[{"Process":{"Description":"The obchiral program is a tool to print the chirality information for all molecules in a file. It also serves as example code for using the Open Babel library (libopenbabel).","Process Name":"obchiral","Link":"https:\/\/linux.die.net\/man\/1\/obchiral"}},{"Process":{"Description":"The obconformer tool can be used as part of a conformational study by generating random conformers using a Monte Carlo search. The best conformer out of the batch of conformers will be output, after taking the supplied number of geometry optimization steps. By default, obconformer uses the MMFF94 force field.","Process Name":"obconformer","Link":"https:\/\/linux.die.net\/man\/1\/obconformer"}},{"Process":{"Description":"The obenergy tool can be used to calculate the energy for molecules inside (multi-)molecule files (e.g., MOL2, etc.)","Process Name":"obenergy","Link":"https:\/\/linux.die.net\/man\/1\/obenergy"}},{"Process":{"Description":"obex-data-server is D-Bus service providing high-level OBEX client and server side functionality. It currently supports OPP (Object Push Profile) and FTP (File Transfer profile) profiles and Bluetooth as transport. obex-data-server exposes it's functionality through 'org.openobex' namespace in DBus Session bus.","Process Name":"obex-data-server","Link":"https:\/\/linux.die.net\/man\/1\/obex-data-server"}},{"Process":{"Description":null,"Process Name":"obex_push","Link":"https:\/\/linux.die.net\/man\/1\/obex_push"}},{"Process":{"Description":"obexftp is used to access files on mobile equipment, i.e. cell phones. With obexftpd you can transfer files between any computers using IrDA, Bluetooth and TCP\/IP . This tool lets you access the ObexFTP library by the means of a command line interface. You might consider using the other means available. E.g. the ObexFS filesystem for Linux.","Process Name":"obexftp","Link":"https:\/\/linux.die.net\/man\/1\/obexftp"}},{"Process":{"Description":null,"Process Name":"obexftpd","Link":"https:\/\/linux.die.net\/man\/1\/obexftpd"}},{"Process":{"Description":"Superimpose two molecules using a quaternion fit. The atoms used to fit the two molecules are defined by the SMARTS pattern given by the user. It is useful to align congeneric series of molecules on a common structural scaffold for 3D-QSAR studies. It can also be useful for displaying the results of conformational generation. Any molecules matching the supplied SMARTS pattern will be rotated and translated to provide the smallest possible RMSD between the matching regions. If a molecule does not match the SMARTS pattern, it will be output with no transformation.","Process Name":"obfit","Link":"https:\/\/linux.die.net\/man\/1\/obfit"}},{"Process":{"Description":null,"Process Name":"obgen","Link":"https:\/\/linux.die.net\/man\/1\/obgen"}},{"Process":{"Description":null,"Process Name":"obgrep","Link":"https:\/\/linux.die.net\/man\/1\/obgrep"}},{"Process":{"Description":"The GNU objcopy utility copies the contents of an object file to another. objcopy uses the GNU BFD Library to read and write the object files. It can write the destination object file in a format different from that of the source object file. The exact behavior of objcopy is controlled by command-line options. Note that objcopy should be able to copy a fully linked file between any two formats. However, copying a relocatable object file between any two formats may not work as expected. objcopy creates temporary files to do its translations and deletes them afterward. objcopy uses BFD to do all its translation work; it has access to all the formats described in BFD and thus is able to recognize most formats without being told explicitly. objcopy can be used to generate S-records by using an output target of srec (e.g., use -O srec). objcopy can be used to generate a raw binary file by using an output target of binary (e.g., use -O binary). When objcopy generates a raw binary file, it will essentially produce a memory dump of the contents of the input object file. All symbols and relocation information will be discarded. The memory dump will start at the load address of the lowest section copied into the output file. When generating an S-record or a raw binary file, it may be helpful to use -S to remove sections containing debugging information. In some cases -R will be useful to remove sections which contain information that is not needed by the binary file. Note---objcopy is not able to change the endianness of its input files. If the input format has an endianness (some formats do not), objcopy can only copy the inputs into file formats that have the same endianness or which have no endianness (e.g., srec). (However, see the --reverse-bytes option.)","Process Name":"objcopy","Link":"https:\/\/linux.die.net\/man\/1\/objcopy"}},{"Process":{"Description":null,"Process Name":"objdump","Link":"https:\/\/linux.die.net\/man\/1\/objdump"}},{"Process":{"Description":"The obminimize tool can be used to minimize the energy for molecules inside (multi-)molecule files (e.g., MOL2, etc.)","Process Name":"obminimize","Link":"https:\/\/linux.die.net\/man\/1\/obminimize"}},{"Process":{"Description":"obnam makes, restores, manipulates, and otherwise deals with backups. It can store backups on a local disk or to a server via sftp. Every backup generation looks like a fresh snapshot, but is really incremental: the user does not need to worry whether it's a full backup or not. Only changed data is backed up, and if a chunk of data is already backed up in another file, that data is re-used. The place where backed up data is placed is called the backup repository. A repository may be, for example, a directory on an sftp server, or a directory on a USB hard disk. A single repository may contain backups from several clients. Their data will intermingle as if they were using separate repositories, but if one client backs up a file, the others may re-use the data. obnam command line syntax consists of a command possibly followed by arguments. The commands are list below. \u2022 backup makes a new backup. The first time it is run, it makes a full backup, after that an incremental one. \u2022 restore is the opposite of a backup. It copies backed up data from the backup repository to a target directory. You can restore everything in a generation, or just selected files. \u2022 clients lists the clients that are backed up to the repository. \u2022 generations lists every backup generation for a given client, plus some metadata about the generation. \u2022 genids lists the identifier for every backup generation for a given client. No other information is shown. This can be useful for scripting. \u2022 ls lists the contents of a given generation, similar to ls -lAR. \u2022 verify compares data in the backup with actual user data, and makes sures they are identical. It is most useful to run immediately after a backup, to check that it actually worked. It can be run at any time, but if the user data has changed, verification fails even though the backup is OK. \u2022 forget removes backup generations that are no longer wanted, so that they don't use disk space. Note that after a backup generation is removed the data can't be restored anymore. You can either specify the generations to remove by listing them on the command line, or use the --keep option to specify a policy for what to keep (everything else will be removed). \u2022 fsck checks the internal consistency of the backup repository. It verifies that all clients, generations, directories, files, and all file contents still exists in the backup repository. It may take quite a long time to run. \u2022 force-lock removes a lock file for a client in the repository. You should only force a lock if you are sure no-one is accessing that client's data in the repository. A dangling lock might happen, for example, if obnam loses its network connection to the backup repository. \u2022 client-keys lists the encryption key associated with each client. \u2022 list-keys lists the keys that can access the repository, and which toplevel directories each key can access. Some of the toplevel directories are shared between clients, others are specific to a client. \u2022 list-toplevels is like list-keys, but lists toplevels and which keys can access them. \u2022 add-key adds an encryption key to the repository. By default, the key is added only to the shared toplevel directories, but it can also be added to specific clients: list the names of the clients on the command line. They key is given with the --keyid option. Whoever has access to the secret key corresponding to the key id can access the backup repository (the shared toplevels plus specified clients). \u2022 remove-key removes a key from the shared toplevel directories, plus any clients specified on the command line. \u2022 nagios-last-backup-age is a check that exits with non-zero return if a backup age exceeds a certain threshold. It is suitable for use as a check plugin for nagios. Thresholds can be given the --warn-age and --critical-age options. \u2022 diff compares two generations and lists files differing between them. Every output line will be prefixed either by a plus sign (+) for files that were added, a minus sign (-) for files that have been removed or an asterisk (*) for files that have changed. If only one generation ID is specified on the command line that generation will be compared with its direct predecessor. If two IDs have been specified, all changes between those two generations will be listed. Making backups When you run a backup, obnam uploads data into the backup repository. The data is divided into chunks, and if a chunk already exists in the backup repository, it is not uploaded again. This allows obnam to deal with files that have been changed or renamed since the previous backup run. It also allows several backup clients to avoid uploading the same data. If, for example, everyone in the office has a copy of the same sales brochures, only one copy needs to be stored in the backup repository. Every backup run is a generation. In addition, obnam will make checkpoint generations every now and then. These are exactly like normal generations, but are not guaranteed to be a complete snapshot of the live data. If the backup run needs to be aborted in the middle, the next backup run can continue from the latest checkpoint, avoiding the need to start completely over. If one backup run drops a backup root directory, the older generations will still keep it: nothing changes in the old generations just because there is a new one. If the root was dropped by mistake, it can be added back and the next backup run will re-use the existing data in the backup repository, and will only back up the file metadata (filenames, permissions, etc). Verifying backups What good is a backup system you cannot rely on? How can you rely on something you cannot test? The obnam verify command checks that data in the backup repository matches actual user data. It retrieves one or more files from the repository and compares them to the user data. This is essentialy the same as doing a restore, then comparing restored files with the original files using cmp(1), but easier to use. By default verification happens on all files. You can also specify the files to be verified by listing them on the command line. You should specify the full paths to the files, not relative to the current directory. The output lists files that fail verification for some reason. If you verify everything, it is likely that some files (e.g., parent directories of backup root) may have changed without it being a problem. Note that you will need to specify the whole path to the files or directories to be verified, not relative to the backup root. You still need to specify at least one of the backup roots on the command line or via the --root option so that obnam will find the filesystem, in case it is a remote one. URL syntax Whenever obnam accepts a URL, it can be either a local pathname, or an sftp URL. An sftp URL has the following form: sftp:\/\/[ user@] domain[: port] \/path where domain is a normal Internet domain name for a server, user is your username on that server, port is an optional port number, and path is a pathname on the server side. Like bzr(1), but unlike the sftp URL standard, the pathname is absolute, unless it starts with \/~\/ in which case it is relative to the user's home directory on the server. See the EXAMPLE section for examples of URLs. You can use sftp URLs for the repository, or the live data (root), but note that due to limitations in the protocol, and its implementation in the paramiko library, some things will not work very well for accessing live data over sftp. Most importantly, the handling of of hardlinks is rather suboptimal. For live data access, you should not end the URL with \/~\/ and should append a dot at the end in this special case. Generation specifications When not using the latest generation, you will need to specify which one you need. This will be done with the --generation option, which takes a generation specification as its argument. The specification is either the word latest, meaning the latest generation (also the default), or a number. See the generations command to see what generations are available, and what their numbers are. Policy for keeping and removing backup generations The forget command can follow a policy to automatically keep some and remove other backup generations. The policy is set with the --keep= POLICY option. POLICY is comma-separated list of rules. Each rule consists of a count and a time period. The time periods are h, d, w, m, and y, for hour, day, week, month, and year. A policy of 30d means to keep the latest backup for each day when a backup was made, and keep the last 30 such backups. Any backup matched by any policy rule is kept, and any backups in between will be removed, as will any backups older than the oldest kept backup. As an example, assume backups are taken every hour, on the hour: at 00:00, 01:00, 02:00, and so on, until 23:00. If the forget command is run at 23:15, with the above policy, it will keep the backup taken at 23:00 on each day, and remove every other backup that day. It will also remove backups older than 30 days. If backups are made every other day, at noon, forget would keep the 30 last backups, or 60 days worth of backups, with the above policy. Note that obnam will only inspect timestamps in the backup repository, and does not care what the actual current time is. This means that if you stop making new backups, the existing ones won't be removed automatically. In essence, obnam pretends the current time is just after the latest backup when forget is run. The rules can be given in any order, but will be sorted to ascending order of time period before applied. (It is an error to give two rules for the same period.) A backup generation is kept if it matches any rule. For example, assume the same backup frequence as above, but a policy of 30d,52w. This will keep the newest daily backup for each day for thirty days, and the newest weekly backup for 52 weeks. Because the hourly backups will be removed daily, before they have a chance to get saved by a weekly rule, the effect is that the 23:00 o'clock backup for each day is saved for a month, and the 23:00 backup on Sundays is saved for a year. If, instead, you use a policy of 72h,30d,52w, obnam would keep the last 72 hourly backups, and the last backup of each calendar day for 30 days, and the last backup of each calendar week for 52 weeks. If the backup frequency was once per day, obnam would keep the backup of each calendar hour for which a backup was made, for 72 such backups. In other words, it would effectively keep the the last 72 daily backups. Sound confusing? Just wonder how confused the developer was when writing the code. If no policy is given, forget will keep everything. A typical policy might be 72h,7d,5w,12m, which means: keep the last 72 hourly backups, the last 7 daily backups, the last 5 weekly backups and the last 12 monthly backups. If the backups are systematically run on an hourly basis, this will mean keeping hourly backups for three days, daily backups for a week, weekly backups for a month, and monthly backups for a year. The way the policy works is a bit complicated. Run forget with the --pretend option to make sure you're removing the right ones. Using encryption obnam can encrypt all the data it writes to the backup repository. It uses gpg(1) to do the encryption. You need to create a key pair using gpg --gen-key (or use an existing one), and then tell obnam about it using the --encrypt-with option. Configuration files obnam will look for configuration files in a number of locations. See the FILES section for a list. All files are treated as if they were one with the contents of all files catenated. The files are in INI format, and only the [config] section is used (any other sections are ignored). The long names of options are used as keys for configuration variables. Any setting that can be set from the command line can be set in a configuration file, in the [config] section. For example, the options in the following command line: obnam --repository=\/backup --exclude='.wav$' backup could be replaced with the following configuration file: [config]\nrepository: \/backup\nexclude: .wav$ (You can use either foo=value or foo: value syntax in the files.) The only unusual thing about the files is the way options that can be used many times are expressed. All values are put in a single line, separated by commas (and optionally spaces as well). For example: [config]\nexclude = foo, bar, \\.mp3$ The above has three values for the exclude option: any files that contain the words foo or bar anywhere in the fully qualified pathname, or files with names ending with a period and mp3 (because the exclusions are regular expressions). Multiple clients and locking obnam supports sharing a repository between multiple clients. The clients can share the file contents (chunks), so that if client A backs up a large file, and client B has the same file, then B does not need to upload the large file to the repository a second time. For this to work without confusion, the clients use a simple locking protocol that allows only one client at a time to modify the shared data structures. Locks do not prevent read-only access at the same time: this allows you to restore while someone else is backing up. Sometimes a read-only operation happens to access a data structure at the same time as it is being modified. This can result in a crash. It will not result in corrupt data, or incorrect restores. However, you may need to restart the read-only operation after a crash. Repository format conversion The convert5to6 subcommand converts a repository of format 5 to format 6. It is somewhat dangerous! It modifies the repository in place, so you should be careful. You should do a hardlink copy of the repository before converting: cp -al repo repo.format5 You should also run this with local filesystem access to the repository, rather than sftp, to avoid abysmal performance.","Process Name":"obnam","Link":"https:\/\/linux.die.net\/man\/1\/obnam"}},{"Process":{"Description":"obnam-benchmark benchmarks the obnam(1) backup application, by measuring how much time it takes to do a backup, restore, etc, in various scenarios. obnam-benchmark uses the seivot(1) tool for actually running the benchmarks, but makes some helpful assumptions about things, to make it simpler to run than running seivot directly. Benchmarks are run using two different usage profiles: mailspool (all files are small), and mediaserver (all files are big). For each profile, test data of the desired total size is generated, backed up, and then several incremental generations are backed up, each adding some more generated test data. Then other operations are run against the backup repository: restoring, listing the contents of, and removing each generation. The result of the benchmark is a .seivot file per profile, plus a Python profiler file for each run of obnam. These are stored in ..\/benchmarks. A set of .seivot files can be summarized for comparison with seivots-summary(1). The profiling files can be viewed with the usual Python tools: see the pstats module. The benchmarks are run against a version of obnam checked out from version control. It is not (currently) possible to run the benchmark against an installed version of obnam. Also the larch Python library, which obnam needs, needs to be checked out from version control. The --obnam-branch and --larch-branch options set the locations, if the defaults are not correct.","Process Name":"obnam-benchmark","Link":"https:\/\/linux.die.net\/man\/1\/obnam-benchmark"}},{"Process":{"Description":"obnam-viewprof shows a plain text version of Python profiler output. You can generate such output from Obnam by setting the OBNAM_PROFILE environment variable to a filename. The profile will be written to that filename, and you should give it to obnam-viewprof as an argument. The sort-order argument defaults to cumulative and can be any of the orderings that the Python pstats library supports. obnam-viewprof is mainly useful for those developing obnam(1).","Process Name":"obnam-viewprof","Link":"https:\/\/linux.die.net\/man\/1\/obnam-viewprof"}},{"Process":{"Description":"The obprobe tool creates a grid around a molecule, placing a probe atom with a specified atom type and partial charge at each point to calculate the MMFF94 energy. This can be used for docking experiments to test hydrogen-bond affinity, electrostatic potential, etc. Output is sent to standard output using the Gaussian Cube format.","Process Name":"obprobe","Link":"https:\/\/linux.die.net\/man\/1\/obprobe"}},{"Process":{"Description":null,"Process Name":"obprop","Link":"https:\/\/linux.die.net\/man\/1\/obprop"}},{"Process":{"Description":"The obrotamer tool can be used as part of a conformational search by generating random isomers based on rotating dihedral angles. These rotamers are not conformers -- that is, obrotamer does not perform geometry optimization after generating the rotamer structure. The obminimize tool can do geometry optimization using molecular mechanics.","Process Name":"obrotamer","Link":"https:\/\/linux.die.net\/man\/1\/obrotamer"}},{"Process":{"Description":"The obrotate program rotates the torsional (dihedral) angle of a specified bond in molecules to that defined by the user. In other words, it does the same as a user setting an angle in a molecular modelling package, but much faster and in batch mode (i.e. across multiple molecules in a file). The four atom IDs required are indexes into the SMARTS pattern, which starts at atom 0 (zero). The angle supplied is in degrees. The two atoms used to set the dihedral angle <atom1> and <atom4> do not need to be connected to the atoms of the bond <atom2> and <atom3> in any way. The order of the atoms matters -- the portion of the molecule attached to <atom1> and <atom2> remain fixed, but the portion bonded to <atom3> and & <atom4> moves.","Process Name":"obrotate","Link":"https:\/\/linux.die.net\/man\/1\/obrotate"}},{"Process":{"Description":null,"Process Name":"obxprop","Link":"https:\/\/linux.die.net\/man\/1\/obxprop"}},{"Process":{"Description":"The ocaml(1) command is the toplevel system for Objective Caml, that permits interactive use of the Objective Caml system through a read-eval-print loop. In this mode, the system repeatedly reads Caml phrases from the input, then typechecks, compiles and evaluates them, then prints the inferred type and result value, if any. The system prints a # (sharp) prompt before reading each phrase. A toplevel phrase can span several lines. It is terminated by ;; (a double-semicolon). The syntax of toplevel phrases is as follows. The toplevel system is started by the command ocaml(1). Phrases are read on standard input, results are printed on standard output, errors on standard error. End-of-file on standard input terminates ocaml(1). If one or more object-files (ending in .cmo or .cma) are given, they are loaded silently before starting the toplevel. If a script-file is given, phrases are read silently from the file, errors printed on standard error. ocaml(1) exits after the execution of the last phrase.","Process Name":"ocaml","Link":"https:\/\/linux.die.net\/man\/1\/ocaml"}},{"Process":{"Description":null,"Process Name":"ocamlbuild","Link":"https:\/\/linux.die.net\/man\/1\/ocamlbuild"}},{"Process":{"Description":"The Objective Caml bytecode compiler ocamlc(1) compiles Caml source files to bytecode object files and links these object files to produce standalone bytecode executable files. These executable files are then run by the bytecode interpreter ocamlrun(1). The ocamlc(1) command has a command-line interface similar to the one of most C compilers. It accepts several types of arguments and processes them sequentially: Arguments ending in .mli are taken to be source files for compilation unit interfaces. Interfaces specify the names exported by compilation units: they declare value names with their types, define public data types, declare abstract data types, and so on. From the file x.mli, the ocamlc(1) compiler produces a compiled interface in the file x.cmi. Arguments ending in .ml are taken to be source files for compilation unit implementations. Implementations provide definitions for the names exported by the unit, and also contain expressions to be evaluated for their side-effects. From the file x.ml, the ocamlc(1) compiler produces compiled object bytecode in the file x.cmo. If the interface file x.mli exists, the implementation x.ml is checked against the corresponding compiled interface x.cmi, which is assumed to exist. If no interface x.mli is provided, the compilation of x.ml produces a compiled interface file x.cmi in addition to the compiled object code file x.cmo. The file x.cmi produced corresponds to an interface that exports everything that is defined in the implementation x.ml. Arguments ending in .cmo are taken to be compiled object bytecode. These files are linked together, along with the object files obtained by compiling .ml arguments (if any), and the Caml Light standard library, to produce a standalone executable program. The order in which .cmo and.ml arguments are presented on the command line is relevant: compilation units are initialized in that order at run-time, and it is a link-time error to use a component of a unit before having initialized it. Hence, a given x.cmo file must come before all .cmo files that refer to the unit x. Arguments ending in .cma are taken to be libraries of object bytecode. A library of object bytecode packs in a single file a set of object bytecode files (.cmo files). Libraries are built with ocamlc -a (see the description of the -a option below). The object files contained in the library are linked as regular .cmo files (see above), in the order specified when the .cma file was built. The only difference is that if an object file contained in a library is not referenced anywhere in the program, then it is not linked in. Arguments ending in .c are passed to the C compiler, which generates a .o object file. This object file is linked with the program if the -custom flag is set (see the description of -custom below). Arguments ending in .o or .a are assumed to be C object files and libraries. They are passed to the C linker when linking in -custom mode (see the description of -custom below). Arguments ending in .so are assumed to be C shared libraries (DLLs). During linking, they are searched for external C functions referenced from the Caml code, and their names are written in the generated bytecode executable. The run-time system ocamlrun(1) then loads them dynamically at program start-up time. The output of the linking phase is a file containing compiled bytecode that can be executed by the Objective Caml bytecode interpreter: the command ocamlrun(1). If caml.out is the name of the file produced by the linking phase, the command ocamlrun caml.out arg1 arg2 ... argn executes the compiled code contained in caml.out, passing it as arguments the character strings arg1 to argn. (See ocamlrun(1) for more details.) On most systems, the file produced by the linking phase can be run directly, as in: .\/caml.out arg1 arg2 ... argn. The produced file has the executable bit set, and it manages to launch the bytecode interpreter by itself. ocamlc.opt is the same compiler as ocamlc, but compiled with the native-code compiler ocamlopt(1). Thus, it behaves exactly like ocamlc, but compiles faster. ocamlc.opt may not be available in all installations of Objective Caml.","Process Name":"ocamlc","Link":"https:\/\/linux.die.net\/man\/1\/ocamlc"}},{"Process":{"Description":null,"Process Name":"ocamlc.opt","Link":"https:\/\/linux.die.net\/man\/1\/ocamlc.opt"}},{"Process":{"Description":"The ocamlcp command is a front-end to ocamlc(1) that instruments the source code, adding code to record how many times functions are called, branches of conditionals are taken, ... Execution of instrumented code produces an execution profile in the file ocamlprof.dump, which can be read using ocamlprof(1). ocamlcp accepts the same arguments and options as ocamlc(1).","Process Name":"ocamlcp","Link":"https:\/\/linux.die.net\/man\/1\/ocamlcp"}},{"Process":{"Description":"ocamldebug is the Objective Caml source-level replay debugger. Before the debugger can be used, the program must be compiled and linked with the -g option: all .cmo and .cma files that are part of the program should have been created with ocamlc -g, and they must be linked together with ocamlc -g. Compiling with -g entails no penalty on the running time of programs: object files and bytecode executable files are bigger and take longer to produce, but the executable files run at exactly the same speed as if they had been compiled without -g.","Process Name":"ocamldebug","Link":"https:\/\/linux.die.net\/man\/1\/ocamldebug"}},{"Process":{"Description":"The ocamldep(1) command scans a set of Objective Caml source files (.ml and .mli files) for references to external compilation units, and outputs dependency lines in a format suitable for the make(1) utility. This ensures that make will compile the source files in the correct order, and recompile those files that need to when a source file is modified. The typical usage is: ocamldep options *.mli *.ml > .depend where .depend is the file that should contain the dependencies. Dependencies are generated both for compiling with the bytecode compiler ocamlc(1) and with the native-code compiler ocamlopt(1).","Process Name":"ocamldep","Link":"https:\/\/linux.die.net\/man\/1\/ocamldep"}},{"Process":{"Description":"The Objective Caml documentation generator ocamldoc(1) generates documentation from special comments embedded in source files. The comments used by OCamldoc are of the form (** ... *) and follow the format described in the The Objective Caml user's manual. OCamldoc can produce documentation in various formats: HTML, LaTeX, TeXinfo, Unix man pages, and dot(1) dependency graphs. Moreover, users can add their own custom generators. In this manpage, we use the word element to refer to any of the following parts of an OCaml source file: a type declaration, a value, a module, an exception, a module type, a type constructor, a record field, a class, a class type, a class method, a class value or a class inheritance clause.","Process Name":"ocamldoc","Link":"https:\/\/linux.die.net\/man\/1\/ocamldoc"}},{"Process":{"Description":"","Process Name":"ocamlfind","Link":"https:\/\/linux.die.net\/man\/1\/ocamlfind"}},{"Process":{"Description":"The ocamllex(1) command generates Objective Caml lexers from a set of regular expressions with associated semantic actions, in the style of lex(1). Running ocamllex(1) on the input file lexer.mll produces Caml code for a lexical analyzer in file lexer.ml. This file defines one lexing function per entry point in the lexer definition. These functions have the same names as the entry points. Lexing functions take as argument a lexer buffer, and return the semantic attribute of the corresponding entry point. Lexer buffers are an abstract data type implemented in the standard library module Lexing. The functions Lexing.from_channel, Lexing.from_string and Lexing.from_function create lexer buffers that read from an input channel, a character string, or any reading function, respectively. When used in conjunction with a parser generated by ocamlyacc(1), the semantic actions compute a value belonging to the type token defined by the generated parsing module.","Process Name":"ocamllex","Link":"https:\/\/linux.die.net\/man\/1\/ocamllex"}},{"Process":{"Description":"The ocamlmktop(1) command builds Objective Caml toplevels that contain user code preloaded at start-up. The ocamlmktop(1) command takes as argument a set of x.cmo and x.cma files, and links them with the object files that implement the Objective Caml toplevel. If the -custom flag is given, C object files and libraries (.o and .a files) can also be given on the command line and are linked in the resulting toplevel.","Process Name":"ocamlmktop","Link":"https:\/\/linux.die.net\/man\/1\/ocamlmktop"}},{"Process":{"Description":"The Objective Caml high-performance native-code compiler ocamlopt(1) compiles Caml source files to native code object files and link these object files to produce standalone executables. The ocamlopt(1) command has a command-line interface very close to that of ocamlc(1). It accepts the same types of arguments and processes them sequentially: Arguments ending in .mli are taken to be source files for compilation unit interfaces. Interfaces specify the names exported by compilation units: they declare value names with their types, define public data types, declare abstract data types, and so on. From the file x.mli, the ocamlopt(1) compiler produces a compiled interface in the file x.cmi. The interface produced is identical to that produced by the bytecode compiler ocamlc(1). Arguments ending in .ml are taken to be source files for compilation unit implementations. Implementations provide definitions for the names exported by the unit, and also contain expressions to be evaluated for their side-effects. From the file x.ml, the ocamlopt(1) compiler produces two files: x.o, containing native object code, and x.cmx, containing extra information for linking and optimization of the clients of the unit. The compiled implementation should always be referred to under the name x.cmx (when given a .o file, ocamlopt(1) assumes that it contains code compiled from C, not from Caml). The implementation is checked against the interface file x.mli (if it exists) as described in the manual for ocamlc(1). Arguments ending in .cmx are taken to be compiled object code. These files are linked together, along with the object files obtained by compiling .ml arguments (if any), and the Caml Light standard library, to produce a native-code executable program. The order in which .cmx and .ml arguments are presented on the command line is relevant: compilation units are initialized in that order at run-time, and it is a link-time error to use a component of a unit before having initialized it. Hence, a given x.cmx file must come before all .cmx files that refer to the unit x. Arguments ending in .cmxa are taken to be libraries of object code. Such a library packs in two files lib.cmxa and lib.a a set of object files (.cmx\/.o files). Libraries are build with ocamlopt -a (see the description of the -a option below). The object files contained in the library are linked as regular .cmx files (see above), in the order specified when the library was built. The only difference is that if an object file contained in a library is not referenced anywhere in the program, then it is not linked in. Arguments ending in .c are passed to the C compiler, which generates a .o object file. This object file is linked with the program. Arguments ending in .o or .a are assumed to be C object files and libraries. They are linked with the program. The output of the linking phase is a regular Unix executable file. It does not need ocamlrun(1) to run. ocamlopt.opt is the same compiler as ocamlopt, but compiled with itself instead of with the bytecode compiler ocamlc(1). Thus, it behaves exactly like ocamlopt, but compiles faster. ocamlopt.opt is not available in all installations of Objective Caml.","Process Name":"ocamlopt","Link":"https:\/\/linux.die.net\/man\/1\/ocamlopt"}},{"Process":{"Description":null,"Process Name":"ocamlopt.opt","Link":"https:\/\/linux.die.net\/man\/1\/ocamlopt.opt"}},{"Process":{"Description":"The ocamlprof command prints execution counts gathered during the execution of a Objective Caml program instrumented with ocamlcp(1). It produces a source listing of the program modules given as arguments where execution counts have been inserted as comments. For instance, ocamlprof foo.ml prints the source code for the foo module, with comments indicating how many times the functions in this module have been called. Naturally, this information is accurate only if the source file has not been modified since the profiling execution took place.","Process Name":"ocamlprof","Link":"https:\/\/linux.die.net\/man\/1\/ocamlprof"}},{"Process":{"Description":null,"Process Name":"ocamlrun","Link":"https:\/\/linux.die.net\/man\/1\/ocamlrun"}},{"Process":{"Description":"The ocamlyacc(1) command produces a parser from a LALR(1) context-free grammar specification with attached semantic actions, in the style of yacc(1). Assuming the input file is grammar.mly, running ocamlyacc produces Caml code for a parser in the file grammar.ml, and its interface in file grammar.mli. The generated module defines one parsing function per entry point in the grammar. These functions have the same names as the entry points. Parsing functions take as arguments a lexical analyzer (a function from lexer buffers to tokens) and a lexer buffer, and return the semantic attribute of the corresponding entry point. Lexical analyzer functions are usually generated from a lexer specification by the ocamllex(1) program. Lexer buffers are an abstract data type implemented in the standard library module Lexing. Tokens are values from the concrete type token, defined in the interface file grammar.mli produced by ocamlyacc(1).","Process Name":"ocamlyacc","Link":"https:\/\/linux.die.net\/man\/1\/ocamlyacc"}},{"Process":{"Description":"ociobakelut -- create a new LUT or icc profile from an OCIO config or lut file(s) usage: ociobakelut [options] <OUTPUTFILE.LUT> example: ociobakelut --inputspace lg10 --outputspace srgb8 --format flame lg_to_srgb.3dl example: ociobakelut --lut filmlut.3dl --lut calibration.3dl --format flame display.3dl example: ociobakelut --lut look.3dl --offset 0.01 -0.02 0.03 --lut display.3dl --format flame display_with_look.3dl example: ociobakelut --inputspace lg10 --outputspace srgb8 --format icc ~\/Library\/ColorSync\/Profiles\/test.icc example: ociobakelut --lut filmlut.3dl --lut calibration.3dl --format icc ~\/Library\/ColorSync\/Profiles\/test.icc Using Existing OCIO Configurations --inputspace %s Input OCIO ColorSpace (or Role) --outputspace %s Output OCIO ColorSpace (or Role) --shaperspace %s the OCIO ColorSpace or Role, for the shaper --iconfig %s Input .ocio configuration file (default: $OCIO) Config-Free LUT Baking (all options can be specified multiple times, each is applied in order) --lut %s Specify a LUT (forward direction) --invlut %s Specify a LUT (inverse direction) --slope %f %f %f slope --offset %f %f %f offset (float) --offset10 %f %f %f offset (10-bit) --power %f %f %f power --sat %f saturation (ASC-CDL luma coefficients) Baking Options --format %s the lut format to bake: flame (.3dl), lustre (.3dl), cinespace (.csp), houdini (.lut), iridas_itx (.itx), icc (.icc) --shapersize %d size of the shaper (default: format specific) --cubesize %d size of the cube (default: format specific) --stdout Write to stdout (rather than file) --v Verbose --help Print help message ICC Options --whitepoint %d whitepoint for the profile (default: 6505) --displayicc %s an icc profile which matches the OCIO profiles target display --description %s a meaningful description, this will show up in UI like photoshop --copyright %s a copyright field ociobakelut -- create a new LUT or icc profile from an OCIO config or lut file(s) usage: ociobakelut [options] <OUTPUTFILE.LUT> example: ociobakelut --inputspace lg10 --outputspace srgb8 --format flame lg_to_srgb.3dl example: ociobakelut --lut filmlut.3dl --lut calibration.3dl --format flame display.3dl example: ociobakelut --lut look.3dl --offset 0.01 -0.02 0.03 --lut display.3dl --format flame display_with_look.3dl example: ociobakelut --inputspace lg10 --outputspace srgb8 --format icc ~\/Library\/ColorSync\/Profiles\/test.icc example: ociobakelut --lut filmlut.3dl --lut calibration.3dl --format icc ~\/Library\/ColorSync\/Profiles\/test.icc Using Existing OCIO Configurations --inputspace %s Input OCIO ColorSpace (or Role) --outputspace %s Output OCIO ColorSpace (or Role) --shaperspace %s the OCIO ColorSpace or Role, for the shaper --iconfig %s Input .ocio configuration file (default: $OCIO) Config-Free LUT Baking (all options can be specified multiple times, each is applied in order) --lut %s Specify a LUT (forward direction) --invlut %s Specify a LUT (inverse direction) --slope %f %f %f slope --offset %f %f %f offset (float) --offset10 %f %f %f offset (10-bit) --power %f %f %f power --sat %f saturation (ASC-CDL luma coefficients) Baking Options --format %s the lut format to bake: flame (.3dl), lustre (.3dl), cinespace (.csp), houdini (.lut), iridas_itx (.itx), icc (.icc) --shapersize %d size of the shaper (default: format specific) --cubesize %d size of the cube (default: format specific) --stdout Write to stdout (rather than file) --v Verbose --help Print help message ICC Options --whitepoint %d whitepoint for the profile (default: 6505) --displayicc %s an icc profile which matches the OCIO profiles target display --description %s a meaningful description, this will show up in UI like photoshop --copyright %s a copyright field Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"ociobakelut","Link":"https:\/\/linux.die.net\/man\/1\/ociobakelut"}},{"Process":{"Description":"ociocheck -- validate an OpenColorIO configuration usage: ociocheck [options] --help Print help message --iconfig %s Input .ocio configuration file (default: $OCIO) --oconfig %s Output .ocio file ociocheck is useful to validate that the specified .ocio configuration is valid, and that all the color transforms are defined. For example, it is possible that the configuration may reference lookup tables that do not exist. ociocheck will find these cases. ociocheck can also be used to clean up formatting on an existing profile that has been manually edited, using the '-o' option. ociocheck -- validate an OpenColorIO configuration usage: ociocheck [options] --help Print help message --iconfig %s Input .ocio configuration file (default: $OCIO) --oconfig %s Output .ocio file ociocheck is useful to validate that the specified .ocio configuration is valid, and that all the color transforms are defined. For example, it is possible that the configuration may reference lookup tables that do not exist. ociocheck will find these cases. ociocheck can also be used to clean up formatting on an existing profile that has been manually edited, using the '-o' option. Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"ociocheck","Link":"https:\/\/linux.die.net\/man\/1\/ociocheck"}},{"Process":{"Description":"Oclock simply displays the current time on an analog display.","Process Name":"oclock","Link":"https:\/\/linux.die.net\/man\/1\/oclock"}},{"Process":{"Description":null,"Process Name":"ocp","Link":"https:\/\/linux.die.net\/man\/1\/ocp"}},{"Process":{"Description":"ocsinventory-agent creates inventory and send or write them. This agent is the successor of the former linux_agent which was released with OCS 1.01 and prior. It also replaces the Solaris\/AIX\/BSD unofficial agents. The detailed list of supported Operating System is available in the Wiki. GNU\/Linux Solaris FreeBSD NetBSD OpenBSD AIX MacOSX GNU\/kFreeBSD","Process Name":"ocsinventory-agent","Link":"https:\/\/linux.die.net\/man\/1\/ocsinventory-agent"}},{"Process":{"Description":"The Online Certificate Status Protocol ( OCSP ) enables applications to determine the (revocation) state of an identified certificate ( RFC 2560). The ocsp command performs many common OCSP tasks. It can be used to print out requests and responses, create requests and send queries to an OCSP responder and behave like a mini OCSP server itself.","Process Name":"ocsp","Link":"https:\/\/linux.die.net\/man\/1\/ocsp"}},{"Process":{"Description":"Octave is a high-level language, primarily intended for numerical computations. It provides a convenient command line interface for solving linear and nonlinear problems numerically.","Process Name":"octave","Link":"https:\/\/linux.die.net\/man\/1\/octave"}},{"Process":{"Description":null,"Process Name":"octave-bug","Link":"https:\/\/linux.die.net\/man\/1\/octave-bug"}},{"Process":{"Description":"octave-config is a tool to obtain directory information for .oct and .m files for octave(1).","Process Name":"octave-config","Link":"https:\/\/linux.die.net\/man\/1\/octave-config"}},{"Process":{"Description":null,"Process Name":"od","Link":"https:\/\/linux.die.net\/man\/1\/od"}},{"Process":{"Description":"The oddjob_request application is a simple oddjob client application which can be used to invoke a particular method provided by the oddjobd server. If no request is given as a command-line argument, oddjob_request will attempt to invoke the list method, which should list all methods for which the invoking user is authorized.","Process Name":"oddjob_request","Link":"https:\/\/linux.die.net\/man\/1\/oddjob_request"}},{"Process":{"Description":"ode is a tool that solves, by numerical integration, the initial value problem for a specified system of first-order ordinary differential equations. Three distinct numerical integration schemes are available: Runge-Kutta-Fehlberg (the default), Adams-Moulton, and Euler. The Adams-Moulton and Runge-Kutta schemes are available with adaptive step size. The operation of ode is specified by a program, written in its input language. The program is simply a list of expressions for the derivatives of the variables to be integrated, together with some control statements. Some examples are given in the EXAMPLES section. ode reads the program from the specified file, or from standard input if no file name is given. If reading from standard input, ode will stop reading and exit when it sees a single period on a line by itself. At each time step, the values of variables specified in the program are written to standard output. So a table of values will be produced, with each column showing the evolution of a variable. If there are only two columns, the output can be piped to graph(1) or a similar plotting program.","Process Name":"ode","Link":"https:\/\/linux.die.net\/man\/1\/ode"}},{"Process":{"Description":"","Process Name":"odf2xliff","Link":"https:\/\/linux.die.net\/man\/1\/odf2xliff"}},{"Process":{"Description":"The ods-hsmspeed utility is part of OpenDNSSEC and can be used to test the performance of the configured HSMs. The components of OpenDNSSEC do not talk directly to the HSMs, but uses an internal library called libhsm. It then talks to the HSMs using PKCS#11. The libhsm simplifies the process of creating keys and signatures for the other components of OpenDNSSEC. ods-hsmspeed will measure the speed by using the libhsm. The result that you get is somewhat lower than what the manufactures promises, because the libhsm creates some overhead to the pure PKCS#11 environment.","Process Name":"ods-hsmspeed","Link":"https:\/\/linux.die.net\/man\/1\/ods-hsmspeed"}},{"Process":{"Description":"The ods-hsmutil utility is mainly used for debugging or testing. It is designed to interact directly with your HSM and can be used to manually list, create or delete keys. It can also be used to perform a set of basics HSM tests. Be careful before creating or deleting keys using ods-hsmutil, as the changes are not synchronized with the KASP Enforcer. The repositories are configured by the user in the OpenDNSSEC configuration file. The configuration contains the name of the repository, the token label, the user PIN, and the path to its shared library.","Process Name":"ods-hsmutil","Link":"https:\/\/linux.die.net\/man\/1\/ods-hsmutil"}},{"Process":{"Description":"The OpenDNSSEC XML configuration files (conf.xml and kasp.xml) offer the user many options to configure the OpenDNSSEC signing system. Some syntactic constraints are placed on the configuration by the .rng definition (for example, whether an element is required or optional), but some semantic constraints cannot be defined this way (for example, if NSEC3 is used to secure the zone, then a consistent DNSKEY algorithm choice should be made). ods-kaspcheck is provided to check that the configuration files (conf.xml and kasp.xml) are semantically sane and contain no inconsistencies. It is advisable to use this tool to check your configuration before starting to use OpenDNSSEC.","Process Name":"ods-kaspcheck","Link":"https:\/\/linux.die.net\/man\/1\/ods-kaspcheck"}},{"Process":{"Description":"ods-ksmutil manages the operation of the KASP Enforcer, which is the part of OpenDNSSEC that triggers key generation and signing operations on domains based on policies with user-defined timing and security requirements. Since everything beyond this management utility is usually automatic, ods-ksmutil is the primary tool for managing OpenDNSSEC. Among the functions of ods-ksmutil are key management, updates to the zone list and manually rolling keys to recover from exceptional situations like key loss. To get started, a first invocation of ods-ksmutil setup is needed; see SETUP AND UPDATE COMMANDS below for details. After this is done, the rest of the functionality of ods-ksmutil becomes available. The following sections discuss the subcommands in logical groups, detailing any options that they support.","Process Name":"ods-ksmutil","Link":"https:\/\/linux.die.net\/man\/1\/ods-ksmutil"}},{"Process":{"Description":"odt2txt is a command-line tool which extracts the text out of OpenDocument Texts, as produced by OpenOffice.org, KOffice, StarOffice and others. odt2txt can also extract text from some file formats similar to OpenDocument Text, such as OpenOffice.org XML (*.sxw), which was used by OpenOffice.org version 1.x and older StarOffice versions. To a lesser extend, odt2txt may be useful to extract content from OpenDocument spreadsheets (*.ods) and OpenDocument presentations (*.odp). The FILENAME argument is mandatory.","Process Name":"odt2txt","Link":"https:\/\/linux.die.net\/man\/1\/odt2txt"}},{"Process":{"Description":null,"Process Name":"odvicopy","Link":"https:\/\/linux.die.net\/man\/1\/odvicopy"}},{"Process":{"Description":"THIS MAN PAGE IS OBSOLETE! See the Texinfo documentation instead. You can read it either in Emacs or with the standalone info program which comes with the GNU texinfo distribution as prep.ai.mit.edu:pub\/gnu\/texinfo*.tar.gz. The program dvips takes a DVI file file[.dvi] produced by TeX (or by some other processor such as GFtoDVI) and converts it to PostScript, normally sending the result directly to the (laser)printer. The DVI file may be specified without the .dvi extension. Fonts used may either be resident in the printer or defined as bitmaps in PK files, or a 'virtual' combination of both. If the mktexpk program is installed, dvips will automatically invoke METAFONT to generate fonts that don't already exist. For more information, see the Texinfo manual dvips.texi, which should be installed somewhere on your system, hopefully accessible through the standard Info tree.","Process Name":"odvips","Link":"https:\/\/linux.die.net\/man\/1\/odvips"}},{"Process":{"Description":"This manual page is not meant to be exhaustive. The complete documentation for this version of TeX can be found in the info file or manual Web2C: A TeX implementation. The dvitype program translates a DVI (DeVice Independent) file output by (for example) tex(1) or gftodvi(1), to a file that humans can read. It also serves as a DVI file-validating program (i.e., if dvitype can read it, it's correct) and as an example of a DVI-reading program for future device drivers. The output file can include all commands, just the important ones, or none at all (in which case only errors are reported). A subinterval of pages may be selected for transliteration; the magnification and resolution of the ''output device'' may be changed; and so on. All options are specified with an on-line dialog. The .dvi extension is supplied if omitted from dvi_name. The output goes to stdout.","Process Name":"odvitype","Link":"https:\/\/linux.die.net\/man\/1\/odvitype"}},{"Process":{"Description":"offconsol reads an OFF file from standard input and writes an OFF file in which vertices closer than a given precision have been combined. The default precision is zero. Color and normal information are not taken into account when comparing vertices. The vertices of the resultant polyline have been sorted, and so there order bears no resemblance to the order of the point in the original polylist.","Process Name":"offconsol","Link":"https:\/\/linux.die.net\/man\/1\/offconsol"}},{"Process":{"Description":"OfflineImap operates on a REMOTE and a LOCAL repository and synchronizes emails between them, so that you can read the same mailbox from multiple computers. The REMOTE repository is some IMAP server, while LOCAL can be either a local Maildir or another IMAP server. Missing folders will be automatically created on both sides if needed. No folders will be deleted at the moment. Configuring OfflineImap in basic mode is quite easy, however it provides an amazing amount of flexibility for those with special needs. You can specify the number of connections to your IMAP server, use arbitrary python functions (including regular expressions) to limit the number of folders being synchronized. You can transpose folder names between repositories using any python function, to mangle and modify folder names on the LOCAL repository. There are six different ways to hand the IMAP password to OfflineImap from console input, specifying in the configuration file, .netrc support, specifying in a separate file, to using arbitrary python functions that somehow return the password. Finally, you can use IMAPs IDLE infrastructure to always keep a connection to your IMAP server open and immediately be notified (and synchronized) when a new mail arrives (aka Push mail). Most configuration is done via the configuration file. However, any setting can also be overriden by command line options handed to OfflineIMAP. OfflineImap is well suited to be frequently invoked by cron jobs, or can run in daemon mode to periodically check your email (however, it will exit in some error situations).","Process Name":"offlineimap","Link":"https:\/\/linux.die.net\/man\/1\/offlineimap"}},{"Process":{"Description":null,"Process Name":"ofm2opl","Link":"https:\/\/linux.die.net\/man\/1\/ofm2opl"}},{"Process":{"Description":"libofx 0.9.4 prints to stdout the created OFX file based on the options you pass it. currently it will only create a statement request file. you can POST this to an OFX server to request a statement from that financial institution for that account. -h, --help Print help and exit -V, --version Print version and exit --fipid= STRING FI partner identifier (looks up fid, org & url from partner server) --fid= STRING FI identifier --org= STRING FI org tag --bank= STRING IBAN bank identifier --broker= STRING Broker identifier --user= STRING User name --pass= STRING Password --acct= STRING Account ID --type= INT Account Type 1=checking 2=invest 3=ccard --past= LONG How far back to look from today (in days) --url= STRING Url to POST the data to (otherwise goes to stdout) --trid= INT Transaction id Group: command -s, --statement-req Request for a statement -a, --accountinfo-req Request for a list of accounts -p, --payment-req Request to make a payment -i, --paymentinquiry-req Request to inquire about the status of a payment -b, --bank-list List all known banks -f, --bank-fipid List all fipids for a given bank -v, --bank-services List supported services for a given fipid --allsupport List all banks which support online banking","Process Name":"ofxconnect","Link":"https:\/\/linux.die.net\/man\/1\/ofxconnect"}},{"Process":{"Description":null,"Process Name":"ofxdump","Link":"https:\/\/linux.die.net\/man\/1\/ofxdump"}},{"Process":{"Description":"ogg123 reads Ogg Vorbis audio files and decodes them to the devices specified on the command line. By default, ogg123 writes to the standard sound device, but output can be sent to any number of devices. Files can be read from the file system, or URLs can be streamed via HTTP. If a directory is given, all of the files in it or its subdirectories will be played.","Process Name":"ogg123","Link":"https:\/\/linux.die.net\/man\/1\/ogg123"}},{"Process":{"Description":null,"Process Name":"oggdec","Link":"https:\/\/linux.die.net\/man\/1\/oggdec"}},{"Process":{"Description":"oggenc reads audio data in either raw, WAV, or AIFF format and encodes it into an Ogg Vorbis stream. oggenc may also read audio data from FLAC and Ogg FLAC files depending upon compile-time options. If the input file \"-\" is specified, audio data is read from stdin and the Vorbis stream is written to stdout unless the -o option is used to redirect the output. By default, disk files are output to Ogg Vorbis files of the same name, with the extension changed to \".ogg\". This naming convention can be overridden by the -o option (in the case of one file) or the -n option (in the case of several files). Finally, if none of these are available, the output filename will be the input filename with the extension (that part after the final dot) replaced with ogg, so file.wav will become file.ogg","Process Name":"oggenc","Link":"https:\/\/linux.die.net\/man\/1\/oggenc"}},{"Process":{"Description":"ogginfo reads one or more Ogg files and prints information about stream contents (including chained and\/or multiplexed streams) to standard output. It will detect (but not correct) a wide range of common defects, with many additional checks specifically for Ogg Vorbis streams. For all stream types ogginfo will print the filename being processed, the stream serial numbers, and various common error conditions. For Vorbis streams, information including the version used for encoding, the sample rate and number of channels, the bitrate and playback length, and the contents of the comment header are printed. Similarly, for Theora streams, basic information about the video is provided, including frame rate, aspect ratio, bitrate, length, and the comment header.","Process Name":"ogginfo","Link":"https:\/\/linux.die.net\/man\/1\/ogginfo"}},{"Process":{"Description":null,"Process Name":"oggz","Link":"https:\/\/linux.die.net\/man\/1\/oggz"}},{"Process":{"Description":"oggz-chop chops a section of an Ogg file. It correctly interprets the granulepos timestamps of Ogg CELT, CMML, Dirac, FLAC, Kate, PCM, Speex, Theora and Vorbis bitstreams. Run oggz-known-codecs(1) for a full list of codecs known by the installed version of oggz. The output file contains copies of the headers of the input file, and all the codec data required to correctly decode the content between the start and end times specified on the commandline. For codecs with data dependencies like video keyframes, the keyframe prior to the starting time will be included in the output. Note that oggz-chop operates by copying pages of Ogg data; it does not strip partial packets from the first or last data page included in the output. It does however ensure to set the end of stream flag on the last page of each logical bitstream. Skeleton handling: By default, the output will contain a Skeleton track specifying the start of the chop as presentation time.","Process Name":"oggz-chop","Link":"https:\/\/linux.die.net\/man\/1\/oggz-chop"}},{"Process":{"Description":"oggz-codecs displays a list of codecs found in one or more Ogg files and their bitstreams. The default comma-separated output is designed for use in an HTML5 <video> codecs attribute.","Process Name":"oggz-codecs","Link":"https:\/\/linux.die.net\/man\/1\/oggz-codecs"}},{"Process":{"Description":null,"Process Name":"oggz-comment","Link":"https:\/\/linux.die.net\/man\/1\/oggz-comment"}},{"Process":{"Description":"oggz-diff is a sh script which uses oggz-dump to generate hexadecimal packet dumps of each input file, then outputs the difference between these dumps using diff.","Process Name":"oggz-diff","Link":"https:\/\/linux.die.net\/man\/1\/oggz-diff"}},{"Process":{"Description":null,"Process Name":"oggz-dump","Link":"https:\/\/linux.die.net\/man\/1\/oggz-dump"}},{"Process":{"Description":"oggz-info displays information about one or more Ogg files and their bitstreams.","Process Name":"oggz-info","Link":"https:\/\/linux.die.net\/man\/1\/oggz-info"}},{"Process":{"Description":"oggz-known-codecs lists the names of codecs for which oggz can interpret timestamps and report general information. These codec names are valid arguments to the -c, --content-type arguments to other oggz commands.","Process Name":"oggz-known-codecs","Link":"https:\/\/linux.die.net\/man\/1\/oggz-known-codecs"}},{"Process":{"Description":null,"Process Name":"oggz-merge","Link":"https:\/\/linux.die.net\/man\/1\/oggz-merge"}},{"Process":{"Description":"oggz-rip extracts one or more tracks from an Ogg file. In Ogg terminology, oggz-rip extracts one or more logical bitstreams. For example, to extract just the Vorbis audio track from any Ogg file: oggz-rip -c vorbis input.ogx > output.ogg This will removed any other tracks: Skeleton, Theora, image, etc. The output will be a valid Ogg Vorbis I file, suitable for use on hardware players that do not support multiplexed (ie. multi-track) Ogg files. If the input file contains multiple Vorbis, the desired once can be specified by serialno with the -s option. Use oggz-info to view the serialno of each track in a file.","Process Name":"oggz-rip","Link":"https:\/\/linux.die.net\/man\/1\/oggz-rip"}},{"Process":{"Description":"oggz-scan scans the content of an Ogg file and outputs timestamps of characteristic features.","Process Name":"oggz-scan","Link":"https:\/\/linux.die.net\/man\/1\/oggz-scan"}},{"Process":{"Description":"oggz-sort sorts an Ogg file, interleaving pages in order of presentation time. It correctly interprets the granulepos timestamps of Ogg CELT, CMML, Dirac, FLAC, Kate, PCM, Speex, Theora and Vorbis bitstreams. Run oggz-known-codecs(1) for a full list of codecs known by the installed version of oggz. Some encoders produce files with incorrect page ordering; for example, some audio and video pages may occur out of order. Although these files are usually playable, it can be difficult to accurately seek or scrub on them, increasing the likelihood of glitches during playback. Players may also need to use more memory in order to buffer the audio and video data for synchronized playback, which can be a problem when the files are viewed on low-memory devices. Some older Ogg encoders also produce pages which incorrectly have granulepos timestamps recorded that shouldn't be: pages which have no completed packets must have a granulepos of -1. oggz-sort will correct such granulepos during the process of sorting. The tool oggz-validate can be used to check the relative ordering of packets in a file, and also to detect incorrect granulepos on pages with no completed packets. If either of these errors are reported, or use oggz-sort to fix the problem.","Process Name":"oggz-sort","Link":"https:\/\/linux.die.net\/man\/1\/oggz-sort"}},{"Process":{"Description":"oggz-validate validates the Ogg framing of one or more Ogg files. oggz-validate detects the following errors in Ogg framing: File contains no Ogg packets Packets out of order Packet belongs to unknown serialno Granulepos decreasing within track Multiple bos pages Multiple eos pages eos marked but no bos Missing eos packets eos marked on page with no completed packets Granulepos on page with no completed packets Theora video bos page after audio bos page Terminal header page has non-zero granulepos Terminal header page contains non-header packet Terminal header page contains non-header segment oggz-validate exits with status 0 if all files are valid Ogg files, and with status 1 if one or more errors are found.","Process Name":"oggz-validate","Link":"https:\/\/linux.die.net\/man\/1\/oggz-validate"}},{"Process":{"Description":"ogmcat does NOT work at the moment. It is work in progress. I included it just out of laziness (otherwise I'd have to remove it from the Makefile\/configure stuff prior to releasing this version). ogmcat can be used to concatenate several OGG\/OGM files into one big file if they are of the same type. For a more in-depth description refer to the LIMITATIONS section. -o, --output outname Output to ' outname'. inname1 Use ' inname1', ' inname2' etc as the sources. -m, --manualsync n Specifies a manual sync value in ms that will be added to each stream's presentation timestamps along with the value calculated by the chosen sync algorithm (see the -s option). This option can be used for each input file although it has no effect if used for the first one as well. -s, --sync nr Uses sync mode nr. Valid values are 0 - 4. The default value is shown on ogmcat's help screen. -n, --nosafetychecks Disable the safety checks made prior to the concatenating. The resulting file may be unplayable. See the LIMITATIONS section for further details. -v, --verbose Be verbose and show each OGG packet. Can be used twice to increase verbosity. -h, --help Show this help. -V, --version Show version information.","Process Name":"ogmcat","Link":"https:\/\/linux.die.net\/man\/1\/ogmcat"}},{"Process":{"Description":"This program extracts all or only some streams from an OGM and writes them to separate files. inname Use 'inname' as the source. -o, --output out Use ' out' as the base for destination file names. '-v1', '-v2', '-a1', '-t1'... will be appended to this name. Default: use ' inname'. -a, --astream n Extract specified audio stream. Can be used more than once. Default: extract all streams. -d, --vstream n Extract specified video stream. Can be used more than once. Default: extract all streams. -t, --tstream n Extract specified text stream. Can be used more than once. Default: extract all streams. -na, --noaudio Don't extract any audio streams. -nv, --novideo Don't extract any video streams. -nt, --notext Don't extract any text streams. Default: extract all streams. -r, --raw Extract the raw streams only. Default: extract to useful formats (AVI, WAV, OGG, SRT...). -v, --verbose Increase verbosity. -h, --help Show this help. -V, --version Show version number.","Process Name":"ogmdemux","Link":"https:\/\/linux.die.net\/man\/1\/ogmdemux"}},{"Process":{"Description":"This program lists all streams contained in an OGM including information about the codecs used. inname Use 'inname' as the source. -v, --verbose Be verbose and show each OGG packet. See the section ' VERBOSITY LEVELS' for details. -s, --summary Will print a short summary for each stream including the total size in bytes, the bitrate, the number of packets\/frames and the length in seconds. This requires the parsing of the complete file. -h, --help Show usage information. -V, --version Show version information.","Process Name":"ogminfo","Link":"https:\/\/linux.die.net\/man\/1\/ogminfo"}},{"Process":{"Description":"This program takes the input from several media files and joins their streams (all of them or just a selection) into an OGM. It was formerly known as 'oggmerge' and is based on the 'oggmerge' CVS module from Xiph's repository (<http:\/\/www.xiph.org\/>). Global options: -v, --verbose Increase verbosity. -q, --quiet Suppress status output. -o, --output out Write to the file ' out'. Options that can be used for each input file: -a, --astreams < n, m,...> Copy the n'th audio stream, NOT the stream with the serial no. n. Default: copy all audio streams. -d, --vstreams < n, m,...> Copy the n'th video stream, NOT the stream with the serial no. n. Default: copy all video streams. -t, --tstreams < n, m,...> Copy the n'th text stream, NOT the stream with the serial no. n. Default: copy all text streams. -A, --noaudio Don't copy any audio stream from this file. -D, --novideo Don't copy any video stream from this file. -T, --notext Don't copy any text stream from this file. -s, --sync < d[, o[\/ p]]> Synchronize manually, delay the audio stream by d ms. d > 0: Pad with silent samples. d < 0: Remove samples from the beginning. o\/ p: adjust the timestamps by o\/ p to fix linear drifts. p defaults to 1000 if omitted. Both o and p can be floating point numbers. Defaults: no manual synch correction (which is the same as d = 0 and o\/ p = 1.0). -r, --range < start- end> Only process from start to end. Both values take the form ' HH:MM:SS.mmm' or ' SS.mmm', e.g. '00:01:00.500' or '60.500'. If one of start or end is omitted then it defaults to 0 or to end of the file respectively. If you want to split a file into smaller ones I strongly suggest you use ogmsplit(1) as it can do a much better job than using the -r option. -c, --comment ' A=B#C=D' or ' @filename' Set additional comment fields for the streams. Sensitive values would be ' LANGUAGE=English' or ' TITLE=Ally McBeal'. If the parameter starts with '@' then the comments will be read from a file with the same name without the leading '@'. -c can be specified multiple times per file. The comments will all be concatenated. -f, --fourcc < FourCC> Forces the FourCC to the specified value. Works only for video streams. Note that you cannot simply use a hex editor and change the FourCC by hand as the OGG file format uses checksums which would be wrong after such a change. --omit-empty-packets Normally, when a subtitle entry should be removed, an empty packet is created and inserted with the appropriate timestamp. With this option these empty packets are omitted completely. --old-headers Assume that the input file has been created with an older version of ogmmerge ( < 1.1). This may be needed if ogmmerge cannot read such a file correctly. --nav-seek < filename> Use an external AVI index file as generated by aviindex from the transcode package. Can be used if an AVI file has a broken index. Other options: -l, --list-types List supported input file types. -h, --help Show usage information. -V, --version Show version information.","Process Name":"ogmmerge","Link":"https:\/\/linux.die.net\/man\/1\/ogmmerge"}},{"Process":{"Description":"ogmsplit can be used to easily split an OGM file after a given size. Several OGM files will be created that each start with a keyframe. inname Use 'inname' as the source. -o, --output out Use ' out' as the base name. Ascending part numbers will be appended to it. Default is ' inname'. Examples: 1) If -o output.ogg is given on the command line then ogmsplit will create output-000001.ogg, output-000002.ogg and so on. 2) If no -o option is given and the input's name is movie.ogm then ogmsplit will create movie-000001.ogm and so on. The operation mode can be set with exactly one of -s, -t, -c or -p. The default mode is to split by size ( -s). -s, --size size Size in MiB ( = 1024 * 1024 bytes) after which a new file will be opened (approximately). Default is 700MiB. Size can end in 'B' to indicate 'bytes' instead of 'MiB'. -t, --time time Split after the given elapsed time (approximately). ' time' takes the form HH:MM:SS.sss or simply SS(. sss), e.g. 00:05:00.000 or 300.000 or simply 300. -c, --cuts cuts Produce output files as specified by cuts, a list of slices of the form \" start- end\" or \" start+ length\", separated by commas. If start is omitted, it defaults to the end of the previous cut. start and end take the same format as the arguments to -t. -n, --num num Don't create more than num separate files. The last one may be bigger than the desired size. Default is an unlimited number of files. Can only be used with -s or -t. --frontend Frontend mode. Progress output will be terminated by \\n instead of \\r. -p, --print-splitpoints Only print the key frames and the number of bytes encountered before each. Useful to find the exact splitting point. -v, --verbose Be verbose and show each OGG packet. Can be used twice to increase verbosity. -h, --help Show this help. -V, --version Show version information.","Process Name":"ogmsplit","Link":"https:\/\/linux.die.net\/man\/1\/ogmsplit"}},{"Process":{"Description":"ogonkify does various munging of PostScript files related to printing in different languages. Its main use is to filter the output of Netscape, Mosaic and other programs in order to print in languages that don't use the standard Western-European encoding (ISO 8859-1).","Process Name":"ogonkify","Link":"https:\/\/linux.die.net\/man\/1\/ogonkify"}},{"Process":{"Description":null,"Process Name":"ogr2ogr","Link":"https:\/\/linux.die.net\/man\/1\/ogr2ogr"}},{"Process":{"Description":null,"Process Name":"ogr_utilities","Link":"https:\/\/linux.die.net\/man\/1\/ogr_utilities"}},{"Process":{"Description":"The ogrinfo program lists various information about an OGR supported data source to stdout (the terminal). -ro: Open the data source in read-only mode. -al: List all features of all layers (used instead of having to give layer names as arguments). -so: Summary Only: supress listing of features, show only the summary information like projection, schema, feature count and extents. -q: Quiet verbose reporting of various information, including coordinate system, layer schema, extents, and feature count. -where restricted_where : An attribute query in a restricted form of the queries used in the SQL WHERE statement. Only features matching the attribute query will be reported. -sql statement : Execute the indicated SQL statement and return the result. -spat xmin ymin xmax ymax : The area of interest. Only features within the rectangle will be reported. -fid fid : If provided, only the feature with this feature id will be reported. Operates exclusive of the spatial or attribute queries. Note: if you want to select several features based on their feature id, you can also use the fact the 'fid' is a special field recognized by OGR SQL. So, '-where 'fid in (1,3,5)'' would select features 1, 3 and 5. -fields={YES\/NO}: (starting with GDAL 1.6.0) If set to NO, the feature dump will not display field values. Default value is YES. -geom={YES\/NO\/SUMMARY}: (starting with GDAL 1.6.0) If set to NO, the feature dump will not display the geometry. If set to SUMMARY, only a summary of the geometry will be displayed. If set to YES, the geometry will be reported in full OGC WKT format. Default value is YES. --formats: List the format drivers that are enabled. datasource_name : The data source to open. May be a filename, directory or other virtual name. See the OGR Vector Formats list for supported datasources. layer : One or more layer names may be reported. If no layer names are passed then ogrinfo will report a list of available layers (and their layerwide geometry type). If layer name(s) are given then their extents, coordinate system, feature count, geometry type, schema and all features matching query parameters will be reported to the terminal. If no query parameters are provided, all features are reported. Geometries are reported in OGC WKT format.","Process Name":"ogrinfo","Link":"https:\/\/linux.die.net\/man\/1\/ogrinfo"}},{"Process":{"Description":"The ogrtindex program can be used to create a tileindex - a file containing a list of the identities of a bunch of other files along with there spatial extents. This is primarily intended to be used with MapServer for tiled access to layers using the OGR connection type. -lnum n : Add layer number 'n' from each source file in the tile index. -lname name : Add the layer named 'name' from each source file in the tile index. -f output_format : Select an output format name. The default is to create a shapefile. -tileindex field_name : The name to use for the dataset name. Defaults to LOCATION. -write_absolute_path: Filenames are written with absolute paths -skip_different_projection: Only layers with same projection ref as layers already inserted in the tileindex will be inserted. If no -lnum or -lname arguments are given it is assumed that all layers in source datasets should be added to the tile index as independent records. If the tile index already exists it will be appended to, otherwise it will be created. It is a flaw of the current ogrtindex program that no attempt is made to copy the coordinate system definition from the source datasets to the tile index (as is expected by MapServer when PROJECTION AUTO is in use).","Process Name":"ogrtindex","Link":"https:\/\/linux.die.net\/man\/1\/ogrtindex"}},{"Process":{"Description":"This program is part of the OpenImageIO (http:\/\/www.openimageio.org) tool suite. Detailed documentation is avaliable in pdf format with the OpenImageIO distribution.","Process Name":"oiiotool","Link":"https:\/\/linux.die.net\/man\/1\/oiiotool"}},{"Process":{"Description":"oLschema2ldif is a simple tool that converts standard OpenLDAP schema files to a LDIF format that is understood by LDB.","Process Name":"olschema2ldif","Link":"https:\/\/linux.die.net\/man\/1\/olschema2ldif"}},{"Process":{"Description":"omcmd is a scriptable CLI program for performing actions on a running ISC dhcp server using OMAPI. It can currently be used to create, remove, modify and look up hosts and leases. Note that host and lease objects have persistent storage in the leases file. The keyname and secret are the same as the values in the dhcpd.conf file; the secret should be base64 encoded. If using a 'keyfile', line1=keyname, line2=secret. The -v option affects the lookup|find command, see below. Commands In the command descriptions below, 'indexproperty' is used to indicate a property that specifies the object uniquely to the dhcp server. I.e., ip-address uniquely specifies a lease object. lookup|find <objecttype> <indexproperty=value> [property ...] Look up a lease or host. For leases, specify the ip-address or hardware-address. For hosts, specify the name or hardware-address. If property names are specified after the lookup property, only those property values will be returned, one per line. if -v is given, they will be returned in property=value format. create|add|new <objecttype> <property=value> ... Create a new object, usually a 'host'. For hosts, specify name, hardware-address, hardware-type (usually 1 for ethernet), and optionally ip-address. remove|delete <objecttype> <indexproperty=value> Delete an object, usually a host. For hosts, you must specify name or hardware-address. At the time this was written, you cannot delete a lease; you can, however, modify the lease and change it's 'state' to 'free', but this requires a patched version of dhcp (as of dhcp-3.0.1, see the README included with the source). See modify, below. modify|change|update <objecttype> <indexproperty=value> <property=value> ... Change an object. Mostly used to change the 'state' of a 'lease'. Object Types Currently omcmd only has support for host and lease object types. Properties The following properties are known to omcmd: state - the lease state ip-address dhcp-client-identifier client-hostname hardware-address hardware-type - normally 1 for ethernet ends tstp tsfp cltt name - unique name for host object statements known Note: check the dhcp man pages listed in SEE ALSO, below, for more information on the properties and object types.","Process Name":"omcmd","Link":"https:\/\/linux.die.net\/man\/1\/omcmd"}},{"Process":{"Description":"Run the Omega typesetter on file, usually creating file.dvi. If the file argument has no extension, \".tex\" will be appended to it. Instead of a filename, a set of Omega commands can be given, the first of which must start with a backslash. With a &format argument Omega uses a different set of precompiled commands, contained in format.fmt; it is usually better to use the -fmt format option instead. Omega is a version of the TeX program modified for multilingual typesetting. It uses unicode, and has additional primitives for (among other things) bidirectional typesetting. The iniomega and viromega commands are Omega's analogues to the initex and virtex commands. In this installation, they are symlinks to the omega executable. Omega's command line options are similar to those of TeX. Omega is experimental software.","Process Name":"omega","Link":"https:\/\/linux.die.net\/man\/1\/omega"}},{"Process":{"Description":"This manual page documents briefly the omnicpp command. omniidl is omniORBs IDL compiler and uses omnicpp as it's C preprocessor. omnicpp is really just the GNU C preprocessor under a different name. Normally you should not need to invoke this directly but should use omniidl instead.","Process Name":"omnicpp","Link":"https:\/\/linux.die.net\/man\/1\/omnicpp"}},{"Process":{"Description":null,"Process Name":"omniidl","Link":"https:\/\/linux.die.net\/man\/1\/omniidl"}},{"Process":{"Description":"orte-checkpoint can be invoked multiple, non-overlapping times. It is convenient to note that the user does not need to spectify the checkpointer to be used here, as that is determined completely by each of the running process in the job being checkpointed.","Process Name":"ompi-checkpoint","Link":"https:\/\/linux.die.net\/man\/1\/ompi-checkpoint"}},{"Process":{"Description":"","Process Name":"ompi-clean","Link":"https:\/\/linux.die.net\/man\/1\/ompi-clean"}},{"Process":{"Description":"ompi-iof displays a copy of the indicated stdout, stderr, and\/or stddiag streams from the designated process. At this time, a ctrl-C must be used to terminate the program. The program will terminate cleanly, telling the associated mpirun to close the requested streams before exiting.","Process Name":"ompi-iof","Link":"https:\/\/linux.die.net\/man\/1\/ompi-iof"}},{"Process":{"Description":null,"Process Name":"ompi-ps","Link":"https:\/\/linux.die.net\/man\/1\/ompi-ps"}},{"Process":{"Description":"ompi-restart can be invoked multiple, non-overlapping times. This allows the user to restart a previously running parallel job.","Process Name":"ompi-restart","Link":"https:\/\/linux.die.net\/man\/1\/ompi-restart"}},{"Process":{"Description":null,"Process Name":"ompi-server","Link":"https:\/\/linux.die.net\/man\/1\/ompi-server"}},{"Process":{"Description":"ompi_info provides detailed information about the Open MPI installation. It can be useful for at least three common scenarios: 1. Checking local configuration and seeing how Open MPI was installed. 2. Submitting bug reports \/ help requests to the Open MPI community (see http:\/\/www.open-mpi.org\/community\/help\/) 3. Seeing a list of installed Open MPI plugins and querying what MCA parameters they support.","Process Name":"ompi_info","Link":"https:\/\/linux.die.net\/man\/1\/ompi_info"}},{"Process":{"Description":"The OMAPI Command Shell, omshell, provides an interactive way to connect to, query, and possibly change, the ISC DHCP Server's state via OMAPI, the Object Management API. By using OMAPI and omshell, you do not have to stop, make changes, and then restart the DHCP server, but can make the changes while the server is running. Omshell provides a way of accessing OMAPI. OMAPI is simply a communications mechanism that allows you to manipulate objects. In order to actually use omshell, you must understand what objects are available and how to use them. Documentation for OMAPI objects can be found in the documentation for the server that provides them - for example, in the dhcpd(1) manual page and the dhclient(1) manual page.","Process Name":"omshell","Link":"https:\/\/linux.die.net\/man\/1\/omshell"}},{"Process":{"Description":"This manual page documents briefly the on_ac_power command. on_ac_power is a command line program to test whether the computer is running on line power","Process Name":"on_ac_power","Link":"https:\/\/linux.die.net\/man\/1\/on_ac_power"}},{"Process":{"Description":null,"Process Name":"onnode","Link":"https:\/\/linux.die.net\/man\/1\/onnode"}},{"Process":{"Description":"onsgmls parses and validates the SGML document whose document entity is specified by the system identifiers and prints on the standard output a simple text representation of its Element Structure Information Set. (This is the information set which a structure-controlled conforming SGML application should act upon.) If more than one system identifier is specified, then the corresponding entities will be concatenated to form the document entity. Thus the document entity may be spread among several files; for example, the SGML declaration, prolog and document instance set could each be in a separate file. If no system identifiers are specified, then onsgmls will read the document entity from the standard input. A command line system identifier of - can be used to refer to the standard input. (Normally in a system identifier, <OSFD>0 is used to refer to standard input.) Part of an SGML System Conforming to International Standard ISO 8879 -- Standard Generalized Markup Language. An SGML Extended Facilities system conforming to Annex A of Internal Standard ISO\/IEC 10744 -- Hypermedia\/Time-based Structuring Language The following options are available: -alinktype, --activate=linktype Make link type linktype active. Not all ESIS information is output in this case: the active LPDs are not explicitly reported, although each link attribute is qualified with its link type name; there is no information about result elements; when there are multiple link rules applicable to the current element, onsgmls always chooses the first. -A architecture, --architecture= architecture Parse with respect to architecture architecture. -b bctf, --bctf= bctf, -b encoding, --encoding= encoding This determines the encoding used for output. If in fixed character set mode it specifies the name of an encoding; if not, it specifies the name of a BCTF. -B, --batch_mode Batch mode. Parse each specified on the command line separately, rather than concatenating them. This is useful mainly with -s. If -tfilename is also specified, then the specified filename will be prefixed to the sysid to make the filename for the RAST result for each sysid. -c sysid, --catalog= sysid Map public identifiers and entity names to system identifiers using the catalog entry file whose system identifier is sysid. Multiple -c options are allowed. If there is a catalog entry file called catalog in the same place as the document entity, it will be searched for immediately after those specified by -c. -C, --catalogs The arguments specify catalog files rather than the document entity. The document entity is specified by the first DOCUMENT entry in the catalog files. -D directory, --directory= directory Search directory for files specified in system identifiers. Multiple -D options are allowed. See the description of the osfile storage manager for more information about file searching. -e, --open-entities Describe open entities in error messages. Error messages always include the position of the most recently opened external entity. -E max_errors, --max-errors= max_errors onsgmls will exit after max_errors errors. If max_errors is 0, there is no limit on the number of errors. The default is 200. -f file, --error-file= file Redirect errors to file. This is useful mainly with shells that do not support redirection of stderr. -g, --open-elements Show the generic identifiers of open elements in error messages. -h, --help Show a help message and exit. -i name, --include= name Pretend that <!ENTITY % name \"INCLUDE\"> occurs at the start of the document type declaration subset in the SGML document entity. Since repeated definitions of an entity are ignored, this definition will take precedence over any other definitions of this entity in the document type declaration. Multiple -i options are allowed. If the SGML declaration replaces the reserved name INCLUDE then the new reserved name will be the replacement text of the entity. Typically the document type declaration will contain <!ENTITY % name \"IGNORE\"> and will use %name; in the status keyword specification of a marked section declaration. In this case the effect of the option will be to cause the marked section not to be ignored. -n, --error-numbers Show message numbers in error messages. -o output_option, --option= output_option Output additional information according to output_option: entity Output definitions of all general entities not just for data or subdoc entities that are referenced or named in an ENTITY or ENTITIES attribute. id Distinguish attributes whose declared value is ID. line Output L commands giving the current line number and filename. included Output an i command for included sub-elements. empty Output an e command for elements which are not allowed to have an end-tag, that is those with a declared content of empty or with a content reference attribute. notation-sysid Output an f command before an N command, if a system identifier could be generated for that notation. nonsgml In fixed character set mode, output \\% escape sequences for non-SGML data characters. Non-SGML data characters can result from numeric character references. data-attribute Output the notation name and attributes for DATA attributes. Otherwise, DATA attributes are treated like CDATA attributes. For more details see clause 4.4.3 of Annex K of ISO 8879. comment Output an _ command with the contents of a comment. Multiple comments in a single comment declaration will result in multiple distinct _ commands, just as if the comments were each in a separate comment declaration. omitted Output an o command before a command which was implied by the input document, but omitted from the actual markup. This currently affects (,), and A commands. tagomit As omitted, but only for ( and ) commands. attromit As omitted, but only for A commands. Multiple -o options are allowed. -p, --only-prolog Parse only the prolog. onsgmls will exit after parsing the document type declaration. Implies -s. -R, --restricted Restrict file reading. This option is intended for use with onsgmls-based Web tools (e.g. CGI scripts) to prevent reading of arbitrary files on the Web server. With this option enabled, onsgmls will not read any local files unless they are located in a directory (or subdirectory) specified by the -D option or included in the SGML_SEARCH_PATH environment variable. As a further security precaution, this option limits filesnames to the characters A-Z, a-z, 0-9, '?', '.', '_', '-' and does not allow filenames containing \"..\". On systems with MS-DOS file names ':' and '\\' are also allowed. -s, --no-output Suppress output. Error messages will still be printed. -t file, --rast-file= file Output to file the RAST result as defined by ISO\/IEC 13673:1995 (actually this isn't quite an IS yet; this implements the Intermediate Editor's Draft of 1994\/08\/29, with changes to implement ISO\/IEC JTC1\/SC18\/WG8 N1777). The normal output is not produced. -v, --version Print the version number. -w type, --warning= type Control warnings and errors. Multiple -w options are allowed. The following values of type enable warnings: xml Warn about constructs that are not allowed by XML. mixed Warn about mixed content models that do not allow #PCDATA anywhere. sgmldecl Warn about various dubious constructions in the SGML declaration. should Warn about various recommendations made in ISO 8879 that the document does not comply with. (Recommendations are expressed with \"should\", as distinct from requirements which are usually expressed with \"shall\".) default Warn about defaulted references. duplicate Warn about duplicate entity declarations. undefined Warn about undefined elements: elements used in the DTD but not defined. unclosed Warn about unclosed start and end-tags. empty Warn about empty start and end-tags. net Warn about net-enabling start-tags and null end-tags. min-tag Warn about minimized start and end-tags. Equivalent to combination of unclosed, empty and net warnings. unused-map Warn about unused short reference maps: maps that are declared with a short reference mapping declaration but never used in a short reference use declaration in the DTD. unused-param Warn about parameter entities that are defined but not used in a DTD. Unused internal parameter entities whose text is INCLUDE or IGNORE won't get the warning. notation-sysid Warn about notations for which no system identifier could be generated. all Warn about conditions that should usually be avoided (in the opinion of the author). Equivalent to: mixed, should, default, undefined, sgmldecl, unused-map, unused-param, empty and unclosed. immediate-recursion Warn about immediately recursive elements. For more detais see clause 2.2.5 of Annex K of ISO 8879. fully-declared Warn if the document instance fails to be fully declared. This has the effect of changing the SGML declaration to specify IMPLYDEF ATTLIST NO ELEMENT NO ENTITY NO NOTATION NO. For more details see clause 2.2.1 of Annex K of ISO 8879. fully-tagged Warn if the document instance fails to be fully-tagged. This has the effect of changing the SGML declaration to specify DATATAG NO, RANK NO, OMITTAG NO, SHORTTAG STARTTAG EMPTY NO and SHORTTAG ATTRIB OMITNAME NO. For more details see clause 2.2.2 of Annex K of ISO 8879. amply-tagged, amply-tagged-recursive Warn if the doucment instance fails to be amply-tagged. Implicitly defined elements may be immediately recurisve if amply-tagged-recursive is specified. This has the effect of changing the SGML declaration to specify DATATAG NO, RANK NO, OMITTAG NO, SHORTTAG ATTRIB OMITNAME NO and either IMPLYDEF ELEMENT ANYOTHER or IMPLYDEF ELEMENT YES. For more details see clause 2.2.4 of Annex K of ISO 8879. type-valid Warn if the document instance fails to be type-valid. This has the effect of changing the SGML declaration to specify VALIDITY YES. For more details see clause 2.2.3 of Annex K of ISO 8879. entity-ref Warn about references to non-predefined entities. This has the effect of changing the SGML declaration to specify ENTITIES REF NONE. For more details see clause 2.3.2 of Annex K of ISO 8879. external-entity-ref Warn about references to external entities. This includes references to an external DTD subset. This has the effect of changing the SGML declaration to specify ENTITIES REF INTERNAL. For more details see clause 2.3.3 of Annex K of ISO 8879. integral Warn if the document instance is not integrally stored. This has the effect of changing the SGML declaration to specify ENTITIES INTEGRAL YES. For more details see clause 2.3.1 of Annex K of ISO 8879. A warning can be disabled by using its name prefixed with no-. Thus -wall -wno-duplicate will enable all warnings except those about duplicate entity declarations. The following values for warning_type disable errors: no-idref Do not give an error for an ID reference value which no element has as its ID. The effect will be as if each attribute declared as an ID reference value had been declared as a name. no-significant Do not give an error when a character that is not a significant character in the reference concrete syntax occurs in a literal in the SGML declaration. This may be useful in conjunction with certain buggy test suites. no-valid Do not require the document to be type-valid. This has the effect of changing the SGML declaration to specify VALIDITY NOASSERT and IMPLYDEF ATTLIST YES ELEMENT YES. An option of -wvalid has the effect of changing the SGML declaration to specify VALIDITY TYPE and IMPLYDEF ATTLIST NO ELEMENT NO. If neither -wvalid nor -wno-valid are specified, then the VALIDITY and IMPLYDEF specified in the SGML declaration will be used. no-afdr Do not give errors when AFDR meta-DTD notation features are used in the DTD. These errors are normally produced when parsing the DTD, but suppressed when parsing meta-DTDs. -x, --references Show information about relevant clauses (from ISO 8879:1986) in error messages. The following options are also supported for backward compatibility with sgmls: -d Same as -wduplicate. -l Same as -oline. -m sysid Same as -c. -r Same as -wdefault. -u Same as -wundef.","Process Name":"onsgmls","Link":"https:\/\/linux.die.net\/man\/1\/onsgmls"}},{"Process":{"Description":"See: http:\/\/translate.sourceforge.net\/wiki\/toolkit\/oo2po for examples and usage instructions","Process Name":"oo2po","Link":"https:\/\/linux.die.net\/man\/1\/oo2po"}},{"Process":{"Description":"User documentation: http:\/\/translate.sourceforge.net\/wiki\/toolkit\/oo2po","Process Name":"oo2xliff","Link":"https:\/\/linux.die.net\/man\/1\/oo2xliff"}},{"Process":{"Description":"The \"oodist\" script is currently only usable on UNIX in combination with Makefile.PL. It is a smart wrapper around the OODoc module, to avoid start work to produce real POD for modules which use OODoc's POD extensions. HTML is not (yet) supported. Configuring All OPTIONS can be specified on the command-line, but you do not want to specify them explicitly each time you produce a new distribution for your product. The options come in two kinds: those which are environment independent and those which are. The first group can be set via the Makefile.PL, the second can be set using environment variables as well. Example: add to the end of your \"Makefile.PL\" sub MY::postamble { <<'__POSTAMBLE' }\nFIRST_YEAR   = 2001\nWEBSITE      = http:\/\/perl.overmeer.net\/oodoc\nEMAIL        = oodoc@overmeer.net\n__POSTAMBLE Main options The process is controlled by four main options. All options are by default \"on\". . --pod or --nopod Produce pod files in the working directory and in the distribution. . --dist or --nodist Create a distribution, containing all files from the MANIFEST plus produced files. . --html or --nohtml Create html manual pages. The \"--html-templates\" options must point to an existing directory (defaults to the \"html\/\" sub-directory). . --raw or --noraw Create a package which contains the files which are needed to produce the distribution: the pm files still including the oodoc markup. General options The other OPTIONS in detail . --readme <filename> Copy the specified README file into the distribution, if there is no README yet. The name will be added to the MANIFEST . If the option is not specified, a simple file will be created. If this option is specified, but the filename is empty, then the README will not be produced. . --distdir | -d <dir> The location where the final file to be distributed is placed, by default in the source directory. Ignored when \"--test\" is used. . --extends | -x <path> A colon seperated list of directories which contains \"raw oodoc\" pm and pod files which are (in some cases) used to provide data for base-classes of this module. . --rawdir | --r <dir> The location where a raw version of the distribution is made. The normal distribution contains expanded POD and stripped PM files. For that reason, you can not use it a backup for your files. If you have co-workers on the module, you may wish to give them the originals. gnored when \"--test\" is used. . --test | -t Run in test mode: the raw and real distributions are produced, but not moved to their final location. You will end-up with these archives in the work-directory (see \"--workdir\"). . --verbose | --v Shows what happens during the process. . --workdir | -w <dir> The processing will take place in a seperate directory: the stripped pm's and produced pod files will end-up there. If not provided, that directory will be named after the project, and located in $ENV{TMPDIR}, which defaults to \"\/tmp\". For instance \"\/tmp\/OODoc\/\" Options for parsers . --skip_links <strings> A blank or comma separated list of packages which will have a manual page, but cannot be loaded under their name. Sub-packages are excluded as well. For instance, XML::LibXML has many manual-pages without a package with the same name. Options to produce POD . --email | -e <email> The email address which can be used to contact the authors. Used in the automatic podtail. . --firstyear| -y <string> The first year that a release for this software was made. Used in the automatic podtail. The specified string can be more complex than a simple year, for instance \"1998-2000,2003\". The last year will be automatically added like \"-2006\", which results in \"1998-2000,2003-2006\". When the current year is detected at the end of the string, the year will not be added again. . --license | -l ['gpl'|'artistic'|filename] The lisense option is a special case: it can contain either the strings \"gpl\" or \"artistic\", or a filename which is used to read the actual license text from. The default is \"artistic\" . --pmhead Each of the stripped \"pm\" files will have content of the file added at the top. Each line will get a comment '# ' marker before it. If not specified, a short notice will be produced automatically. Implicitly, if a file \"PMHEAD.txt\" exists, that will be used. variables found in the text will be filled-in. . --podtail Normally, a trailing text is automatically produced, based on all kinds of facts. However, you can specify your own file. If exists, the content is read from a file named \"PODTAIL.txt\". After reading the file, variables will be filled in: scalars like $version, $project, $website etc are to your disposal. . --website | -u <url> Where the HTML documentation related to this module is publicly visible. Options to produce HTML . --html-docroot <url> This (usually relative) URL is prepended to all absolute links in the HTML output. . --html-output <dir> The directory where the produced HTML pages are written to. . --html-templates <dir> The directory which contains the templates for HTML pages which are used to construct the html version of the manual-pages. . --html-package <dir> When specified, the html-output directory will get packaged into a a tar achive in this specified directory. Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"oodist","Link":"https:\/\/linux.die.net\/man\/1\/oodist"}},{"Process":{"Description":"This manual page documents briefly the oogl2rib command. oogl2rib Convert OOGL file to RenderMan rib format. Default: read from stdin, write to stdout. Accepts \"-\" as infile\/outfile.","Process Name":"oogl2rib","Link":"https:\/\/linux.die.net\/man\/1\/oogl2rib"}},{"Process":{"Description":"This manual page documents briefly the oogl2vrml command. oogl2vrml Convert OOGL file to VRML format. Writes to standard output. Reads from stdin if no file specified.","Process Name":"oogl2vrml","Link":"https:\/\/linux.die.net\/man\/1\/oogl2vrml"}},{"Process":{"Description":"The op tool provides a flexible means for system administrators to grant trusted users access to certain root operations without having to give them full superuser privileges. Different sets of users may access different operations, and the security-related aspects of environment of each operation can be carefully controlled.","Process Name":"op","Link":"https:\/\/linux.die.net\/man\/1\/op"}},{"Process":{"Description":"By default, op_help lists the available performance counter options. If you give it a symbolic event name, it will return the hardware value (e.g. \"op_help DATA_MEM_REFS\").","Process Name":"op_help","Link":"https:\/\/linux.die.net\/man\/1\/op_help"}},{"Process":{"Description":"opal_wrapper is not meant to be called directly by end users. It is automatically invoked as the back-end by the Open MPI wrapper commands such as: mpicc, mpiCC, mpic++, mpif77, and mpif90. Some Open MPI installations may have additional wrapper commands, and\/or have renamed the wrapper compilers listed above to avoid executable name conflicts with other MPI implementations. Hence, you may also have wrapper compilers installed including the following names: mpif90.openmpi, mpif77.openmpi, mpicxx.openmpi, mpiCC.openmpi, mpicc.openmpi, mpic++.openmpi, opalcc, opalc++, ortecc, and ortec++,","Process Name":"opal_wrapper","Link":"https:\/\/linux.die.net\/man\/1\/opal_wrapper"}},{"Process":{"Description":"opannotate outputs annotated source and\/or assembly from profile data of an OProfile session. See oprofile(1) for how to write profile specifications.","Process Name":"opannotate","Link":"https:\/\/linux.die.net\/man\/1\/opannotate"}},{"Process":{"Description":"oparchive generates a directory populated with executable, debug, and oprofile sample files. This directory can be move to another machine via tar and analyzed without further use of the data collection machine. See oprofile(1) for how to write profile specifications.","Process Name":"oparchive","Link":"https:\/\/linux.die.net\/man\/1\/oparchive"}},{"Process":{"Description":"opcontrol can be used to start profiling, end a profiling session, dump profile data, and set up the profiling parameters.","Process Name":"opcontrol","Link":"https:\/\/linux.die.net\/man\/1\/opcontrol"}},{"Process":{"Description":"THIS MAN PAGE IS OBSOLETE! See the Texinfo documentation instead. You can read it either in Emacs or with the standalone info program which comes with the GNU texinfo distribution as prep.ai.mit.edu:pub\/gnu\/texinfo*.tar.gz. The program dvips takes a DVI file file[.dvi] produced by TeX (or by some other processor such as GFtoDVI) and converts it to PostScript, normally sending the result directly to the (laser)printer. The DVI file may be specified without the .dvi extension. Fonts used may either be resident in the printer or defined as bitmaps in PK files, or a 'virtual' combination of both. If the mktexpk program is installed, dvips will automatically invoke METAFONT to generate fonts that don't already exist. For more information, see the Texinfo manual dvips.texi, which should be installed somewhere on your system, hopefully accessible through the standard Info tree.","Process Name":"opdvips","Link":"https:\/\/linux.die.net\/man\/1\/opdvips"}},{"Process":{"Description":"","Process Name":"open","Link":"https:\/\/linux.die.net\/man\/1\/open"}},{"Process":{"Description":"The openapp command allows you to launch graphical GNUstep applications from the command line. application is the complete or relative name of the application program with or without the .app extension, like Ink.app. arguments are the arguments passed to the application. openapp first checks whether the application is in the current working directory. If not then searches the GNUstep domains' Applications folders in the following order: User (i.e. ~\/GNUstep\/Applications), Local, Network, System. First match wins.","Process Name":"openapp","Link":"https:\/\/linux.die.net\/man\/1\/openapp"}},{"Process":{"Description":"Openbox is minimalistic, highly configurable, next generation window manager with extensive standards support. You can start Openbox in three ways: If you run a display manager such as GDM, you will find 3 entries in the login session type menu for Openbox: GNOME\/Openbox, KDE\/Openbox and Openbox. If you want to use Openbox within GNOME or KDE, you can choose the appropriate entry, and it will launch GNOME or KDE with Openbox as the window manager. The third option at log in, which is Openbox without a session manager, uses the openbox-session command to start Openbox. On log in, openbox will run the ~\/.config\/openbox\/autostart.sh script if it exists, and will run the system-wide script \/etc\/xdg\/openbox\/autostart.sh otherwise. You may place anything you want to run automatically in those files, for example: xsetroot -solid grey &\ngnome-settings-daemon & Make sure that each line is followed by a \"&\" or else the script will stop there and further commands will not be executed. You can use the \/etc\/xdg\/openbox\/autostart.sh file as an example for creating your own. The default \/etc\/xdg\/openbox\/autostart.sh runs a number of things with Openbox. Lastly, if you use startx to launch your X session, you can set up a ~\/.xinitrc file to run openbox-session and follow the same directions as above regarding the autostart.sh file. You can use the obconf tool to configure Openbox easily with a graphical interface, however more in-depth configuration is possible by editing the configuration files by hand. The default configuration and menu files are installed in \/etc\/xdg\/openbox\/, and the user configuration is placed in ~\/.config\/openbox\/. You can copy the default configuration and menus to ~\/.config\/openbox and edit it to your liking.","Process Name":"openbox","Link":"https:\/\/linux.die.net\/man\/1\/openbox"}},{"Process":{"Description":"openbox-gnome-session runs a GNOME session with Openbox as the window manager. openbox-gnome-session does not take any command line arguments.","Process Name":"openbox-gnome-session","Link":"https:\/\/linux.die.net\/man\/1\/openbox-gnome-session"}},{"Process":{"Description":"openbox-kde-session runs a KDE session with Openbox as the window manager. openbox-kde-session does not take any command line arguments.","Process Name":"openbox-kde-session","Link":"https:\/\/linux.die.net\/man\/1\/openbox-kde-session"}},{"Process":{"Description":"openbox-session runs an openbox session without any session manager. Without a session manager, you will not be able to save your state from one log in to the next. openbox-session does not take any command line arguments. On log in, openbox-session will run the ~\/.config\/openbox\/autostart.sh script if it exists, and will run the system-wide script \/etc\/xdg\/openbox\/autostart.sh otherwise. You may place anything you want to run automatically in those files, for example: xsetroot -solid grey &\ngnome-settings-daemon & Make sure that each line is followed by a \"&\" or else the script will stop there and further commands will not be executed. You can use the \/etc\/xdg\/openbox\/autostart.sh file as an example for creating your own.","Process Name":"openbox-session","Link":"https:\/\/linux.die.net\/man\/1\/openbox-session"}},{"Process":{"Description":"openchangeclient is a MAPI command line tool designed to facilitate mail send, receive and delete operations using the MAPI protocol. It also provides operations on tasks, contacts (address book) and calendar operations.","Process Name":"openchangeclient","Link":"https:\/\/linux.die.net\/man\/1\/openchangeclient"}},{"Process":{"Description":"openchangepfadmin is a administrative command line tool designed to facilitate user management related operations (create, delete, modify) on a remote Exchange server and manage public folder store and permissions.","Process Name":"openchangepfadmin","Link":"https:\/\/linux.die.net\/man\/1\/openchangepfadmin"}},{"Process":{"Description":"","Process Name":"openct-tool","Link":"https:\/\/linux.die.net\/man\/1\/openct-tool"}},{"Process":{"Description":"opendkim-spam accepts a regular format message (RFC5322) on standard input and uses it to update a local SQL database being updated by opendkim(8) with an indiciation that a user believes the input message is spam or otherwise abusive. This feedback is important input toward developing DKIM-based domain reputation systems. The tool is intended to be used directly from within shell-based mail readers such as alpine(1) or mutt(1) using a \"pipe\" command, which feeds the message being read to the specified program. This tool is experimental. If the experiment proves useful, the feedback could be used as an input stream to a larger-scale collaborative feedback system that can be used to identify sources of signed mail that have good reputations.","Process Name":"opendkim-spam","Link":"https:\/\/linux.die.net\/man\/1\/opendkim-spam"}},{"Process":{"Description":"OpenGrok is a fast and usable source code search and cross reference engine written in pure Java. It helps you search, cross-referenece and navigate your source tree. It can understand various program file formats and version control histories.","Process Name":"opengrok","Link":"https:\/\/linux.die.net\/man\/1\/opengrok"}},{"Process":{"Description":"The openipmicmd program allows a user to execute direct IPMI commands. It can work with direct interface with the OpenIPMI driver or with IPMI LAN interfaces.","Process Name":"openipmicmd","Link":"https:\/\/linux.die.net\/man\/1\/openipmicmd"}},{"Process":{"Description":"openipmigui is a GUI interface using the OpenIPMI library. It provides a tree-structured view of the IPMI domains it connected to. By default openipmigui starts up with no connections or anything of that nature. You must open connection to domains yourself. However, you may save your current configuration and the current domains and some GUI settings; these will be automatically restored at startup. The main window has a tree on the left and a log window on the right. The log window captures informational and error logs from OpenIPMI. Note that events are not reported in the log window, you must open an SEL window for a domain to view the events.","Process Name":"openipmigui","Link":"https:\/\/linux.die.net\/man\/1\/openipmigui"}},{"Process":{"Description":"The openipmish is a command interpreter that gives the full power of the OpenIPMI library to a user-level command language. It is designed so it can easily be driven with a scripting language like TCL, it has well-formed output. openipmish starts up with no connections or anything of that nature. You must enter commands to make connections to domains.","Process Name":"openipmish","Link":"https:\/\/linux.die.net\/man\/1\/openipmish"}},{"Process":{"Description":"openjade is an implementation of the ISO\/IEC 10179:1996 standard DSSSL language. The DSSSL engine receives as input an SGML or XML document and transforms it into formats like: * XML representation of the flow object tree. * RTF format that can be rendered and printed with Microsoft's free Word Viewer 97 * TeX format * MIF format that can be rendered and printed with Framemaker * SGML or XML format. This is used in conjunction with non-standard flow object classes to generate SGML, thus allowing openjade to be used for SGML\/XML transformations. The system identifier of the document to be processed is specified as an argument to openjade. If this is omitted, standard input will be read. openjade determines the system identifier for the DSSSL specification as follows: 1. If the -d option is specified, it will use the argument as the system identifier. 2. Otherwise, it will look for processing instructions in the prolog of the document. Two kinds of processing instruction are recognized: <?stylesheet href=\"sysid\" type=\"text\/dsssl\"> The system data of the processing instruction is parsed like an SGML start-tag. It will be parsed using the reference concrete syntax whatever the actual concrete syntax of the document. The name that starts the processing instruction can be either stylesheet, xml-stylesheet or xml:stylesheet. The processing instruction will be ignored unless the value of the type attribute is one of text\/dsssl, text\/x-dsssl, application\/dsssl, or application\/x-dsssl. The value of href attribute is the system identifier of the DSSSL specification. <?dsssl sysid> The system identifier is the portion of the system data of the processing instruction following the initial name and any whitespace. Although the processing instruction is only recognized in the prolog, it need not occur in the document entity. For example, it could occur in a DTD. The system identifier will be interpreted relative to where the the processing instruction occurs. 3. Otherwise, it will use the system identifier of the document with any extension changed to .dsl. A DSSSL specification document can contain more than one style-specification. If the system identifier of the DSSSL specification is followed by #id, then openjade will use the style-specification whose unique identifier is id. This is allowed both with the -d option and with the processing instructions. The DSSSL specification must be an SGML document conforming to the DSSSL architecture. For an example, see dsssl\/demo.dsl. openjade supports the following options in addition to the normal OpenSP (see onsgmls(1)) options (note that all options are case-sensitive, ie -g and -G are different options): -d dsssl_spec This specifies that dsssl_spec is the system identifier of the DSSSL specification to be used. -G Debug mode. When an error occurs in the evaluation of an expression, openjade will display a stack trace. Note that this disables tail-call optimization. -c filename The filename arguments specify catalog files rather than the document entity. The document entity is specified by the first DOCUMENT entry in the catalog files. -s Strict compliance mode. Currently the only effect is that jade doesn't use any predefined character names, sdata-entity mappings or name-characters. This is useful for checking that your stylesheet is portable to other DSSSL implementations and that it is strictly compliant to the DSSSL specifications. -t output_type output_type specifies the type of output as follows: fot An XML representation of the flow object tree rtf rtf-95 RTF (used for SGML\/XML to RTF transformations) Microsoft's Rich Text Format. rtf-95 produces output optimized for Word 95 rather than Word 97. tex TeX (used for SGML\/XML to TeX transformations) sgml sgml-raw SGML (used for SGML\/XML to SGML transformations). sgml-raw doesn't emit linebreaks in tags. xml xml-raw XML (used for SGML\/XML to XML transformations). xml-raw doesn't emit linebreaks in tags. html HTML (used for SGML\/XML to HTML transformations) mif MIF (used for SGML\/XML to MIF transformations) -o output_file Write output to output_file instead of the default. The default filename is the name of the last input file with its extension replaced by the name of the type of output. If there is no input filename, then the extension is added onto jade-out. -V variable This is equivalent to doing (define variable #t) except that this definition will take priority over any definition of variable in a style-sheet. -V variable=value This is equivalent to doing (define variable \" value\") except that this definition will take priority over any definition of variable in a style-sheet. -V (define variable value) This is equivalent to doing (define variable value) except that this definition will take priority over any definition of variable in a style-sheet. Note that you will probably have to use some escaping mechanism for the spaces to get the entire scheme expression parsed as one cmdline argument. -w type Control warnings and errors. Multiple -w options are allowed. The following values of type enable warnings: xml Warn about constructs that are not allowed by XML. mixed Warn about mixed content models that do not allow #pcdata anywhere. sgmldecl Warn about various dubious constructions in the SGML declaration. should Warn about various recommendations made in ISO 8879 that the document does not comply with. (Recommendations are expressed with ''should'', as distinct from requirements which are usually expressed with ''shall''.) default Warn about defaulted references. duplicate Warn about duplicate entity declarations. undefined Warn about undefined elements: elements used in the DTD but not defined. unclosed Warn about unclosed start and end-tags. empty Warn about empty start and end-tags. net Warn about net-enabling start-tags and null end-tags. min-tag Warn about minimized start and end-tags. Equivalent to combination of unclosed, empty and net warnings. unused-map Warn about unused short reference maps: maps that are declared with a short reference mapping declaration but never used in a short reference use declaration in the DTD. unused-param Warn about parameter entities that are defined but not used in a DTD. Unused internal parameter entities whose text is INCLUDE or IGNORE won't get the warning. notation-sysid Warn about notations for which no system identifier could be generated. all Warn about conditions that should usually be avoided (in the opinion of the author). Equivalent to: mixed, should, default, undefined, sgmldecl, unused-map, unused-param, empty and unclosed. A warning can be disabled by using its name prefixed with no-. Thus -wall -wno-duplicate will enable all warnings except those about duplicate entity declarations. The following values for warning_type disable errors: no-idref Do not give an error for an ID reference value which no element has as its ID. The effect will be as if each attribute declared as an ID reference value had been declared as a name. no-significant Do not give an error when a character that is not a significant character in the reference concrete syntax occurs in a literal in the SGML declaration. This may be useful in conjunction with certain buggy test suites. no-valid Do not require the document to be type-valid. This has the effect of changing the SGML declaration to specify VALIDITY NOASSERT and IMPLYDEF ATTLIST YES ELEMENT YES. An option of -wvalid has the effect of changing the SGML declaration to specify VALIDITY TYPE and IMPLYDEF ATTLIST NO ELEMENT NO. If neither -wvalid nor -wno-valid are specified, then the VALIDITY and IMPLYDEF specified in the SGML declaration will be used.","Process Name":"openjade","Link":"https:\/\/linux.die.net\/man\/1\/openjade"}},{"Process":{"Description":"OpenOCD is an on-chip debugging, in-system programming and boundary-scan testing tool for various ARM and MIPS systems. The debugger uses an IEEE 1149-1 compliant JTAG TAP bus master to access on-chip debug functionality available on ARM based microcontrollers or system-on-chip solutions. For MIPS systems the EJTAG interface is supported. User interaction is realized through a telnet command line interface, a gdb (the GNU debugger) remote protocol server, and a simplified RPC connection that can be used to interface with OpenOCD's Jim Tcl engine. OpenOCD supports various different types of JTAG interfaces\/programmers, please check the openocd info page for the complete list.","Process Name":"openocd","Link":"https:\/\/linux.die.net\/man\/1\/openocd"}},{"Process":{"Description":null,"Process Name":"openrisc-linux-gnu-addr2line","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-addr2line"}},{"Process":{"Description":"The GNU ar program creates, modifies, and extracts from archives. An archive is a single file holding a collection of other files in a structure that makes it possible to retrieve the original individual files (called members of the archive). The original files' contents, mode (permissions), timestamp, owner, and group are preserved in the archive, and can be restored on extraction. GNU ar can maintain archives whose members have names of any length; however, depending on how ar is configured on your system, a limit on member-name length may be imposed for compatibility with archive formats maintained with other tools. If it exists, the limit is often 15 characters (typical of formats related to a.out) or 16 characters (typical of formats related to coff). ar is considered a binary utility because archives of this sort are most often used as libraries holding commonly needed subroutines. ar creates an index to the symbols defined in relocatable object modules in the archive when you specify the modifier s. Once created, this index is updated in the archive whenever ar makes a change to its contents (save for the q update operation). An archive with such an index speeds up linking to the library, and allows routines in the library to call each other without regard to their placement in the archive. You may use nm -s or nm --print-armap to list this index table. If an archive lacks the table, another form of ar called ranlib can be used to add just the table. GNU ar can optionally create a thin archive, which contains a symbol index and references to the original copies of the member files of the archives. Such an archive is useful for building libraries for use within a local build, where the relocatable objects are expected to remain available, and copying the contents of each object would only waste time and space. Thin archives are also flattened, so that adding one or more archives to a thin archive will add the elements of the nested archive individually. The paths to the elements of the archive are stored relative to the archive itself. GNU ar is designed to be compatible with two different facilities. You can control its activity using command-line options, like the different varieties of ar on Unix systems; or, if you specify the single command-line option -M, you can control it with a script supplied via standard input, like the MRI \"librarian\" program.","Process Name":"openrisc-linux-gnu-ar","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-ar"}},{"Process":{"Description":null,"Process Name":"openrisc-linux-gnu-as","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-as"}},{"Process":{"Description":"The C ++ and Java languages provide function overloading, which means that you can write many functions with the same name, providing that each function takes parameters of different types. In order to be able to distinguish these similarly named functions C ++ and Java encode them into a low-level assembler name which uniquely identifies each different version. This process is known as mangling. The c++filt [1] program does the inverse mapping: it decodes (demangles) low-level names into user-level names so that they can be read. Every alphanumeric word (consisting of letters, digits, underscores, dollars, or periods) seen in the input is a potential mangled name. If the name decodes into a C ++ name, the C ++ name replaces the low-level name in the output, otherwise the original word is output. In this way you can pass an entire assembler source file, containing mangled names, through c++filt and see the same source file containing demangled names. You can also use c++filt to decipher individual symbols by passing them on the command line: c++filt <symbol> If no symbol arguments are given, c++filt reads symbol names from the standard input instead. All the results are printed on the standard output. The difference between reading names from the command line versus reading names from the standard input is that command line arguments are expected to be just mangled names and no checking is performed to separate them from surrounding text. Thus for example: c++filt -n _Z1fv will work and demangle the name to \"f()\" whereas: c++filt -n _Z1fv, will not work. (Note the extra comma at the end of the mangled name which makes it invalid). This command however will work: echo _Z1fv, | c++filt -n and will display \"f(),\", i.e., the demangled name followed by a trailing comma. This behaviour is because when the names are read from the standard input it is expected that they might be part of an assembler source file where there might be extra, extraneous characters trailing after a mangled name. For example: .type   _Z1fv, @function","Process Name":"openrisc-linux-gnu-c++filt","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-c++filt"}},{"Process":{"Description":null,"Process Name":"openrisc-linux-gnu-dlltool","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-dlltool"}},{"Process":{"Description":"elfedit updates the ELF header of ELF files which have the matching ELF machine and file types. The options control how and which fields in the ELF header should be updated. elffile... are the ELF files to be updated. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files.","Process Name":"openrisc-linux-gnu-elfedit","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-elfedit"}},{"Process":{"Description":"\"gprof\" produces an execution profile of C, Pascal, or Fortran77 programs. The effect of called routines is incorporated in the profile of each caller. The profile data is taken from the call graph profile file (gmon.out default) which is created by programs that are compiled with the -pg option of \"cc\", \"pc\", and \"f77\". The -pg option also links in versions of the library routines that are compiled for profiling. \"Gprof\" reads the given object file (the default is \"a.out\") and establishes the relation between its symbol table and the call graph profile from gmon.out. If more than one profile file is specified, the \"gprof\" output shows the sum of the profile information in the given profile files. \"Gprof\" calculates the amount of time spent in each routine. Next, these times are propagated along the edges of the call graph. Cycles are discovered, and calls into a cycle are made to share the time of the cycle. Several forms of output are available from the analysis. The flat profile shows how much time your program spent in each function, and how many times that function was called. If you simply want to know which functions burn most of the cycles, it is stated concisely here. The call graph shows, for each function, which functions called it, which other functions it called, and how many times. There is also an estimate of how much time was spent in the subroutines of each function. This can suggest places where you might try to eliminate function calls that use a lot of time. The annotated source listing is a copy of the program's source code, labeled with the number of times each line of the program was executed.","Process Name":"openrisc-linux-gnu-gprof","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-gprof"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"openrisc-linux-gnu-ld","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-ld"}},{"Process":{"Description":null,"Process Name":"openrisc-linux-gnu-ld.bfd","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-ld.bfd"}},{"Process":{"Description":"nlmconv converts the relocatable i386 object file infile into the NetWare Loadable Module outfile, optionally reading headerfile for NLM header information. For instructions on writing the NLM command file language used in header files, see the linkers section, NLMLINK in particular, of the NLM Development and Tools Overview, which is part of the NLM Software Developer's Kit (\" NLM SDK \"), available from Novell, Inc. nlmconv uses the GNU Binary File Descriptor library to read infile; nlmconv can perform a link step. In other words, you can list more than one object file for input if you list them in the definitions file (rather than simply specifying one input file on the command line). In this case, nlmconv calls the linker for you.","Process Name":"openrisc-linux-gnu-nlmconv","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-nlmconv"}},{"Process":{"Description":"GNU nm lists the symbols from object files objfile.... If no object files are listed as arguments, nm assumes the file a.out. For each symbol, nm shows: \u2022 The symbol value, in the radix selected by options (see below), or hexadecimal by default. \u2022 The symbol type. At least the following types are used; others are, as well, depending on the object file format. If lowercase, the symbol is usually local; if uppercase, the symbol is global (external). There are however a few lowercase symbols that are shown for special global symbols (\"u\", \"v\" and \"w\"). \"A\" The symbol's value is absolute, and will not be changed by further linking. \"B\" \"b\" The symbol is in the uninitialized data section (known as BSS ). \"C\" The symbol is common. Common symbols are uninitialized data. When linking, multiple common symbols may appear with the same name. If the symbol is defined anywhere, the common symbols are treated as undefined references. \"D\" \"d\" The symbol is in the initialized data section. \"G\" \"g\" The symbol is in an initialized data section for small objects. Some object file formats permit more efficient access to small data objects, such as a global int variable as opposed to a large global array. \"i\" For PE format files this indicates that the symbol is in a section specific to the implementation of DLLs. For ELF format files this indicates that the symbol is an indirect function. This is a GNU extension to the standard set of ELF symbol types. It indicates a symbol which if referenced by a relocation does not evaluate to its address, but instead must be invoked at runtime. The runtime execution will then return the value to be used in the relocation. \"N\" The symbol is a debugging symbol. \"p\" The symbols is in a stack unwind section. \"R\" \"r\" The symbol is in a read only data section. \"S\" \"s\" The symbol is in an uninitialized data section for small objects. \"T\" \"t\" The symbol is in the text (code) section. \"U\" The symbol is undefined. \"u\" The symbol is a unique global symbol. This is a GNU extension to the standard set of ELF symbol bindings. For such a symbol the dynamic linker will make sure that in the entire process there is just one symbol with this name and type in use. \"V\" \"v\" The symbol is a weak object. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the weak symbol becomes zero with no error. On some systems, uppercase indicates that a default value has been specified. \"W\" \"w\" The symbol is a weak symbol that has not been specifically tagged as a weak object symbol. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the symbol is determined in a system-specific manner without error. On some systems, uppercase indicates that a default value has been specified. \"-\" The symbol is a stabs symbol in an a.out object file. In this case, the next values printed are the stabs other field, the stabs desc field, and the stab type. Stabs symbols are used to hold debugging information. \"?\" The symbol type is unknown, or object file format specific. \u2022 The symbol name.","Process Name":"openrisc-linux-gnu-nm","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-nm"}},{"Process":{"Description":"The GNU objcopy utility copies the contents of an object file to another. objcopy uses the GNU BFD Library to read and write the object files. It can write the destination object file in a format different from that of the source object file. The exact behavior of objcopy is controlled by command-line options. Note that objcopy should be able to copy a fully linked file between any two formats. However, copying a relocatable object file between any two formats may not work as expected. objcopy creates temporary files to do its translations and deletes them afterward. objcopy uses BFD to do all its translation work; it has access to all the formats described in BFD and thus is able to recognize most formats without being told explicitly. objcopy can be used to generate S-records by using an output target of srec (e.g., use -O srec). objcopy can be used to generate a raw binary file by using an output target of binary (e.g., use -O binary). When objcopy generates a raw binary file, it will essentially produce a memory dump of the contents of the input object file. All symbols and relocation information will be discarded. The memory dump will start at the load address of the lowest section copied into the output file. When generating an S-record or a raw binary file, it may be helpful to use -S to remove sections containing debugging information. In some cases -R will be useful to remove sections which contain information that is not needed by the binary file. Note---objcopy is not able to change the endianness of its input files. If the input format has an endianness (some formats do not), objcopy can only copy the inputs into file formats that have the same endianness or which have no endianness (e.g., srec). (However, see the --reverse-bytes option.)","Process Name":"openrisc-linux-gnu-objcopy","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-objcopy"}},{"Process":{"Description":null,"Process Name":"openrisc-linux-gnu-objdump","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-objdump"}},{"Process":{"Description":"ranlib generates an index to the contents of an archive and stores it in the archive. The index lists each symbol defined by a member of an archive that is a relocatable object file. You may use nm -s or nm --print-armap to list this index. An archive with such an index speeds up linking to the library and allows routines in the library to call each other without regard to their placement in the archive. The GNU ranlib program is another form of GNU ar; running ranlib is completely equivalent to executing ar -s.","Process Name":"openrisc-linux-gnu-ranlib","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-ranlib"}},{"Process":{"Description":null,"Process Name":"openrisc-linux-gnu-readelf","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-readelf"}},{"Process":{"Description":"The GNU size utility lists the section sizes---and the total size---for each of the object or archive files objfile in its argument list. By default, one line of output is generated for each object file or each module in an archive. objfile... are the object files to be examined. If none are specified, the file \"a.out\" will be used.","Process Name":"openrisc-linux-gnu-size","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-size"}},{"Process":{"Description":"For each file given, GNU strings prints the printable character sequences that are at least 4 characters long (or the number given with the options below) and are followed by an unprintable character. By default, it only prints the strings from the initialized and loaded sections of object files; for other types of files, it prints the strings from the whole file. strings is mainly useful for determining the contents of non-text files.","Process Name":"openrisc-linux-gnu-strings","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-strings"}},{"Process":{"Description":"GNU strip discards all symbols from object files objfile. The list of object files may include archives. At least one object file must be given. strip modifies the files named in its argument, rather than writing modified copies under different names.","Process Name":"openrisc-linux-gnu-strip","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-strip"}},{"Process":{"Description":null,"Process Name":"openrisc-linux-gnu-windmc","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-windmc"}},{"Process":{"Description":"windres reads resources from an input file and copies them into an output file. Either file may be in one of three formats: \"rc\" A text format read by the Resource Compiler. \"res\" A binary format generated by the Resource Compiler. \"coff\" A COFF object or executable. The exact description of these different formats is available in documentation from Microsoft. When windres converts from the \"rc\" format to the \"res\" format, it is acting like the Windows Resource Compiler. When windres converts from the \"res\" format to the \"coff\" format, it is acting like the Windows \"CVTRES\" program. When windres generates an \"rc\" file, the output is similar but not identical to the format expected for the input. When an input \"rc\" file refers to an external filename, an output \"rc\" file will instead include the file contents. If the input or output format is not specified, windres will guess based on the file name, or, for the input file, the file contents. A file with an extension of .rc will be treated as an \"rc\" file, a file with an extension of .res will be treated as a \"res\" file, and a file with an extension of .o or .exe will be treated as a \"coff\" file. If no output file is specified, windres will print the resources in \"rc\" format to standard output. The normal use is for you to write an \"rc\" file, use windres to convert it to a COFF object file, and then link the COFF file into your application. This will make the resources described in the \"rc\" file available to Windows.","Process Name":"openrisc-linux-gnu-windres","Link":"https:\/\/linux.die.net\/man\/1\/openrisc-linux-gnu-windres"}},{"Process":{"Description":"The opensc-explorer utility can be used interactively to perform miscellaneous operations such as exploring the contents of or sending arbitrary APDU commands to a smart card or similar security token.","Process Name":"opensc-explorer","Link":"https:\/\/linux.die.net\/man\/1\/opensc-explorer"}},{"Process":{"Description":"The opensc-tool utility can be used from the command line to perform miscellaneous smart card operations such as getting the card ATR or sending arbitrary APDU commands to a card.","Process Name":"opensc-tool","Link":"https:\/\/linux.die.net\/man\/1\/opensc-tool"}},{"Process":{"Description":"OpenSSL is a cryptography toolkit implementing the Secure Sockets Layer ( SSL v2\/v3) and Transport Layer Security ( TLS v1) network protocols and related cryptography standards required by them. The openssl program is a command line tool for using the various cryptography functions of OpenSSL's crypto library from the shell. It can be used for o  Creation and management of private keys, public keys and parameters\no  Public key cryptographic operations\no  Creation of X.509 certificates, CSRs and CRLs\no  Calculation of Message Digests\no  Encryption and Decryption with Ciphers\no  SSL\/TLS Client and Server Tests\no  Handling of S\/MIME signed or encrypted mail\no  Time Stamp requests, generation and verification","Process Name":"openssl","Link":"https:\/\/linux.die.net\/man\/1\/openssl"}},{"Process":{"Description":"Provide functionality to add\/edit\/delete items and sections in the OpenStack ini format config files.","Process Name":"openstack-config","Link":"https:\/\/linux.die.net\/man\/1\/openstack-config"}},{"Process":{"Description":"Set up a local database (MySQL) for use with openstack-<service>. This script will create a '<service>' database that is accessible only on localhost by user '<service>' with password '<service>'. The setup of a database with a multi-server OpenStack installation is outside of the scope of this simple helper script.","Process Name":"openstack-db","Link":"https:\/\/linux.die.net\/man\/1\/openstack-db"}},{"Process":{"Description":"Install all OpenStack services on a single node with a default configuration, useful for testing. Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"openstack-demo-install","Link":"https:\/\/linux.die.net\/man\/1\/openstack-demo-install"}},{"Process":{"Description":null,"Process Name":"openstack-status","Link":"https:\/\/linux.die.net\/man\/1\/openstack-status"}},{"Process":{"Description":"opentool is obsolete. You should make sure that the Tools directories are in your PATH, and then opentool is not needed anymore.","Process Name":"opentool","Link":"https:\/\/linux.die.net\/man\/1\/opentool"}},{"Process":{"Description":"This manual page documents briefly the openuniverse command. OpenUniverse (also called OU) is a fun, fast and free OpenGL space simulator. It currently focusses on the Solar System and lets you visit all of its planets, most major moons and a vast collection of smaller bodies in colorful, glorious and realtime 3D. If you've ever had a chance to visit Mercury or asteroid Geographos, here you'll find them looking exactly the same way, following exactly the same path as when you've left them. This program really benefits of a 3D accelerator graphic card. It's the only way to ensure a decent framerate (speed). When OpenUniverse starts up we suggest to simply lay back for a few seconds. Because you have just reached Earth! OU's main screen with it's helpful information shows up now and the blue planet will slowly rotate in front of you, while you have a moment to enjoy it's beauty and fragility. To view the Help press the 'H' key, you can swith on and off the demo mode with the 'd' key. For other commands and functions see OU's manual. This manual page was written for the Debian GNU\/Linux distribution because the original program does not have a manual page. Instead, it has documentation in the HTML format; see below.","Process Name":"openuniverse","Link":"https:\/\/linux.die.net\/man\/1\/openuniverse"}},{"Process":{"Description":null,"Process Name":"openvas-client","Link":"https:\/\/linux.die.net\/man\/1\/openvas-client"}},{"Process":{"Description":"The OpenVAS Security Scanner protects the communication between the client and the server by using SSL. SSL requires the server to present a certificate to the client, and the client can optionally present a certificate to the server. This script openvas-mkcert-client generates a client certificate.","Process Name":"openvas-mkcert-client","Link":"https:\/\/linux.die.net\/man\/1\/openvas-mkcert-client"}},{"Process":{"Description":"The OpenVAS Security Scanner protects the communication between the client and the scanner by using SSL. To work securely, the OpenSSL library needs a suitable amount of random bytes. On most system, it uses EGD or \/dev\/random (or \/dev\/urandom). On systems which have none of these, OpenVAS will provide OpenSSL with a file full of random bytes generated by openvas-mkrand which is stored by default under $HOME\/.rnd. openvas-mkcert and openvas-mkcert-client require user input as a random seed. By default, openvas-mkrand produces a file stored under $HOME\/.rnd containing 1024 bytes of entropy.","Process Name":"openvas-mkrand","Link":"https:\/\/linux.die.net\/man\/1\/openvas-mkrand"}},{"Process":{"Description":null,"Process Name":"openvas-nasl","Link":"https:\/\/linux.die.net\/man\/1\/openvas-nasl"}},{"Process":{"Description":"The OpenVAS Security Scanner protects the communication between the client and the server by using SSL. SSL requires the server to present a certificate to the client, and the client can optionally present a certificate to the server. This script openvasclient-mkcert generates a client certificate.","Process Name":"openvasclient-mkcert","Link":"https:\/\/linux.die.net\/man\/1\/openvasclient-mkcert"}},{"Process":{"Description":"openvt will find the first available VT, and run on it the given command with the given command options, standard input, output and error are directed to that terminal. The current search path ($PATH) is used to find the requested command. If no command is specified then the environment variable $SHELL is used. Options -c vtnumber Use the given VT number and not the first available. Note you must have write access to the supplied VT for this to work. -f Force opening a VT without checking whether it is already in use. -e Directly execute the given command, without forking. This option is meant for use in \/etc\/inittab. -s Switch to the new VT when starting the command. The VT of the new command will be made the new current VT. -u Figure out the owner of the current VT, and run login as that user. Suitable to be called by init. Shouldn't be used with -c or -l. -l Make the command a login shell. A - is prepended to the name of the command to be executed. -v Be a bit more verbose. -w wait for command to complete. If -w and -s are used together then openvt will switch back to the controlling terminal when the command completes. -- end of options to openvt.","Process Name":"openvt","Link":"https:\/\/linux.die.net\/man\/1\/openvt"}},{"Process":{"Description":"opgprof outputs gprof-format profile data for a given binary image, from an OProfile session. See oprofile(1) for how to write profile specifications.","Process Name":"opgprof","Link":"https:\/\/linux.die.net\/man\/1\/opgprof"}},{"Process":{"Description":"By default, ophelp lists the available performance counter options. If you give it a symbolic event name, it will return the hardware value (e.g. \"ophelp DATA_MEM_REFS\").","Process Name":"ophelp","Link":"https:\/\/linux.die.net\/man\/1\/ophelp"}},{"Process":{"Description":null,"Process Name":"opimport","Link":"https:\/\/linux.die.net\/man\/1\/opimport"}},{"Process":{"Description":"opl2ofm translates a property-list format file, OPLFILE, into the binary Omega Font Metric format. The program writes to standard output (by default) or to a file specified as OFMFILE. The program also works with TeX PL files, producing TeX TFM files. (opl2ofm is based on the WEB source code for pltotf(1).)","Process Name":"opl2ofm","Link":"https:\/\/linux.die.net\/man\/1\/opl2ofm"}},{"Process":{"Description":"opreport outputs binary image summaries, or per-symbol data, from OProfile profiling sessions. See oprofile(1) for how to write profile specifications.","Process Name":"opreport","Link":"https:\/\/linux.die.net\/man\/1\/opreport"}},{"Process":{"Description":"OProfile is a profiling system for systems running Linux 2.2, 2.4, and 2.6. Profiling runs transparently in the background and profile data can be collected at any time. OProfile makes use of the hardware performance counters provided on Intel, AMD, and other processors, and uses a timer-interrupt based mechanism on CPUs without counters. OProfile can profile the whole system in high detail. For a gentle guide to using OProfile, please read the HTML documentation listed in SEE ALSO.","Process Name":"oprofile","Link":"https:\/\/linux.die.net\/man\/1\/oprofile"}},{"Process":{"Description":"opstack outputs binary callgraph symbol summaries. Currently, this requires an x86-based box and a 2.6 kernel patch; see http:\/\/oprofile.sf.net\/patches\/.","Process Name":"opstack","Link":"https:\/\/linux.die.net\/man\/1\/opstack"}},{"Process":{"Description":"The opt command is the modular LLVM optimizer and analyzer. It takes LLVM source files as input, runs the specified optimizations or analyses on it, and then outputs the optimized file or the analysis results. The function of opt depends on whether the -analyze option is given. When -analyze is specified, opt performs various analyses of the input source. It will usually print the results on standard output, but in a few cases, it will print output to standard error or generate a file with the analysis output, which is usually done when the output is meant for another program. While -analyze is not given, opt attempts to produce an optimized output file. The optimizations available via opt depend upon what libraries were linked into it as well as any additional libraries that have been loaded with the -load option. Use the -help option to determine what optimizations you can use. If filename is omitted from the command line or is -, opt reads its input from standard input. Inputs can be in either the LLVM assembly language format (.ll) or the LLVM bitcode format (.bc). If an output filename is not specified with the -o option, opt writes its output to the standard output.","Process Name":"opt","Link":"https:\/\/linux.die.net\/man\/1\/opt"}},{"Process":{"Description":"The OptiPNG program shall attempt to optimize PNG files, i.e. reduce their size to a minimum, without losing any information. In addition, this program shall perform a suite of auxiliary functions like integrity checks, metadata recovery and pixmap-to-PNG conversion. The optimization attempts are not guaranteed to succeed. Valid PNG files that cannot be optimized by this program are normally left intact; their size will not grow. The user may request to override this default behavior.","Process Name":"optipng","Link":"https:\/\/linux.die.net\/man\/1\/optipng"}},{"Process":{"Description":"","Process Name":"or","Link":"https:\/\/linux.die.net\/man\/1\/or"}},{"Process":{"Description":"orage is a fast and easy to use graphical calendar for the Xfce Desktop Environment. It uses portable ical format and includes common calendar features like repeating appointments and multiple alarming possibilities. orage does not have group calendar features, but can only be used for single user. orage takes a list of FILEs for ical files that should be imported. Contents of those files are read and converted into orage, but those files are left untouched. For a more detailed explanation of orage, please see the Help menu.","Process Name":"orage","Link":"https:\/\/linux.die.net\/man\/1\/orage"}},{"Process":{"Description":"The Server Manager included with the orbd tool is used to enable clients to transparently locate and invoke persistent objects on servers in the CORBA environment. The persistent servers, while publishing the persistent object references in the Naming Service, include the port number of the ORBD in the object reference instead of the port number of the Server. The inclusion of an ORBD port number in the object reference for persistent object references has the following advantages: o The object reference in the Naming Service remains independent of the server life cycle. For example, the object reference could be published by the server in the Naming Service when it is first installed, and then, independent of how many times the server is started or shutdown, the ORBD will always return the correct object reference to the invoking client. o The client needs to lookup the object reference in the Naming Service only once, and can keep re-using this reference independent of the changes introduced due to server life cycle. To access ORBD's Server Manager, the server must be started using servertool, which is a command-line interface for application programmers to register, unregister, startup, and shutdown a persistent server. For more information on the Server Manager, see the section in this document titled Server Manager. When orbd starts up, it also starts a naming service. For more information on the naming service, link to Naming Service.","Process Name":"orbd-java-1.6.0-openjdk","Link":"https:\/\/linux.die.net\/man\/1\/orbd-java-1.6.0-openjdk"}},{"Process":{"Description":"The Server Manager included with the orbd tool is used to enable clients to transparently locate and invoke persistent objects on servers in the CORBA environment. The persistent servers, while publishing the persistent object references in the Naming Service, include the port number of the ORBD in the object reference instead of the port number of the Server. The inclusion of an ORBD port number in the object reference for persistent object references has the following advantages: o The object reference in the Naming Service remains independent of the server life cycle. For example, the object reference could be published by the server in the Naming Service when it is first installed, and then, independent of how many times the server is started or shutdown, the ORBD will always return the correct object reference to the invoking client. o The client needs to lookup the object reference in the Naming Service only once, and can keep re-using this reference independent of the changes introduced due to server life cycle. To access ORBD's Server Manager, the server must be started using servertool(1), which is a command-line interface for application programmers to register, unregister, startup, and shutdown a persistent server. For more information on the Server Manager, see the section in this document titled Server Manager. When orbd starts up, it also starts a naming service. For more information on the naming service, link to Naming Service @ http:\/\/docs.oracle.com\/javase\/7\/docs\/technotes\/guides\/idl\/jidlNaming.html.","Process Name":"orbd-java-1.7.0-openjdk","Link":"https:\/\/linux.die.net\/man\/1\/orbd-java-1.7.0-openjdk"}},{"Process":{"Description":null,"Process Name":"orca","Link":"https:\/\/linux.die.net\/man\/1\/orca"}},{"Process":{"Description":"originator reads (longitude, latitude, height, radius, crustal_age) records from infiles [or standard input] and uses the given Absolute Plate Motion (APM) stage poles and the list of hotspot locations to determine the most likely origin (hotspot) for each seamount. It does so by calculating flowlines back in time and determining the closest approach to all hotspots. The output consists of the input records with four additional fields added for each of the n_hs closest hotspots. The four fields are the hotspot id (e.g., HWI), the stage id of the flowline segment that came closest, the pseudo-age of the seamount, and the closest distance to the hotspot (in km). See option -: on how to read (latitude, longitude,height, radius, crustal_age) files. No space between the option flag and the associated arguments. Use upper case for the option flags and lower case for modifiers. infile(s) Seamount data file(s) to be analyzed. If not given, standard input is read. -E Give file with rotation parameters. This file must contain one record for each rotation; each record must be of the following format: lon lat tstart [tstop] angle [ khat a b c d e f g df ] where tstart and tstop are in Myr and lon lat angle are in degrees. tstart and tstop are the ages of the old and young ends of a stage. If -C is set then a total reconstruction rotation is expected and tstop is implicitly set to 0 and should not be specified in the file. If a covariance matrix C for the rotation is available it must be specified in a format using the nine optional terms listed in brackets. Here, C = (g\/khat)*[ a b d; b c e; d e f ] which shows C made up of three row vectors. If the degrees of freedom (df) in fitting the rotation is 0 or not given it is set to 10000. Blank lines and records whose first column contains # will be ignored. -F Give file with hotspot locations. This file must contain one record for each hotspot to be considered; each record must be of the following format: lon lat hs_abbrev hs_id r t_off t_on create fit plot name E.g., for Hawaii this may look like 205 20 HWI 1 25 0 90 Y Y Y Hawaii Most applications only need the first 4 columns which thus represents the minimal hotspot information record type. The abbreviation may be maximum 3 characters long. The id must be an integer from 1-32. The positional uncertainty of the hotspot is given by r (in km). The t_off and t_on variables are used to indicate the active time-span of the hotspot. The create, fit, and plot indicators are either Y or N and are used by some programs to indicate if the hotspot is included in the ID-grids used to determine rotations, if the hotspot chain will be used to determine rotations, and if the hotspot should be included in various plots. The name is a 32-character maximum text string with the full hotspot name. Blank lines and records whose first column contains # will be ignored.","Process Name":"originator","Link":"https:\/\/linux.die.net\/man\/1\/originator"}},{"Process":{"Description":"","Process Name":"orte-clean","Link":"https:\/\/linux.die.net\/man\/1\/orte-clean"}},{"Process":{"Description":"ompi-iof displays a copy of the indicated stdout, stderr, and\/or stddiag streams from the designated process. At this time, a ctrl-C must be used to terminate the program. The program will terminate cleanly, telling the associated mpirun to close the requested streams before exiting.","Process Name":"orte-iof","Link":"https:\/\/linux.die.net\/man\/1\/orte-iof"}},{"Process":{"Description":null,"Process Name":"orte-ps","Link":"https:\/\/linux.die.net\/man\/1\/orte-ps"}},{"Process":{"Description":"orted starts an Open RTE daemon for the Open MPI system.","Process Name":"orted","Link":"https:\/\/linux.die.net\/man\/1\/orted"}},{"Process":{"Description":"One invocation of mpirun starts an MPI application running under Open MPI. If the application is single process multiple data (SPMD), the application can be specified on the mpirun command line. If the application is multiple instruction multiple data (MIMD), comprising of multiple programs, the set of programs and argument can be specified in one of two ways: Extended Command Line Arguments, and Application Context. An application context describes the MIMD program set including all arguments in a separate file. This file essentially contains multiple mpirun command lines, less the command name itself. The ability to specify different options for different instantiations of a program is another reason to use an application context. Extended command line arguments allow for the description of the application layout on the command line using colons (:) to separate the specification of programs and arguments. Some options are globally set across all specified programs (e.g. --hostfile), while others are specific to a single program (e.g. -np). Specifying Host Nodes Host nodes can be identified on the mpirun command line with the -host option or in a hostfile. For example, mpirun -H aa,aa,bb .\/a.out launches two processes on node aa and one on bb. Or, consider the hostfile % cat myhostfile aa slots=2 bb slots=2 cc slots=2 Here, we list both the host names (aa, bb, and cc) but also how many \"slots\" there are for each. Slots indicate how many processes can potentially execute on a node. For best performance, the number of slots may be chosen to be the number of cores on the node or the number of processor sockets. If the hostfile does not provide slots information, a default of 1 is assumed. When running under resource managers (e.g., SLURM, Torque, etc.), Open MPI will obtain both the hostnames and the number of slots directly from the resource manger. mpirun -hostfile myhostfile .\/a.out will launch two processes on each of the three nodes. mpirun -hostfile myhostfile -host aa .\/a.out will launch two processes, both on node aa. mpirun -hostfile myhostfile -host dd .\/a.out will find no hosts to run on and abort with an error. That is, the specified host dd is not in the specified hostfile. Specifying Number of Processes As we have just seen, the number of processes to run can be set using the hostfile. Other mechanisms exist. The number of processes launched can be specified as a multiple of the number of nodes or processor sockets available. For example, mpirun -H aa,bb -npersocket 2 .\/a.out launches processes 0-3 on node aa and process 4-7 on node bb, where aa and bb are both dual-socket nodes. The -npersocket option also turns on the -bind-to-socket option, which is discussed in a later section. mpirun -H aa,bb -npernode 2 .\/a.out launches processes 0-1 on node aa and processes 2-3 on node bb. mpirun -H aa,bb -npernode 1 .\/a.out launches one process per host node. mpirun -H aa,bb -pernode .\/a.out is the same as -npernode 1. Another alternative is to specify the number of processes with the -np option. Consider now the hostfile % cat myhostfile aa slots=4 bb slots=4 cc slots=4 Now, mpirun -hostfile myhostfile -np 6 .\/a.out will launch ranks 0-3 on node aa and ranks 4-5 on node bb. The remaining slots in the hostfile will not be used since the -np option indicated that only 6 processes should be launched. Mapping Processes to Nodes The examples above illustrate the default mapping of process ranks to nodes. This mapping can also be controlled with various mpirun options. Here, we consider the same hostfile as above with -np 6 again: node aa node bb node cc mpirun 0 1 2 3 4 5 mpirun -loadbalance 0 1 2 3 4 5 mpirun -bynode 0 3 1 4 2 5 mpirun -nolocal 0 1 2 3 4 5 The -loadbalance option tries to spread processes out fairly among the nodes. The -bynode option does likewise but numbers the processes in \"by node\" in a round-robin fashion. The -nolocal option prevents any processes from being mapped onto the local host (in this case node aa). While mpirun typically consumes few system resources, -nolocal can be helpful for launching very large jobs where mpirun may actually need to use noticable amounts of memory and\/or processing time. Just as -np can specify fewer processes than there are slots, it can also oversubscribe the slots. For example, with the same hostfile: mpirun -hostfile myhostfile -np 14 .\/a.out will launch processes 0-3 on node aa, 4-7 on bb, and 8-11 on cc. It will then add the remaining two processes to whichever nodes it chooses. One can also specify limits to oversubscription. For example, with the same hostfile: mpirun -hostfile myhostfile -np 14 -nooversubscribe .\/a.out will produce an error since -nooversubscribe prevents oversubscription. Limits to oversubscription can also be specified in the hostfile itself: % cat myhostfile aa slots=4 max_slots=4 bb max_slots=4 cc slots=4 The max_slots field specifies such a limit. When it does, the slots value defaults to the limit. Now: mpirun -hostfile myhostfile -np 14 .\/a.out causes the first 12 processes to be launched as before, but the remaining two processes will be forced onto node cc. The other two nodes are protected by the hostfile against oversubscription by this job. Using the --nooversubscribe option can be helpful since Open MPI currently does not get \"max_slots\" values from the resource manager. Of course, -np can also be used with the -H or -host option. For example, mpirun -H aa,bb -np 8 .\/a.out launches 8 processes. Since only two hosts are specified, after the first two processes are mapped, one to aa and one to bb, the remaining processes oversubscribe the specified hosts. And here is a MIMD example: mpirun -H aa -np 1 hostname : -H bb,cc -np 2 uptime will launch process 0 running hostname on node aa and processes 1 and 2 each running uptime on nodes bb and cc, respectively. Process Binding Processes may be bound to specific resources on a node. This can improve performance if the operating system is placing processes suboptimally. For example, it might oversubscribe some multi-core processor sockets, leaving other sockets idle; this can lead processes to contend unnecessarily for common resources. Or, it might spread processes out too widely; this can be suboptimal if application performance is sensitive to interprocess communication costs. Binding can also keep the operating system from migrating processes excessively, regardless of how optimally those processes were placed to begin with. To bind processes, one must first associate them with the resources on which they should run. For example, the -bycore option associates the processes on a node with successive cores. Or, -bysocket associates the processes with successive processor sockets, cycling through the sockets in a round-robin fashion if necessary. And -cpus-per-proc indicates how many cores to bind per process. But, such association is meaningless unless the processes are actually bound to those resources. The binding option specifies the granularity of binding -- say, with -bind-to-core or -bind-to-socket. One can also turn binding off with -bind-to-none, which is typically the default. Finally, -report-bindings can be used to report bindings. As an example, consider a node with two processor sockets, each comprising four cores. We run mpirun with -np 4 -report-bindings and the following additional options: % mpirun ... -bycore -bind-to-core [...] ... binding child [...,0] to cpus 0001 [...] ... binding child [...,1] to cpus 0002 [...] ... binding child [...,2] to cpus 0004 [...] ... binding child [...,3] to cpus 0008 % mpirun ... -bysocket -bind-to-socket [...] ... binding child [...,0] to socket 0 cpus 000f [...] ... binding child [...,1] to socket 1 cpus 00f0 [...] ... binding child [...,2] to socket 0 cpus 000f [...] ... binding child [...,3] to socket 1 cpus 00f0 % mpirun ... -cpus-per-proc 2 -bind-to-core [...] ... binding child [...,0] to cpus 0003 [...] ... binding child [...,1] to cpus 000c [...] ... binding child [...,2] to cpus 0030 [...] ... binding child [...,3] to cpus 00c0 % mpirun ... -bind-to-none Here, -report-bindings shows the binding of each process as a mask. In the first case, the processes bind to successive cores as indicated by the masks 0001, 0002, 0004, and 0008. In the second case, processes bind to all cores on successive sockets as indicated by the masks 000f and 00f0. The processes cycle through the processor sockets in a round-robin fashion as many times as are needed. In the third case, the masks show us that 2 cores have been bind per process. In the fourth case, binding is turned off and no bindings are reported. Open MPI's support for process binding depends on the underlying operating system. Therefore, processing binding may not be available on every system. Process binding can also be set with MCA parameters. Their usage is less convenient than that of mpirun options. On the other hand, MCA parameters can be set not only on the mpirun command line, but alternatively in a system or user mca-params.conf file or as environment variables, as described in the MCA section below. The correspondences are: mpirun option MCA parameter key value -bycore rmaps_base_schedule_policy core -bysocket rmaps_base_schedule_policy socket -bind-to-core orte_process_binding core -bind-to-socket orte_process_binding socket -bind-to-none orte_process_binding none The orte_process_binding value can also take on the :if-avail attribute. This attribute means that processes will be bound only if this is supported on the underlying operating system. Without the attribute, if there is no such support, the binding request results in an error. For example, you could have % cat $HOME\/.openmpi\/mca-params.conf rmaps_base_schedule_policy = socket orte_process_binding = socket:if-avail Rankfiles Rankfiles provide a means for specifying detailed information about how process ranks should be mapped to nodes and how they should be bound. Consider the following: cat myrankfile rank 0=aa slot=1:0-2 rank 1=bb slot=0:0,1 rank 2=cc slot=1-2 mpirun -H aa,bb,cc,dd -rf myrankfile .\/a.out So that Rank 0 runs on node aa, bound to socket 1, cores 0-2. Rank 1 runs on node bb, bound to socket 0, cores 0 and 1. Rank 2 runs on node cc, bound to cores 1 and 2. Application Context or Executable Program? To distinguish the two different forms, mpirun looks on the command line for --app option. If it is specified, then the file named on the command line is assumed to be an application context. If it is not specified, then the file is assumed to be an executable program. Locating Files If no relative or absolute path is specified for a file, Open MPI will first look for files by searching the directories specified by the --path option. If there is no --path option set or if the file is not found at the --path location, then Open MPI will search the user's PATH environment variable as defined on the source node(s). If a relative directory is specified, it must be relative to the initial working directory determined by the specific starter used. For example when using the rsh or ssh starters, the initial directory is $HOME by default. Other starters may set the initial directory to the current working directory from the invocation of mpirun. Current Working Directory The -wdir mpirun option (and its synonym, -wd) allows the user to change to an arbitrary directory before the program is invoked. It can also be used in application context files to specify working directories on specific nodes and\/or for specific applications. If the -wdir option appears both in a context file and on the command line, the context file directory will override the command line value. If the -wdir option is specified, Open MPI will attempt to change to the specified directory on all of the remote nodes. If this fails, mpirun will abort. If the -wdir option is not specified, Open MPI will send the directory name where mpirun was invoked to each of the remote nodes. The remote nodes will try to change to that directory. If they are unable (e.g., if the directory does not exit on that node), then Open MPI will use the default directory determined by the starter. All directory changing occurs before the user's program is invoked; it does not wait until MPI_INIT is called. Standard I\/O Open MPI directs UNIX standard input to \/dev\/null on all processes except the MPI_COMM_WORLD rank 0 process. The MPI_COMM_WORLD rank 0 process inherits standard input from mpirun. Note: The node that invoked mpirun need not be the same as the node where the MPI_COMM_WORLD rank 0 process resides. Open MPI handles the redirection of mpirun's standard input to the rank 0 process. Open MPI directs UNIX standard output and error from remote nodes to the node that invoked mpirun and prints it on the standard output\/error of mpirun. Local processes inherit the standard output\/error of mpirun and transfer to it directly. Thus it is possible to redirect standard I\/O for Open MPI applications by using the typical shell redirection procedure on mpirun. % mpirun -np 2 my_app < my_input > my_output Note that in this example only the MPI_COMM_WORLD rank 0 process will receive the stream from my_input on stdin. The stdin on all the other nodes will be tied to \/dev\/null. However, the stdout from all nodes will be collected into the my_output file. Signal Propagation When orterun receives a SIGTERM and SIGINT, it will attempt to kill the entire job by sending all processes in the job a SIGTERM, waiting a small number of seconds, then sending all processes in the job a SIGKILL. SIGUSR1 and SIGUSR2 signals received by orterun are propagated to all processes in the job. One can turn on forwarding of SIGSTOP and SIGCONT to the program executed by mpirun by setting the MCA parameter orte_forward_job_control to 1. A SIGTSTOP signal to mpirun will then cause a SIGSTOP signal to be sent to all of the programs started by mpirun and likewise a SIGCONT signal to mpirun will cause a SIGCONT sent. Other signals are not currently propagated by orterun. Process Termination \/ Signal Handling During the run of an MPI application, if any rank dies abnormally (either exiting before invoking MPI_FINALIZE, or dying as the result of a signal), mpirun will print out an error message and kill the rest of the MPI application. User signal handlers should probably avoid trying to cleanup MPI state (Open MPI is, currently, neither thread-safe nor async-signal-safe). For example, if a segmentation fault occurs in MPI_SEND (perhaps because a bad buffer was passed in) and a user signal handler is invoked, if this user handler attempts to invoke MPI_FINALIZE, Bad Things could happen since Open MPI was already \"in\" MPI when the error occurred. Since mpirun will notice that the process died due to a signal, it is probably not necessary (and safest) for the user to only clean up non-MPI state. Process Environment Processes in the MPI application inherit their environment from the Open RTE daemon upon the node on which they are running. The environment is typically inherited from the user's shell. On remote nodes, the exact environment is determined by the boot MCA module used. The rsh launch module, for example, uses either rsh\/ ssh to launch the Open RTE daemon on remote nodes, and typically executes one or more of the user's shell-setup files before launching the Open RTE daemon. When running dynamically linked applications which require the LD_LIBRARY_PATH environment variable to be set, care must be taken to ensure that it is correctly set when booting Open MPI. See the \"Remote Execution\" section for more details. Remote Execution Open MPI requires that the PATH environment variable be set to find executables on remote nodes (this is typically only necessary in rsh- or ssh-based environments -- batch\/scheduled environments typically copy the current environment to the execution of remote jobs, so if the current environment has PATH and\/or LD_LIBRARY_PATH set properly, the remote nodes will also have it set properly). If Open MPI was compiled with shared library support, it may also be necessary to have the LD_LIBRARY_PATH environment variable set on remote nodes as well (especially to find the shared libraries required to run user MPI applications). However, it is not always desirable or possible to edit shell startup files to set PATH and\/or LD_LIBRARY_PATH. The --prefix option is provided for some simple configurations where this is not possible. The --prefix option takes a single argument: the base directory on the remote node where Open MPI is installed. Open MPI will use this directory to set the remote PATH and LD_LIBRARY_PATH before executing any Open MPI or user applications. This allows running Open MPI jobs without having pre-configured the PATH and LD_LIBRARY_PATH on the remote nodes. Open MPI adds the basename of the current node's \"bindir\" (the directory where Open MPI's executables are installed) to the prefix and uses that to set the PATH on the remote node. Similarly, Open MPI adds the basename of the current node's \"libdir\" (the directory where Open MPI's libraries are installed) to the prefix and uses that to set the LD_LIBRARY_PATH on the remote node. For example: Local bindir: \/local\/node\/directory\/bin Local libdir: \/local\/node\/directory\/lib64 If the following command line is used: % mpirun --prefix \/remote\/node\/directory Open MPI will add \"\/remote\/node\/directory\/bin\" to the PATH and \"\/remote\/node\/directory\/lib64\" to the D_LIBRARY_PATH on the remote node before attempting to execute anything. Note that --prefix can be set on a per-context basis, allowing for different values for different nodes. The --prefix option is not sufficient if the installation paths on the remote node are different than the local node (e.g., if \"\/lib\" is used on the local node, but \"\/lib64\" is used on the remote node), or if the installation paths are something other than a subdirectory under a common prefix. Note that executing mpirun via an absolute pathname is equivalent to specifying --prefix without the last subdirectory in the absolute pathname to mpirun. For example: % \/usr\/local\/bin\/mpirun ... is equivalent to % mpirun --prefix \/usr\/local Exported Environment Variables All environment variables that are named in the form OMPI_* will automatically be exported to new processes on the local and remote nodes. The -x option to mpirun can be used to export specific environment variables to the new processes. While the syntax of the -x option allows the definition of new variables, note that the parser for this option is currently not very sophisticated - it does not even understand quoted values. Users are advised to set variables in the environment and use -x to export them; not to define them. Setting MCA Parameters The -mca switch allows the passing of parameters to various MCA (Modular Component Architecture) modules. MCA modules have direct impact on MPI programs because they allow tunable parameters to be set at run time (such as which BTL communication device driver to use, what parameters to pass to that BTL, etc.). The -mca switch takes two arguments: <key> and <value>. The <key> argument generally specifies which MCA module will receive the value. For example, the <key> \"btl\" is used to select which BTL to be used for transporting MPI messages. The <value> argument is the value that is passed. For example: mpirun -mca btl tcp,self -np 1 foo Tells Open MPI to use the \"tcp\" and \"self\" BTLs, and to run a single copy of \"foo\" an allocated node. mpirun -mca btl self -np 1 foo Tells Open MPI to use the \"self\" BTL, and to run a single copy of \"foo\" an allocated node. The -mca switch can be used multiple times to specify different <key> and\/or <value> arguments. If the same <key> is specified more than once, the <value>s are concatenated with a comma (\",\") separating them. Note that the -mca switch is simply a shortcut for setting environment variables. The same effect may be accomplished by setting corresponding environment variables before running mpirun. The form of the environment variables that Open MPI sets is: OMPI_MCA_<key>=<value> Thus, the -mca switch overrides any previously set environment variables. The -mca settings similarly override MCA parameters set in the $OPAL_PREFIX\/etc\/openmpi-mca-params.conf or $HOME\/.openmpi\/mca-params.conf file. Unknown <key> arguments are still set as environment variable -- they are not checked (by mpirun) for correctness. Illegal or incorrect <value> arguments may or may not be reported -- it depends on the specific MCA module. To find the available component types under the MCA architecture, or to find the available parameters for a specific component, use the ompi_info command. See the ompi_info(1) man page for detailed information on the command.","Process Name":"orterun","Link":"https:\/\/linux.die.net\/man\/1\/orterun"}},{"Process":{"Description":"osage draws clustered graphs. As input, it takes any graph in the dot format. It draws the graph recursively. At each level, there will be a collection of nodes and a collection of cluster subgraphs. The internals of each cluster subgraph are laid out, then the cluster subgraphs and nodes at the current level are positioned relative to each other, treating each cluster subgraph as a node. At each level, the nodes and cluster subgraphs are viewed as rectangles to be packed together. At present, edges are ignored during packing. Packing is done using the standard packing functions. In particular, the graph attributes pack and packmode control the layout. Each graph and cluster can specify its own values for these attributes. Remember also that a cluster inherits its attribute values from its parent graph. After all nodes and clusters, edges are routed based on the value of the splines attribute.","Process Name":"osage","Link":"https:\/\/linux.die.net\/man\/1\/osage"}},{"Process":{"Description":null,"Process Name":"osc","Link":"https:\/\/linux.die.net\/man\/1\/osc"}},{"Process":{"Description":null,"Process Name":"osd_cat","Link":"https:\/\/linux.die.net\/man\/1\/osd_cat"}},{"Process":{"Description":"osgmlnorm prints on the standard output a normalized document instance for the SGML document contained in the concatenation of the entities with system identifiers When the normalized instance is prefixed with the original SGML declaration and prolog, it will have the same ESIS as the original SGML document, with the following exceptions: * The output of osgmlnorm does not protect against the recognition of short reference delimiters, so any USEMAP declarations must be removed from the DTD. * The normalized instance will use the reference delimiters, even if the original instance did not. * If marked sections are included in the output using the -m option, the reference reserved names will be used for the status keywords even if the original instance did not. * Any ESIS information relating to the SGML LINK feature will be lost. The normalized instance will not use any markup minimization features except that: * Any attributes that were not specified in the original instance will not be included in the normalized instance. (Current attributes will be included.) * If the declared value of an attribute was a name token group, and a value was specified that was the same as the name of the attribute, then the attribute name and value indicator will be omitted. For example, with HTML osgmlnorm would output <DL COMPACT> rather than <DL COMPACT=\"COMPACT\"> Part of an SGML System Conforming to International Standard ISO 8879 -- Standard Generalized Markup Language. An SGML Extended Facilities system conforming to Annex A of International Standard ISO\/IEC 10744 -- Hypermedia\/Time-based Structuring Language. The following options are available: -aname, --activate=name Make doctype or linktype name active. -A architecture, --architecture= architecture Parse with respect to architecture architecture. -b bctf, --bctf= bctf Use the BCTF with name bctf for output. -c sysid, --catalog= sysid Map public identifiers and entity names to system identifiers using the catalog entry file whose system identifier is sysid. -C, --catalogs This has the same effect as in onsgmls(1). -d, --dtd Output a document type declaration with the same external identifier as the input document, and with no internal declaration subset. No check is performed that the document instance is valid with respect to this DTD. -D directory, --directory= directory Search directory for files specified in system identifiers. This has the same effect as in onsgmls(1). -e, --open-entities Describe open entities in error messages. --error-numbers Show error numbers in error messages. -h, --help Display a help text and exit. -i name, --include= name This has the same effect as in onsgmls(1). -m, --marked-sections Output any marked sections that were in the input document instance. -n, --comments Output any comments that were in the input document instance. -r, --raw Raw output. Don't perform any conversion on RSs and REs when printing the entity. The entity would typically have the storage manager attribute records=asis. -R, --restricted This has the same effect as in onsgmls(1). -v, --version Print the version number. -w type Control warnings and errors according to type. This has the same effect as in onsgmls(1).","Process Name":"osgmlnorm","Link":"https:\/\/linux.die.net\/man\/1\/osgmlnorm"}},{"Process":{"Description":"The osinfo command displays information regarding the operating system, gathered via WBEM using the PG_OperatingSystem class supported by the OperatingSystem Provider (bundled with OpenPegasus). osinfo requires the CIM Server to be installed and running. By default, the information is formatted for display, converting CIMDateTime strings into a more user-readable format (see Examples), and converting the total number of seconds of uptime into the appropriate number of days, hours, minutes, and seconds. If a property value is unavailable, it will be shown as \"Unknown\" Options osinfo recognizes the following options: -c Use the CIM formats for DateTime and SystemUpTime values (not the formatting done by default). As specified by the DMTF, the CIMDateTime format is yyyymmddhhmmss.mmmmmmsutc, where yyyy is a 4-digit year, mm is the month, dd is the day, hh is the hour on a 24-hour clock, mm is the minute, ss is the second, mmmmmm is the number of microseconds, s is a \"+\" or \"-\" indicating the sign of the UTC (Universal Time Code) correction field (since the DateTime is returned in the local time zone of the system), and utc is the offset from UTC in minutes. -h hostname Connect to the CIM Server on the specified host. If this option is not specified, osinfo connects to the localhost. --help Display command usage information. -p portnumber Connect to the CIM Server on the specified port number. If this option is not specified, osinfo connects to the default port for the wbem-http service, or if the -s option is specified, to the default port for the wbem-https service. -s Enable the use of the SSL protocol between osinfo and the CIM server. The -s option should be specified if the CIM Server on the specified hostname\/portnumber expects clients to connect using HTTPS. -t timeout Wait the specified number of milliseconds on sending a request, before timing out if no response has been received. The timeout value must be an integer value greater than 0. -u username Connect as the specified R username . If username is not specified, the current logged in user is used for authentication. This option is ignored if neither hostname nor portnumber is specified. --version Display CIM Server version. -w password Authenticate the connection using the specified password . This option is ignored if neither hostname nor portnumber is specified. WARNING: A password should not be specified on the command line on a multi-user system, since command-line options are typically world-readable for a short window of time. If the remote host requests authentication and this option is not specified, osinfo will prompt for the password.","Process Name":"osinfo","Link":"https:\/\/linux.die.net\/man\/1\/osinfo"}},{"Process":{"Description":"This manual page documents briefly the libosip library. API reference & FAQ can be found at http:\/\/www.fsf.org\/software\/osip\/","Process Name":"osip","Link":"https:\/\/linux.die.net\/man\/1\/osip"}},{"Process":{"Description":"This manual page documents briefly the libosip2 library.","Process Name":"osip2","Link":"https:\/\/linux.die.net\/man\/1\/osip2"}},{"Process":{"Description":"ospam (OpenSP Add Markup) is an SGML markup stream editor implemented using the OpenSP parser. ospam parses the SGML document contained in sysid and copies to the standard output the portion of the document entity containing the document instance, adding or changing markup as specified by the -m options. The -p option can be used to include the SGML declaration and prolog in the output. The -o option can be used to output other entities. The -x option can be used to expand entity references. Part of an SGML System Conforming to International Standard ISO 8879 -- Standard Generalized Markup Language. An SGML Extended Facilities system conforming to Annex A of Internation Standard ISO\/IEC 10744 -- Hypermedia\/Time-based Structuring Language. The following options are available: -aname, --activate=name Make doctype or linktype name active. -A architecture, --architecture= architecture Parse with respect to architecture architecture. -b bctf, --bctf= bctf Use bctf bctf for output. -c sysid, --catalog= sysid Use the catalog entry file sysid. -C, --catalogs This has the same effect as in onsgmls(1). -D directory, --directory= directory Search directory for files specified in system identifiers. This has the same effect as in onsgmls(1). -e, --open-entities Describe open entities in error messages. -E max_errors, --max-errors= max_errors Exit after max_errors errors are encountered. -f file, --error-file= file Redirect errors to file. This is useful mainly with shells that do not support redirection of stderr. -h, --hoist-omitted-tags Hoist omitted tags out from the start of internal entities. If the text at the beginning of an internal entity causes a tag to be implied, the tag will usually be treated as being in that internal entity; this option will instead cause it to be treated as being in the entity that referenced the internal entity. This option makes a difference in conjunction with -momittag or -x -x. --help Display a help text and exit. -i name, --include= name This has the same effect as in onsgmls(1). -l, --lowercase Prefer lower-case. Added names that were subject to upper-case substitution will be converted to lower-case. -m markup_option, --markup-option= markup_option Change the markup in the output according to the value of markup_option as follows: omittag Add tags that were omitted using omitted tag minimization. End tags that were omitted because the element has a declared content of EMPTY or an explicit content reference will not be added. shortref Replace short references by named entity references. net Change null end-tags into unminimized end-tags, and change net-enabling start-tags into unminimized start-tags. emptytag Change empty tags into unminimized tags. unclosed Change unclosed tags into unminimized tags. attname Add omitted attribute names and vis. attvalue Add literal delimiters omitted from attribute values. attspec Add omitted attribute specifications. current Add omitted attribute specifications for current attributes. This option is implied by the attspec option. shorttag Equivalent to combination of net, emptytag, unclosed, attname, attvalue and attspec options. rank Add omitted rank suffixes. reserved Put reserved names in upper-case. ms Remove marked section declarations whose effective status is IGNORE, and replace each marked section declaration whose effective status is INCLUDE by its marked section. In the document instance, empty comments will be added before or after the marked section declaration to ensure that ignored record ends remain ignored. Multiple -m options are allowed. -n, --error-numbers Show error numbers in error messages. -o name, --output-entity= name Output the general entity name instead of the document entity. The output will correspond to the first time that the entity is referenced in content. -p, --output-prolog Output the part of the document entity containing the SGML declaration (if it was explicitly present in the document entity) and the prolog before anything else. If this option is specified two or more times, then all entity references occurring between declarations in the prolog will be expanded; this includes the implicit reference to the entity containing the external subset of the DTD, if there is one. Note that the SGML declaration will not be included if it was specified by an SGMLDECL entry in a catalog. -r, --raw Don't perform any conversion on RSs and REs when outputting the entity. The entity would typically have the storage manager attribute records=asis. -R, --restricted This as the same effect as in onsgmls(1) -v, --version Print the version number. -w type, --warning= type Control warnings and errors according to type. This has the same effect as in onsgmls(1). -x, --expand-references Expand references to entities that are changed. If this option is specified two or more times, then all references to entities that contain tags will be expanded.","Process Name":"ospam","Link":"https:\/\/linux.die.net\/man\/1\/ospam"}},{"Process":{"Description":null,"Process Name":"ospcat","Link":"https:\/\/linux.die.net\/man\/1\/ospcat"}},{"Process":{"Description":"ospent (OpenSP print entity) prints the concatenation of the entities with system identifiers on the standard output. The following options are available: -bbctf, --bctf=bctf Use the BCTF with name bctf for output. -c sysid, --catalog= sysid Map public identifiers and entity names to system identifiers using the catalog entry file whose system identifier is sysid. -C, --catalogs This has the same effect as in onsgmls(1). -D directory, --directory= directory Search directory for files specified in system identifiers. This has the same effect as in onsgmls(1). -f file, --error-file= file Redirect error messages to file. -h, --help Display a help text and exit. -n, --non-sgml The entity is a non-SGML data entity. This option forces the octets in the storage objects comprising the entity to be copied exactly without any of the conversions that are done for text entities. Implies -r. -r, --raw Raw output. Don't perform any conversion on RSs and REs when printing the entity. The entity would typically have the storage manager attribute records=asis. -R, --restricted This has the same effect as in onsgmls(1). -v, --version Print the version number.","Process Name":"ospent","Link":"https:\/\/linux.die.net\/man\/1\/ospent"}},{"Process":{"Description":"osql is a diagnostic tool provided as part of FreeTDS. It is a Bourne shell script that checks and reports on your configuration files. If everything checks out OK, it invokes isql. osql works only with the isql that comes with unixODBC.","Process Name":"osql","Link":"https:\/\/linux.die.net\/man\/1\/osql"}},{"Process":{"Description":null,"Process Name":"osx","Link":"https:\/\/linux.die.net\/man\/1\/osx"}},{"Process":{"Description":"otf2bdf will convert an OpenType font to a BDF font using the Freetype2 renderer ( http:\/\/www.freetype.org).","Process Name":"otf2bdf","Link":"https:\/\/linux.die.net\/man\/1\/otf2bdf"}},{"Process":{"Description":"otp2ocp is used to create or recreate Omega Compiled Process files (binary) from Omega Translation Process (text) files. Omega Translation Processes are rules used to translate one character set to another and to choose between characters with one or more context-sensitive variants. OTPs could also be used to change the case or hyphenation of text.","Process Name":"otp2ocp","Link":"https:\/\/linux.die.net\/man\/1\/otp2ocp"}},{"Process":{"Description":null,"Process Name":"otr_mackey","Link":"https:\/\/linux.die.net\/man\/1\/otr_mackey"}},{"Process":{"Description":"Off-the-Record (OTR) Messaging allows you to have private conversations over IM by providing: - Encryption - No one else can read your instant messages. - Authentication - You are assured the correspondent is who you think it is. - Deniability - The messages you send do not have digital signatures that are checkable by a third party. Anyone can forge messages after a conversation to make them look like they came from you. However, during a conversation, your correspondent is assured the messages he sees are authentic and unmodified. - Perfect forward secrecy - If you lose control of your private keys, no previous conversation is compromised. The OTR Toolkit is useful for analyzing and\/or forging OTR messages. Why do we offer this? Primarily, to make absolutely sure that transcripts of OTR conversations are really easy to forge after the fact. [Note that during an OTR conversation, messages can't be forged without real-time access to the secret keys on the participants' computers, and in that case, all security has already been lost.] Easily-forgeable transcripts help us provide the \"Deniability\" property: if someone claims you said something over OTR, they'll have no proof, as anyone at all can modify a transcript to make it say whatever they like, and still have all the verification come out correctly. Here are the six programs in the toolkit: - otr_parse - Parse OTR messages given on stdin, showing the values of all the fields in OTR protocol messages. - otr_sesskeys our_privkey their_pubkey - Shows our public key, the session id, two AES and two MAC keys derived from the given Diffie-Hellman keys (one private, one public). - otr_mackey aes_enc_key - Shows the MAC key derived from the given AES key. - otr_readforge aes_enc_key [newmsg] - Decrypts an OTR Data message using the given AES key, and displays the message. - If newmsg is given, replace the message with that one, encrypt and MAC it properly, and output the resulting OTR Data Message. This works even if the given key was not correct for the original message, so as to enable complete forgeries. - otr_modify mackey old_text new_text offset - Even if you can't read the data because you don't know either the AES key or the Diffie-Hellman private key, but you can make a good guess that the substring \"old_text\" appears at the given offset in the message, replace the old_text with the new_text (which must be of the same length), recalculate the MAC with the given mackey, and output the resulting Data message. - Note that, even if you don't know any text in an existing message, you can still forge messages of your choice using the otr_readforge command, above. - otr_remac mackey flags snd_keyid rcv_keyid pubkey counter encdata revealed_mackeys - Make a new OTR Data Message, with the given pieces (note that the data part is already encrypted). MAC it with the given mackey.","Process Name":"otr_modify","Link":"https:\/\/linux.die.net\/man\/1\/otr_modify"}},{"Process":{"Description":"Off-the-Record (OTR) Messaging allows you to have private conversations over IM by providing: - Encryption - No one else can read your instant messages. - Authentication - You are assured the correspondent is who you think it is. - Deniability - The messages you send do not have digital signatures that are checkable by a third party. Anyone can forge messages after a conversation to make them look like they came from you. However, during a conversation, your correspondent is assured the messages he sees are authentic and unmodified. - Perfect forward secrecy - If you lose control of your private keys, no previous conversation is compromised. The OTR Toolkit is useful for analyzing and\/or forging OTR messages. Why do we offer this? Primarily, to make absolutely sure that transcripts of OTR conversations are really easy to forge after the fact. [Note that during an OTR conversation, messages can't be forged without real-time access to the secret keys on the participants' computers, and in that case, all security has already been lost.] Easily-forgeable transcripts help us provide the \"Deniability\" property: if someone claims you said something over OTR, they'll have no proof, as anyone at all can modify a transcript to make it say whatever they like, and still have all the verification come out correctly. Here are the six programs in the toolkit: - otr_parse - Parse OTR messages given on stdin, showing the values of all the fields in OTR protocol messages. - otr_sesskeys our_privkey their_pubkey - Shows our public key, the session id, two AES and two MAC keys derived from the given Diffie-Hellman keys (one private, one public). - otr_mackey aes_enc_key - Shows the MAC key derived from the given AES key. - otr_readforge aes_enc_key [newmsg] - Decrypts an OTR Data message using the given AES key, and displays the message. - If newmsg is given, replace the message with that one, encrypt and MAC it properly, and output the resulting OTR Data Message. This works even if the given key was not correct for the original message, so as to enable complete forgeries. - otr_modify mackey old_text new_text offset - Even if you can't read the data because you don't know either the AES key or the Diffie-Hellman private key, but you can make a good guess that the substring \"old_text\" appears at the given offset in the message, replace the old_text with the new_text (which must be of the same length), recalculate the MAC with the given mackey, and output the resulting Data message. - Note that, even if you don't know any text in an existing message, you can still forge messages of your choice using the otr_readforge command, above. - otr_remac mackey flags snd_keyid rcv_keyid pubkey counter encdata revealed_mackeys - Make a new OTR Data Message, with the given pieces (note that the data part is already encrypted). MAC it with the given mackey.","Process Name":"otr_parse","Link":"https:\/\/linux.die.net\/man\/1\/otr_parse"}},{"Process":{"Description":null,"Process Name":"otr_readforge","Link":"https:\/\/linux.die.net\/man\/1\/otr_readforge"}},{"Process":{"Description":"Off-the-Record (OTR) Messaging allows you to have private conversations over IM by providing: - Encryption - No one else can read your instant messages. - Authentication - You are assured the correspondent is who you think it is. - Deniability - The messages you send do not have digital signatures that are checkable by a third party. Anyone can forge messages after a conversation to make them look like they came from you. However, during a conversation, your correspondent is assured the messages he sees are authentic and unmodified. - Perfect forward secrecy - If you lose control of your private keys, no previous conversation is compromised. The OTR Toolkit is useful for analyzing and\/or forging OTR messages. Why do we offer this? Primarily, to make absolutely sure that transcripts of OTR conversations are really easy to forge after the fact. [Note that during an OTR conversation, messages can't be forged without real-time access to the secret keys on the participants' computers, and in that case, all security has already been lost.] Easily-forgeable transcripts help us provide the \"Deniability\" property: if someone claims you said something over OTR, they'll have no proof, as anyone at all can modify a transcript to make it say whatever they like, and still have all the verification come out correctly. Here are the six programs in the toolkit: - otr_parse - Parse OTR messages given on stdin, showing the values of all the fields in OTR protocol messages. - otr_sesskeys our_privkey their_pubkey - Shows our public key, the session id, two AES and two MAC keys derived from the given Diffie-Hellman keys (one private, one public). - otr_mackey aes_enc_key - Shows the MAC key derived from the given AES key. - otr_readforge aes_enc_key [newmsg] - Decrypts an OTR Data message using the given AES key, and displays the message. - If newmsg is given, replace the message with that one, encrypt and MAC it properly, and output the resulting OTR Data Message. This works even if the given key was not correct for the original message, so as to enable complete forgeries. - otr_modify mackey old_text new_text offset - Even if you can't read the data because you don't know either the AES key or the Diffie-Hellman private key, but you can make a good guess that the substring \"old_text\" appears at the given offset in the message, replace the old_text with the new_text (which must be of the same length), recalculate the MAC with the given mackey, and output the resulting Data message. - Note that, even if you don't know any text in an existing message, you can still forge messages of your choice using the otr_readforge command, above. - otr_remac mackey flags snd_keyid rcv_keyid pubkey counter encdata revealed_mackeys - Make a new OTR Data Message, with the given pieces (note that the data part is already encrypted). MAC it with the given mackey.","Process Name":"otr_remac","Link":"https:\/\/linux.die.net\/man\/1\/otr_remac"}},{"Process":{"Description":"Off-the-Record (OTR) Messaging allows you to have private conversations over IM by providing: - Encryption - No one else can read your instant messages. - Authentication - You are assured the correspondent is who you think it is. - Deniability - The messages you send do not have digital signatures that are checkable by a third party. Anyone can forge messages after a conversation to make them look like they came from you. However, during a conversation, your correspondent is assured the messages he sees are authentic and unmodified. - Perfect forward secrecy - If you lose control of your private keys, no previous conversation is compromised. The OTR Toolkit is useful for analyzing and\/or forging OTR messages. Why do we offer this? Primarily, to make absolutely sure that transcripts of OTR conversations are really easy to forge after the fact. [Note that during an OTR conversation, messages can't be forged without real-time access to the secret keys on the participants' computers, and in that case, all security has already been lost.] Easily-forgeable transcripts help us provide the \"Deniability\" property: if someone claims you said something over OTR, they'll have no proof, as anyone at all can modify a transcript to make it say whatever they like, and still have all the verification come out correctly. Here are the six programs in the toolkit: - otr_parse - Parse OTR messages given on stdin, showing the values of all the fields in OTR protocol messages. - otr_sesskeys our_privkey their_pubkey - Shows our public key, the session id, two AES and two MAC keys derived from the given Diffie-Hellman keys (one private, one public). - otr_mackey aes_enc_key - Shows the MAC key derived from the given AES key. - otr_readforge aes_enc_key [newmsg] - Decrypts an OTR Data message using the given AES key, and displays the message. - If newmsg is given, replace the message with that one, encrypt and MAC it properly, and output the resulting OTR Data Message. This works even if the given key was not correct for the original message, so as to enable complete forgeries. - otr_modify mackey old_text new_text offset - Even if you can't read the data because you don't know either the AES key or the Diffie-Hellman private key, but you can make a good guess that the substring \"old_text\" appears at the given offset in the message, replace the old_text with the new_text (which must be of the same length), recalculate the MAC with the given mackey, and output the resulting Data message. - Note that, even if you don't know any text in an existing message, you can still forge messages of your choice using the otr_readforge command, above. - otr_remac mackey flags snd_keyid rcv_keyid pubkey counter encdata revealed_mackeys - Make a new OTR Data Message, with the given pieces (note that the data part is already encrypted). MAC it with the given mackey.","Process Name":"otr_sesskeys","Link":"https:\/\/linux.die.net\/man\/1\/otr_sesskeys"}},{"Process":{"Description":"Off-the-Record (OTR) Messaging allows you to have private conversations over IM by providing: - Encryption - No one else can read your instant messages. - Authentication - You are assured the correspondent is who you think it is. - Deniability - The messages you send do not have digital signatures that are checkable by a third party. Anyone can forge messages after a conversation to make them look like they came from you. However, during a conversation, your correspondent is assured the messages he sees are authentic and unmodified. - Perfect forward secrecy - If you lose control of your private keys, no previous conversation is compromised. The OTR Toolkit is useful for analyzing and\/or forging OTR messages. Why do we offer this? Primarily, to make absolutely sure that transcripts of OTR conversations are really easy to forge after the fact. [Note that during an OTR conversation, messages can't be forged without real-time access to the secret keys on the participants' computers, and in that case, all security has already been lost.] Easily-forgeable transcripts help us provide the \"Deniability\" property: if someone claims you said something over OTR, they'll have no proof, as anyone at all can modify a transcript to make it say whatever they like, and still have all the verification come out correctly. Here are the six programs in the toolkit: - otr_parse - Parse OTR messages given on stdin, showing the values of all the fields in OTR protocol messages. - otr_sesskeys our_privkey their_pubkey - Shows our public key, the session id, two AES and two MAC keys derived from the given Diffie-Hellman keys (one private, one public). - otr_mackey aes_enc_key - Shows the MAC key derived from the given AES key. - otr_readforge aes_enc_key [newmsg] - Decrypts an OTR Data message using the given AES key, and displays the message. - If newmsg is given, replace the message with that one, encrypt and MAC it properly, and output the resulting OTR Data Message. This works even if the given key was not correct for the original message, so as to enable complete forgeries. - otr_modify mackey old_text new_text offset - Even if you can't read the data because you don't know either the AES key or the Diffie-Hellman private key, but you can make a good guess that the substring \"old_text\" appears at the given offset in the message, replace the old_text with the new_text (which must be of the same length), recalculate the MAC with the given mackey, and output the resulting Data message. - Note that, even if you don't know any text in an existing message, you can still forge messages of your choice using the otr_readforge command, above. - otr_remac mackey flags snd_keyid rcv_keyid pubkey counter encdata revealed_mackeys - Make a new OTR Data Message, with the given pieces (note that the data part is already encrypted). MAC it with the given mackey.","Process Name":"otr_toolkit","Link":"https:\/\/linux.die.net\/man\/1\/otr_toolkit"}},{"Process":{"Description":null,"Process Name":"outocp","Link":"https:\/\/linux.die.net\/man\/1\/outocp"}},{"Process":{"Description":"ovf2ovp translates a virtual font (OVF) file, OVFNAME, and its companion font metric (OFM) file, OFMNAME, into a human-readable property-list format. The program writes to standard output (by default) or to a file specified as OVPNAME. The program also works with TeX VF and TFM files, producing TeX VP files. (ovf2ovp is based on the WEB source code for vftovp(1).)","Process Name":"ovf2ovp","Link":"https:\/\/linux.die.net\/man\/1\/ovf2ovp"}},{"Process":{"Description":"ovp2ovf translates a human-readable virtual property list (OVP) file, OVPFILE, into a virtual font (OVF) file, OVFFILE and its companion font metric (OFM) file, OFMFILE. The program also works with TeX VP files, producing VF and TFM files. (ovp2ovf is based on the WEB source code for vptovf(1).)","Process Name":"ovp2ovf","Link":"https:\/\/linux.die.net\/man\/1\/ovp2ovf"}},{"Process":{"Description":"Xdvi is a program for previewing dvi files, as produced e.g. by the tex(1) program, under the X window system. Xdvi can show the file shrunken by various integer factors, and it has a ''magnifying glass'' for viewing parts of the page enlarged (see the section MAGNIFIER below). This version of xdvi is also referred to as xdvik since it uses the kpathsea library to locate and generate font files. In addition to that, it supports the following features: - hyperlinks in DVI files (section HYPERLINKS), - direct rendering of Postscript<tm> Type1 fonts (section T1LIB), - source specials in the DVI file (section SOURCE SPECIALS), - string search in DVI files (section STRING SEARCH), - saving or printing (parts of) the DVI file (sections PRINT DIALOG and SAVE DIALOG). Xdvi can be compiled with the Motif toolkit or the Xaw (Athena) toolkit (and variants of it), and the Motif version has a slightly different GUI; these differences are noted below. Before displaying a page of a DVI file, xdvi will check to see if the file has changed since the last time it was displayed. If this is the case, it will reload the file. This feature allows you to preview many versions of the same file while running xdvi only once. Since it cannot read partial DVI files, xdvik versions starting from 22.74.3 will create a temporary copy of the DVI file being viewed, to ensure that the file can be viewed without interruptions. (The -notempfile can be used to turn off this feature). Xdvi can show PostScript<tm> specials by any of three methods. It will try first to use Display PostScript<tm>, then NeWS, then it will try to use Ghostscript to render the images. All of these options depend on additional software to work properly; moreover, some of them may not be compiled into this copy of xdvi. For performance reasons, xdvi does not render PostScript specials in the magnifying glass. If no file name has been specified on the command line, xdvi will try to open the most recently opened file; if the file history (accessible via the File > Open Recent menu) is empty, or if none of the files in the history are valid DVI files, it will pop up a file selector for choosing a file name. (In previous versions, which didn't have a file history, the file selector was always used; you can set the X resource noFileArgUseHistory to false to get back the old behaviour.)","Process Name":"oxdvi","Link":"https:\/\/linux.die.net\/man\/1\/oxdvi"}},{"Process":{"Description":null,"Process Name":"oz-cleanup-cache","Link":"https:\/\/linux.die.net\/man\/1\/oz-cleanup-cache"}},{"Process":{"Description":"This is a tool to modify already installed operating systems. Modifications typically involve installing additional packages. This program can be used as a counterpart to oz-install, though it does not have to be used in conjunction. Note that oz-customize does the actual customization using a combination of KVM and libvirt, so both of these must be available (and working) for oz-customize to have a chance to succeed.","Process Name":"oz-customize","Link":"https:\/\/linux.die.net\/man\/1\/oz-customize"}},{"Process":{"Description":"The oz-install(1) , oz-customize(1) , and oz-generate-icicle(1) man pages explain the command-line usage of the Oz commands. One of the required input parameters to all of the above commands is a TDL (Template Description Language) file, which describes the OS the user wants to install, where to get the media from, and any additional packages or actions the user wants to take on the operating system. This man page describes a number of TDL examples and what happens when they are used. Since the TDL is XML, standard XPath notation is used to describe various elements of the XML.","Process Name":"oz-examples","Link":"https:\/\/linux.die.net\/man\/1\/oz-examples"}},{"Process":{"Description":"This is a tool to generate a package manifest (also called ICICLE) from a disk image. This program can be used as a counterpart to oz-install, though it does not have to be used in conjunction. Note that oz-generate-icicle does the actual work using a combination of KVM and libvirt, so both of these must be available (and working) for oz-generate-icicle to have a chance to succeed.","Process Name":"oz-generate-icicle","Link":"https:\/\/linux.die.net\/man\/1\/oz-generate-icicle"}},{"Process":{"Description":"This is a tool to automatically install operating system into files that represent disk images. The input is an XML file representing the operating system and packages to be installed. By default (and by design), the first stage of the install only installs a JEOS (Just Enough Operating System); customization, including the installation of additional packages can also be done, but requires additional flags. Note that oz-install does the actual installation using a combination of KVM and libvirt, so both of these must be available (and working) for oz-install to have a chance to succeed.","Process Name":"oz-install","Link":"https:\/\/linux.die.net\/man\/1\/oz-install"}}]