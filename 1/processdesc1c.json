[{"Process":{"Description":"Cdecl (and c++decl) is a program for encoding and decoding C (or C++) type declarations. The C language is based on the (draft proposed) X3J11 ANSI Standard; optionally, the C language may be based on the pre-ANSI definition defined by Kernighan & Ritchie's The C Programming Language book, or the C language defined by the Ritchie PDP-11 C compiler. The C++ language is based on Bjarne Stroustrup's The C++ Programming Language, plus the version 2.0 additions to the language.","Process Name":"c++decl","Link":"https:\/\/linux.die.net\/man\/1\/c++decl"}},{"Process":{"Description":"The C ++ and Java languages provide function overloading, which means that you can write many functions with the same name, providing that each function takes parameters of different types. In order to be able to distinguish these similarly named functions C ++ and Java encode them into a low-level assembler name which uniquely identifies each different version. This process is known as mangling. The c++filt [1] program does the inverse mapping: it decodes (demangles) low-level names into user-level names so that they can be read. Every alphanumeric word (consisting of letters, digits, underscores, dollars, or periods) seen in the input is a potential mangled name. If the name decodes into a C ++ name, the C ++ name replaces the low-level name in the output, otherwise the original word is output. In this way you can pass an entire assembler source file, containing mangled names, through c++filt and see the same source file containing demangled names. You can also use c++filt to decipher individual symbols by passing them on the command line: c++filt <symbol> If no symbol arguments are given, c++filt reads symbol names from the standard input instead. All the results are printed on the standard output. The difference between reading names from the command line versus reading names from the standard input is that command line arguments are expected to be just mangled names and no checking is performed to separate them from surrounding text. Thus for example: c++filt -n _Z1fv will work and demangle the name to \"f()\" whereas: c++filt -n _Z1fv, will not work. (Note the extra comma at the end of the mangled name which makes it invalid). This command however will work: echo _Z1fv, | c++filt -n and will display \"f(),\", i.e., the demangled name followed by a trailing comma. This behaviour is because when the names are read from the standard input it is expected that they might be part of an assembler source file where there might be extra, extraneous characters trailing after a mangled name. For example: .type   _Z1fv, @function","Process Name":"c++filt","Link":"https:\/\/linux.die.net\/man\/1\/c++filt"}},{"Process":{"Description":"The following is the old c2ph.doc documentation by Tom Christiansen <tchrist@perl.com> Date: 25 Jul 91 08:10:21 GMT Once upon a time, I wrote a program called pstruct. It was a perl program that tried to parse out C structures and display their member offsets for you. This was especially useful for people looking at binary dumps or poking around the kernel. Pstruct was not a pretty program. Neither was it particularly robust. The problem, you see, was that the C compiler was much better at parsing C than I could ever hope to be. So I got smart: I decided to be lazy and let the C compiler parse the C, which would spit out debugger stabs for me to read. These were much easier to parse. It's still not a pretty program, but at least it's more robust. Pstruct takes any .c or .h files, or preferably .s ones, since that's the format it is going to massage them into anyway, and spits out listings like this: struct tty {\n  int                          tty.t_locker                         000      4\n  int                          tty.t_mutex_index                    004      4\n  struct tty *                 tty.t_tp_virt                        008      4\n  struct clist                 tty.t_rawq                           00c     20\n    int                        tty.t_rawq.c_cc                      00c      4\n    int                        tty.t_rawq.c_cmax                    010      4\n    int                        tty.t_rawq.c_cfx                     014      4\n    int                        tty.t_rawq.c_clx                     018      4\n    struct tty *               tty.t_rawq.c_tp_cpu                  01c      4\n    struct tty *               tty.t_rawq.c_tp_iop                  020      4\n    unsigned char *            tty.t_rawq.c_buf_cpu                 024      4\n    unsigned char *            tty.t_rawq.c_buf_iop                 028      4\n  struct clist                 tty.t_canq                           02c     20\n    int                        tty.t_canq.c_cc                      02c      4\n    int                        tty.t_canq.c_cmax                    030      4\n    int                        tty.t_canq.c_cfx                     034      4\n    int                        tty.t_canq.c_clx                     038      4\n    struct tty *               tty.t_canq.c_tp_cpu                  03c      4\n    struct tty *               tty.t_canq.c_tp_iop                  040      4\n    unsigned char *            tty.t_canq.c_buf_cpu                 044      4\n    unsigned char *            tty.t_canq.c_buf_iop                 048      4\n  struct clist                 tty.t_outq                           04c     20\n    int                        tty.t_outq.c_cc                      04c      4\n    int                        tty.t_outq.c_cmax                    050      4\n    int                        tty.t_outq.c_cfx                     054      4\n    int                        tty.t_outq.c_clx                     058      4\n    struct tty *               tty.t_outq.c_tp_cpu                  05c      4\n    struct tty *               tty.t_outq.c_tp_iop                  060      4\n    unsigned char *            tty.t_outq.c_buf_cpu                 064      4\n    unsigned char *            tty.t_outq.c_buf_iop                 068      4\n  (*int)()                     tty.t_oproc_cpu                      06c      4\n  (*int)()                     tty.t_oproc_iop                      070      4\n  (*int)()                     tty.t_stopproc_cpu                   074      4\n  (*int)()                     tty.t_stopproc_iop                   078      4\n  struct thread *              tty.t_rsel                           07c      4 etc. Actually, this was generated by a particular set of options. You can control the formatting of each column, whether you prefer wide or fat, hex or decimal, leading zeroes or whatever. All you need to be able to use this is a C compiler than generates BSD\/GCC-style stabs. The -g option on native BSD compilers and GCC should get this for you. To learn more, just type a bogus option, like -\\?, and a long usage message will be provided. There are a fair number of possibilities. If you're only a C programmer, than this is the end of the message for you. You can quit right now, and if you care to, save off the source and run it when you feel like it. Or not. But if you're a perl programmer, then for you I have something much more wondrous than just a structure offset printer. You see, if you call pstruct by its other incybernation, c2ph, you have a code generator that translates C code into perl code! Well, structure and union declarations at least, but that's quite a bit. Prior to this point, anyone programming in perl who wanted to interact with C programs, like the kernel, was forced to guess the layouts of the C structures, and then hardwire these into his program. Of course, when you took your wonderfully crafted program to a system where the sgtty structure was laid out differently, your program broke. Which is a shame. We've had Larry's h2ph translator, which helped, but that only works on cpp symbols, not real C, which was also very much needed. What I offer you is a symbolic way of getting at all the C structures. I've couched them in terms of packages and functions. Consider the following program: #!\/usr\/local\/bin\/perl\n\nrequire 'syscall.ph';\nrequire 'sys\/time.ph';\nrequire 'sys\/resource.ph';\n\n$ru = \"\\0\" x &rusage'sizeof();\n\nsyscall(&SYS_getrusage, &RUSAGE_SELF, $ru)      && die \"getrusage: $!\";\n\n@ru = unpack($t = &rusage'typedef(), $ru);\n\n$utime =  $ru[ &rusage'ru_utime + &timeval'tv_sec  ]\n       + ($ru[ &rusage'ru_utime + &timeval'tv_usec ]) \/ 1e6;\n\n$stime =  $ru[ &rusage'ru_stime + &timeval'tv_sec  ]\n       + ($ru[ &rusage'ru_stime + &timeval'tv_usec ]) \/ 1e6;\n\nprintf \"you have used %8.3fs+%8.3fu seconds.\\n\", $utime, $stime; As you see, the name of the package is the name of the structure. Regular fields are just their own names. Plus the following accessor functions are provided for your convenience: struct      This takes no arguments, and is merely the number of first-level\n            elements in the structure.  You would use this for indexing\n            into arrays of structures, perhaps like this\n\n                $usec = $u[ &user'u_utimer\n                            + (&ITIMER_VIRTUAL * &itimerval'struct)\n                            + &itimerval'it_value\n                            + &timeval'tv_usec\n                          ];\n\nsizeof      Returns the bytes in the structure, or the member if\n            you pass it an argument, such as\n\n                    &rusage'sizeof(&rusage'ru_utime)\n\ntypedef     This is the perl format definition for passing to pack and\n            unpack.  If you ask for the typedef of a nothing, you get\n            the whole structure, otherwise you get that of the member\n            you ask for.  Padding is taken care of, as is the magic to\n            guarantee that a union is unpacked into all its aliases.\n            Bitfields are not quite yet supported however.\n\noffsetof    This function is the byte offset into the array of that\n            member.  You may wish to use this for indexing directly\n            into the packed structure with vec() if you're too lazy\n            to unpack it.\n\ntypeof      Not to be confused with the typedef accessor function, this\n            one returns the C type of that field.  This would allow\n            you to print out a nice structured pretty print of some\n            structure without knoning anything about it beforehand.\n            No args to this one is a noop.  Someday I'll post such\n            a thing to dump out your u structure for you. The way I see this being used is like basically this: % h2ph <some_include_file.h  >  \/usr\/lib\/perl\/tmp.ph\n% c2ph  some_include_file.h  >> \/usr\/lib\/perl\/tmp.ph\n% install It's a little tricker with c2ph because you have to get the includes right. I can't know this for your system, but it's not usually too terribly difficult. The code isn't pretty as I mentioned -- I never thought it would be a 1000- line program when I started, or I might not have begun. :-) But I would have been less cavalier in how the parts of the program communicated with each other, etc. It might also have helped if I didn't have to divine the makeup of the stabs on the fly, and then account for micro differences between my compiler and gcc. Anyway, here it is. Should run on perl v4 or greater. Maybe less. --tom Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"c2ph","Link":"https:\/\/linux.die.net\/man\/1\/c2ph"}},{"Process":{"Description":"c3270 opens a telnet connection to an IBM host in a console window. It implements RFCs 2355 (TN3270E), 1576 (TN3270) and 1646 (LU name selection), and supports IND$FILE file transfer. If the console is capable of displaying colors, then c3270 emulates an IBM 3279. Otherwise, it emulates a 3278. The full syntax for host is: [ prefix:]...[ LUname@] hostname[: port] Prepending a P: onto hostname causes the connection to go through the telnet-passthru service rather than directly to the host. See PASSTHRU below. Prepending an S: onto hostname removes the \"extended data stream\" option reported to the host. See -tn below for further information. Prepending an N: onto hostname turns off TN3270E support for the session. Prepending an L: onto hostname causes c3270 to first create an SSL tunnel to the host, and then create a TN3270 session inside the tunnel. (This function is supported only if c3270 was built with SSL\/TLS support). Note that TLS-encrypted sessions using the TELNET START-TLS option are negotiated with the host automatically; for these sessions the L: prefix should not be used. A specific LU name to use may be specified by prepending it to the hostname with an '@'. Multiple LU names to try can be separated by commas. An empty LU can be placed in the list with an extra comma. The hostname may optionally be placed inside square-bracket characters '[' and ']'. This will prevent any colon ':' characters in the hostname from being interpreted as indicating option prefixes or port numbers. This allows numeric IPv6 addresses to be used as hostnames. On systems that support the forkpty library call, the hostname may be replaced with -e and a command string. This will cause c3270 to connect to a local child process, such as a shell. The port to connect to defaults to telnet. This can be overridden with the -port option, or by appending a port to the hostname with a colon ':'. (For compatability with previous versions of c3270 and with tn3270(1), the port may also be specified as a second, separate argument.)","Process Name":"c3270","Link":"https:\/\/linux.die.net\/man\/1\/c3270"}},{"Process":{"Description":"Produces a DjVuPhoto encoded image. The input image file inputfilename can be either a portable gray-map ( PGM ) or a portable pix-map ( PPM ). Input images compressed with JPEG are also accepted. It is however suggested to only use high quality JPEG files (low compression ratio, large size) because the wavelet compression will increase the defects already present in highly compressed JPEG files. The program produces a DjVuPhoto file outputfilename. If the output file name is not specified, a default file name will be generated by replacing the input file name suffix by suffix djvu. The main design objective for the DjVu wavelets consisted of allowing progressive rendering and smooth scrolling of large images with limited memory requirements. Decoding functions process the compressed data and update a memory efficient representation of the wavelet coefficients. Imaging function then can quickly render an arbitrary segment of the image using the available data. Both process can be carried out in two threads of execution. This design plays an important role in the DjVu system. We investigated various state-of-the-art wavelet compression schemes. Although these schemes may achieve slightly smaller file sizes, the decoding functions did not even approach our requirements. The IW44 wavelets reach these requirements today and may in the future implement more modern refinements if these refinements can be implemented within our constraints.","Process Name":"c44","Link":"https:\/\/linux.die.net\/man\/1\/c44"}},{"Process":{"Description":"addr2line translates addresses into file names and line numbers. Given an address in an executable or an offset in a section of a relocatable object, it uses the debugging information to figure out which file name and line number are associated with it. The executable or relocatable object to use is specified with the -e option. The default is the file a.out. The section in the relocatable object to use is specified with the -j option. addr2line has two modes of operation. In the first, hexadecimal addresses are specified on the command line, and addr2line displays the file name and line number for each address. In the second, addr2line reads hexadecimal addresses from standard input, and prints the file name and line number for each address on standard output. In this mode, addr2line may be used in a pipe to convert dynamically chosen addresses. The format of the output is FILENAME:LINENO . The file name and line number for each input address is printed on separate lines. If the -f option is used, then each FILENAME:LINENO line is preceded by FUNCTIONNAME which is the name of the function containing the address. If the -i option is used and the code at the given address is present there because of inlining by the compiler then the { FUNCTIONNAME } FILENAME:LINENO information for the inlining function will be displayed afterwards. This continues recursively until there is no more inlining to report. If the -a option is used then the output is prefixed by the input address. If the -p option is used then the output for each input address is displayed on one, possibly quite long, line. If -p is not used then the output is broken up into multiple lines, based on the paragraphs above. If the file name or function name can not be determined, addr2line will print two question marks in their place. If the line number can not be determined, addr2line will print 0.","Process Name":"c6x-linux-gnu-addr2line","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-addr2line"}},{"Process":{"Description":"The GNU ar program creates, modifies, and extracts from archives. An archive is a single file holding a collection of other files in a structure that makes it possible to retrieve the original individual files (called members of the archive). The original files' contents, mode (permissions), timestamp, owner, and group are preserved in the archive, and can be restored on extraction. GNU ar can maintain archives whose members have names of any length; however, depending on how ar is configured on your system, a limit on member-name length may be imposed for compatibility with archive formats maintained with other tools. If it exists, the limit is often 15 characters (typical of formats related to a.out) or 16 characters (typical of formats related to coff). ar is considered a binary utility because archives of this sort are most often used as libraries holding commonly needed subroutines. ar creates an index to the symbols defined in relocatable object modules in the archive when you specify the modifier s. Once created, this index is updated in the archive whenever ar makes a change to its contents (save for the q update operation). An archive with such an index speeds up linking to the library, and allows routines in the library to call each other without regard to their placement in the archive. You may use nm -s or nm --print-armap to list this index table. If an archive lacks the table, another form of ar called ranlib can be used to add just the table. GNU ar can optionally create a thin archive, which contains a symbol index and references to the original copies of the member files of the archives. Such an archive is useful for building libraries for use within a local build, where the relocatable objects are expected to remain available, and copying the contents of each object would only waste time and space. Thin archives are also flattened, so that adding one or more archives to a thin archive will add the elements of the nested archive individually. The paths to the elements of the archive are stored relative to the archive itself. GNU ar is designed to be compatible with two different facilities. You can control its activity using command-line options, like the different varieties of ar on Unix systems; or, if you specify the single command-line option -M, you can control it with a script supplied via standard input, like the MRI \"librarian\" program.","Process Name":"c6x-linux-gnu-ar","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-ar"}},{"Process":{"Description":"GNU as is really a family of assemblers. If you use (or have used) the GNU assembler on one architecture, you should find a fairly similar environment when you use it on another architecture. Each version has much in common with the others, including object file formats, most assembler directives (often called pseudo-ops) and assembler syntax. as is primarily intended to assemble the output of the GNU C compiler \"gcc\" for use by the linker \"ld\". Nevertheless, we've tried to make as assemble correctly everything that other assemblers for the same machine would assemble. Any exceptions are documented explicitly. This doesn't mean as always uses the same syntax as another assembler for the same architecture; for example, we know of several incompatible versions of 680x0 assembly language syntax. Each time you run as it assembles exactly one source program. The source program is made up of one or more files. (The standard input is also a file.) You give as a command line that has zero or more input file names. The input files are read (from left file name to right). A command line argument (in any position) that has no special meaning is taken to be an input file name. If you give as no file names it attempts to read one input file from the as standard input, which is normally your terminal. You may have to type ctl-D to tell as there is no more program to assemble. Use -- if you need to explicitly name the standard input file in your command line. If the source is empty, as produces a small, empty object file. as may write warnings and error messages to the standard error file (usually your terminal). This should not happen when a compiler runs as automatically. Warnings report an assumption made so that as could keep assembling a flawed program; errors report a grave problem that stops the assembly. If you are invoking as via the GNU C compiler, you can use the -Wa option to pass arguments through to the assembler. The assembler arguments must be separated from each other (and the -Wa) by commas. For example: gcc -c -g -O -Wa,-alh,-L file.c This passes two options to the assembler: -alh (emit a listing to standard output with high-level and assembly source) and -L (retain local symbols in the symbol table). Usually you do not need to use this -Wa mechanism, since many compiler command-line options are automatically passed to the assembler by the compiler. (You can call the GNU compiler driver with the -v option to see precisely what options it passes to each compilation pass, including the assembler.)","Process Name":"c6x-linux-gnu-as","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-as"}},{"Process":{"Description":"The C ++ and Java languages provide function overloading, which means that you can write many functions with the same name, providing that each function takes parameters of different types. In order to be able to distinguish these similarly named functions C ++ and Java encode them into a low-level assembler name which uniquely identifies each different version. This process is known as mangling. The c++filt [1] program does the inverse mapping: it decodes (demangles) low-level names into user-level names so that they can be read. Every alphanumeric word (consisting of letters, digits, underscores, dollars, or periods) seen in the input is a potential mangled name. If the name decodes into a C ++ name, the C ++ name replaces the low-level name in the output, otherwise the original word is output. In this way you can pass an entire assembler source file, containing mangled names, through c++filt and see the same source file containing demangled names. You can also use c++filt to decipher individual symbols by passing them on the command line: c++filt <symbol> If no symbol arguments are given, c++filt reads symbol names from the standard input instead. All the results are printed on the standard output. The difference between reading names from the command line versus reading names from the standard input is that command line arguments are expected to be just mangled names and no checking is performed to separate them from surrounding text. Thus for example: c++filt -n _Z1fv will work and demangle the name to \"f()\" whereas: c++filt -n _Z1fv, will not work. (Note the extra comma at the end of the mangled name which makes it invalid). This command however will work: echo _Z1fv, | c++filt -n and will display \"f(),\", i.e., the demangled name followed by a trailing comma. This behaviour is because when the names are read from the standard input it is expected that they might be part of an assembler source file where there might be extra, extraneous characters trailing after a mangled name. For example: .type   _Z1fv, @function","Process Name":"c6x-linux-gnu-c++filt","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-c++filt"}},{"Process":{"Description":"The C preprocessor, often known as cpp, is a macro processor that is used automatically by the C compiler to transform your program before compilation. It is called a macro processor because it allows you to define macros, which are brief abbreviations for longer constructs. The C preprocessor is intended to be used only with C, C ++ , and Objective-C source code. In the past, it has been abused as a general text processor. It will choke on input which does not obey C's lexical rules. For example, apostrophes will be interpreted as the beginning of character constants, and cause errors. Also, you cannot rely on it preserving characteristics of the input which are not significant to C-family languages. If a Makefile is preprocessed, all the hard tabs will be removed, and the Makefile will not work. Having said that, you can often get away with using cpp on things which are not C. Other Algol-ish programming languages are often safe (Pascal, Ada, etc.) So is assembly, with caution. -traditional-cpp mode preserves more white space, and is otherwise more permissive. Many of the problems can be avoided by writing C or C ++ style comments instead of native language comments, and keeping macros simple. Wherever possible, you should use a preprocessor geared to the language you are writing in. Modern versions of the GNU assembler have macro facilities. Most high level programming languages have their own conditional compilation and inclusion mechanism. If all else fails, try a true general text processor, such as GNU M4. C preprocessors vary in some details. This manual discusses the GNU C preprocessor, which provides a small superset of the features of ISO Standard C. In its default mode, the GNU C preprocessor does not do a few things required by the standard. These are features which are rarely, if ever, used, and may cause surprising changes to the meaning of a program which does not expect them. To get strict ISO Standard C, you should use the -std=c90, -std=c99 or -std=c11 options, depending on which version of the standard you want. To get all the mandatory diagnostics, you must also use -pedantic. This manual describes the behavior of the ISO preprocessor. To minimize gratuitous differences, where the ISO preprocessor's behavior does not conflict with traditional semantics, the traditional preprocessor should behave the same way. The various differences that do exist are detailed in the section Traditional Mode. For clarity, unless noted otherwise, references to CPP in this manual refer to GNU CPP .","Process Name":"c6x-linux-gnu-cpp","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-cpp"}},{"Process":{"Description":"dlltool reads its inputs, which can come from the -d and -b options as well as object files specified on the command line. It then processes these inputs and if the -e option has been specified it creates a exports file. If the -l option has been specified it creates a library file and if the -z option has been specified it creates a def file. Any or all of the -e, -l and -z options can be present in one invocation of dlltool. When creating a DLL , along with the source for the DLL , it is necessary to have three other files. dlltool can help with the creation of these files. The first file is a .def file which specifies which functions are exported from the DLL , which functions the DLL imports, and so on. This is a text file and can be created by hand, or dlltool can be used to create it using the -z option. In this case dlltool will scan the object files specified on its command line looking for those functions which have been specially marked as being exported and put entries for them in the .def file it creates. In order to mark a function as being exported from a DLL , it needs to have an -export:<name_of_function> entry in the .drectve section of the object file. This can be done in C by using the asm() operator: asm (\".section .drectve\");\nasm (\".ascii \\\"-export:my_func\\\"\");\n\nint my_func (void) { ... } The second file needed for DLL creation is an exports file. This file is linked with the object files that make up the body of the DLL and it handles the interface between the DLL and the outside world. This is a binary file and it can be created by giving the -e option to dlltool when it is creating or reading in a .def file. The third file needed for DLL creation is the library file that programs will link with in order to access the functions in the DLL (an 'import library'). This file can be created by giving the -l option to dlltool when it is creating or reading in a .def file. If the -y option is specified, dlltool generates a delay-import library that can be used instead of the normal import library to allow a program to link to the dll only as soon as an imported function is called for the first time. The resulting executable will need to be linked to the static delayimp library containing __delayLoadHelper2(), which in turn will import LoadLibraryA and GetProcAddress from kernel32. dlltool builds the library file by hand, but it builds the exports file by creating temporary files containing assembler statements and then assembling these. The -S command line option can be used to specify the path to the assembler that dlltool will use, and the -f option can be used to pass specific flags to that assembler. The -n can be used to prevent dlltool from deleting these temporary assembler files when it is done, and if -n is specified twice then this will prevent dlltool from deleting the temporary object files it used to build the library. Here is an example of creating a DLL from a source file dll.c and also creating a program (from an object file called program.o) that uses that DLL: gcc -c dll.c\ndlltool -e exports.o -l dll.lib dll.o\ngcc dll.o exports.o -o dll.dll\ngcc program.o dll.lib -o program dlltool may also be used to query an existing import library to determine the name of the DLL to which it is associated. See the description of the -I or --identify option.","Process Name":"c6x-linux-gnu-dlltool","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-dlltool"}},{"Process":{"Description":"elfedit updates the ELF header of ELF files which have the matching ELF machine and file types. The options control how and which fields in the ELF header should be updated. elffile... are the ELF files to be updated. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files.","Process Name":"c6x-linux-gnu-elfedit","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-elfedit"}},{"Process":{"Description":"When you invoke GCC , it normally does preprocessing, compilation, assembly and linking. The \"overall options\" allow you to stop this process at an intermediate stage. For example, the -c option says not to run the linker. Then the output consists of object files output by the assembler. Other options are passed on to one stage of processing. Some options control the preprocessor and others the compiler itself. Yet other options control the assembler and linker; most of these are not documented here, since you rarely need to use any of them. Most of the command-line options that you can use with GCC are useful for C programs; when an option is only useful with another language (usually C ++ ), the explanation says so explicitly. If the description for a particular option does not mention a source language, you can use that option with all supported languages. The gcc program accepts options and file names as operands. Many options have multi-letter names; therefore multiple single-letter options may not be grouped: -dv is very different from -d -v. You can mix options and other arguments. For the most part, the order you use doesn't matter. Order does matter when you use several options of the same kind; for example, if you specify -L more than once, the directories are searched in the order specified. Also, the placement of the -l option is significant. Many options have long names starting with -f or with -W---for example, -fmove-loop-invariants, -Wformat and so on. Most of these have both positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"c6x-linux-gnu-gcc","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-gcc"}},{"Process":{"Description":"gcov is a test coverage program. Use it in concert with GCC to analyze your programs to help create more efficient, faster running code and to discover untested parts of your program. You can use gcov as a profiling tool to help discover where your optimization efforts will best affect your code. You can also use gcov along with the other profiling tool, gprof, to assess which parts of your code use the greatest amount of computing time. Profiling tools help you analyze your code's performance. Using a profiler such as gcov or gprof, you can find out some basic performance statistics, such as: \u2022 how often each line of code executes \u2022 what lines of code are actually executed \u2022 how much computing time each section of code uses Once you know these things about how your code works when compiled, you can look at each module to see which modules should be optimized. gcov helps you determine where to work on optimization. Software developers also use coverage testing in concert with testsuites, to make sure software is actually good enough for a release. Testsuites can verify that a program works as expected; a coverage program tests to see how much of the program is exercised by the testsuite. Developers can then determine what kinds of test cases need to be added to the testsuites to create both better testing and a better final product. You should compile your code without optimization if you plan to use gcov because the optimization, by combining some lines of code into one function, may not give you as much information as you need to look for 'hot spots' where the code is using a great deal of computer time. Likewise, because gcov accumulates statistics by line (at the lowest resolution), it works best with a programming style that places only one statement on each line. If you use complicated macros that expand to loops or to other control structures, the statistics are less helpful---they only report on the line where the macro call appears. If your complex macros behave like functions, you can replace them with inline functions to solve this problem. gcov creates a logfile called sourcefile.gcov which indicates how many times each line of a source file sourcefile.c has executed. You can use these logfiles along with gprof to aid in fine-tuning the performance of your programs. gprof gives timing information you can use along with the information you get from gcov. gcov works only on code compiled with GCC . It is not compatible with any other profiling or test coverage mechanism.","Process Name":"c6x-linux-gnu-gcov","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-gcov"}},{"Process":{"Description":"\"gprof\" produces an execution profile of C, Pascal, or Fortran77 programs. The effect of called routines is incorporated in the profile of each caller. The profile data is taken from the call graph profile file (gmon.out default) which is created by programs that are compiled with the -pg option of \"cc\", \"pc\", and \"f77\". The -pg option also links in versions of the library routines that are compiled for profiling. \"Gprof\" reads the given object file (the default is \"a.out\") and establishes the relation between its symbol table and the call graph profile from gmon.out. If more than one profile file is specified, the \"gprof\" output shows the sum of the profile information in the given profile files. \"Gprof\" calculates the amount of time spent in each routine. Next, these times are propagated along the edges of the call graph. Cycles are discovered, and calls into a cycle are made to share the time of the cycle. Several forms of output are available from the analysis. The flat profile shows how much time your program spent in each function, and how many times that function was called. If you simply want to know which functions burn most of the cycles, it is stated concisely here. The call graph shows, for each function, which functions called it, which other functions it called, and how many times. There is also an estimate of how much time was spent in the subroutines of each function. This can suggest places where you might try to eliminate function calls that use a lot of time. The annotated source listing is a copy of the program's source code, labeled with the number of times each line of the program was executed.","Process Name":"c6x-linux-gnu-gprof","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-gprof"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"c6x-linux-gnu-ld","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-ld"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"c6x-linux-gnu-ld.bfd","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-ld.bfd"}},{"Process":{"Description":"nlmconv converts the relocatable i386 object file infile into the NetWare Loadable Module outfile, optionally reading headerfile for NLM header information. For instructions on writing the NLM command file language used in header files, see the linkers section, NLMLINK in particular, of the NLM Development and Tools Overview, which is part of the NLM Software Developer's Kit (\" NLM SDK \"), available from Novell, Inc. nlmconv uses the GNU Binary File Descriptor library to read infile; nlmconv can perform a link step. In other words, you can list more than one object file for input if you list them in the definitions file (rather than simply specifying one input file on the command line). In this case, nlmconv calls the linker for you.","Process Name":"c6x-linux-gnu-nlmconv","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-nlmconv"}},{"Process":{"Description":"GNU nm lists the symbols from object files objfile.... If no object files are listed as arguments, nm assumes the file a.out. For each symbol, nm shows: \u2022 The symbol value, in the radix selected by options (see below), or hexadecimal by default. \u2022 The symbol type. At least the following types are used; others are, as well, depending on the object file format. If lowercase, the symbol is usually local; if uppercase, the symbol is global (external). There are however a few lowercase symbols that are shown for special global symbols (\"u\", \"v\" and \"w\"). \"A\" The symbol's value is absolute, and will not be changed by further linking. \"B\" \"b\" The symbol is in the uninitialized data section (known as BSS ). \"C\" The symbol is common. Common symbols are uninitialized data. When linking, multiple common symbols may appear with the same name. If the symbol is defined anywhere, the common symbols are treated as undefined references. \"D\" \"d\" The symbol is in the initialized data section. \"G\" \"g\" The symbol is in an initialized data section for small objects. Some object file formats permit more efficient access to small data objects, such as a global int variable as opposed to a large global array. \"i\" For PE format files this indicates that the symbol is in a section specific to the implementation of DLLs. For ELF format files this indicates that the symbol is an indirect function. This is a GNU extension to the standard set of ELF symbol types. It indicates a symbol which if referenced by a relocation does not evaluate to its address, but instead must be invoked at runtime. The runtime execution will then return the value to be used in the relocation. \"N\" The symbol is a debugging symbol. \"p\" The symbols is in a stack unwind section. \"R\" \"r\" The symbol is in a read only data section. \"S\" \"s\" The symbol is in an uninitialized data section for small objects. \"T\" \"t\" The symbol is in the text (code) section. \"U\" The symbol is undefined. \"u\" The symbol is a unique global symbol. This is a GNU extension to the standard set of ELF symbol bindings. For such a symbol the dynamic linker will make sure that in the entire process there is just one symbol with this name and type in use. \"V\" \"v\" The symbol is a weak object. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the weak symbol becomes zero with no error. On some systems, uppercase indicates that a default value has been specified. \"W\" \"w\" The symbol is a weak symbol that has not been specifically tagged as a weak object symbol. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the symbol is determined in a system-specific manner without error. On some systems, uppercase indicates that a default value has been specified. \"-\" The symbol is a stabs symbol in an a.out object file. In this case, the next values printed are the stabs other field, the stabs desc field, and the stab type. Stabs symbols are used to hold debugging information. \"?\" The symbol type is unknown, or object file format specific. \u2022 The symbol name.","Process Name":"c6x-linux-gnu-nm","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-nm"}},{"Process":{"Description":"The GNU objcopy utility copies the contents of an object file to another. objcopy uses the GNU BFD Library to read and write the object files. It can write the destination object file in a format different from that of the source object file. The exact behavior of objcopy is controlled by command-line options. Note that objcopy should be able to copy a fully linked file between any two formats. However, copying a relocatable object file between any two formats may not work as expected. objcopy creates temporary files to do its translations and deletes them afterward. objcopy uses BFD to do all its translation work; it has access to all the formats described in BFD and thus is able to recognize most formats without being told explicitly. objcopy can be used to generate S-records by using an output target of srec (e.g., use -O srec). objcopy can be used to generate a raw binary file by using an output target of binary (e.g., use -O binary). When objcopy generates a raw binary file, it will essentially produce a memory dump of the contents of the input object file. All symbols and relocation information will be discarded. The memory dump will start at the load address of the lowest section copied into the output file. When generating an S-record or a raw binary file, it may be helpful to use -S to remove sections containing debugging information. In some cases -R will be useful to remove sections which contain information that is not needed by the binary file. Note---objcopy is not able to change the endianness of its input files. If the input format has an endianness (some formats do not), objcopy can only copy the inputs into file formats that have the same endianness or which have no endianness (e.g., srec). (However, see the --reverse-bytes option.)","Process Name":"c6x-linux-gnu-objcopy","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-objcopy"}},{"Process":{"Description":"objdump displays information about one or more object files. The options control what particular information to display. This information is mostly useful to programmers who are working on the compilation tools, as opposed to programmers who just want their program to compile and work. objfile... are the object files to be examined. When you specify archives, objdump shows information on each of the member object files.","Process Name":"c6x-linux-gnu-objdump","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-objdump"}},{"Process":{"Description":"ranlib generates an index to the contents of an archive and stores it in the archive. The index lists each symbol defined by a member of an archive that is a relocatable object file. You may use nm -s or nm --print-armap to list this index. An archive with such an index speeds up linking to the library and allows routines in the library to call each other without regard to their placement in the archive. The GNU ranlib program is another form of GNU ar; running ranlib is completely equivalent to executing ar -s.","Process Name":"c6x-linux-gnu-ranlib","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-ranlib"}},{"Process":{"Description":"readelf displays information about one or more ELF format object files. The options control what particular information to display. elffile... are the object files to be examined. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files. This program performs a similar function to objdump but it goes into more detail and it exists independently of the BFD library, so if there is a bug in BFD then readelf will not be affected.","Process Name":"c6x-linux-gnu-readelf","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-readelf"}},{"Process":{"Description":"The GNU size utility lists the section sizes---and the total size---for each of the object or archive files objfile in its argument list. By default, one line of output is generated for each object file or each module in an archive. objfile... are the object files to be examined. If none are specified, the file \"a.out\" will be used.","Process Name":"c6x-linux-gnu-size","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-size"}},{"Process":{"Description":"For each file given, GNU strings prints the printable character sequences that are at least 4 characters long (or the number given with the options below) and are followed by an unprintable character. By default, it only prints the strings from the initialized and loaded sections of object files; for other types of files, it prints the strings from the whole file. strings is mainly useful for determining the contents of non-text files.","Process Name":"c6x-linux-gnu-strings","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-strings"}},{"Process":{"Description":"GNU strip discards all symbols from object files objfile. The list of object files may include archives. At least one object file must be given. strip modifies the files named in its argument, rather than writing modified copies under different names.","Process Name":"c6x-linux-gnu-strip","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-strip"}},{"Process":{"Description":"windmc reads message definitions from an input file (.mc) and translate them into a set of output files. The output files may be of four kinds: \"h\" A C header file containing the message definitions. \"rc\" A resource file compilable by the windres tool. \"bin\" One or more binary files containing the resource data for a specific message language. \"dbg\" A C include file that maps message id's to their symbolic name. The exact description of these different formats is available in documentation from Microsoft. When windmc converts from the \"mc\" format to the \"bin\" format, \"rc\", \"h\", and optional \"dbg\" it is acting like the Windows Message Compiler.","Process Name":"c6x-linux-gnu-windmc","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-windmc"}},{"Process":{"Description":"windres reads resources from an input file and copies them into an output file. Either file may be in one of three formats: \"rc\" A text format read by the Resource Compiler. \"res\" A binary format generated by the Resource Compiler. \"coff\" A COFF object or executable. The exact description of these different formats is available in documentation from Microsoft. When windres converts from the \"rc\" format to the \"res\" format, it is acting like the Windows Resource Compiler. When windres converts from the \"res\" format to the \"coff\" format, it is acting like the Windows \"CVTRES\" program. When windres generates an \"rc\" file, the output is similar but not identical to the format expected for the input. When an input \"rc\" file refers to an external filename, an output \"rc\" file will instead include the file contents. If the input or output format is not specified, windres will guess based on the file name, or, for the input file, the file contents. A file with an extension of .rc will be treated as an \"rc\" file, a file with an extension of .res will be treated as a \"res\" file, and a file with an extension of .o or .exe will be treated as a \"coff\" file. If no output file is specified, windres will print the resources in \"rc\" format to standard output. The normal use is for you to write an \"rc\" file, use windres to convert it to a COFF object file, and then link the COFF file into your application. This will make the resources described in the \"rc\" file available to Windows.","Process Name":"c6x-linux-gnu-windres","Link":"https:\/\/linux.die.net\/man\/1\/c6x-linux-gnu-windres"}},{"Process":{"Description":"The c99 utility is an interface to the standard C compilation system; it shall accept source code conforming to the ISO C standard. The system conceptually consists of a compiler and link editor. The files referenced by operands shall be compiled and linked to produce an executable file. (It is unspecified whether the linking occurs entirely within the operation of c99; some implementations may produce objects that are not fully resolved until the file is executed.) If the -c option is specified, for all pathname operands of the form file .c, the files: $(basename pathname .c).o shall be created as the result of successful compilation. If the -c option is not specified, it is unspecified whether such .o files are created or deleted for the file .c operands. If there are no options that prevent link editing (such as -c or -E), and all operands compile and link without error, the resulting executable file shall be written according to the -o outfile option (if present) or to the file a.out. The executable file shall be created as specified in File Read, Write, and Creation, except that the file permission bits shall be set to: S_IRWXO | S_IRWXG | S_IRWXU and the bits specified by the umask of the process shall be cleared.","Process Name":"c99","Link":"https:\/\/linux.die.net\/man\/1\/c99"}},{"Process":{"Description":"The ca command is a minimal CA application. It can be used to sign certificate requests in a variety of forms and generate CRLs it also maintains a text database of issued certificates and their status. The options descriptions will be divided into each purpose.","Process Name":"ca","Link":"https:\/\/linux.die.net\/man\/1\/ca"}},{"Process":{"Description":"The CA .pl script is a perl script that supplies the relevant command line arguments to the openssl command for some common certificate operations. It is intended to simplify the process of certificate creation and management by the use of some simple options.","Process Name":"ca.pl","Link":"https:\/\/linux.die.net\/man\/1\/ca.pl"}},{"Process":{"Description":"Cabal-rpm generates RPM .spec files from Haskell Cabal package. If no I]path-or-pkg] is specified, cabal-rpm looks for a .cabal file in the current directory. Otherwise, it will look for I]path-or-pkg]. If the argument is a directory then it will look there for a .cabal file. If the argument is a path to a .cabal or .tar.gz file then it will use it. Otherwise if there is no [aq]\/[aq] in the argument and it does not exist then cabal-rpm will try to unpack the package and use its .cabal file. Cabal-rpm uses a temporary directory for unpackaging tarballs or packages. Cabal-rpm then parses the above specified .cabal file and uses it to generate a .spec file that can be built. If a .spec already exists, cabal-rpm output to .spec.cblrpm instead. The cabal-rpm-diff command can be used in the same way to output a diff of .spec and .spec.cblrpm directly.","Process Name":"cabal-rpm","Link":"https:\/\/linux.die.net\/man\/1\/cabal-rpm"}},{"Process":{"Description":"cabextract is a program that un-archives files in the Microsoft cabinet file format (.cab) or any binary file which contains an embedded cabinet file (frequently found in .exe files). cabextract will extract all files from all cabinet files specified on the command line. To extract a multi-part cabinet consisting of several files, only the first cabinet file needs to be given as an argument to cabextract as it will automatically look for the remaining files. To prevent cabextract from extracting cabinet files you did not specify, use the -s option.","Process Name":"cabextract","Link":"https:\/\/linux.die.net\/man\/1\/cabextract"}},{"Process":{"Description":"caca-config is a tool that is used to configure and determine the compiler and linker flags that should be used to compile and link progams, libraries, and plugins that use libcaca. The use of caca-config is deprecated. The more generic tool pkg-config should be used instead.","Process Name":"caca-config","Link":"https:\/\/linux.die.net\/man\/1\/caca-config"}},{"Process":{"Description":"This manual page documents briefly the cacademo and cacafire programs. cacademo displays ASCII art effects with animated transitions: metaballs, moire pattern of concentric circles, old school plasma, Matrix-like scrolling. cacafire displays burning ASCII art flames.","Process Name":"cacademo","Link":"https:\/\/linux.die.net\/man\/1\/cacademo"}},{"Process":{"Description":"This manual page documents briefly the cacademo and cacafire programs. cacademo displays ASCII art effects with animated transitions: metaballs, moire pattern of concentric circles, old school plasma, Matrix-like scrolling. cacafire displays burning ASCII art flames.","Process Name":"cacafire","Link":"https:\/\/linux.die.net\/man\/1\/cacafire"}},{"Process":{"Description":"cacaplay plays libcaca animation files. These files can be created by any libcaca program by setting the CACA_DRIVER environment variable to raw and storing the program's standard output. If no file argument is provided or '-' is used, cacaplay will read its data from the standard input.","Process Name":"cacaplay","Link":"https:\/\/linux.die.net\/man\/1\/cacaplay"}},{"Process":{"Description":"cacaserver reads libcaca animation files in its standard input and serves them as ANSI art on network port 51914. These animations can be created by any libcaca program by setting the CACA_DRIVER environment variable to raw and piping the program's standard output to cacaserver. Clients can then connect to port 51914 using telnet or netcat to see the output.","Process Name":"cacaserver","Link":"https:\/\/linux.die.net\/man\/1\/cacaserver"}},{"Process":{"Description":"cacaview is a lightweight text mode image viewer. It renders images using colour ASCII characters. It is a powerful add-on to famous console programs such as the mutt email client, the slrn newsreader and the links or w3m web browsers. cacaview can load the most widespread image formats: PNG, JPEG, GIF, PNG, BMP etc. You can zoom and scroll the image around for more details, and choose four different dithering modes. All commands are accessible through a single key press.","Process Name":"cacaview","Link":"https:\/\/linux.die.net\/man\/1\/cacaview"}},{"Process":{"Description":"cache-clean is a tool for administrators of ARC server installations to safely remove A-REX cache data and to provide an overview of the contents of the cache. It is used by the A-REX to automatically manage cache contents. There are two modes of operation - printing statistics and deleting files. If -s is used, then statistics are printed on each cache. If -m and -M are used then files in each cache are deleted if the free space on the file system is less than that given by -m, in the order of least recently accessed, until the free space is equal to what is specified by -M. If -E is used, then all files accessed less recently than the given time are deleted. -E can be used in combination with -m and -M but deleting files using -E is carried out first. If after this the cache free space is still less than that given by -m then cleaning according to those options is performed. Cache directories are given by dir1, dir2.. or taken from the config file specified by -c or the ARC_CONFIG environment variable. -h - print short help -s - print cache statistics, without deleting anything. The output displays for each cache the number of deletable (and locked) files, the total size of these files, the percentage usage of the file system in which the cache is stored, and a histogram of access times of the files in the cache. -m - the minimum free space (as % of the file system) at which to start cleaning -M - the minimum free space (as % of the file system) at which to stop cleaning -E - files accessed less recently than the given time period will be deleted. Example values of this option are 1800, 90s, 24h, 30d. The default when no suffix is given is seconds. -D - debug level. Possible values are FATAL, ERROR, WARNING, INFO, VERBOSE or DEBUG. Default level is INFO. -c - path to an A-REX config file, xml or ini format This tool is run periodically by the A-REX to keep the size of each cache within the limits specified in the configuration file. Therefore cleaning should not be performed manually, unless the cache size needs to be reduced temporarily. For performance reasons it may however be desirable to run cache-clean independently on the machine hosting the cache file system, if this is different from the A-REX host. The most useful function for administrators is to give an overview of the contents of the cache, using the -s option. Within each cache directory specified in the configuration file, there is a subdirectory for data (data\/) and one for per-job hard links (joblinks\/). See the A-REX Administration Guide for more details. cache-clean should only operate on the data subdirectory, therefore when giving dir arguments they should be the top level cache directory. cache-clean will then automatically only look at files within the data directory.","Process Name":"cache-clean","Link":"https:\/\/linux.die.net\/man\/1\/cache-clean"}},{"Process":{"Description":"-h - print short help -c - configuration file from which to read cache information. The ARC_CONFIG environment variable can be set in place of this option. cache-list is used to list all files present in each cache or, given a list of URLs as arguments, shows the location of each URL in the cache if present. If no arguments are given, it prints to stdout each cache directory specified in the configuration file then a list of files in each cache directory and the corresponding URLs of their source in the format: url filename If arguments are given, each cache is checked for the existence of each URL. If a URL is present in the cache then the URL and filename are printed to stdout in the above format. This tool can be useful for finding out if a certain URL is stored in the cache, or simply to give a list of all URLs in the cache.","Process Name":"cache-list","Link":"https:\/\/linux.die.net\/man\/1\/cache-list"}},{"Process":{"Description":"cadaver supports file upload, download, on-screen display, namespace operations (move and copy), collection creation and deletion, and locking operations. Its operation is similar to the standard BSD ftp(1) client and the Samba Project's smbclient(1). A user familiar with these tools should be quite comfortable with cadaver.","Process Name":"cadaver","Link":"https:\/\/linux.die.net\/man\/1\/cadaver"}},{"Process":{"Description":"This draws Escher's \"Impossible Cage\", a 3d analog of a moebius strip, and rotates it in three dimensions.","Process Name":"cage","Link":"https:\/\/linux.die.net\/man\/1\/cage"}},{"Process":{"Description":"Cal displays a simple calendar. If arguments are not specified, the current month is displayed. The options are as follows:       -1'        Display single month output.  (This is the default.) -3' Display prev\/current\/next month output. -s' Display Sunday as the first day of the week. -m' Display Monday as the first day of the week. -j' Display Julian dates (days one-based, numbered from January 1). -y' Display a calendar for the current year. -V' Display version information and exit. A single parameter specifies the year (1 - 9999) to be displayed; note the year must be fully specified: ''cal 89'' will not display a calendar for 1989. Two parameters denote the month (1 - 12) and year. Three parameters denote the day (1-31), month and year, and the day will be highlighted if the calendar is displayed on a terminal. If no parameters are specified, the current month's calendar is displayed. A year starts on Jan 1. The first day of the week is determined by the locale. The Gregorian Reformation is assumed to have occurred in 1752 on the 3rd of September. By this time, most countries had recognized the reformation (although a few did not recognize it until the early 1900's.) Ten days following that date were eliminated by the reformation, so the calendar for that month is a bit unusual.","Process Name":"cal","Link":"https:\/\/linux.die.net\/man\/1\/cal"}},{"Process":{"Description":"A source or destination cal3d file is either a skeleton (.xsf or .csf), a mesh (.xmf or .cmf), an animation (.xaf or .caf) or a material (.xrf or .crf). The files with an extension starting with an x denote XML files (.xsf, .xmf, .xaf, xrf) and the files with an extension starting with a c denot binary files (.csf, .cmf, .caf, .crf). If the source is a file of a given type, the destination is expected to be a file of the same type.","Process Name":"cal3d_converter","Link":"https:\/\/linux.die.net\/man\/1\/cal3d_converter"}},{"Process":{"Description":"Calamaris is used to produce statistical output from Squid, NetCache, Inktomi Traffic Server, Oops! proxy server, Compaq Tasksmart, Cisco Content Engines or related Proxy log files. The resulting output can be ascii or html. It is possible to cache calculated data in a file to use them in later runs. This manual page describes the options of Calamaris and gives a few examples.","Process Name":"calamaris","Link":"https:\/\/linux.die.net\/man\/1\/calamaris"}},{"Process":{"Description":"The FOX Calculator is a simple desktop calculator geared toward the programmer. It supports not only a full complement scientific functions, but also common operations that programmers need, such as bitwise operations, bitwise shifting, and base-2 logarithm and exponents, and numeric conversion between hexadecimal, octal, binary, and decimal. The FOX Calculator implements correct operator precedences, so expressions like 2+3*5 yield the correct result, which is 17, and not 25. Also featured is a constant memory, which permanently stores its value even if you exit the calculator and restart it later.","Process Name":"calculator","Link":"https:\/\/linux.die.net\/man\/1\/calculator"}},{"Process":{"Description":"Calcurse is a text-based calendar and scheduling application. It helps keeping track of events, appointments and everyday tasks. A configurable notification system reminds user of upcoming deadlines, and the curses based interface can be customized to suit user needs. All of the commands are documented within an online help system.","Process Name":"calcurse","Link":"https:\/\/linux.die.net\/man\/1\/calcurse"}},{"Process":{"Description":"The calendar utility checks the current directory or the directory specified by the CALENDAR_DIR environment variable for a file named calendar and displays lines that begin with either today's date or tomorrow's. On Fridays, events on Friday through Monday are displayed. The options are as follows:        -A num Print lines from today and next num days (forward, future). -a' Process the ''calendar'' files of all users and mail the results to them. This requires superuser privileges. -B num Print lines from today and previous num days (backward, past). -b' Enforce special date calculation mode for KOI8 calendars. -f calendarfile Use calendarfile as the default calendar file. -t [[[cc]yy][mm]]dd Act like the specified value is ''today'' instead of using the current date. To handle calendars in your national code table you can specify ''LANG=<locale_name>'' in the calendar file as early as possible. To handle national Easter names in the calendars, ''Easter=<national_name>'' (for Catholic Easter) or ''Paskha=<national_name>'' (for Orthodox Easter) can be used. The ''CALENDAR'' variable can be used to specify the style. Only 'Julian' and 'Gregorian' styles are currently supported. Use ''CALENDAR='' to return to the default (Gregorian). To enforce special date calculation mode for Cyrillic calendars you should specify ''LANG=<local_name>'' and ''BODUN=<bodun_prefix>'' where <local_name> can be ru_RU.KOI8-R, uk_UA.KOI8-U or by_BY.KOI8-B. Other lines should begin with a month and day. They may be entered in almost any format, either numeric or as character strings. If proper locale is set, national months and weekdays names can be used. A single asterisk ('*') matches every month. A day without a month matches that day of every week. A month without a day matches the first of that month. Two numbers default to the month followed by the day. Lines with leading tabs default to the last entered date, allowing multiple line specifications for a single date. ''Easter'' (may be followed by a positive or negative integer) is Easter for this year. ''Paskha'' (may be followed by a positive or negative integer) is Orthodox Easter for this year. Weekdays may be followed by ''-4'' ... ''+5'' (aliases last, first, second, third, fourth) for moving events like ''the last Monday in April''. By convention, dates followed by an asterisk ('*') are not fixed, i.e., change from year to year. Day descriptions start after the first <tab> character in the line; if the line does not contain a <tab> character, it isn't printed out. If the first character in the line is a <tab> character, it is treated as the continuation of the previous description. The calendar file is preprocessed by cpp(1), allowing the inclusion of shared files such as company holidays or meetings. If the shared file is not referenced by a full pathname, cpp(1) searches in the current (or home) directory first, and then in the directory \/usr\/share\/calendar. Empty lines and lines protected by the C commenting syntax (\/* ... *\/) are ignored. Some possible calendar entries (a \\t sequence denotes a <tab> character): LANG=C Easter=Ostern #include <calendar.usholiday> #include <calendar.birthday> 6\/15\\tJune 15 (if ambiguous, will default to month\/day). Jun. 15\\tJune 15. 15 June\\tJune 15. Thursday\\tEvery Thursday. June\\tEvery June 1st. 15 *\\t15th of every month. May Sun+2\\tsecond Sunday in May (Muttertag) 04\/SunLast\\tlast Sunday in April, \\tsummer time in Europe Easter\\tEaster Ostern-2\\tGood Friday (2 days before Easter) Paskha\\tOrthodox Easter","Process Name":"calendar","Link":"https:\/\/linux.die.net\/man\/1\/calendar"}},{"Process":{"Description":"Calife requests user's own password for becoming login (or root, if no login is provided), and switches to that user and group ID after verifying proper rights to do so. A shell is then executed. If calife is executed by root, no password is requested and a shell with the appropriate user ID is executed. The invoked shell is the user's own except when a shell is specified in the configuration file calife.auth. If ''-'' is specified on the command line, user's profile files are read as if it was a login shell. This is not the traditional behavior of su. Only users specified in calife.auth can use calife to become another one with this method. You can specify in the calife.auth file the list of logins allowed for users when using calife. See calife.auth(5) for more details. calife.auth is installed as \/etc\/calife.auth.","Process Name":"calife","Link":"https:\/\/linux.die.net\/man\/1\/calife"}},{"Process":{"Description":"","Process Name":"caller","Link":"https:\/\/linux.die.net\/man\/1\/caller"}},{"Process":{"Description":"Callgrind is a profiling tool similar to gprof, but by being able to observe a program run in great detail - using Valgrind - it can give much more information. The binary does not have to be prepared for profiling with callgrind in any special way. Still, it is recommand to compile with debug information. Callgrind builds up the call graph of a program while it is running, and optionally does cache simulation. The collected profiling data can be stored into an output file multiple times in a program run, optionally separately for every thread in the case of multithreaded code. For interactive inspection and control, see callgrind_control. The data produced (callgrind.out.PID) can be analysed with callgrind_annotate or better with the graphical profile visualization KCachegrind. Further documentation can be found in HTML format; see below.","Process Name":"callgrind","Link":"https:\/\/linux.die.net\/man\/1\/callgrind"}},{"Process":{"Description":"callgrind_annotate takes an output file produced by the Valgrind tool Callgrind and prints the information in an easy-to-read form.","Process Name":"callgrind_annotate","Link":"https:\/\/linux.die.net\/man\/1\/callgrind_annotate"}},{"Process":{"Description":"callgrind_control controls programs being run by the Valgrind tool Callgrind. When a pid\/program name argument is not specified, all applications currently being run by Callgrind on this system will be used for actions given by the specified option(s). The default action is to give some brief information about the applications being run by Callgrind.","Process Name":"callgrind_control","Link":"https:\/\/linux.die.net\/man\/1\/callgrind_control"}},{"Process":{"Description":"This program is part of Netpbm(1). cameratopam converts from any of dozens of raw camera image formats to PAM. Digital still cameras often can produce images in a special raw format in addition to something more standard such as TIFF or JFIF (JPEG). Software supplied with the camera allows you to manipulate the image using information which is lost when the camera converts to the common format. A particular camera model often has a unique raw format.","Process Name":"cameratopam","Link":"https:\/\/linux.die.net\/man\/1\/cameratopam"}},{"Process":{"Description":"cancel cancels existing print jobs. The -a option will remove all jobs from the specified destination.","Process Name":"cancel-cups","Link":"https:\/\/linux.die.net\/man\/1\/cancel-cups"}},{"Process":{"Description":"cannacheck displays the problems concerning use of Canna and the related information. It examines the following items: - Customize file to be used - Romaji-to-Kana conversion dictionary to be used - Destination to which the server is to be connected. Is it connectable? - Dictionaries to be mounted. Are they mountable? - Errors in the customize file The problem(s) are displayed on the standard output if found.","Process Name":"cannacheck","Link":"https:\/\/linux.die.net\/man\/1\/cannacheck"}},{"Process":{"Description":"cannakill terminates operation of Kana-Kanji conversion server cannaserver(1M). This causes the server to delete the socket and close all open dictionaries before terminating its processing. The user who has started cannaserver(1M) or the super user can only use cannakill(1M) to terminate operation of canncannaer.","Process Name":"cannakill","Link":"https:\/\/linux.die.net\/man\/1\/cannakill"}},{"Process":{"Description":"cannaserver(1M) provides the Kana-Kanji conversion service. Most commonly this daemon starts at daemon bootup framework like \/etc\/rc depending on your system, so you do not usually start it up manually. By default, cannaserver(1M) only accepts connections from clients at the same host where the server is running (via UNIX domain socket). You can make it accessible from other hosts via TCP by using option -inet (or -inet6). Whether you use this option or not, you can get access control based on user and host using \/etc\/hosts.canna. cannaserver(1M) immediately forks and gets into the background after it starts. You do not need to use '&' explicitly to make it run in the background. When it starts, cannaserver(1M) creates the UNIX domain socket \/var\/run\/.iroha_unix\/IROHA[:num] to communicate clients. The socket is deleted automatically if cannaserver(1M) terminates normally. The socket may exist while cannaserver(1M) is not running (because, for example, cannaserver(1M) terminated abnormally). In this case, delete the file manually or you cannot start it. \/var\/lib\/canna\/dic\/*\/dics.dir includes the list of dictionaries which are available to clients. Users (i.e. clients) can choose which to use and specify them in ~\/.canna.","Process Name":"cannaserver","Link":"https:\/\/linux.die.net\/man\/1\/cannaserver"}},{"Process":{"Description":"cannastat(1) displays information about the Kana-Kanji conversion server cannaserver(1M). The information includes the following: the name of the machine on which the connected server exists, the cannaserver(1M)'s version, the number of clients connecting to the server, the name of the user connecting to the server, its user number, the socket number, the number of contexts, the time of connection to the server, the time of use of the server, the idle time, the name of the host under which the client exists, and the frequency of using each protocol. If cannastat(1) is called without options, the following is displayed: Connected to machine1\nCanna Server (Ver. 2.1)\nTotal connecting clients 1\nUSER_NAME    ID   NO U_CX          C_TIME   U_TIME   I_TIME  HOST_NAME\nkon           5    0    4  Tue 21  8:49am       11    12:48   machine2 The items represent the following: USER_NAME User name ID Socket number NO User number U_CX Context number C_TIME Time of connection (h:min) U_TIME User time (h:min:s) I_TIME Idle time (h:min) HOST_NAME Name of the host under which the client exists","Process Name":"cannastat","Link":"https:\/\/linux.die.net\/man\/1\/cannastat"}},{"Process":{"Description":"Capinfos is a program that reads one or more capture files and returns some or all available statistics of each <infile>. The user specifies which statistics to report by specifying flags corresponding to the statistic. If no flags are specified, Capinfos will report all statistics available. Capinfos is able to detect and read the same capture files that are supported by Wireshark. The input files don't need a specific filename extension; the file format and an optional gzip compression will be automatically detected. Near the beginning of the DESCRIPTION section of wireshark(1) or <http:\/\/www.wireshark.org\/docs\/man-pages\/wireshark.html> is a detailed description of the way Wireshark handles this, which is the same way Capinfos handles this.","Process Name":"capinfos","Link":"https:\/\/linux.die.net\/man\/1\/capinfos"}},{"Process":{"Description":"Linux capability support and use can be explored and constrained with this tool. This tool provides a handy wrapper for certain types of capability testing and environment creation. It also provides some debugging features useful for summarizing capability state.","Process Name":"capsh","Link":"https:\/\/linux.die.net\/man\/1\/capsh"}},{"Process":{"Description":"captoinfo looks in file for termcap descriptions. For each one found, an equivalent terminfo description is written to standard output. Termcap tc capabilities are translated directly to terminfo use capabilities. If no file is given, then the environment variable TERMCAP is used for the filename or entry. If TERMCAP is a full pathname to a file, only the terminal whose name is specified in the environment variable TERM is extracted from that file. If the environment variable TERMCAP is not set, then the file \/usr\/share\/terminfo is read. -v print out tracing information on standard error as the program runs. -V print out the version of the program in use on standard error and exit. -1 cause the fields to print out one to a line. Otherwise, the fields will be printed several to a line to a maximum width of 60 characters. -w change the output to width characters.","Process Name":"captoinfo","Link":"https:\/\/linux.die.net\/man\/1\/captoinfo"}},{"Process":{"Description":"Print a reference card of the PROGRAMs thanks to their inline help.","Process Name":"card","Link":"https:\/\/linux.die.net\/man\/1\/card"}},{"Process":{"Description":"card_eventmgr is a smart card monitoring tool that listen to the status of the card reader and dispatch actions on several events. card_eventmgr can be used for several actions: like screen lock on card removal. Three events are supported: card insertion, card removal and timeout on removed card. Actions are specified in a configuration file.","Process Name":"card_eventmgr","Link":"https:\/\/linux.die.net\/man\/1\/card_eventmgr"}},{"Process":{"Description":"The cardos-tool utility is used to display information about smart cards and similar security tokens based on Siemens Card\/OS M4.","Process Name":"cardos-tool","Link":"https:\/\/linux.die.net\/man\/1\/cardos-tool"}},{"Process":{"Description":"cas is a tool used to automatically configure an environment for viewing coredumps.","Process Name":"cas","Link":"https:\/\/linux.die.net\/man\/1\/cas"}},{"Process":{"Description":"cas-admin is a tool used to build necessary data for use with cas.","Process Name":"cas-admin","Link":"https:\/\/linux.die.net\/man\/1\/cas-admin"}},{"Process":{"Description":"","Process Name":"case","Link":"https:\/\/linux.die.net\/man\/1\/case"}},{"Process":{"Description":"The program downloads RSS enclosures, for example podcasts, from RSS feeds. RSS feeds are assigned channel identifiers in the configuration file .castgetrc. Channels to be processed by castget are indicated by supplying the channel identifiers as arguments. If no channel identifiers are provided, all available channels are processed. If run without any options, castget will perform the default action on all channels to be processed. The default action is to download any enclosure not already downloaded. Other actions may be performed by supplying one or more options as arguments.","Process Name":"castget","Link":"https:\/\/linux.die.net\/man\/1\/castget"}},{"Process":{"Description":"Concatenate FILE(s), or standard input, to standard output. -A, --show-all equivalent to -vET -b, --number-nonblank number nonempty output lines -e equivalent to -vE -E, --show-ends display $ at end of each line -n, --number number all output lines -s, --squeeze-blank suppress repeated empty output lines -t equivalent to -vT -T, --show-tabs display TAB characters as ^I -u (ignored) -v, --show-nonprinting use ^ and M- notation, except for LFD and TAB --help display this help and exit --version output version information and exit With no FILE, or when FILE is -, read standard input.","Process Name":"cat","Link":"https:\/\/linux.die.net\/man\/1\/cat"}},{"Process":{"Description":"rancid is a perl(1) script which uses the login scripts (see clogin(1)) to login to a device, execute commands to display the configuration, etc, then filters the output for formatting, security, and so on. rancid's product is a file with the name of it's last argument plus the suffix .new. For example, hostname.new. There are complementary scripts for other platforms and\/or manufacturers that are supported by rancid(1). Briefly, these are: agmrancid Cisco Anomaly Guard Module (AGM) arancid Alteon WebOS switches arrancid Arista Networks devices brancid Bay Networks (nortel) cat5rancid Cisco catalyst switches cssrancid Cisco content services switches erancid ADC-kentrox EZ-T3 mux f10rancid Force10 f5rancid F5 BigIPs fnrancid Fortinet Firewalls francid Foundry and HP procurve OEMs of Foundry hrancid HP Procurve Switches htranicd Hitachi Routers jerancid Juniper Networks E-series jrancid Juniper Networks mrancid MRTd mrvrancid MRV optical switches nrancid Netscreen firewalls nsrancid Netscaler nxrancid Cisco Nexus boxes prancid Procket Networks rivrancid Riverstone rrancid Redback srancid SMC switch (some Dell OEMs) trancid Netopia sDSL\/T1 routers tntrancid Lucent TNT xrancid Extreme switches xrrancid Cisco IOS-XR boxes zrancid Zebra routing software The command-line options are as follows: -V Prints package name and version strings. -d Display debugging information. -l Display somewhat less debugging information. -f rancid should interpret the next argument as a filename which contains the output it would normally collect from the device ( hostname) with clogin(1).","Process Name":"cat5rancid","Link":"https:\/\/linux.die.net\/man\/1\/cat5rancid"}},{"Process":{"Description":"The \"catalyst.pl\" script bootstraps a Catalyst application, creating a directory structure populated with skeleton files. The application name must be a valid Perl module name. The name of the directory created is formed from the application name supplied, with double colons replaced with hyphens (so, for example, the directory for \"My::App\" is \"My-App\"). Using the example application name \"My::App\", the application directory will contain the following items: README a skeleton README file, which you are encouraged to expand on Changes a changes file with an initial entry for the creation of the application Makefile.PL Makefile.PL uses the \"Module::Install\" system for packaging and distribution of the application. lib contains the application module (\"My\/App.pm\") and subdirectories for model, view, and controller components (\"My\/App\/M\", \"My\/App\/V\", and \"My\/App\/C\"). root root directory for your web document content. This is left empty. script a directory containing helper scripts: \"myapp_create.pl\" helper script to generate new component modules \"myapp_server.pl\" runs the generated application within a Catalyst test server, which can be used for testing without resorting to a full-blown web server configuration. \"myapp_cgi.pl\" runs the generated application as a CGI script \"myapp_fastcgi.pl\" runs the generated application as a FastCGI script \"myapp_test.pl\" runs an action of the generated application from the command line. t test directory The application module generated by the \"catalyst.pl\" script is functional, although it reacts to all requests by outputting a friendly welcome screen.","Process Name":"catalyst.pl","Link":"https:\/\/linux.die.net\/man\/1\/catalyst.pl"}},{"Process":{"Description":"catdic downloads a dictionary file to the current directory. The file downloaded is a dictionary file the dictionary name of which is remote-dic. It is in the user dictionary directory of the machine on which cannaserver(1M) is in operation. (Hereafter, this machine is called the remote host.)","Process Name":"catdic","Link":"https:\/\/linux.die.net\/man\/1\/catdic"}},{"Process":{"Description":"catdoc behaves much like cat(1) but it reads MS-Word file and produces human-readable text on standard output. Optionally it can use latex(1) escape sequences for characters which have special meaning for LaTeX. It also makes some effort to recognize MS-Word tables, although it never tries to write correct headers for LaTeX tabular environment. Additional output formats, such is HTML can be easily defined. catdoc doesn't attempt to extract formatting information other than tables from MS-Word document, so different output modes means mainly that different characters should be escaped and different ways used to represent characters, missing from output charset. See CHARACTER SUBSTITUTION below catdoc uses internal unicode(4) representation of text, so it is able to convert texts when charset in source document doesn't match charset on target system. See CHARACTER SETS below. If no file names supplied, catdoc processes its standard input unless it is terminal. It is unlikely that somebody could type Word document from keyboard, so if catdoc invoked without arguments and stdin is not redirected, it prints brief usage message and exits. Processing of standard input (even among other files) can be forced using dash '-' as file name. By default, catdoc wraps lines which are more than 72 chars long and separates paragraphs by blank lines. This behavior can be turned of by -w switch. In wide mode catdoc prints each paragraph as one long line, suitable for import into word processors which perform word wrapping theirselves.","Process Name":"catdoc","Link":"https:\/\/linux.die.net\/man\/1\/catdoc"}},{"Process":{"Description":"catior displays the components of the stringified object reference supplied to it.","Process Name":"catior","Link":"https:\/\/linux.die.net\/man\/1\/catior"}},{"Process":{"Description":"catppt reads MS-PowerPoint presentations and dumps its content to stdout.","Process Name":"catppt","Link":"https:\/\/linux.die.net\/man\/1\/catppt"}},{"Process":{"Description":"This program acts almost identicly like the cat(1) program, with the exception that all output is redirected to \/dev\/speech.","Process Name":"catspeech","Link":"https:\/\/linux.die.net\/man\/1\/catspeech"}},{"Process":{"Description":"cb_console_runner is part of the Code::Blocks IDE. It is used to launch console programs and wait for them to finish. When the console program is finished, cb_console_runner displays \"Press ENTER to continue\". That's its sole job.","Process Name":"cb_console_runner","Link":"https:\/\/linux.die.net\/man\/1\/cb_console_runner"}},{"Process":{"Description":"cb_share_config allows you to share (import\/export) parts of Code::Blocks configuration. It is useful, for example, when you want to partially transfer settings between computers or even between different configurations.","Process Name":"cb_share_config","Link":"https:\/\/linux.die.net\/man\/1\/cb_share_config"}},{"Process":{"Description":"ccache is a compiler cache. It speeds up recompilation by caching the result of previous compilations and detecting when the same compilation is being done again. Supported languages are C, C++, Objective-C and Objective-C++. ccache has been carefully written to always produce exactly the same compiler output that you would get without the cache. The only way you should be able to tell that you are using ccache is the speed. Currently known exceptions to this goal are listed under BUGS. If you ever discover an undocumented case where ccache changes the output of your compiler, please let us know. Features \u2022 Keeps statistics on hits\/misses. \u2022 Automatic cache size management. \u2022 Can cache compilations that generate warnings. \u2022 Easy installation. \u2022 Low overhead. \u2022 Optionally uses hard links where possible to avoid copies. \u2022 Optionally compresses files in the cache to reduce disk space. Limitations \u2022 Only knows how to cache the compilation of a single C\/C++\/Objective-C\/Objective-C++ file. Other types of compilations (multi-file compilation, linking, etc) will silently fall back to running the real compiler. \u2022 Only works with GCC and compilers that behave similar enough. \u2022 Some compiler flags are not supported. If such a flag is detected, ccache will silently fall back to running the real compiler.","Process Name":"ccache","Link":"https:\/\/linux.die.net\/man\/1\/ccache"}},{"Process":{"Description":"ccache-swig is a compiler cache. It speeds up re-compilation of C\/C++\/SWIG code by caching previous compiles and detecting when the same compile is being done again. ccache-swig is ccache plus support for SWIG. ccache and ccache-swig are used interchangeably in this document.","Process Name":"ccache-swig","Link":"https:\/\/linux.die.net\/man\/1\/ccache-swig"}},{"Process":{"Description":"\"ccconfig\" will try to determine a usable configuration for Convert::Binary::C from testing a compiler executable. It is not necessary that the binaries generated by the compiler can be executed, so \"ccconfig\" can also be used for cross-compilers. This tool is still experimental, and you should neither rely on its output without checking, nor expect it to work in your environment.","Process Name":"ccconfig","Link":"https:\/\/linux.die.net\/man\/1\/ccconfig"}},{"Process":{"Description":"Hercules support tool used to assist in the diagnosis of CCKD DASD problems. Operates on Hercules CCKD DASD volumes in read-only mode, and displays various information contained in CCKD DASD files. Effective usage requires internal knowledge of Hercules CCKD DASD files.","Process Name":"cckddiag","Link":"https:\/\/linux.die.net\/man\/1\/cckddiag"}},{"Process":{"Description":"The \"ccmake\" executable is the CMake curses interface. Project configuration settings may be specified interactively through this GUI. Brief instructions are provided at the bottom of the terminal when the program is running. CMake is a cross-platform build system generator. Projects specify their build process with platform-independent CMake listfiles included in each directory of a source tree with the name CMakeLists.txt. Users build a project by using CMake to generate a build system for a native tool on their platform.","Process Name":"ccmake","Link":"https:\/\/linux.die.net\/man\/1\/ccmake"}},{"Process":{"Description":"The \"ccmake\" executable is the CMake curses interface. Project configuration settings may be specified interactively through this GUI. Brief instructions are provided at the bottom of the terminal when the program is running. CMake is a cross-platform build system generator. Projects specify their build process with platform-independent CMake listfiles included in each directory of a source tree with the name CMakeLists.txt. Users build a project by using CMake to generate a build system for a native tool on their platform.","Process Name":"ccmake28","Link":"https:\/\/linux.die.net\/man\/1\/ccmake28"}},{"Process":{"Description":"The ccmakedep program calls a C compiler to preprocess each sourcefile, and uses the output to construct makefile rules describing their dependencies. These rules instruct make(1) on which object files must be recompiled when a dependency has changed. By default, ccmakedep places its output in the file named makefile if it exists, otherwise Makefile. An alternate makefile may be specified with the -f option. It first searches the makefile for a line beginning with # DO NOT DELETE or one provided with the -s option, as a delimiter for the dependency output. If it finds it, it will delete everything following this up to the end of the makefile and put the output after this line. If it doesn't find it, the program will append the string to the makefile and place the output after that.","Process Name":"ccmakedep","Link":"https:\/\/linux.die.net\/man\/1\/ccmakedep"}},{"Process":{"Description":"The ccom utility provides a C compiler. The frontend is usually pcc(1). It is not intended to be run directly. ccom reads the C source from infile or standard input and writes the assembler source to outfile or to standard output. The options are as follows:        -f feature Enable language features. Multiple -f options can be given, the following features are supported: stack-protector Enable stack smashing protection. Currently the same as stack-protector-all. stack-protector-all Enable stack smashing protection for all functions. pack-struct[ =n] Specify maximum alignment for structure members, similar to a #pragma pack statement at the start of the file. If no value is given, the default is 1. freestanding Emit code for a freestanding environment. Currently not implemented. -g' Include debugging information in the output code for use by symbolic and source-level debuggers. Currently this uses the stabs format, encoding information in symbol table entries. -k' Generate PIC code. -m option Target-specific options, used in machine-dependent code. Multiple -m options can be given, the following options are supported: AMD64' ARM' little-endian, big-endian, fpe=fpa, fpe=vpf, soft-float, arch=armv1, arch=armv2, arch=armv2a, arch=armv3, arch=armv4, arch=armv4t, arch=armv4tej, arch=armv5, arch=armv5te, arch=armv5tej, arch=armv6, arch=armv6t2, arch=armv6kz, arch=armv6k & arch=armv7. HPPA' i386' M16C' MIPS' little-endian & big-endian. NOVA' PDP-10 PDP-11 PowerPC little-endian, big-endian, soft-float & hard-float. Sparc64 VAX' -p' Generate profiling code. -s' Print statistics to standard error when complete. This includes: name table entries, name string size, permanent allocated memory, temporary allocated memory, lost memory, argument list unions, dimension\/function unions, struct\/union\/enum blocks, inline node count, inline control blocks, and permanent symtab entries. -v' Display version. -W warning Do some basic checks and emit warnings about possible coding problems. Multiple -W options can be given, the following warnings are supported: error[ =warning] Enable warning, and treat it as an error condition. If a specific warning is not given, producing any warning will cause an error. implicit-function-declaration (TODO) Require explicit prototypes for all called functions. implicit-int (TODO) Warn when a function declaration lacks a type. missing-prototypes Require explicit prototypes for all global function definitions. pointer-sign Warn when pointer operations are done with mismatched signed and unsigned values. sign-compare (TODO) Warn about comparisons between signed and unsigned values. strict-prototypes (TODO) Require that function prototypes are strictly C99. shadow Report when a local variable shadows something from a higher scope. truncate Report when integer values may be implicitly truncated to fit a smaller type. unknown-pragmas Report unhandled pragma statements. unreachable-code Report statements that cannot be executed. Any of the above may be prefixed with ''no-'' in order to disable the effect. -X flags C specific debugging where flags is one or more of the following: b' Building of parse trees d' Declarations (using multiple d flags gives more output) e' Pass1 trees at exit i' Initializations n' Memory allocations o' Turn off optimisations p' Prototypes s' Inlining t' Type conversions x' Target-specific flag, used in machine-dependent code -x setting Enable setting in the compiler. Multiple -x options can be given, the following settings are supported: ccp' Apply sparse conditional constant propagation techniques for optimization. Currently not implemented. dce' Do dead code elimination. deljumps Delete redundant jumps and dead code. gnu89 gnu99 Use gcc semantics rather than C99 for some things. Currently only inline. inline Replace calls to functions marked with an inline specifier with a copy of the actual function. ssa' Convert statements into static single assignment form for optimization. Not yet finished. tailcall Enable optimization of tail-recursion functions. Currently not implemented. temps Locate automatic variables into registers where possible, for further optimization by the register allocator. uchar Treat character constants as unsigned values. -Z flags Code generator (pass2) specific debugging where flags is one or more of the following: b' Basic block and SSA building c' Code printout e' Trees when entering pass2 f' Instruction matcher, may provide much output n' Memory allocation o' Instruction generator r' Register allocator s' Shape matching in instruction generator t' Type matching in instruction generator u' Sethi-Ullman computations x' Target-specific flag, used in machine-dependent code","Process Name":"ccom","Link":"https:\/\/linux.die.net\/man\/1\/ccom"}},{"Process":{"Description":"ccomps decomposes graphs into their connected components, printing the components to standard output.","Process Name":"ccomps","Link":"https:\/\/linux.die.net\/man\/1\/ccomps"}},{"Process":{"Description":"Generates self-similar linear fractals, including the classic ''C Curve.''","Process Name":"ccurve","Link":"https:\/\/linux.die.net\/man\/1\/ccurve"}},{"Process":{"Description":"This manual page documents briefly the ccze utility, which is a drop-in replacement for colorize, but written in C, to be faster and less resource-hungry. The goal was to be fully backwards compatible, yet superior with respect to speed and features.","Process Name":"ccze","Link":"https:\/\/linux.die.net\/man\/1\/ccze"}},{"Process":{"Description":"This manual page documents briefly the ccze-cssdump utility, which is a simple tool to dump the color setup of CCZE into Cascading Style Sheet format, to be used later by ccze -h -o cssfile.","Process Name":"ccze-cssdump","Link":"https:\/\/linux.die.net\/man\/1\/ccze-cssdump"}},{"Process":{"Description":"","Process Name":"cd","Link":"https:\/\/linux.die.net\/man\/1\/cd"}},{"Process":{"Description":"In order to do CDDB queries over the Internet, you must know the DiscID of the CD you are querying. cd-discid provides you with that information. It outputs the discid, the number of tracks, the frame offset of all of the tracks, and the total length of the CD in seconds, on one line in a space-delimited format. cd-discid was designed as a backend tool for cdgrab (now abcde) but will work independently of it.","Process Name":"cd-discid","Link":"https:\/\/linux.die.net\/man\/1\/cd-discid"}},{"Process":{"Description":"-d, --debug= INT Set debugging to LEVEL -i, --cdrom-device[= DEVICE] show only info about CD-ROM device -q, --quiet Don't produce warning output -V, --version display version and copyright information and exit Help options: -?, --help Show this help message --usage Display brief usage message","Process Name":"cd-drive","Link":"https:\/\/linux.die.net\/man\/1\/cd-drive"}},{"Process":{"Description":"-a, --access-mode= STRING Set CD access method -d, --debug= INT Set debugging to LEVEL -T, --no-tracks Don't show track information -A, --no-analyze Don't filesystem analysis --no-cddb Does nothing since this program is not -P, --cddb-port= INT CDDB-enabled -H, --cddb-http --cddb-server= STRING --cddb-cache= STRING --cddb-email= STRING --no-cddb-cache --cddb-timeout= INT --no-device-info Don't show device info, just CD info --no-disc-mode Don't show disc-mode info --dvd Attempt to give DVD information if a DVD is found. -v, --no-vcd Don't look up Video CD information - for this build, this is always set -I, --no-ioctl Don't show ioctl() information -b, --bin-file[= FILE] set \"bin\" CD-ROM disk image file as source -c, --cue-file[= FILE] set \"cue\" CD-ROM disk image file as source -N, --nrg-file[= FILE] set Nero CD-ROM disk image file as source -t, --toc-file[= FILE] set cdrdao CD-ROM disk image file as source -i, --input[= FILE] set source and determine if \"bin\" image or device --iso9660 print directory contents of any ISO-9660 filesystems -C, --cdrom-device[= DEVICE] set CD-ROM device as source -l, --list-drives Give a list of CD-drives --no-header Don't display header and copyright (for regression testing) --no-joliet Don't use Joliet extensions --no-rock-ridge Don't use Rock-Ridge-extension information --no-xa Don't use XA-extension information -q, --quiet Don't produce warning output -V, --version display version and copyright information and exit Help options: -?, --help Show this help message --usage Display brief usage message","Process Name":"cd-info","Link":"https:\/\/linux.die.net\/man\/1\/cd-info"}},{"Process":{"Description":"cd-paranoia retrieves audio tracks from CDDA capable CD-ROM drives. The data can be saved to a file or directed to standard output in WAV, AIFF, AIFF-C or raw format. Most ATAPI, SCSI and several proprietary CD-ROM drive makes are supported; cd-paranoia can determine if the target drive is CDDA capable. In addition to simple reading, cd-paranoia adds extra-robust data verification, synchronization, error handling and scratch reconstruction capability. This version uses the libcdio library for interaction with a CD-ROM drive. The jitter and error correction however are the same as used in Xiph's cdparanoia.","Process Name":"cd-paranoia","Link":"https:\/\/linux.die.net\/man\/1\/cd-paranoia"}},{"Process":{"Description":"-a, --access-mode= STRING Set CD control access mode -m, --mode= MODE-TYPE set CD-ROM read mode (audio, auto, m1f1, m1f2, m2mf1, m2f2) -d, --debug= INT Set debugging to LEVEL -x, --hexdump Show output as a hex dump. The default is a hex dump when output goes to stdout and no hex dump when output is to a file. -j, --just-hex Don't display printable chars on hex dump. The default is print chars too. --no-header Don't display header and copyright (for regression testing) --no-hexdump Don't show output as a hex dump. -s, --start= INT Set LBA to start reading from -e, --end= INT Set LBA to end reading from -n, --number= INT Set number of sectors to read -b, --bin-file[= FILE] set \"bin\" CD-ROM disk image file as source -c, --cue-file[= FILE] set \"cue\" CD-ROM disk image file as source -i, --input[= FILE] set source and determine if \"bin\" image or device -C, --cdrom-device[= DEVICE] set CD-ROM device as source -N, --nrg-file[= FILE] set Nero CD-ROM disk image file as source -t, --toc-file[= FILE] set \"TOC\" CD-ROM disk image file as source -o, --output-file= FILE Output blocks to file rather than give a hexdump. -V, --version display version and copyright information and exit Help options: -?, --help Show this help message --usage Display brief usage message","Process Name":"cd-read","Link":"https:\/\/linux.die.net\/man\/1\/cd-read"}},{"Process":{"Description":"With cdargs you can jump to various places throughout the file-system that are defined using a plain text, line oriented bookmarks file. You can create bookmarks by editing your bookmarks file \"$HOME\/.cdargs\" using your favorite editor, by using the \"--add\" option to cdargs or by using the built-in mechanism that will be described later. While you are in cdargs you can use various commands to navigate through your list of bookmarks and through the file-system. These are described below in the section \"COMMANDS\". The most commonly used and most obvious commands are the up\/down keys for navigation, \"ENTER\" for selection of a path and \"q\" for quit. To be able to actually use this program together with the shell built-in \"cd\" command you must use a little trick by defining a shell function. Of course the syntax for this is different between the csh-like shells (like tcsh) and the sh-like shells (like bash). For sh-like shells: function cv () { cdargs \"$1\" && cd \"'cat \"$HOME\/.cdargsresult\"'\" ; } For csh-like shells: alias cv 'cdargs \\!* && cd 'cat $HOME\/.cdargsresult'' Now you only have to put the cdargs binary somewhere into your path. Of course you can create functions and\/or aliases for different purposes. You might want to add a pwd call to echo your current directories or put --nowrap in your functions to enable a different scrolling behavior. See section SHELL FUNCTIONS below. Since version 1.19 cdargs brings a collection of bash functions (and since 1.20 tcsh aliases, too) with it. You only have to source this in your ~\/.profile or ~\/.bashrc (or ~\/.tcsh) and you get nice kinds of magic prepared (completion...). Please read the cdargs-bash.sh or cdargs-tcsh.csh respectively for the documentation of these functions.","Process Name":"cdargs","Link":"https:\/\/linux.die.net\/man\/1\/cdargs"}},{"Process":{"Description":"cdda2ogg is a simple script that uses the icedax <fileprefix> command to extract all audio tracks with the icedax <fileprefix> command and encode them using the ogg123 encoder. The scripts are not intended to be full-featured music archiving programs, but only for quick storing of few audio data. It does not use databases like CDDB or have any extra features. You may look at icedax if you need them. ogg123 is provided by the vorbis-tools which needs to be installed separately. See www.ogg.org for more information.","Process Name":"cdda2ogg","Link":"https:\/\/linux.die.net\/man\/1\/cdda2ogg"}},{"Process":{"Description":"cdda2wav can retrieve audio tracks from CDROM drives (see README for a list of drives) that are capable of reading audio data digitally to the host (CDDA).","Process Name":"cdda2wav","Link":"https:\/\/linux.die.net\/man\/1\/cdda2wav"}},{"Process":{"Description":"cddb-tool is a backend tool for abcde. It is passed information from cd-discid and uses it look up the CD title, artist, and track information on the cddb database ( http:\/\/freedb.freedb.org) on the internet.","Process Name":"cddb-tool","Link":"https:\/\/linux.die.net\/man\/1\/cddb-tool"}},{"Process":{"Description":"Cdecl (and c++decl) is a program for encoding and decoding C (or C++) type declarations. The C language is based on the (draft proposed) X3J11 ANSI Standard; optionally, the C language may be based on the pre-ANSI definition defined by Kernighan & Ritchie's The C Programming Language book, or the C language defined by the Ritchie PDP-11 C compiler. The C++ language is based on Bjarne Stroustrup's The C++ Programming Language, plus the version 2.0 additions to the language.","Process Name":"cdecl","Link":"https:\/\/linux.die.net\/man\/1\/cdecl"}},{"Process":{"Description":"Intention of this document: to provide some examples of the commonly used parts of RRDtool's CDEF language. If you think some important feature is not explained properly, and if adding it to this document would benefit most users, please do ask me to add it. I will then try to provide an answer in the next release of this tutorial. No feedback equals no changes! Additions to this document are also welcome. -- Alex van den Bogaerdt <alex@vandenbogaerdt.nl> Why this tutorial? One of the powerful parts of RRDtool is its ability to do all sorts of calculations on the data retrieved from its databases. However, RRDtool's many options and syntax make it difficult for the average user to understand. The manuals are good at explaining what these options do; however they do not (and should not) explain in detail why they are useful. As with my RRDtool tutorial: if you want a simple document in simple language you should read this tutorial. If you are happy with the official documentation, you may find this document too simple or even boring. If you do choose to read this tutorial, I also expect you to have read and fully understand my other tutorial. More reading If you have difficulties with the way I try to explain it please read Steve Rader's rpntutorial. It may help you understand how this all works.","Process Name":"cdeftutorial","Link":"https:\/\/linux.die.net\/man\/1\/cdeftutorial"}},{"Process":{"Description":"cdiff is a wrapper for colordiff and will add colour to a diff given in a URL or a file, e.g. cdiff http:\/\/some.url.com\/foo\/thing.patch It also adds support for reading gzip, bzip2, xz, and lzma compressed diffs, and like colordiff, also supports reading diffs from standard input if URL or a filename is not specified. cdiff pipes colordiff's output through less -R.","Process Name":"cdiff","Link":"https:\/\/linux.die.net\/man\/1\/cdiff"}},{"Process":{"Description":"cdlabelgen's purpose in life is twofold: * To be run automatically and swiftly from a shell script and automatically generate a frontcard and a traycard for a cd--usually data archive cd's. The traycard (which goes behind the CD itself) is U-shaped and the ends of the CD case bear the label of what the CD is. Inside inserts for DVDs are also supported. * To have a minimum of dependencies--cdlabelgen only requires perl. cdlabelgen was designed to simplify the process of generating labels for CD 's. It originated as a program to allow auto generation of frontcards and traycards for CD 's burned via an automated mechanism (specifically for archiving data), but has now become popular for labelling CD compilations of mp3's, and copies of CDs. Note that cdlabelgen does not actually print anything--it just spits out postscript, which you can then do with as you please. It can also be combined with output from other programs such as \"barcodegen\" - to print a barcode as a tray overlay image. The latest version of cdlabelgen as well as this document can be found at http:\/\/www.aczoom.com\/tools\/cdinsert\/. The software package includes CGI scripts that can be used to serve cdlabelgen over the internet. An older version may be available at: http:\/\/www.red-bean.com\/~bwf\/software\/cdlabelgen\/. cdlabelgen comes with several eps images for you to use on your labels. These images can be found in \/usr\/local\/lib\/cdlabelgen or \/usr\/share\/cdlabelgen or \/opt\/lib\/cdlabelgen\/ or \/usr\/local\/share\/cdlabelgen, depending on your installation. Included are a Recycling icon, an mp3 icon, the Compact Disc icon (with and without 'Digital' on it), Tux the penguin, and the new Debian 'swirl' logo. Two color background images called Music Notes are also available. CDs: cdlabelgen prints a 'tongue' as part of the traycard. This folds around and is viewable from the front in jewel boxes that are entirely clear ( CD holder piece is not opaque). If you do not have a clear CD holder in your jewel box, you may find it easier to just cut the 'tongue' off--it's a bit easier to fold without it. Paper Sizes: Normal CD cases, Slim CD cases, DVD inside inserts can be printed on a letter or A4 sized page. CD\/DVD Envelopes and DVD outside inserts will not fit on a letter sized paper, a larger paper size will be needed to make it fit. cdlabelgen requires Perl Version 5.003 or greater. Ghostscript is not required, but is recommended so that you can test out your labels without wasting paper.","Process Name":"cdlabelgen","Link":"https:\/\/linux.die.net\/man\/1\/cdlabelgen"}},{"Process":{"Description":"cdparanoia retrieves audio tracks from CDDA capable CDROM drives. The data can be saved to a file or directed to standard output in WAV, AIFF, AIFF-C or raw format. Most ATAPI, SCSI and several proprietary CDROM drive makes are supported; cdparanoia can determine if the target drive is CDDA capable. In addition to simple reading, cdparanoia adds extra-robust data verification, synchronization, error handling and scratch reconstruction capability.","Process Name":"cdparanoia","Link":"https:\/\/linux.die.net\/man\/1\/cdparanoia"}},{"Process":{"Description":"cdpinger is a program that informs ltspfsd(1) that a cd has been inserted or removed. This program is not intended to be run manually, but by udev scripts from the ltspfsd package installed on a thin client.","Process Name":"cdpinger","Link":"https:\/\/linux.die.net\/man\/1\/cdpinger"}},{"Process":{"Description":"cdrdao creates audio and data CD-Rs in disk-at-once (DAO) mode driven by a description file called toc-file. In DAO mode it is possible to create non standard track pre-gaps that have other lengths than 2 seconds and contain nonzero audio data. This is for example useful to divide live recordings into tracks where 2 second gaps would be kind of irritating. Instead of a toc-file a cue file (used by a famous DOS\/Windows mastering tool) may be used. See the CUE FILES section for more details.","Process Name":"cdrdao","Link":"https:\/\/linux.die.net\/man\/1\/cdrdao"}},{"Process":{"Description":"Cdrecord is used to record data or audio Compact Discs on an Orange Book CD-Recorder or to write DVD media on a DVD-Recorder. The device refers to scsibus\/target\/lun of the CD\/DVD-Recorder. Communication on SunOS is done with the SCSI general driver scg. Other operating systems are using a library simulation of this driver. Possible syntax is: dev= scsibus,target,lun or dev= target,lun. In the latter case, the CD\/DVD-Recorder has to be connected to the default SCSI bus of the machine. Scsibus, target and lun are integer numbers. Some operating systems or SCSI transport implementations may require to specify a filename in addition. In this case the correct syntax for the device is: dev= devicename:scsibus,target,lun or dev= devicename:target,lun. If the name of the device node that has been specified on such a system refers to exactly one SCSI device, a shorthand in the form dev= devicename:@ or dev= devicename:@,lun may be used instead of dev= devicename:scsibus,target,lun. To access remote SCSI devices, you need to prepend the SCSI device name by a remote device indicator. The remote device indicator is either REMOTE:user@host: or REMOTE:host: A valid remote SCSI device name may be: REMOTE:user@host: to allow remote SCSI bus scanning or REMOTE:user@host:1,0,0 to access the SCSI device at host connected to SCSI bus # 1,target 0 lun 0. Cdrecord is completely based on SCSI commands but this is no problem as all CD\/DVD writers ever made use SCSI commands for the communication. Even ATAPI drives are just SCSI drives that inherently use the ATA packet interface as SCSI command transport layer build into the IDE (ATA) transport. You may need to specify an alternate transport layer on the command line if your OS does not implement a fully integrated kernel driver subsystem that allows to access any drive using SCSI commands via a single unique user interface. To access SCSI devices via alternate transport layers, you need to prepend the SCSI device name by a transport layer indicator. The transport layer indicator may be something like USCSI: or ATAPI:. To get a list of supported transport layers for your platform, use dev= HELP: To make cdrecord portable to all UNIX platforms, the syntax dev= devicename:scsibus,target,lun is preferred as it hides OS specific knowledge about device names from the user. A specific OS may not necessarily support a way to specify a real device file name nor a way to specify scsibus,target,lun. Scsibus 0 is the default SCSI bus on the machine. Watch the boot messages for more information or look into \/var\/adm\/messages for more information about the SCSI configuration of your machine. If you have problems to figure out what values for scsibus,target,lun should be used, try the -scanbus option of cdrecord described below. If a file \/etc\/cdrecord.conf exists, the parameter to the dev= option may also be a drive name label in said file (see FILES section). On SVr4 compliant systems, cdrecord uses the real time class to get the highest scheduling priority that is possible (higher than all kernel processes). On systems with POSIX real time scheduling cdrecord uses real time scheduling too, but may not be able to gain a priority that is higher than all kernel processes. In order to be able to use the SCSI transport subsystem of the OS, run at highest priority and lock itself into core cdrecord either needs to be run as root, needs to be installed suid root or must be called via RBACs pfexec mechanism. In Track At Once mode, each track corresponds to a single file that contains the prepared data for that track. If the argument is '-', standard input is used for that track. Only one track may be taken from stdin. In the other write modes, the direct file to track relation may not be implemented. In -clone mode, a single file contains all data for the whole disk. To allow DVD writing on platforms that do not implement large file support, cdrecord concatenates all file arguments to a single track when writing to DVD media.","Process Name":"cdrecord","Link":"https:\/\/linux.die.net\/man\/1\/cdrecord"}},{"Process":{"Description":"cdrskin is a program that provides some of cdrecord's options in a compatible way for CD media. With DVD and BD it has its own ways. You do not need to be superuser for its daily usage. Overview of features: Blanking of CD-RW and DVD-RW. Formatting of DVD-RW, DVD+RW, DVD-RAM, BD. Burning of data or audio tracks to CD, either in versatile Track at Once mode (TAO) or in Session at Once mode for seamless tracks. Multi session on CD (follow-up sessions in TAO only) or on DVD-R[W] (in Incremental mode) or DVD+R[\/DL] or BD-R. Single session on DVD-RW or DVD-R (Disk-at-once). Single session or emulated ISO-9660 multi-session on overwriteable DVD+RW, DVD-RW, DVD-RAM, BD-RE or on data file or block device. Bus scan, burnfree, speed options, retrieving media info, padding, fifo. See section EXAMPLES at the end of this text. General information paragraphs: Track recording model Write mode selection Recordable CD Media Sequentially Recordable DVD or BD Media Overwriteable DVD or BD Media Drive preparation and addressing Emulated drives Track recording model: The input-output entities which get processed are called tracks. A track stores a stream of bytes. Each track is initiated by one track source address argument, which may either be \"-\" for standard input or the address of a readable file. If no write mode is given explicitely then one will be chosen which matches the peculiarities of track sources and the state of the output media. More than one track can be burned by a single run of cdrskin. In the terms of the MMC standard all tracks written by the same run constitute a session. Some media types can be kept appendable so that further tracks can be written to them in subsequent runs of cdrskin (see option -multi). Info about the addresses of burned tracks is kept in a table of content (TOC) on media and can be retrieved via cdrskin option -toc. These informations are also used by the operating systems' CD-ROM read drivers. In general there are two types of tracks: data and audio. They differ in sector size, throughput and readability via the systems' CD-ROM drivers resp. by music CD players. With DVD and BD there is only type data. If not explicitely option -audio is given, then any track is burned as type data, unless the track source is a file with suffix \".wav\" or \".au\" and has a header part which identifies it as MS-WAVE resp. SUN Audio with suitable parameters. Such files are burned as audio tracks by default. While audio tracks just contain a given time span of acoustic vibrations, data tracks may have an arbitray meaning. Nevertheless, ISO-9660 filesystems are established as a format which can represent a tree of directories and files on all major operating systems. Such filesystem images can be produced by programs mkisofs or genisoimage or xorriso. They can also be extended by follow-up tracks if prepared properly. See the man pages of said programs. cdrskin is able to fulfill the needs about their option -C. Another type of data track content are archive formats which originally have been developed for magnetic tapes. Only formats which mark a detectable end-of-archive in their data are suitable, though. Well tested are the archivers afio and star. Not suitable seems GNU tar. Write mode selection: In general there are two approaches for writing media: A permissive mode depicted by option -tao which needs no predicted track size and allows to make use of eventual multi-session capabilities. A more restrictive mode -sao (alias -dao) which usually demands a predictable track size and is not necessarily capable of multi-session. It may have advantages for some readers resp. players of the recorded tracks. If none of the options -dao, -tao or -sao is given then the program will try to choose a write mode which matches the defined recording job, the capabilities of the drive and the state of the present media. So the mentioning of write modes in the following paragraphs and in the examples is not so much a demand that the user shall choose one explicitely, but rather an illustration of what to expect with particular media types. Recordable CD Media: CD-R can be initially written only once and eventually extended until they get closed (or are spoiled because they are overly full). After that they are read-only. Closing is done automatically unless option -multi is given which keeps the media appendable. Write mode -tao allows to use track sources of unpredictable length (like stdin) and allows to write further sessions to appendable media. -sao produces audio sessions with seamless tracks but needs predicted track sizes and cannot append sessions to media. CD-RW media can be blanked to make them re-usable for another round of overwriting. Usually blank=fast is the appropriate option. Blanking damages the previous content but does not make it completely unreadable. It is no effective privacy precaution. Multiple cycles of blanking and overwriting with random numbers might be needed. Sequentially Recordable DVD or BD Media: Currently DVD-RW, DVD-R , DVD+R[\/DL], and BD-R can be used for the Sequential recording model. It resembles the model of CD media. Only DVD-RW can be blanked and re-used from scratch. DVD-RW are sequential media if they are in state \"Sequential Recording\". The media must be either blank or appendable. Newly purchased DVD-RW and DVD-R media are in this state. Used DVD-RW get into blank sequential state by option blank=deformat_sequential . With DVD-R[W] two write modes may be available: Mode DAO has many restrictions. It does not work with appendable media, allows no -multi and only a single track. The size of the track needs to be known in advance. So either its source has to be a disk file of recognizable size or the size has to be announced explicitely by options tsize= or tao_to_sao_tsize= . DAO is the only mode for media which do not offer feature 21h Incremental Streaming. DAO may also be selected explicitely by option -sao . Program growisofs uses DAO on sequential DVD-R[W] media for maximum DVD-ROM\/-Video compatibility. The other mode, Incremental Streaming, is the default write mode if it is available and if the restrictions of DAO would prevent the job. Incremental Streaming may be selected explicitely by option -tao as it resembles much CD TAO by allowing track sources of unpredicted length and to keep media appendable by option -multi . The only restriction towards CD-R[W] is the lack of support for -audio tracks. Multiple tracks per session are permissible. The write modes for DVD+R[\/DL] and BD-R resemble those with DVD-R except that each track gets wrapped in an own session. There is no -dummy writing with DVD+R[\/DL] or BD-R. Quite deliberately write mode -sao insists in the tradition of a predicted track size and blank media, whereas -tao writes the tracks open ended and allows appendable media. BD-R may be formatted before first use to enable the Defect Management which might catch and repair some bad spots at the expense of slow speed even with flawless media. Note: Option -multi might make DVD media unreadable in some DVD-ROM drives. Best reader compatibility is achieved without it (i.e. by single session media). Overwriteable DVD or BD Media: Currently types DVD+RW, DVD-RW, DVD-RAM and BD-RE can be overwritten via cdrskin. Option -audio is not allowed. Only one track is allowed. Option -multi cannot mark a recognizeable end of overwriteable media. Therefore -multi is banned unless ISO-9660 images shall be expandable by help of option --grow_overwriteable_iso . Without this option or without an ISO-9660 filesystem image present on media, -toc does not return information about the media content and media get treated as blank regardless wether they hold data or not. Currently there is no difference between -sao and -tao. If ever, then -tao will be the mode which preserves the current behavior. DVD+RW and DVD-RAM media need no special initial formatting. They offer a single continuous data area for blockwise random access. BD-RE need explicit formatting before use. See blank=as_needed or blank=format_defectmgt . DVD-RW are sold in state \"Sequential Recording\". To become suitable for the Overwriteable DVD recording model they need to get formatted to state \"Restricted Overwrite\". Then they behave much like DVD+RW. This formatting can be done by option blank=format_overwrite . Several programs like dvd+rw-format, cdrecord, wodim, or cdrskin can bring a DVD-RW out of overwriteable state so that it has to be formatted again. If in doubt, just give it a try. Drive preparation and addressing: The drives, CD, DVD, or BD burners, are accessed via addresses which are specific to libburn and the operating system. Those addresses get listed by a run of cdrskin --devices. On Linux, they are device files which traditionally do not offer w-permissions for normal users. Because libburn needs rw-permission, it might be only the superuser who is able to get this list without further precautions. It is consensus that chmod a+rw \/dev\/sr0 or chmod a+rw \/dev\/hdc is less security sensitive than chmod u+s,a+x \/usr\/bin\/cdrskin. The risk for the drive is somewhat higher but the overall system is much less at stake. Consider to restrict rw-access to a single group which bundles the users who are allowed to use the burner drive (like group \"floppy\"). If you only got one CD capable drive then you may leave out cdrskin option dev=. Else you should use this option to address the drive you want. cdrskin option dev= not only accepts the listed addresses but also traditional cdrecord SCSI addresses which on Linux consist of three numbers: Bus,Target,Lun. There is also a related address family \"ATA\" which accesses IDE drives not under control of Linux SCSI drivers: ATA:Bus,Target,Lun. See option -scanbus for getting a list of cdrecord style addresses. Further are accepted on Linux: links to libburn-suitable device files, device files which have the same major and minor device number, and device files which have the same SCSI address parameters (e.g. \/dev\/sg0). Emulated drives: Option --allow_emulated_drives enables addressing of pseudo-drives which get emulated on top of filesystem objects. Regular data files and block devices result in pseudo-drives which behave much like DVD-RAM. If the given address does not exist yet but its directory exists, then it gets created as regular file. Other file types like character devices or pipes result in pseudo-drives which behave much like blank DVD-R. The target file address is given after prefix \"stdio:\". E.g.: dev=stdio:\/tmp\/my_pseudo_drive Addresses of the form \"stdio:\/dev\/fd\/<number>\" are treated special. The number is read literally and used as open file descriptor. With dev=\"stdio:\/dev\/fd\/1\" the normal standard output of the program is redirected to stderr and the stream data of a burn run will appear on stdout. Not good for terminals ! Redirect it. Pseudo-drives allow -dummy. Their reply with --tell_media_space can be utopic. -dummy burn runs touch the file but do not modify its data content. Note: --allow_emulated_drives is restricted to stdio:\/dev\/null if cdrskin is run by the superuser or if it has changed user identity via the setuid bit of its access permissions. The ban for the superuser can be lifted by a skillfully created file. See section FILES below.","Process Name":"cdrskin","Link":"https:\/\/linux.die.net\/man\/1\/cdrskin"}},{"Process":{"Description":"The cdwrtool command can perform certain actions on a CD-R, CD-RW, or DVD-R device. Mainly these are blanking the media, formating it for use with the packet-cd device, and applying an UDF filesystem. The most common usage is probably the 'quick setup' option: cdrwtool -d device -q which will blank the disc, format it as one large track, and write the UDF filesystem structures. Other options get and set various parameters of how the device is set up, and provide for different offsets, modes and settings from the defaults. The usefulness of most of the options is not explained.","Process Name":"cdrwtool","Link":"https:\/\/linux.die.net\/man\/1\/cdrwtool"}},{"Process":{"Description":"cdw is a front-end for command-line tools used for burning data CD and DVD discs and for related tasks. The tools are: cdrecord\/wodim, mkisofs\/genisoimage, growisofs, dvd+rw-mediainfo, dvd+rw-format, xorriso. You can also use cdw to rip tracks from your audio CD to raw audio files. Limited support for copying content of CD and DVD discs to image files is also provided. cdw can utilize md5sum program to verify correctness of writing ISO image to CD and DVD disc. cdw uses ncurses library to build user-friendly interface and it can be used in UNIX terminal window and in terminal emulator (like konsole, rxvt or gnome-console) in X environment. cdw supports only ISO9660 filesystem for optical media. Other filesystem for optical media are not supported nor recognized.","Process Name":"cdw","Link":"https:\/\/linux.die.net\/man\/1\/cdw"}},{"Process":{"Description":"-v, --verbose be verbose -q, --quiet show only critical messages -V, --version display version and copyright information and exit Help options: -?, --help Show this help message --usage Display brief usage message","Process Name":"cdxa2mpeg","Link":"https:\/\/linux.die.net\/man\/1\/cdxa2mpeg"}},{"Process":{"Description":"This manual page documents briefly the centerim command. centerim is a text mode menu- and window-driven IM interface. Currently ICQ2000, Yahoo!, AIM, IRC, MSN, Gadu-Gadu and Jabber protocols are supported. It allows you to send, receive, and forward messages, URLs, SMSes and, contacts, mass message send, search for users (including extended \"whitepages search\"), view users' details, maintain your contact list directly from the program (including non-icq contacts), view the messages history, register a new UIN and update your details, be informed on receiving email messages, automatically set away after the defined period of inactivity (on any console), and have your own ignore, visible and invisible lists. It can also associate events with sounds, add events to the outgoing queue from command line, define external event processing actions (like events auto-forwarding or elizatalk), has support for Hebrew and Arabic languages and allows to arrange contacts into groups.","Process Name":"centerim","Link":"https:\/\/linux.die.net\/man\/1\/centerim"}},{"Process":{"Description":"cernlib is a tool to list the compiler and linker options necessary to compile a CERNLIB program that has the given library dependencies. It is generally used within a command substitution, as in the following example: gfortran -o myprogram myprogram.F 'cernlib -G Motif pawlib' This version of cernlib has been completely rewritten from the original script provided by CERN. It now does recursive library dependency checking and removes duplicate entries. Note that by default, the cernlib script assumes that the CERN libraries are to be linked against statically; if the environment variables $CERN or $CERN_ROOT are specified, it looks for the libraries only in the \"lib\" subdirectory of those locations, not any \"shlib\" subdirectory. Furthermore, the script brackets the CERN libraries with linker instructions to link statically. This is done to preserve the original upstream behavior, in which all CERNLIB libraries exist only in static form. For instance, \"cernlib packlib\" outputs: -Wl,-static -lpacklib -lkernlib -Wl,-dy -lm -lnsl -lcrypt -ldl -lg2c If you want to link against ALL libraries (including CERNLIB) either statically or dynamically, call the cernlib script with its -safe flag to omit these bracketing linker flags. That is: if you want to link against all libraries (not just CERNLIB) statically, use the -static compiler flag and call cernlib with its -safe flag: gfortran -o myprogram myprogram.F -static 'cernlib -safe -G Motif pawlib' and if you want to link against all libraries (including the CERN libraries) dynamically, use the same command without the -static compiler flag (the linker assumes dynamic linking by default): gfortran -o myprogram myprogram.F 'cernlib -safe -G Motif pawlib'","Process Name":"cernlib","Link":"https:\/\/linux.die.net\/man\/1\/cernlib"}},{"Process":{"Description":"cernlib-static writes to standard output a list of object libraries needed to link a program with the given library(s) of CERNlib; other libraries needed implicitely, both CERNlib and system libraries, are added automatically. library may be any of the CERNlib libraries using a short name like packlib, mathlib, graflib, phtools... version allows to specify which version to use for this library, see usage notes. Options cernlib-static recognizes the following options: -G driver specifies the graphics interface to use. Defaults to X11. The choices available depend on the system in use: X11 all systems. On HP\/UX X11R5 is used when available on the system. Motif all systems. On HP\/UX Motif1.2 is used when available on the system. GKS Vendor specific graphics, DEC Ultrix, IBM\/RS6000, SUN only GPR Vendor specific graphics, Apollo only. GL Vendor specific graphics, IBM\/RS6000 and SGI only. -s obsolete option. Ignored. -l Don't link against system lapack and blas. -u do not automatically add vendor libraries needed to link, but not specified by the user. -v version Use level version of CERNlib libraries; version can be pro, new or old, or you may specify the explicit version number; the default version is pro. The global version can be overwritten for an individual library by specifying the library as library\/version. Usage cernlib-static expects to find the CERNlib directory tree in \/cern; if your system does not have this, set the environment variable CERN to contain the name of the root directory for the CERNlib tree. After the library a version for that library different from the default version may be specified. The possible values are as for the -v option, or in the case of graflib as for the -G option. Examples As cernlib-static writes the list of libraries to standard output, using it as a command will show the list of libraries generated by cernlib-static. Also, it can be used like other commands in the command line surrounded by back quotes('). At CERN you should use the hepf77 command to link on your system as this gives the compiler parameters which are compatible with Cernlib (in particular for xlf on IBM\/RS6000 and fort77 on HP\/UX ) you could type: hepf77 your.o 'cernlib-static' to link your object file with routines from packlib and kernlib; you need not specify packlib, as this is added automatically. Also all kernlib routines are included in packlib. If your application uses graflib and the Motif interface, the command would be: hepf77 your.o 'cernlib-static graflib\/Motif' Again packlib need not be specified. If you prefer to have an environment variable CERNLIB set at login time, put the following into .profile for sh, ksh or similar CERNLIB='cernlib-static graflib'; export CERNLIB .login for users of csh of similar setenv CERNLIB \"'cernlib-static graflib'\" The CERNLIB variable can then be used in the link command instead of calling the cernlib-static command. Bugs There is no checking that any of the libraries (or options) are valid or exist. Most loaders only check the libraries once for symbols, hence the order the libraries given is important. This command does not check that the order given is correct. For systems other than Alliant, Apollo, DEC Ultrix, Digital Unix (or DEC OSF), HP\/UX, SGI, Linux, Sun OS, Sun Solaris the libraries given for the graphics interface are likely to be wrong, even for X11 and Motif. Files The cernlib-static command is available in the \/cern\/pro\/bin directory in the CERNlib tree. CONTACT Address If you have suggestions for improvements, or find bugs, please report them to cernlib@cern.ch. Your report must state which version of CERNlib you are using, or the output of the command: what 'which cernlib-static' Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"cernlib-static","Link":"https:\/\/linux.die.net\/man\/1\/cernlib-static"}},{"Process":{"Description":"Creates an X.509 certificate and key.","Process Name":"cert-init.pl","Link":"https:\/\/linux.die.net\/man\/1\/cert-init.pl"}},{"Process":{"Description":"Creates a 512 bit proxy certificate optionally including a VOMS attribute certificate.","Process Name":"cert-req.pl","Link":"https:\/\/linux.die.net\/man\/1\/cert-req.pl"}},{"Process":{"Description":"Cert2ldap is used to import a certificate into an LDAP directory in such a as to allow the mod_authz_ldap Apache module to authenticate and authorize users based on their certificates. The certificate is either specified as a certificatefilename argument on the command line or read from standard input. There are essentially two ways to use the program: either a certificate is added as a userCertifcate attribute to a users node, or a certificate mapping node is added somewhere else in the directory, referencing the user. The second form is active as soon as one if the options -i, -s, -o or -n are used. The first form uses only the -c option. The correct configuration of the entires can be checked using the certfind(1) program. If the node to be updated does not exist yet, a minimal node is created. However this is only marginally useful in the case of a node containing the certificate proper.","Process Name":"cert2ldap","Link":"https:\/\/linux.die.net\/man\/1\/cert2ldap"}},{"Process":{"Description":"Combine a list of X.509 certificates or X.509 CRL (Certificate Revocation List) into a Software Publisher Certificate (PKCS#7 file). The SPC file is required for signing a PE file (like an assembly) using signcode.","Process Name":"cert2spc","Link":"https:\/\/linux.die.net\/man\/1\/cert2spc"}},{"Process":{"Description":"Certfind searches the directory for a node either having a userCertificate attribute with the given certificate as its value, or having an issuerDN attribute with the DN of the certificate issuer, and either a subjectDN or a serialNumber attribute with the corresponding values from the certificate. The certificate is either specified as a certificatefilename argument on the command line or read from standard input.","Process Name":"certfind","Link":"https:\/\/linux.die.net\/man\/1\/certfind"}},{"Process":{"Description":"See https:\/\/fedorahosted.org\/certmaster Certmaster is a daemon that runs on a \"master\" machine to hand out certificates to machines that want them. Certificates can then be used by applications like func. Certmaster is configured by \/etc\/certmaster\/certmaster.conf","Process Name":"certmaster","Link":"https:\/\/linux.die.net\/man\/1\/certmaster"}},{"Process":{"Description":"\"certmaster-ca --list\" The list command prints all certificates that have been requested from certmaster by a remote application (such as funcd or certmaster-request) but are not yet signed. \"certmaster-ca --sign [hostname]\" This command is used to sign a certificate and send it back to the requester.","Process Name":"certmaster-ca","Link":"https:\/\/linux.die.net\/man\/1\/certmaster-ca"}},{"Process":{"Description":"The certmaster-getcert tool issues requests to a org.fedorahosted.certmonger service on behalf of the invoking user. It can ask the service to begin enrollment, optionally generating a key pair to use, it can ask the service to begin monitoring a certificate in a specified location for expiration, and optionally to refresh it when expiration nears, it can list the set of certificates that the service is already monitoring, or it can list the set of CAs that the service is capable of using. If no command is given as the first command-line argument, ipa-getcert will print short usage information for each of its functions. The certmaster-getcert tool behaves identically to the generic getcert tool when it is used with the -c certmaster option.","Process Name":"certmaster-getcert","Link":"https:\/\/linux.die.net\/man\/1\/certmaster-getcert"}},{"Process":{"Description":"FIXME: To be added later once we split this out from func.","Process Name":"certmaster-request","Link":"https:\/\/linux.die.net\/man\/1\/certmaster-request"}},{"Process":{"Description":"certmaster-sync synchronizes client certificates amongst certmaster clients via Func. It is assumed that the hosts who have requested certificates are reachable via Func for synchronization operations. certmaster-sync by default is called as a post-sign and post-clean trigger. In order to enable synchronization you must set sync_certs to True, see CONFIGURATION VALUES below. The synchronization occurs by querying remote Func methods in certmastermod on the minion hosts. This will gather information, copy any new certificates, and remove any certificates that have been cleaned.","Process Name":"certmaster-sync","Link":"https:\/\/linux.die.net\/man\/1\/certmaster-sync"}},{"Process":{"Description":"This tool allow to list, add, remove or extract certificates, certificate revocation lists (CRL) or certificate trust lists (CTL) to\/from a certificate store. Certificate stores are used to build and validate certificate chains for Authenticode(r) code signing validation and SSL server certificates.","Process Name":"certmgr","Link":"https:\/\/linux.die.net\/man\/1\/certmgr"}},{"Process":{"Description":"Generate X.509 certificates, certificate requests, and private keys.","Process Name":"certtool","Link":"https:\/\/linux.die.net\/man\/1\/certtool"}},{"Process":{"Description":"The certwatch program is used to issue warning mail when an SSL certificate is about to expire. The program has two modes of operation: normal mode and quiet mode. In normal mode, the certificate given by the filename argument is examined, and a warning email is issued to standard output if the certificate is outside its validity period, or approaching expiry. If the certificate cannot be found, or any errors occur whilst parsing the certificate, the certificate is ignored and no output is produced. In quiet mode, no output is given, but the exit status can still be used. The certificate can be specified by its nickname or by a path to the containing file.","Process Name":"certwatch","Link":"https:\/\/linux.die.net\/man\/1\/certwatch"}},{"Process":{"Description":"Cervisia is a graphical user interface for the Concurrent Versions System. It is based on the KDE libraries and therefore shares their Look'n'Feel, configuration and help system.","Process Name":"cervisia","Link":"https:\/\/linux.die.net\/man\/1\/cervisia"}},{"Process":{"Description":"cfg-clex builds the initial configuration file for the CLEX file manager. Its use is optional. It is intended for new users running CLEX for the first time or when migrating from an older version. Once the initial configuration file is created, all further changes should be made using the CLEX's configuration panel.","Process Name":"cfg-clex","Link":"https:\/\/linux.die.net\/man\/1\/cfg-clex"}},{"Process":{"Description":"Maintains widget palette configuration for the Visual Builder. It can be stored in the system-wide and the local user config files. \"cfgmaint\" allows adding, renaming, moving, and deleting the classes and pages in the Visual Builder widget palette.","Process Name":"cfgmaint","Link":"https:\/\/linux.die.net\/man\/1\/cfgmaint"}},{"Process":{"Description":"Cfgmaker creates MRTG configuration files based on information pulled from a router or another SNMP manageable device. [community@]router Community is the community name of the device you want to create a configuration for. If not specified, it defaults to 'public'; you might want to try this first if you do not know the community name of a device. If you are using the wrong community name you will get no response from the device. Router is the DNS name or the IP number of an SNMP-managable device. Following the name you can specify 6 further options separated by colons. The full syntax looks like this: router[:[prt][:[tmout][:[retr][:[backoff][:vers]]]]] Of special interest may be the last parameter, vers. If you set this to '2' then your device will be queried with SNMP version 2 requests. This allows to poll the 64 bit traffic counters in the device and will thus work much better with fast interfaces (no more counter overrun). Note that the order in which the routers are specified on the command line do matter as the same order is used when the configuration file is generated. The first specified router has it's configuration lines genrated first, followed by the lines belonging to the next router and so on. Note that the first line of the generated cfg file will contain all the commandline options you used for generating it. This is to allow for the easy 'regeneration' in case you want to add newhosts or make some other global change. Configuration Except for the --output and --global options, all options affect only the routers following them on the command line. If an option specified earlier on the command line reappears later on the command line with another value, the new value overrides the old value as far as remaining routers are concerned. This way options might be tailored for groups of routers or for individual routers. See --output and --global for how their behaviour is affected by where or how many times they appear on the command line. See the Examples below on how to set an option differently for multiple routers. --help Print a brief help message and exit. --man Prints the manual page and exits. --version Print the version of cfgmaker. This should match the version of MRTG for which config files are being created. --ifref nr| ip| eth| descr| name Select the interface identification method. Default is nr which identifies the router interfaces by their number. Unfortunately the interface numbering scheme in an SNMP tree can change. Some routers change their numbering when new interfaces are added, others change thier numbering every full moon just for fun. To work around this sad problem MRTG can identify interfaces by 4 other properties. None of these works for all interfaces, but you should be able to find one which does fine for you. Note that especially ethernet addrsses can be problematic as some routers have the same ethernet address on most of their interface cards. Select ip to identify the interface by its IP number. Use eth to use the ethernet address for identification. Use descr to use the Interface description. Or use name to use the Interface name. If your chosen method does not allow unique interface identification on the device you are querying, cfgmaker will tell you about it. --ifdesc nr| ip| eth| descr| name| type| alias Select what to use as the description of the interface. The description appears in the \"Title[]\" property for the target as well as the text header in the HTML code defined in the target's \"PageTop[]\". Default is to use nr which is just the interface number which isn't always useful to the viewer of the graphs. There are 6 other properties which could be used. Use ip if you want to use the interface's IP-address. Use eth if you want to use the interface's ethernet address. If you want a better description, you can use either descr, name or alias. Exactly what each of these do varies between different equipment so you might need to experiment. For instance, for a serial interface on a Cisco router running IOS using name might result in \"S0\" being the interface description , descr might result in \"Serial0\" and alias might result in \"Link to HQ\" (provided that is what is used as the interface's \"description\" in the router's configuration). Finally, if you want to describe the interface by it's Btype (i.e \"ethernetCSMA\", \"propPointtoPoint\" etc) you can use type. --if-filter ' filter-expression' First of all, this is under some developement and is experimental. Use this if you want to have better control over what interfaces gets included into the configuration. The filter-expression is evaluated as a piece of Perl code and is expected to return a truth value. If true, include the interface and if false, exclude the interface. For a further discussion on how these filters work, see the section \"Details on Filters\" below. --if-template template-file First of all, this is under some development and is experimental. Use this if you want to control what the line for each target should look like in the configuration file. The contents of the file template-file will be evaluated as a Perl program which generates the lines using certain variables for input and output. For a further discussion on how these templates work, see the section \"Details on Temaplates\" below. --host-template template-file First of all, this is under some development and is experimental. Use this if you want to have some extra targets related to the host itself such as CPU utilization, ping response time to the host, number of busy modems etc. The contents of the file template-file will be evaluated once per host as a Perl program which generates the lines using certain variables for input and output. For a further discussion on how these templates work, see the section \"Details on Templates\" below. --community community-string Use this to set the community for the routers following on the command line to community-string. Individual routers might overrride this community string by using the syntax community@router. --enable-ipv6 This option enables IPv6 support. It requires the appropriate perl modules; if they are not found then IPv6 is disabled (see the ipv6 documentation). cfgmaker will use IPv6 or IPv4 depending on the target. If the target is a numeric address, the protocol depends on the type of address. If the target is a hostname, cfgmaker will try to resolve the name first to an IPv6 address then to an IPv4 address. IPv6 numeric addresses must be specified between square braces. For example: cfgmaker --enable-ipv6 [2001:760:4::1]:165:::2 If the target has both an IPv6 address and an IPv4 address with the same hostname, cfgmaker first queries the target using IPv6 and falls back to IPv4 if it fails. This is useful for targets which don't support SNMP over IPv6. --use-16bit This option forces the use of 16bit SNMP request IDs. Some broken SNMP agents do not accept 32bit request IDs. Try to avoid this option as much as possible, complain to your agent vendor instead. --snmp-options :[ port][:[ timeout][:[ retries][:[ backoff][: version]]]] Use this to set the default SNMP options for all routers following on the command line. Individual values might be omitted as well as trailing colons. Note that routers might override individual (or all) values specified by --snmp-options by using the syntax router[:[port][:[timeout][:[retries][:[backoff][:version]]]]] --global \" bla: abc \" Use this to add global options to the generated config file. You can call --global several times to add multiple options. The line will appear in the configuration just before the config for the next router appearing on the command line. --global \"workdir: \/home\/mrtg\" If you want some default Options you might want to put --global \"options[_]: growright,bits\" Specifying --global after the last router on the command line will create a line in the configuration file which will appear after all the routers. --noreversedns Do not try to reverse lookup IP numbers ... a must for DNS free environments. --no-down Normally cfgmaker will not include interfaces which are marked anything but administratively and operationally UP . With this switch you get them all. --show-op-down Include interfaces which are operatively down. --zero-speed speed Assign this speed in bits-per-second to all interfaces which return 0 for ifSpeed and ifHighSpeed. Some switches, notably Foundry equipment, return a speed of zero for some interfaces. For example, to have all interfaces reporting zero set to 100Mbps, use --zero-speed=100000000. --subdirs format Give each router its own subdirectory for the HTML and graphics (or .rrd) files. The directory name is the given format string with a couple of pattern replacements. The string \" HOSTNAME \" will be replaced by the hostname of the router (however you specified it on the cfgmaker commandline -- it may be an actual hostname or just an IP address), and \" SNMPNAME \" will be replaced with the device's idea of its own name (the same name that appears on the right side of the \"Title\" lines). For instance, a call like: cfgmaker --subdirs=HOSTNAME__SNMPNAME public@10.10.0.18 would result in the generation of lines looking something like: Directory[10.10.0.18_1]: 10.10.0.18__fp2200-bothrip-1.3 --output file Write the output from cfgmaker into the file file. The default is to use \"STDOUT\". --output is expected to appear only once on the command line. If used multiple times, the file specified by the last --output will be used. --nointerfaces Don't generate configuration lines for interfaces. This makes cfgmaker skip all steps related to interfaces which means it will not do any polling of the router to retrieve interface information which speeds up the execution of cfgmaker and it will neither run any interface templates. --interfaces This makes cfgmaker generate configuration lines for interfaces (the default behaviour). The main usage of this option is to negate an --nointerfaces appearing earlier on the command line. SNMP V3 Options Cfgmaker supports SNMP V3 using the Net:SNMP perl module. There are optional parameters affecting SNMP operation. --enablesnmpv3 {yes|no} The --enablesnmpv3 option is an optional flag to check for the presence of the Net::SNMP libraries. Cfgmaker will try to determine whether this flag is required and will set the values automatically. SNMPv3 Arguments A SNMP context is a collection of management information accessible by a SNMP entity. An item of management information may exist in more than one context and a SNMP entity potentially has access to many contexts. The combination of a contextEngineID and a contextName unambiguously identifies a context within an administrative domain. In a SNMPv3 message, the contextEngineID and contextName are included as part of the scopedPDU. All methods that generate a SNMP message optionally take a --contextengineid and --contextname argument to configure these fields. Context Engine ID The --contextengineid argument expects a hexadecimal string representing the desired contextEngineID. The string must be 10 to 64 characters (5 to 32 octets) long and can be prefixed with an optional \"0x\". Once the --contextengineid is specified it stays with the object until it is changed again or reset to default by passing in the undefined value. By default, the contextEngineID is set to match the authoritativeEngineID of the authoritative SNMP engine. Context Name The contextName is passed as a string which must be 0 to 32 octets in length using the --contextname argument. The contextName stays with the object until it is changed. The contextName defaults to an empty string which represents the \"default\" context. User-based Security Model Arguments The User-based Security Model ( USM ) used by SNMPv3 requires that a securityName be specified using the --username argument. The creation of a Net::SNMP object with the version set to SNMPv3 will fail if the --username argument is not present. The --username argument expects a string 1 to 32 octets in length. Different levels of security are allowed by the User-based Security Model which address authentication and privacy concerns. A SNMPv3 target will derive the security level (securityLevel) based on which of the following arguments are specified. By default a securityLevel of 'noAuthNoPriv' is assumed. If the --authkey or --authpassword arguments are specified, the securityLevel becomes 'authNoPriv'. The --authpassword argument expects a string which is at least 1 octet in length. Optionally, the --authkey argument can be used so that a plain text password does not have to be specified in a script. The --authkey argument expects a hexadecimal string produced by localizing the password with the authoritativeEngineID for the specific destination device. The \"snmpkey\" utility included with the Net::SNMP distribution can be used to create the hexadecimal string (see snmpkey). Two different hash algorithms are defined by SNMPv3 which can be used by the Security Model for authentication. These algorithms are HMAC-MD5-96 \" MD5 \" ( RFC 1321) and HMAC-SHA-96 \" SHA-1 \" ( NIST FIPS PUB 180-1). The default algorithm used by the module is HMAC-MD5-96 . This behavior can be changed by using the --authprotocol argument. This argument expects either the string 'md5' or 'sha' to be passed to modify the hash algorithm. By specifying the arguments --privkey or --privpassword the securityLevel associated with the object becomes 'authPriv'. According to SNMPv3, privacy requires the use of authentication. Therefore, if either of these two arguments are present and the --authkey or --authpassword arguments are missing, the creation of the object fails. The --privkey and --privpassword arguments expect the same input as the --authkey and --authpassword arguments respectively. The User-based Security Model described in RFC 3414 defines a single encryption protocol to be used for privacy. This protocol, CBC-DES \" DES \" ( NIST FIPS PUB 46-1), is used by default or if the string 'des' is passed to the --privprotocol argument. By working with the Extended Security Options Consortium http:\/\/www.snmp.com\/eso\/, the module also supports additional protocols which have been defined in draft specifications. The draft http:\/\/www.snmp.com\/eso\/draft-reeder-snmpv3-usm-3desede-00.txt defines the support of CBC-3DES-EDE \"Triple-DES\" ( NIST FIPS 46-3) in the User-based Security Model. This protocol can be selected using the --privprotocol argument with the string '3desede'. The draft http:\/\/www.snmp.com\/eso\/draft-blumenthal-aes-usm-04.txt describes the use of CFB128-AES-128\/192\/256 \" AES \" ( NIST FIPS PUB 197) in the USM . The three AES encryption protocols, differentiated by their key sizes, can be selected by passing 'aescfb128', 'aescfb192', or 'aescfb256' to the -privprotocol argument. Details on Filters The purpose of the filters is to decide which interfaces to accept and which interfaces to reject. This decision is done for each interface by evaluating the filter expression as a piece of Perl code and investigating the result of the evaluation. If true, accept the interface otherwise reject it. When working with filters, remember that Perl has it's own idea of what truth and false is. The empty string \"\" and the string \"0\" are false, all other strings are true. This further imples that any integer value of 0 is false as well as any undef value. It also implies that all references are considered true. As the filter is evaluated as a Perl expression, several useful constructs in Perl are worth mentioning: Expressions might be grouped by using parentheses \"()\". Expressions might be combined using boolean operators such as the following: \" and\" (equivalent with \" &&\") Boolean \"and\" of the two expressions, is only true if both expressions are true. Example: expression1 and expression2 \" or\" (equivalent with \" ||\") Boolean \"or\" of the two expressions, is true if either or both expressions are true. Example: expression1 or expression2 \" not\" (equivalent with \" !\") Boolean negation of a single expression. Example: not expression . Yet another example: ! expression (For more details on this I recommend a book on Perl) Predefined Filter Variables To facilitate, there are a number of predefined values available to use in the filter. Note that these variables are also available when templates interfaces are evaluated (but not host templates). Caveat: All these variables' names begin with a dollar sign ($), which is a syntactic requirement for scalar variables in Perl. The danger here is that the dollar sign in many shells is an active character (often used for shell variables exactly as in Perl variables) so it is important to ensure that the Perl expression isn't evaluated by the command line shell as shell code before being passed to cfgmaker as command line arguments. In shells like Bourne shell, ksh shell or bash shell, placing the entire expression within single qoutes will avoid such accidental evaluation: '--if-filter=($default_iftype && $if_admin)' $if_type This is an integer specifying the interface type as per the SNMP standards and as reported by the polled device. A complete list of interface types would be impractical for this document , but there are a number predefined varables below. Normally, cfgmaker puts in the target's PageTop this iftype value within paranthesis after the name of the interface type. (e.g \"propPointToPointSerial (22)\"). Here's a list of some of the most common interface types by number:   6 ethernetCsmacd\n  7 iso88023Csmacd\n  9 iso88025TokenRing\n 15 fddi\n 19 E1\n 20 basicISDN\n 21 primaryISDN\n 22 propPointToPointSerial\n 23 ppp\n 24 softwareLoopback\n 30 ds3\n 32 frame-relay\n 33 rs232\n 37 atm\n 39 sonet\n 44 frameRelayService\n 46 hssi\n 49 aal5\n 53 propVirtual\n 62 Fast Ethernet (100BaseT)\n 63 ISDN & X.25\n 69 Full Duplex Fast Ethernet (100BaseFX)\n 94 Asymetric Digital Subscriber Loop (ADSL)\n117 Gigabit Ethernet\n134 ATM Sub Interface $default True if and only if cfgmaker normally should accepted the interface based on the interfaces administrative and operational state (taking the flags --no-down and --show-op-down into account) and it's type (and a few other things). $default_ifstate True if and only if cfgmaker would have accepted the interface based on it's operational and administrative states (also taking into account the presence of the flags --no-down and --show-op-down). $default_iftype True if and only if cfgmaker would have accepted the interface based on it's type (and a few type specific details in addition). $if_admin True if and only if the interface is in an adminstrative up state. $if_oper True if and only if the interface is in an operational up state. A number of variables are also predefined to easily decide if an interface belong to a certain cathegory or not. Below is all those variables listed together with which if_type numbers each variable will be true for. Note that some variables refer to other variables as well. $if_is_ethernet True for ethernet interfaces (nr 6, 7, 26, 62, 69 and 117). $if_is_isdn True for various ISDN interface types (nr 20, 21, 63, 75, 76 and 77) $if_is_dialup True for dial-up interfaces such as PPP as well as ISDN . (nr 23, 81, 82 and 108 in addition to the numbers of $if_is_isdn). $if_is_atm True for miscellaneous ATM related interface types (nr 37, 49, 107, 105, 106, 114 and 134). $if_is_wan True for WAN interfaces point to point, Frame Relay and High Speed Serial ( 22,32,44,46) $if_is_lan True for LAN interfaces (8, 9, 11, 15, 26, 55, 59, 60 and 115 in addition to the numbers of $if_is_ethernet). $if_is_dsl True for ADSL , RDSL , HDSL and SDSL (nr 94, 95, 96, 97) $if_is_loopback True for software loopback interfaces (nr 24) $if_is_ciscovlan True for Cisco VLAN interfaces (interfaces with the word Vlan or VLAN in their ifdescs) $if_vlan_id Returns the vlan id associated with a specific port on Cisco Catalyst switches under both Catalyst OS and IOS , and 3Com switches. If it is not a vlan interface, will return undef. $if_cisco_trunk Returns the trunking state of a specific port on Cisco Catalyst switches under both Catalyst OS and IOS . Returns \"1\" if the interface is a trunk, undef otherwise. $if_MTU Returns the Maximum Transfer Unit associated with a specific port. Besides that, you can also use the variables defined for templates below. Further, all the variables available in cfgmaker is at the scripts disposal even if the use of such features is discouraged. More \"shortcuts\" in the form of variables and functions will be made avaiable in the future instead. Examples on Filters The following filter will not affect which interfaces get's included or excluded, it will make cfgmaker behave as normally. '--if-filter=$default' The following filter will make cfgmaker exclude PPP (23) interfaces: '--if-filter=$default && $if_type!=23' The following filter will make cfgmaker behave as usual except that it will consider the operational state of an interface irrelevant but still reject all interfaces which are administratively down. '--if-filter=$if_admin && $default_iftype' Details on Templates The contents of the template files are evaluated as a Perl program. A number or Perl variables are available for the program to read and others are used to be written to. As quite a few of the predefined variables has values which are are supposed to be used in HTML code some of them have an \"HTML-escaped\" variant, e.g $html_syslocation is the HTML escaped variant of $syslocation. The HTML escaping means that the chars \"<\", \">\" and \"&\" are replaced by \"&lt;\", \"&gt;\" and \"&amp;\" and that newlines embedded in the string are prepended with \"< BR >\" and appended with a space character (if a newline is last in the string it is not touched). Writable Template Variables These are the variables available to store the configuration lines in. Some of them are initialized prior to the evaluation of the template but such content normally is comments for inclusion in the final configuration file so those variables might be reset to the empty string in the template code to eliminate the comments. The other way around is also possible, the contents of these variables might be extended with further information for various reasons such as debugging etc. Once the template has been evaluated, the following happens: if the template is a interface template and the actual interface for some reason is rejected and thus needs to be commented out, all the lines in the variable $target_lines are turned into comments by adding a hash mark (\"#\") at their beginning. Then all the variables $head_lines, $problem_lines , $target_lines and $separator_lines are concatenated together to form the lines to add to the configuration file. $target_lines This variable is the placeholder for the configuration lines created by the template. $target_lines is predefined to be empty when the template code is evaluated. $head_lines This variable is intended to be the placeholder for the comment line appearing just before the target in the configuration file. It is initialized with that comment line before the evaluation of the template code and if the template doesn't modify $head_lines during evaluation, the comment will look like usual in the config file. $problem_lines This variable is intended to be the placholder for the comment lines describing any problems which might have been encountered when trying to add the target into the configuration. For host templates it's normally not used and for those it's predefined as the empty string. For interface templates $problem_lines is predefined with the error description comments which cfgmaker normally would use for rejected interfaces or as the empty string for accepted interfaces. It is possible to test against $problem_lines to find out if an interface will be included or rejected but this is not recommended. Test against $if_ok instead. $separator_lines This variable is the placeholder for the string to use as the separator between the code for individual targets. The contents of this variable is put after each target (so the lines will appear after the end of the last target in the config as well). Predefined Template Variables All the variables below are available for interface templates to use. For host templates, only those listed under \"Host and System Variables\" are available. For interface templates the variables listed under \"Predefined Filter Variables\" are also available. Host and System Variables $router_name This is the fully qualified name for the router. It is affected by the following items on the command line: the router name itself and --dns-domain. $router_connect This is the reference string for the router being polled. It is on the form community@router possibly followed by some snmp options. It is affected by the following items on the command line: the router name itself, --community, --snmp-options and --dns-domain. (There's no HTML escaped variant available) $directory_name This variable should contain the directory name as cfgmaker normally would use as the value for the \"Directory[]\" directive. The value is determined by the --subdirs command line option. If --subdirs isn't specified $directory_name will be the empty string. (There's no HTML escaped variant available) $syscontact This variable is the router's SNMP sysContact value. ( HTML escaped variant: $html_syscontact) $sysname This variable is the router's SNMP sysName value. (No HTML escaped variant available) $syslocation This variable is the router's SNMP sysLocation value. ( HTML escaped variant: $html_syslocation) $sysdescr This variable is the router's SNMP sysDescr value. It is normally not used by cfgmaker but might be useful in a template. ( HTML escaped variant: $html_sysdescr) Interface Target Related Variables $target_name This is what cfgmaker normally would use as the the name of the target. The target name is what is found within the square brackets, \"[]\", for target directives. (There's no HTML escaped variant available) $if_ref This the reference string for the interface. It is expected to be used in the \"Target[xyz]\" directive to distinguish what interface to use. The value of this variable is affected by the --ifref command line option. It is normally used together with $router_connect. (There's no HTML escaped variant available) $if_ok This variable is true if the interface is going to be included into the configuration file, otherwise false. Don't test against other variables such as $problem_lines to find out if an interface will be rejected or not, use this $if_ok instead. $default_target_lines This variable contains all the target lines which cfgmaker by default outputs for this interface. It's useful if you want to have the \"standard target\" but want to add some extra lines to it by using a template. By default cfgmaker uses the following directives for each target it generates: Target[], SetEnv[], MaxBytes[], Title[], PageTop[] and if there is any directory specified also the Directory[] directive. To facilitate the creation of templates which generates target configs which are similar to the default one, each of the above mentioned directive lines have a corresponding variable containing the line as cfgmaker would have output it by default. Note that none of these have a HTML escaped variant, text in them is HTML escaped where needed. Also note that they do not have any newline at the end. $default_target_directive This variable contains the default string for the Target[] directive line. $default_setenv_directive This variable contains the default string for the SetEnv[] directive line. $default_directory_directive This variable contains the default string for the Directory[] directive line which means it is an empty string (with no newline) if there's no directory. $default_maxbytes_directive This variable contains the default string for the MaxBytes[] directive line. $default_title_directive This variable contains the default string for the Title[] directive line. $default_pagetop_directive This variable contains the default string for the PageTop[] directive lines. Interface Network Configuration Variables $if_ip This variable should contain the IP-address of the interface, if any has been assigned to it. (There's no HTML escaped variant available) $ifindex This variable is the SNMP ifIndex for the interface which per definition always is an integer. (There's no HTML escaped variant available) $if_index Equivalent with $ifindex. $if_eth Contains the ethernet address of the interface, if any. (There's no HTML escaped variant available) $if_speed This variable is the speed in bytes\/second (with prefixes). (There's no HTML escaped variant available) $if_speed_str This variable is a cooked speed description which is either in bits or bytes depending on wether or not the bits option is active and also with the proper prefix for the speed (k, M, G etc). (No HTML escaped variant available) $if_type_desc This variable is a textual description of the interface type. ( HTML escaped variant: $html_if_type_desc) $if_type_num This variable the integer value corresponding to the interface type (for a listing for the value for the more common interface types, see the section DETAILS ON FILTERS above). (No HTML escaped variant available) $if_dns_name This is the DNS name for the interface. (No HTML escaped variant available) Interface Name, Description and Alias Variables It might seem confusing with both Name, Description and Alias in this context and to some extent it is. Name and Description are usually supported on most equipment but how they are used varies, both between manufacturers as well as between different cathegories of equipment from the same manufacturer. The Alias is at least supported by Cisco IOS , and that variable contains whatever is used in the IOS statement called \"description\" for the interface (not to be confused with the SNMP variables for Description). For better control from the command line consider $if_title_desc which contents are controlled by the --if-descr command line option. $if_snmp_descr This variable should contain the \"raw\" description of the interface as determined by the SNMP polling of the router. ( HTML escaped variant: $html_if_snmp_descr) $if_snmp_name The \"raw\" name for the interface as provided by SNMP polling. ( HTML escaped variant: $html_if_snmp_name) $if_snmp_alias The \"raw\" ifAlias for the interface as provided by SNMP polling. ( HTML escaped variant: $html_if_snmp_alias) $if_cisco_descr The \"raw\" CiscolocIfDescr for the interface as provided by SNMP polling. ( HTML escaped variant: $html_if_cisco_descr) $if_description This is the \"cooked\" description string for the interface, taking into account the SNMP values found for the interface's RDescr, ifAlias and CiscolocIfDescr. ( HTML escaped variant: $html_if_description) $if_title The full string cfgmaker by default would have used for the Title[] directive in the configuration as well as the content of the topmost H1 tag in the PageTop[]. Is composed by the contents of $desc_prefix, $if_title_desc and $sysname. As $if_title depends on $if_title_desc, it is possible to indirectly control $if_title by using the command line option --if-descr. ( HTML escaped variant: $html_if_title) $if_port_name If the host is a Cisco Catalyst LAN switch, this variable is the name of that port. (No HTML escaped variant available) $if_pp_port_name If the host is a Nortel Passport LAN switch, this variable is the name of that port. (No HTML escaped variant available) $desc_prefix This variable is a prefix of the description of what the target is to use in the \"Title[]\" directive and in the H1 section of the \"PageTop[]\". Default is \"Traffic analysis for \". ( HTML escaped variant: $html_desc_prefix) $if_title_desc This is the description of the interface normally used by cfgmaker as part of the variable $if_title. The latter is used as the full string in the \"Title[]\" directove and the H1 section in the PageTop[]. $if_title_desc is controlled by the command line option --if-descr which indirectly controls the contents of $if_title ( HTML escaped variant: $html_if_title_desc) Help Functions for Templates The following functions exists to facilitate the writing of host and interface templates. html_escape(string) html_escape() takes a string as an argument and returns a new string where the following substitutions has been done: the chars \"<\", \">\" and \"&\" are replaced by \"&lt;\", \"&gt;\" and \"&amp;\" and that newlines embedded in the string are prepended with \"< BR >\" and appended with a space character (newlines at the end of the string are not touched). oid_pick($router_connect,$v3opt,\"oid1\",\"oid2\"...) This function will try to poll each of the oids specified until it is successful or has run out of oids. It will return the name of the first oid that worked or undef if it is not successful Example Template Files Template Example 1: Eliminating Rejected Targets From Appearing This template file generates exactly the same configuration code per interface as cfgmaker does by default, with the exception that it eliminates all lines (comments as well as config code) for an interface if the interface happens to be rejected. if(not $problem_lines)\n{\n  $target_lines .= <<ECHO;\n\nTarget[$target_name]: $if_ref:$router_connect\nSetEnv[$target_name]: MRTG_INT_IP=\"$if_ip\" MRTG_INT_DESCR=\"$if_snmp_descr\"\nECHO\n\n  if ($directory_name) {\n      $target_lines .= \"Directory[$target_name]: $directory_name\\n\";\n  }\n\n  $target_lines .= <<ECHO;\nMaxBytes[$target_name]: $if_speed\nTitle[$target_name]: $html_desc_prefix$html_if_title_desc -- $sysname\nPageTop[$target_name]: <h1>$html_desc_prefix$html_if_title_desc -- $sysname<\/h1>\n               <div id=\"sysdetails\">\n                       <table>\n                               <tr>\n                                       <td>System:<\/td>\n                                       <td>$sysname in $html_syslocation<\/td>\n                               <\/tr>\n                               <tr>\n                                       <td>Maintainer:<\/td>\n                                       <td>$html_syscontact<\/td>\n                               <\/tr>\n                               <tr>\n                                       <td>Description:<\/td>\n                                       <td>$html_if_description<\/td>\n                               <\/tr>\n                               <tr>\n                                       <td>ifType:<\/td>\n                                       <td>$html_if_type_desc ($if_type_num)<\/td>\n                               <\/tr>\n                               <tr>\n                                       <td>ifName:<\/td>\n                                       <td>$html_if_snmp_name<\/td>\n                               <\/tr>\nECHO\n\n  $target_lines .= <<ECHO if defined $if_port_name;\n                               <tr>\n                                       <td>Port Name:<\/td>\n                                       <td>$if_port_name<\/td>\n                               <\/tr>\nECHO\n\n  $target_lines .= <<ECHO if defined $if_pp_port_name;\n                               <tr>\n                                       <td>Port Name:<\/td>\n                                       <td>$if_pp_port_name<\/td>\n                               <\/tr>\nECHO\n\n  $target_lines .= <<ECHO;\n                               <tr>\n                                       <td>Max Speed:<\/td>\n                                       <td>$if_speed_str<\/td>\n                               <\/tr>\nECHO\n\n  $target_lines .= <<ECHO if $if_ip;\n                               <tr>\n                                       <td>Ip:<\/td>\n                                       <td>$if_ip ($if_dns_name)<\/td>\n                               <\/tr>\nECHO\n\n  $target_lines .= <<ECHO;\n                       <\/table>\n               <\/div>\nECHO\n} else {\n  $head_lines=\"\";\n  $problem_lines=\"\";\n  $target_lines=\"\";\n  $separator_lines=\"\";\n} Template Example 2: Simplier Version of Example 1 Example 1 was partly intended to demonstrate how to customize the generation of interface targets but also to provide a hint of how the variables are used in the \"default\" template which one could consider that cfgmaker normally uses. If you're only intrested in the easiest way of entirely eliminating those reject interfaces, the template below would do the job as well by using $default_target_lines. if($if_ok) {\n $target_lines = $default_target_lines;\n} else {\n  $head_lines=\"\";\n  $problem_lines=\"\";\n  $target_lines=\"\";\n  $separator_lines=\"\";\n} Template Example 3: Creating CPU Targets for Hosts Below is an example of a host template. $head_lines .= <<ECHO;\n#---------------------------------------------------------------------\nECHO\n\nmy $target_name = $router_name . \".cpu\";\n\n$target_lines .= <<ECHO;\n\nYLegend[$target_name]: Percentage CPU load\nShortLegend[$target_name]: %\nLegend1[$target_name]: CPU load in %\nLegend2[$target_name]:\nLegend3[$target_name]: Max Observed CPU load\nLegend4[$target_name]:\nLegendI[$target_name]: &nbsp;CPU Load:\nLegendO[$target_name]:\nWithPeak[$target_name]: ywm\nMaxBytes[$target_name]: 100\nOptions[$target_name]: growright, gauge, nopercent\nTitle[$target_name]: $router_name CPU load\nTarget[$target_name]: 1.3.6.1.4.1.9.2.1.58.0&1.3.6.1.4.1.9.2.1.58.0:$router_connect\nPageTop[$target_name]: <h1>$router_name CPU load<\/h1>\n               <div>\n                       <table>\n                               <tr>\n                                       <td>System:<\/td>\n                                       <td>$router_name in $html_syslocation<\/td>\n                               <\/tr>\n                               <tr>\n                                       <td>Maintainer:<\/td>\n                                       <td>$html_syscontact<\/td>\n                               <\/tr>\n                               <tr>\n                                       <td>Description:<\/td>\n                                       <td>$html_sysdescr<\/td>\n                               <\/tr>\n                               <tr>\n                                       <td>Resource:<\/td>\n                                       <td>CPU.<\/td>\n                               <\/tr>\n                       <\/table>\n               <\/div>\nECHO","Process Name":"cfgmaker","Link":"https:\/\/linux.die.net\/man\/1\/cfgmaker"}},{"Process":{"Description":"The cflow utility shall analyze a collection of object files or assembler, C-language, lex, or yacc source files, and attempt to build a graph, written to standard output, charting the external references.","Process Name":"cflow","Link":"https:\/\/linux.die.net\/man\/1\/cflow"}},{"Process":{"Description":"cftp is a client for logging into a remote machine and executing commands to send and receive file information. It can wrap a number of file transfer subsystems The options are as follows:       -B'        Specifies the default size of the buffer to use for sendingand receiving. (Default value: 32768 bytes.) -b' File to read commands from, '-' for stdin. (Default value: interactive\/stdin.) -R' Number of requests to make before waiting for a reply. -s' Subsystem\/server program to connect to. The following commands are recognised by cftp : cd path Change the remote directory to 'path'. chgrp gid path Change the gid of 'path' to 'gid'. chmod mode path Change mode of 'path' to 'mode'. chown uid path Change uid of 'path' to 'uid'. exit' Disconnect from the server. get remote-path [local-path] Get remote file and optionally store it at specified local path. help' Get a list of available commands. lcd path Change local directory to 'path'. lls [ls-options [path]] Display local directory listing. lmkdir path Create local directory. ln linkpath targetpath Symlink remote file. lpwd' Print the local working directory. ls [-l [path]] Display remote directory listing. mkdir path Create remote directory. progress Toggle progress bar. put local-path [remote-path] Transfer local file to remote location pwd' Print the remote working directory. quit' Disconnect from the server. rename oldpath newpath Rename remote file. rmdir path Remove remote directory. rm path Remove remote file. version Print the SFTP version. ?' Synonym for 'help'.","Process Name":"cftp","Link":"https:\/\/linux.die.net\/man\/1\/cftp"}},{"Process":{"Description":"Takes a variable number of arguments where the first argument should either be a Cogito command or one of the supported options. If no arguments are specified an overview of all the Cogito commands will be shown. Enables all Cogito commands to be accessed as subcommands, for example is: cg help\ncg-help equivalent.","Process Name":"cg","Link":"https:\/\/linux.die.net\/man\/1\/cg"}},{"Process":{"Description":"Takes a list of file names at the command line, and schedules them for addition to the GIT repository at the next commit. The command will fail if one of the given files does not exist. Note that directories never have to be added to the repository, and are not tracked on their own. That means, you cannot currently add an empty directory to Cogito. The reason for this is that Cogito manages content and empty directories have no content. Directories are added automatically when adding files inside them, or you can add all files in a directory using cg-add -r.","Process Name":"cg-add","Link":"https:\/\/linux.die.net\/man\/1\/cg-add"}},{"Process":{"Description":"Cat a file of a given filename in a given revision (or in the current one) to stdout.","Process Name":"cg-admin-cat","Link":"https:\/\/linux.die.net\/man\/1\/cg-admin-cat"}},{"Process":{"Description":"Optionally takes a commit or tree ID as a parameter, defaulting to HEAD.","Process Name":"cg-admin-ls","Link":"https:\/\/linux.die.net\/man\/1\/cg-admin-ls"}},{"Process":{"Description":"Lists IDs of all the objects of a given type found in the dircache. Takes the object type as the first parameter, defaults to all objects. WARNING! This command does not list packed objects, so its usefulness is rather limited right now. That's a big TODO. The possible object types are: blob This object is a pure storage object containing some user data. commit This object ties directory hierarchies together into a DAG of revisions. tree This object is an object that ties one or more blob objects into a directory structure. tag This object ties a symbolic release tag to an object in the database. See the git README for more information about the object types.","Process Name":"cg-admin-lsobj","Link":"https:\/\/linux.die.net\/man\/1\/cg-admin-lsobj"}},{"Process":{"Description":"Setup a public GIT repository, that is, one that has no attached working copy and you typically only push into it and pull from it. You need to run this command before you will be able to push into the repository for the first time, but you will be also unable to pull from the repository until you push into it at first. Therefore the workflow is to first init a regular private repository, then use this command to create the public one, then add the appropriate remote branch (cg-branch-add origin ...) in your private repository and then push to the public repository. Use gitlink:cg-init[1] if you want to instead create a new GIT project. The command will create the repository to reside in DIRECTORY (the directory must not exist before calling this command). By default, it will be world-readable, but writable only by you. If you want to make it possible for multiple users to push, create a group for them and use the -g parameter, which will make gitlink:cg-admin-setuprepo[1] set up the permissions properly. The repository will also be set up so that git-update-server-info will be automagically re-ran after each push, in short making it suitable for HTTP access. Note that you might need to do other additional steps, like touching the git-daemon-export-ok file if you want to make the repository accessible by the git daemon (serving the git:\/\/... URIs).","Process Name":"cg-admin-setuprepo","Link":"https:\/\/linux.die.net\/man\/1\/cg-admin-setuprepo"}},{"Process":{"Description":"Takes a commit ID which is the earliest commit to be removed from the repository. If no parameter is passed, it uncommits the latest commit (HEAD). Read the CAVEATS section before using it for the first time. This command is a close relative of cg-switch -f (which does essentially the same thing, but is slightly more powerful at the expense of a more elaborate usage). Do not confuse either with the operation performed by gitlink:cg-seek[1], which is meant only for temporary excursions to the project history.","Process Name":"cg-admin-uncommit","Link":"https:\/\/linux.die.net\/man\/1\/cg-admin-uncommit"}},{"Process":{"Description":"Takes the desired branch name and source location as parameters. This command lets you add to the list of remote branches. Those are branches in your local repository which correspond to some branches in other repositories. After you add a remote branch, you can gitlink:cg-fetch[1] from it to get the latest changes on the branch, as they appeared in the remote repository. Terminology note: This command concerns remote branches, not the local ones (those managed by gitlink:cg-switch[1]). The possible source location specifiers are: 1. Local path - note that fetching will hardlink the objects if possible. 2. rsync - rsync:\/\/host\/path THE rsync REPOSITORY ACCESS METHOD IS DEPRECATED AND WILL BE REMOVED IN THE FUTURE! The problem is that it will download all data from the remote repository, including objects which do not belong to the one particular branch you want to fetch. 3. HTTP - http:\/\/host\/path 4. SSH - git+ssh:\/\/host\/path or host:path (the latter can change); note that the path must be absolute in the first case. 5. GIT-Daemon - git:\/\/host\/path - this won't clone tags. The URL can have a fragment part, which identifies a branch inside of the remote repository. Otherwise, Cogito defaults to whatever branch the repository's HEAD points to at the time of each fetch.","Process Name":"cg-branch-add","Link":"https:\/\/linux.die.net\/man\/1\/cg-branch-add"}},{"Process":{"Description":"Takes the branch name and new source location as parameters. Terminology note: This command concerns remote branches, not the local ones (those managed by gitlink:cg-switch[1]).","Process Name":"cg-branch-chg","Link":"https:\/\/linux.die.net\/man\/1\/cg-branch-chg"}},{"Process":{"Description":"Print a listing of all known branches. The output has the following format: BRANCH LOCATION For example origin  http:\/\/www.kernel.org\/pub\/scm\/cogito\/cogito.git Terminology note: This command concerns remote branches, not the local ones (those managed by gitlink:cg-switch[1] and listed by cg-status -g).","Process Name":"cg-branch-ls","Link":"https:\/\/linux.die.net\/man\/1\/cg-branch-ls"}},{"Process":{"Description":"Cleans file and directories that are not under version control. When run without arguments, files ignored by cg-status and directories are not removed.","Process Name":"cg-clean","Link":"https:\/\/linux.die.net\/man\/1\/cg-clean"}},{"Process":{"Description":"This clones a remote GIT repository and checks it out locally. Takes a parameter specifying the location of the source repository and an optional second parameter specifying the destination. If the second parameter is omitted, the basename of the source repository is used as the destination.","Process Name":"cg-clone","Link":"https:\/\/linux.die.net\/man\/1\/cg-clone"}},{"Process":{"Description":"Commits your changes to the GIT repository. Accepts the commit message from stdin. If the commit message is not modified the commit will be aborted. By default, the commit is recorded as made by you, but you can change that - useful if you are e.g. applying a patch submitted by someone else. See the ENVIRONMENT section below. Note that you can undo a commit by the gitlink:cg-admin-uncommit[1] command, but that is possible only under special circumstances. See the CAVEATS section of its documentation.","Process Name":"cg-commit","Link":"https:\/\/linux.die.net\/man\/1\/cg-commit"}},{"Process":{"Description":"Outputs a diff for converting the first tree to the second one. By default compares the current working tree to the state at the last commit. The output will automatically be displayed in a pager unless it is piped to a program.","Process Name":"cg-diff","Link":"https:\/\/linux.die.net\/man\/1\/cg-diff"}},{"Process":{"Description":"Takes a destination and optionally a tree ID as a parameter, defaulting to HEAD. The destination can be either a .tar, .tar.gz, .tar.bz2 or .tgz for generating a tarball. Other destination specifiers are assumed to be directory names, and the tree is exported to the given directory.","Process Name":"cg-export","Link":"https:\/\/linux.die.net\/man\/1\/cg-export"}},{"Process":{"Description":"Takes the branch name as an argument, defaulting to origin. This will fetch the latest changes from a remote repository to the corresponding branch in your local repository. Note that this operation does not involve merging those changes to your own branch - that is being done by the gitlink:cg-merge[1] command. gitlink:cg-update[1] exists to conveniently bundle the act of fetching and merging to your working branch together. Before the first fetch, you have to tell Cogito about the remote branch. This should be done by the gitlink:cg-branch-add[1] command. See its documentation for the list of supported fetching protocols and other details. Note that one exception to this is the origin branch, which was set to the location of the source repository if you created yours using the gitlink:cg-clone[1] command. Note that the operation now being performed by gitlink:cg-fetch[1] (fetching) was called pulling in the past. However, GIT recently changed this terminology, and after sufficient transition period, the pulling expression will be instead used for the operation now performed by the gitlink:cg-update[1] command. Please do not let this confuse you.","Process Name":"cg-fetch","Link":"https:\/\/linux.die.net\/man\/1\/cg-fetch"}},{"Process":{"Description":"Takes an optional argument describing the command to show the help for. The command can be specified either as COMMAND or cg-COMMAND. If the argument is left out an overview of all the Cogito commands will be shown. Note, short help for a command is also available by passing --help or -h to the command. The complete command manual is shown when passing --long-help (and is the same as doing \"cg-help command\").","Process Name":"cg-help","Link":"https:\/\/linux.die.net\/man\/1\/cg-help"}},{"Process":{"Description":"gitlink:cg-init[1] called in a non-empty directory will automatically add its contents in the initial commit. (Please note that certain default ignore rules are applied during this operation. If any files were not added due to this, cg-init will advise you what to do.) This command is intended for creating repositories for work on new projects. If you want to clone an existing project, see gitlink:cg-clone[1]. If you want to set up a public repository not for direct work but only for pushing\/pulling, see gitlink:cg-admin-setuprepo[1]. It is also possible to import repositories from other SCMs to GIT, see git-cvsimport(1), git-svnimport(1) and git-archimport(1).","Process Name":"cg-init","Link":"https:\/\/linux.die.net\/man\/1\/cg-init"}},{"Process":{"Description":"Display log information for files or a range of commits. The output will automatically be displayed in a pager unless it is piped to a program.","Process Name":"cg-log","Link":"https:\/\/linux.die.net\/man\/1\/cg-log"}},{"Process":{"Description":"Takes a parameter identifying the branch to be merged, defaulting to origin. This command merges all changes currently in the given branch to your current branch. This can produce a merging commit on your branch sticking the two branch together (so-called tree merge). However in case there are no changes in your branch that wouldn't be in the remote branch, no merge commit is done and commit pointer of your branch is just updated to the last commit on the remote branch (so-called fast-forward merge). In case of conflicts being generated by the merge, you have to examine the tree (cg-merge will tell you which files contain commits; the commits are denoted by rcsmerge-like markers <<<<, ====, and >>>>) and then do gitlink:cg-commit[1] yourself. gitlink:cg-commit[1] will know that you are committing a merge and will record it properly. Note that when you are merging remote branches, gitlink:cg-merge[1] will use them in the state they are currently at in your repository. If you want to fetch the latest changes from the remote repository, use gitlink:cg-fetch[1]. If you want to fetch the changes and then merge them to your branch, use the command gitlink:cg-update[1]. Also note that if you have local changes in your tree that you did not commit, cg-merge will always preserve them when fast-forwarding. When doing a tree merge, it will preserve them if they don't conflict with the merged changes, and report an error otherwise. In short, it should do the Right Thing (tm), never lose your local changes and never let them mix up with the merge.","Process Name":"cg-merge","Link":"https:\/\/linux.die.net\/man\/1\/cg-merge"}},{"Process":{"Description":"Generate a patch with diff statistics and meta info about each commit. Note that if you want to use this as the output interface for your GIT tree containing changes against upstream GIT tree, please consider using the StGIT tool (\"quilt for GIT\"), which will enable you to update your patches seamlessly, rebase them against the latest upstream version, directly send them over mail, etc.","Process Name":"cg-mkpatch","Link":"https:\/\/linux.die.net\/man\/1\/cg-mkpatch"}},{"Process":{"Description":"Takes either two filenames and removes a file from the first one to the second one, or a list of filenames and a dirname and moves all the files to the directory. The changes will be executed in your working tree immediately, but recorded to the repository only at the time of the next commit. Note that so far, GIT\/Cogito does not track file renames\/moves per se. Therefore, doing gitlink:cg-mv[1] is currently the same as doing gitlink:cg-rm[1], gitlink:cg-add[1] from the Cogito perspective, and no special information is recorded that the file moved around. When any rename tracking gets involved currently, it is purely heuristical method executed at the time of examination.","Process Name":"cg-mv","Link":"https:\/\/linux.die.net\/man\/1\/cg-mv"}},{"Process":{"Description":"If the ID is not provided, HEAD is used. The default behavior is to show the commit ID.","Process Name":"cg-object-id","Link":"https:\/\/linux.die.net\/man\/1\/cg-object-id"}},{"Process":{"Description":"This is basically just a smart patch wrapper. It handles stuff like mode changes, removal of files vs. zero-size files etc. For now the script can process the old-style cg-diff patches (those having the \"(mode: 0123)\" stuff on the + and --- lines) as well as the new-style git-diff patches (those with the \"diff --git\" lines).","Process Name":"cg-patch","Link":"https:\/\/linux.die.net\/man\/1\/cg-patch"}},{"Process":{"Description":"It will push your commits on the current branch (or as specified by the -r option) to the remote repository, provided that your commits follow the last commit in the remote repository. Note that if the remote repository is associated with a working tree copy, this command won't update that. Use cg-reset at the remote side to bring it in sync (but throw away any local changes in that tree). Consider setting up a standalone repository (see gitlink:cg-admin-setuprepo[1]). Takes the branch name as an argument, defaulting to \"origin\".","Process Name":"cg-push","Link":"https:\/\/linux.die.net\/man\/1\/cg-push"}},{"Process":{"Description":"Reverts the working tree to a consistent state before any changes to it (including merges etc.) were done. This command will rebuild the state of the tree according to the commit of .git\/refs\/heads\/master, so if your working tree got into basically any inconsistent state, this will cure it. Basically, this is the opposite of gitlink:cg-commit[1] in some sense. This command is complementary to gitlink:cg-restore[1], which only brings individual files in sync with their state at the time of the last commit.","Process Name":"cg-reset","Link":"https:\/\/linux.die.net\/man\/1\/cg-reset"}},{"Process":{"Description":"Restore given files to their original state. It recovers any files (or files passed as arguments to the command, respectively) removed locally whose removal was not recorded by gitlink:cg-rm[1]. If passed the -f parameter, it restores the files to their state as of the last commit (including bringing files removed with gitlink:cg-rm[1] back to life). If passed the -r parameter, it will not restore the file as of the last commit, but to the state in the given commit, tree, or blob. The list of files is mandatory in this case. For the \"restore-to-last-commit\" usage, this command is complementary to the gitlink:cg-reset[1] command, which forcefully abandons all the changes in the working tree and restores everything to a proper state (including unseeking, cancelling merge in progress and rebuilding indexes).","Process Name":"cg-restore","Link":"https:\/\/linux.die.net\/man\/1\/cg-restore"}},{"Process":{"Description":"Takes a list of file names at the command line, and schedules them for removal from the GIT repository at the next commit.","Process Name":"cg-rm","Link":"https:\/\/linux.die.net\/man\/1\/cg-rm"}},{"Process":{"Description":"Seeking will bring the working tree from its current HEAD to a given commit. Note that it changes just the HEAD of the working tree, not the branch it is corresponding to. It will return to the HEAD of the appropriate branch if passed no arguments. Therefore, for a quick excurse to the past of the master branch: $ cg-seek git-pasky-0.1\n$ cg-diff this master   # will do the correct thing\n$ cg-seek               # will restore what we had before For intuitiveness, specifying the branch name (cg-seek master) will do the right thing too. If you want to migrate your working tree to another branch, use gitlink:cg-clone[1] to create a new tree for the new branch, or gitlink:cg-switch[1] to also change your current tree to use the new branch. Note that during the time you are seeked out, commits, merges, and some other operations are blocked, since the next gitlink:cg-seek[1] or gitlink:cg-reset[1] invocation will happily wipe out their products silently. You can override this in the gitlink:cg-commit[1] command by passing it a -f parameter - this can be useful e.g. when you seeked to a commit which cannot be compiled and you want to commit a compilation fix, as long as you are aware that the commit of the fix will be rendered unreachable (you will be able to get back to it only if you remember its ID) at the moment you do next seek or a reset. If you want to save the commit, you can save it to a separate branch using cg-switch -n. Takes the target commit ID to seek to as an argument.","Process Name":"cg-seek","Link":"https:\/\/linux.die.net\/man\/1\/cg-seek"}},{"Process":{"Description":"The output includes the list of branches and merge status. Current branch is marked by \">\", remote branches are marked by \"R\". Then, the files in the working tree are printed out. The output has the following format: <status flag> <file> where <status flag> can be one of the following: ? <file> is unknown. A <file> has been added. D <file> has been deleted. ! <file> is gone from your working copy but not deleted by gitlink:cg-rm[1]. M <file> has been touched or modified. m <file> has been touched or modified, but will not be automatically committed the next time you call gitlink:cg-commit[1]. This is used during a merge to mark files which contained local changes before the merge.","Process Name":"cg-status","Link":"https:\/\/linux.die.net\/man\/1\/cg-status"}},{"Process":{"Description":"gitlink:cg-switch[1] can switch your current local branch (and working copy) to an existing branch, or create a new branch based on a given commit. Terminology note: This command concerns local branches (also called \"heads\"), not remote branches (those managed by gitlink:cg-branch-add[1]). Note that gitlink:cg-switch[1] is meant for permanent switching of your current local branch (permanent in the sense that you are going to work on it; you can obviously gitlink:cg-switch[1] again later). If you want to just causually explore the current state of a particular branch of commit, use gitlink:cg-seek[1].","Process Name":"cg-switch","Link":"https:\/\/linux.die.net\/man\/1\/cg-switch"}},{"Process":{"Description":"Creates a tag referencing the given commit (or HEAD). You can then use the tag anywhere you specify a commit or tree ID. cg-tag will try to sign the tag if you give it the -s option. You can override the default key choice by passing it the -k argument. Takes the tag name and optionally the associated ID as arguments.","Process Name":"cg-tag","Link":"https:\/\/linux.die.net\/man\/1\/cg-tag"}},{"Process":{"Description":"This command takes no arguments, and lists all tags in a given repository in alphabetical order, along with their corresponding SHA1 hash IDs.","Process Name":"cg-tag-ls","Link":"https:\/\/linux.die.net\/man\/1\/cg-tag-ls"}},{"Process":{"Description":"Takes the branch name as an argument, defaulting to origin. This is similar to running cg-fetch and cg-merge commands successively. Please refer to the documentation of those commands for more details about the operation. Note that if you are not doing own development but only following some project, it is recommended to use this command instead of gitlink:cg-fetch[1] + gitlink:cg-merge[1] since gitlink:cg-update[1] can handle some additional corner cases (in particular, if the remote branch rebases, gitlink:cg-update[1] will fast-forward instead of doing a tree merge and diverging). Note that in the GIT newspeak, the operation being performed by cg-update is now called pull, even though in the past, pull was the name for the operation being now done by cg-fetch, which is even still aliased to cg-pull. Please do not let this confuse you. (Cogito won't call this update operation pull, since about everyone but GIT and BK users uses it in the fetch meaning.)","Process Name":"cg-update","Link":"https:\/\/linux.die.net\/man\/1\/cg-update"}},{"Process":{"Description":"Show which version of Cogito is installed. Additionally, the HEAD of the installed Cogito is also shown if this information was available at the build time.","Process Name":"cg-version","Link":"https:\/\/linux.die.net\/man\/1\/cg-version"}},{"Process":{"Description":"cg_annotate takes an output file produced by the Valgrind tool Cachegrind and prints the information in an easy-to-read form.","Process Name":"cg_annotate","Link":"https:\/\/linux.die.net\/man\/1\/cg_annotate"}},{"Process":{"Description":"cgcc provides a wrapper around a C compiler ( cc by default) which also invokes the Sparse static analysis tool. cgcc accepts all Sparse command-line options, such as warning options, and passes all other options through to the compiler. By providing the same interface as the C compiler, cgcc allows projects to run Sparse as part of their build without modifying their build system, by using cgcc as the compiler. For many projects, setting CC=cgcc on the make command-line will work.","Process Name":"cgcc","Link":"https:\/\/linux.die.net\/man\/1\/cgcc"}},{"Process":{"Description":"this command moves processes defined by the list of processes (pidlist) to given control groups. The pids in the pidlist are separated by spaces -g <controllers>:<path> defines control groups where the task will be moved. controllers is a list of controllers and path is the relative path to control groups in the given controllers list. This flag can be used multiple times to define multiple pairs of lists of controllers and relative paths. Instead of the list of all mounted controllers, wildcard * can be used. If this option is not used then cgclassify will automatically place the task to the control group based on \/etc\/cgrules.conf. --sticky If this option is used, the daemon of service cgred (cgrulesengd process) does not change both the specified pidlist and their children tasks. Without this option, the daemon does not change the specified pidlist but it changes their children tasks to the right cgroup based on \/etc\/cgrules.conf automatically. --cancel-sticky If this option is used, the daemon of service cgred (cgrulesengd process) can change both the specified pidlist and their children tasks to right cgroup based on \/etc\/cgrules.conf automatically.","Process Name":"cgclassify","Link":"https:\/\/linux.die.net\/man\/1\/cgclassify"}},{"Process":{"Description":"This command moves all the tasks inside various cgroups to the root cgroup, deletes all the cgroups and finally unmounts the cgroup filesystem from the system.","Process Name":"cgclear","Link":"https:\/\/linux.die.net\/man\/1\/cgclear"}},{"Process":{"Description":"The command creates new cgroup(s) defined by option -g.","Process Name":"cgcreate","Link":"https:\/\/linux.die.net\/man\/1\/cgcreate"}},{"Process":{"Description":"CGDB Usage: cgdb [cgdb options] [--] [gdb options] CGDB Options: --version Print version information and then exit. --help Print help (this message) and then exit. -d Set debugger to use. -- Marks the end of CGDB's options.","Process Name":"cgdb","Link":"https:\/\/linux.die.net\/man\/1\/cgdb"}},{"Process":{"Description":"The cgdelete program removes all specified control groups. <controllers>:<path> Defines control group to delete. There can be multiple control groups specified. -r Recursively remove all subgroups.","Process Name":"cgdelete","Link":"https:\/\/linux.die.net\/man\/1\/cgdelete"}},{"Process":{"Description":"cget is a web downloading tool","Process Name":"cget","Link":"https:\/\/linux.die.net\/man\/1\/cget"}},{"Process":{"Description":"The cgexec program executes the task command with arguments arguments in given control groups. -g <controllers>:<path> defines control groups in which the task will be run. controllers is a list of controllers and path is the relative path to control groups in the given controllers list. This flag can be used multiple times to define multiple pairs of lists of controllers and relative paths. Instead of the list of all mounted controllers, wildcard b\"*b\" can be used. If this option is not used then cgexec will automatically place the task to the right cgroup based on \/etc\/cgrules.conf. --sticky If running the task command with this option, the daemon of service cgred (cgrulesengd process) does not change both the task of the command and the children tasks. Without this option, the daemon does not change the task of the command but it changes the children tasks to the right cgroup based on \/etc\/cgrules.conf automatically.","Process Name":"cgexec","Link":"https:\/\/linux.die.net\/man\/1\/cgexec"}},{"Process":{"Description":"The command prints the parameter(s) of input cgroup(s). If there is not set any controller or variable, then values of all possible variables are displayed. <path> is the name of the cgroup which should be read. This parameter can be used multiple times. -r, --variable <name> defines parameter to display. This option can be used multiple times. -g <contoller> defines controller which values should be displaied. This option can be used multiple times. -a, --all print the variables for all controllers which consists given cgroup -n do not print headers, i.e. name of groups. -v, --values-only print only values, not parameter names. -h, --help display help and exit","Process Name":"cgget","Link":"https:\/\/linux.die.net\/man\/1\/cgget"}},{"Process":{"Description":"","Process Name":"cgm2ncgm","Link":"https:\/\/linux.die.net\/man\/1\/cgm2ncgm"}},{"Process":{"Description":"The command set the parameters of input cgroup(s). <path> is the name of the cgroup which should be changed. This parameter can be used multiple times. -r <name=value> defines name of the file to set and the value which should be written to that file. This parameter can be used multiple times. -copy-from <source_cgrup_path> defines name of cgroup whose parameters will be copied to input cgroup.","Process Name":"cgset","Link":"https:\/\/linux.die.net\/man\/1\/cgset"}},{"Process":{"Description":"cgsnapshot generates the cgconfig compatible configuration file for given controllers. If no controller is set, then cgsnapshot shows all mounted hierarchies. The output is in the same format as cgconfig.conf configuration file. -b file Display only variables from blacklist. The default location of the blacklist is \/etc\/cgsnapshot_blacklist.conf. This list contains all variables which should be ignored by cgsnapshot tool. If the variable is blacklisted, then it will not be displayed. If it is not present on the blacklist, then the whitelist is checked. -h, --help display this help and exit -f, --file Redirect the output to output_file -s, --silent Ignore all warnings -t, --strict Do not display the variables which are not on the whitelist -w file Set the blacklist configuration file. This list contains all variables which should be displayed by cgsnapshot tool. If the variable is not blacklisted then whitelist is checked. If the variable is on the whitelist, then it is displayed by cgsnapshot tool. If the variable is not on the whitelist, then the variable is displayed and warning message is produced. By default the whitelist is not used. The warning message can be omitted using -s, --silent flag. If -t, --strict flag is used then the variable which is not on whitelist is not displayed. controller defines controller whose hierarchies will be output","Process Name":"cgsnapshot","Link":"https:\/\/linux.die.net\/man\/1\/cgsnapshot"}},{"Process":{"Description":"chacl is an IRIX-compatibility command, and is maintained for those users who are familiar with its use from either XFS or IRIX. Refer to the SEE ALSO section below for a description of tools which conform more closely to the (withdrawn draft) POSIX 1003.1e standard which describes Access Control Lists (ACLs). chacl changes the ACL(s) for a file or directory. The ACL(s) specified are applied to each file in the pathname arguments. Each ACL is a string which is interpreted using the acl_from_text(3) routine. These strings are made up of comma separated clauses each of which is of the form, tag:name:perm. Where tag can be: \"user\" (or \"u\") indicating that the entry is a user ACL entry. \"group\" (or \"g\") indicating that the entry is a group ACL entry. \"other\" (or \"o\") indicating that the entry is an other ACL entry. \"mask\" (or \"m\") indicating that the entry is a mask ACL entry. name is a string which is the user or group name for the ACL entry. A null name in a user or group ACL entry indicates the file's owner or file's group. perm is the string \"rwx\" where each of the entries may be replaced by a \"-\" indicating no access of that type, e.g. \"r-x\", \"--x\", \"---\".","Process Name":"chacl","Link":"https:\/\/linux.die.net\/man\/1\/chacl"}},{"Process":{"Description":"The chage command changes the number of days between password changes and the date of the last password change. This information is used by the system to determine when a user must change his\/her password.","Process Name":"chage","Link":"https:\/\/linux.die.net\/man\/1\/chage"}},{"Process":{"Description":"This program is used for extracting one or more charts from an Excel file in binary format. The charts can then be included in a \"Spreadsheet::WriteExcel\" file. See the \"add_chart_ext()\" section of the Spreadsheet::WriteExcel documentation for more details.","Process Name":"chartex","Link":"https:\/\/linux.die.net\/man\/1\/chartex"}},{"Process":{"Description":"chattr changes the file attributes on a Linux file system. The format of a symbolic mode is +-=[acdeijstuADST]. The operator '+' causes the selected attributes to be added to the existing attributes of the files; '-' causes them to be removed; and '=' causes them to be the only attributes that the files have. The letters 'acdeijstuADST' select the new attributes for the files: append only (a), compressed (c), no dump (d), extent format (e), immutable (i), data journalling (j), secure deletion (s), no tail-merging (t), undeletable (u), no atime updates (A), synchronous directory updates (D), synchronous updates (S), and top of directory hierarchy (T). The following attributes are read-only, and may be listed by lsattr(1) but not modified by chattr: huge file (h), compression error (E), indexed directory (I), compression raw access (X), and compressed dirty file (Z).","Process Name":"chattr","Link":"https:\/\/linux.die.net\/man\/1\/chattr"}},{"Process":{"Description":"Change the SELinux security context of each FILE to CONTEXT. With --reference, change the security context of each FILE to that of RFILE. -h, --no-dereference affect symbolic links instead of any referenced file --reference= RFILE use RFILE's security context rather than specifying a CONTEXT value -R, --recursive operate on files and directories recursively -v, --verbose output a diagnostic for every file processed -u, --user= USER set user USER in the target security context -r, --role= ROLE set role ROLE in the target security context -t, --type= TYPE set type TYPE in the target security context -l, --range= RANGE set range RANGE in the target security context The following options modify how a hierarchy is traversed when the -R option is also specified. If more than one is specified, only the final one takes effect. -H if a command line argument is a symbolic link to a directory, traverse it -L traverse every symbolic link to a directory encountered -P do not traverse any symbolic links (default) --help display this help and exit --version output version information and exit","Process Name":"chcon","Link":"https:\/\/linux.die.net\/man\/1\/chcon"}},{"Process":{"Description":"cheatmake is used to save time when recompiling. It can fool make into skipping files that haven't changed in a meaningful way. This can be used for instance when you change a comment in a file but none of the actual code. This utility is part of the KDE Software Development Kit.","Process Name":"cheatmake","Link":"https:\/\/linux.die.net\/man\/1\/cheatmake"}},{"Process":{"Description":"check-regexp (GNU Source-highlight) You simply pass as the first command line argument the regular expression and then the strings you want to try to match. It is crucial, in order to avoid shell substitutions, to enclose both the expression and the strings in single quotes. The program then prints some information about the possibly successful matching. In the output the what[0] part represents the whole match, and the what[i] part represents the i-th marked subexpression that matched. The program also prints possible prefix and suffix.","Process Name":"check-regexp","Link":"https:\/\/linux.die.net\/man\/1\/check-regexp"}},{"Process":{"Description":"The check-zone-exiration script reports how long until a zone will expire by querying for the zone's (top level) RRSIG and calculating how much time is left before the signatures will no longer be valid. It will then report how much time is left in human readable form. If the -m switch is provided with a time argument (in seconds), it will only print output for zones that have less than that time left.","Process Name":"check-zone-expiration","Link":"https:\/\/linux.die.net\/man\/1\/check-zone-expiration"}},{"Process":{"Description":"check_updates is a Nagios plugin to check if Red Hat or Fedora system is up-to-date","Process Name":"check_updates","Link":"https:\/\/linux.die.net\/man\/1\/check_updates"}},{"Process":{"Description":"checkbashisms, based on one of the checks from the lintian system, performs basic checks on \/bin\/sh shell scripts for the possible presence of bashisms. It takes the names of the shell scripts on the command line, and outputs warnings if possible bashisms are detected. Note that the definition of a bashism in this context roughly equates to \"a shell feature that is not required to be supported by POSIX\"; this means that some issues flagged may be permitted under optional sections of POSIX, such as XSI or User Portability. In cases where POSIX and Debian Policy disagree, checkbashisms by default allows extensions permitted by Policy but may also provide options for stricter checking.","Process Name":"checkbashisms","Link":"https:\/\/linux.die.net\/man\/1\/checkbashisms"}},{"Process":{"Description":"This manual page documents briefly the checkisomd5 command. checkisomd5 is a program that checks an embedded MD5 checksum in a ISO9660 image (.iso), or block device. The checksum is embedded by the corresponding implantisomd5 command. The check can be aborted by pressing Esc key.","Process Name":"checkisomd5","Link":"https:\/\/linux.die.net\/man\/1\/checkisomd5"}},{"Process":{"Description":"The checkmail program checks the status of the user's mail drop. If new mail is waiting, checkmail runs playbucket to play a sound and returns an exit status of 0. If mail has been cleared, an exit status of 2 is returned. If there has been no change, an exit status of 1 is returned. The checkmail program is intended to be used with the checkCommand resource of xbiff. To set this resource, add the following line to your .Xdefaults file: xbiff*checkCommand: checkmail","Process Name":"checkmail","Link":"https:\/\/linux.die.net\/man\/1\/checkmail"}},{"Process":{"Description":"checkXML is a tool to check for syntax errors in KDE DocBook XML files. It can also be used for other DocBook based XML files, but you should use the less specific xmllint(1) tool if you are not writing or otherwise working on KDE documentation.","Process Name":"checkxml","Link":"https:\/\/linux.die.net\/man\/1\/checkxml"}},{"Process":{"Description":"See Acme::Chef.","Process Name":"chef","Link":"https:\/\/linux.die.net\/man\/1\/chef"}},{"Process":{"Description":"chelonia is a client tool for accessing the Chelonia storage system. With it you can create, remove and list collections, upload, download and remove files, and move and stat collections and files, using Logical Names. Running without arguments it prints usage information. If you just specify the method without any other argument, it prints usage information for that method.","Process Name":"chelonia","Link":"https:\/\/linux.die.net\/man\/1\/chelonia"}},{"Process":{"Description":"chemtool is a program for drawing organic molecules and exporting them as a X bitmap, PNG, PicTeX, Xfig, SVG, SXD, MDL or EPS file. It runs under the X Window System using the GTK widget set. The program offers essentially unlimited undo\/redo, two text fonts plus symbols, seven colors, drawing at several zoom scales, and square and hexagonal backdrop grids for easier alignment.","Process Name":"chemtool","Link":"https:\/\/linux.die.net\/man\/1\/chemtool"}},{"Process":{"Description":"cherokee is an extremely fast, flexible and embeddable web server.","Process Name":"cherokee","Link":"https:\/\/linux.die.net\/man\/1\/cherokee"}},{"Process":{"Description":"cherokee-admin runs the server for the administrative interface used to configure Cherokee. The interface itself will be available via your Web browser.","Process Name":"cherokee-admin","Link":"https:\/\/linux.die.net\/man\/1\/cherokee-admin"}},{"Process":{"Description":"cherokee-admin-launcher is part of Cherokee, an extremely fast, flexible and embeddable web server. It is used as a wrapper for the main administrative interface used to configure Cherokee. This executable is meant to be called from a desktop launcher, automatically accessing an authenticated web browser session of the administration interface.","Process Name":"cherokee-admin-launcher","Link":"https:\/\/linux.die.net\/man\/1\/cherokee-admin-launcher"}},{"Process":{"Description":"cherokee-config is a tool that is used to determine the compile and linker flags that should be used to compile and link programs that use Cherokee.","Process Name":"cherokee-config","Link":"https:\/\/linux.die.net\/man\/1\/cherokee-config"}},{"Process":{"Description":"cherokee-tweak connects to a running cherokee instance, either local or remote, and requests it to perform one of several actions.","Process Name":"cherokee-tweak","Link":"https:\/\/linux.die.net\/man\/1\/cherokee-tweak"}},{"Process":{"Description":"cherokee-worker is a part of Cherokee, an extremely fast, flexible and embeddable web server. This executable is meant to be invoked only by cherokee(1). Executing it directly is a discouraged practice.","Process Name":"cherokee-worker","Link":"https:\/\/linux.die.net\/man\/1\/cherokee-worker"}},{"Process":{"Description":"chfn is used to change your finger information. This information is stored in the \/etc\/passwd file, and is displayed by the finger program. The Linux finger command will display four pieces of information that can be changed by chfn: your real name, your work room and phone, and your home phone. Command Line Any of the four pieces of information can be specified on the command line. If no information is given on the command line, chfn enters interactive mode. Interactive Mode In interactive mode, chfn will prompt for each field. At a prompt, you can enter the new information, or just press return to leave the field unchanged. Enter the keyword \"none\" to make the field blank.","Process Name":"chfn","Link":"https:\/\/linux.die.net\/man\/1\/chfn"}},{"Process":{"Description":"Change the group of each FILE to GROUP. With --reference, change the group of each FILE to that of RFILE. -c, --changes like verbose but report only when a change is made --dereference affect the referent of each symbolic link (this is the default), rather than the symbolic link itself -h, --no-dereference affect each symbolic link instead of any referenced file (useful only on systems that can change the ownership of a symlink) --no-preserve-root do not treat '\/' specially (the default) --preserve-root fail to operate recursively on '\/' -f, --silent, --quiet suppress most error messages --reference= RFILE use RFILE's group rather than specifying a GROUP value -R, --recursive operate on files and directories recursively -v, --verbose output a diagnostic for every file processed The following options modify how a hierarchy is traversed when the -R option is also specified. If more than one is specified, only the final one takes effect. -H if a command line argument is a symbolic link to a directory, traverse it -L traverse every symbolic link to a directory encountered -P do not traverse any symbolic links (default) --help display this help and exit --version output version information and exit","Process Name":"chgrp","Link":"https:\/\/linux.die.net\/man\/1\/chgrp"}},{"Process":{"Description":"The program suggests width\/height combinations to resize an image. The suggestions have the same aspect ratio as the original image. If a maximum width widthmax is specified -- and optionally a minimum width widthmin -- the program only suggests combinations for this range. If no maximum width is specified the program suggests sizes for decreasing the image size. Each suggestion is shown in one line containing classification, width and height. The suggestions are classified as follows: If the scale factor is not a natural number the first character in the line is ''-''. If the scale factor is a natural number but not a power of 2 the first character in the line is ''+''. If the scale factor is a power of 2 the first character in the line is ''*''.","Process Name":"chimgsize","Link":"https:\/\/linux.die.net\/man\/1\/chimgsize"}},{"Process":{"Description":"chkhelp checks the consistency of Performance Co-Pilot help text files generated by newhelp(1) and used by Performance Metric Domain Agents (PMDAs). The checking involves scanning the files, and optionally displaying selected entries. The files helpfile.dir and helpfile.pag are created by newhelp(1), and are assumed to already exist. Without any options or metricname arguments, chkhelp silently verifies the structural integrity of the help files. If any metricname arguments are specified, then the help entries for only the corresponding metrics will be processed. If no metricname arguments are specified, then at least one of the options -i or -p must be given. The -i option causes entries for all instance domains to be processed (ignoring entries for performance metrics). The -p option causes entries for all metrics to be displayed (ignoring entries for instance domains). When metric entries are to be processed (via either the metricname arguments or the -p option or the -i option), the -O and -H options request the display of the one-line and verbose help text respectively. The default is -O. Although historically there have been multiple help text file formats, the only format currently supported using the -v option is version 2, and this is the default if no -v flag is provided. Normally chkhelp operates on the default Performance Metrics Namespace (PMNS), however if the -n option is specified an alternative namespace is loaded from the file pmnsfile. The -e option provides an existence check where all of the specified metrics from the PMNS (note, not from helpfile) are scanned, and only the names of the metrics for which no help text exists are reported. The -e option is mutually exclusive with the -i and\/or -p options.","Process Name":"chkhelp","Link":"https:\/\/linux.die.net\/man\/1\/chkhelp"}},{"Process":{"Description":"Verify if an PE executable (CLR assembly, Win32 EXE or DLL) has a valid Authenticode(r) signature that can be traced back to a trusted certificate authority (CA). This means that (a) the signature is valid (i.e. file integrity) and, (b) the code-signing certificate can be chained back to one of the certificates in the Trust store.","Process Name":"chktrust","Link":"https:\/\/linux.die.net\/man\/1\/chktrust"}},{"Process":{"Description":"chmcmd - creates a Compressed HTML help file (chm) using a XML file made with the TCHMProject class in the fpc unit chmfilewriter.pas","Process Name":"chmcmd","Link":"https:\/\/linux.die.net\/man\/1\/chmcmd"}},{"Process":{"Description":"chmls - lists the complete contents of a chm file The section number can be 0 or 1 and is a filter to show only files contained in the uncompressed section of a chm or the compressed section respectively. When not specified files from both sections are shown.","Process Name":"chmls","Link":"https:\/\/linux.die.net\/man\/1\/chmls"}},{"Process":{"Description":"This manual page documents the GNU version of chmod. chmod changes the file mode bits of each given file according to mode, which can be either a symbolic representation of changes to make, or an octal number representing the bit pattern for the new mode bits. The format of a symbolic mode is [ugoa...][[+-=][perms...]...], where perms is either zero or more letters from the set rwxXst, or a single letter from the set ugo. Multiple symbolic modes can be given, separated by commas. A combination of the letters ugoa controls which users' access to the file will be changed: the user who owns it (u), other users in the file's group (g), other users not in the file's group (o), or all users (a). If none of these are given, the effect is as if a were given, but bits that are set in the umask are not affected. The operator + causes the selected file mode bits to be added to the existing file mode bits of each file; - causes them to be removed; and = causes them to be added and causes unmentioned bits to be removed except that a directory's unmentioned set user and group ID bits are not affected. The letters rwxXst select file mode bits for the affected users: read (r), write (w), execute (or search for directories) (x), execute\/search only if the file is a directory or already has execute permission for some user (X), set user or group ID on execution (s), restricted deletion flag or sticky bit (t). Instead of one or more of these letters, you can specify exactly one of the letters ugo: the permissions granted to the user who owns the file (u), the permissions granted to other users who are members of the file's group (g), and the permissions granted to users that are in neither of the two preceding categories (o). A numeric mode is from one to four octal digits (0-7), derived by adding up the bits with values 4, 2, and 1. Omitted digits are assumed to be leading zeros. The first digit selects the set user ID (4) and set group ID (2) and restricted deletion or sticky (1) attributes. The second digit selects permissions for the user who owns the file: read (4), write (2), and execute (1); the third selects permissions for other users in the file's group, with the same values; and the fourth for other users not in the file's group, with the same values. chmod never changes the permissions of symbolic links; the chmod system call cannot change their permissions. This is not a problem since the permissions of symbolic links are never used. However, for each symbolic link listed on the command line, chmod changes the permissions of the pointed-to file. In contrast, chmod ignores symbolic links encountered during recursive directory traversals.","Process Name":"chmod","Link":"https:\/\/linux.die.net\/man\/1\/chmod"}},{"Process":{"Description":"Add and\/or remove READ\/WRITE rights for the dictionary dicname. The access rights for the dictionary cannot be changed if it is being used.","Process Name":"chmoddic","Link":"https:\/\/linux.die.net\/man\/1\/chmoddic"}},{"Process":{"Description":"This manual page documents the GNU version of chown. chown changes the user and\/or group ownership of each given file. If only an owner (a user name or numeric user ID) is given, that user is made the owner of each given file, and the files' group is not changed. If the owner is followed by a colon and a group name (or numeric group ID), with no spaces between them, the group ownership of the files is changed as well. If a colon but no group name follows the user name, that user is made the owner of the files and the group of the files is changed to that user's login group. If the colon and group are given, but the owner is omitted, only the group of the files is changed; in this case, chown performs the same function as chgrp. If only a colon is given, or if the entire operand is empty, neither the owner nor the group is changed.","Process Name":"chown","Link":"https:\/\/linux.die.net\/man\/1\/chown"}},{"Process":{"Description":"This script can be used when there is a working remote shell program to start the secure servers that the ch_p4 device can use to speed up process startup.","Process Name":"chp4_servs","Link":"https:\/\/linux.die.net\/man\/1\/chp4_servs"}},{"Process":{"Description":"chrony is a pair of programs for keeping computer clocks accurate. chronyd is a background (daemon) program and chronyc is a command-line interface to it. Time reference sources for chronyd can be RFC1305 NTP servers, human (via keyboard and chronyc), or the computer's real-time clock at boot time (Linux only). chronyd can determine the rate at which the computer gains or loses time and compensate for it while no external reference is present. Its use of NTP servers can be switched on and off (through chronyc) to support computers with dial-up\/intermittent access to the Internet, and it can also act as an RFC1305-compatible NTP server.","Process Name":"chrony","Link":"https:\/\/linux.die.net\/man\/1\/chrony"}},{"Process":{"Description":"chrony is a pair of programs for maintaining the accuracy of computer clocks. chronyc is a command-line interface program which can be used to monitor chronyd's performance and to change various operating parateters whilst it is running.","Process Name":"chronyc","Link":"https:\/\/linux.die.net\/man\/1\/chronyc"}},{"Process":{"Description":"Run COMMAND with root directory set to NEWROOT. --userspec= USER:GROUP specify user and group (ID or name) to use --groups= G_LIST specify supplementary groups as g1,g2,..,gN --help display this help and exit --version output version information and exit If no command is given, run ''${SHELL} -i'' (default: \/bin\/sh).","Process Name":"chroot","Link":"https:\/\/linux.die.net\/man\/1\/chroot"}},{"Process":{"Description":"chrpath changes, lists or removes the rpath or runpath setting in a binary. The rpath, or runpath if it is present, is where the runtime linker should look for the libraries needed for a program.","Process Name":"chrpath","Link":"https:\/\/linux.die.net\/man\/1\/chrpath"}},{"Process":{"Description":"chrt(1) sets or retrieves the real-time scheduling attributes of an existing PID or runs COMMAND with the given attributes. Both policy (one of SCHED_OTHER, SCHED_FIFO, SCHED_RR, SCHED_BATCH, or SCHED_IDLE) and priority can be set and retrieved. The SCHED_BATCH policy is supported since Linux 2.6.16. The SCHED_IDLE policy is supported since Linux 2.6.23.","Process Name":"chrt","Link":"https:\/\/linux.die.net\/man\/1\/chrt"}},{"Process":{"Description":"chsh is used to change your login shell. If a shell is not given on the command line, chsh prompts for one. Valid Shells chsh will accept the full pathname of any executable file on the system. However, it will issue a warning if the shell is not listed in the \/etc\/shells file. On the other hand, it can also be configured such that it will only accept shells listed in this file, unless you are root.","Process Name":"chsh","Link":"https:\/\/linux.die.net\/man\/1\/chsh"}},{"Process":{"Description":"cht is a commandline tool to derive the sum formula and molecular mass of the molecule depicted in a chemtool drawing file. It is also available from within chemtool to calculate these data for the current structure or a marked fragment of it. cht currently recognizes the element symbols C, H, O, N, P, S, Si, B, Br, Cl, F, I, Al, As, Ba, Be, Bi, Ca, Cd, Co, Cr, Cs, Cu, Fe, Ga, Ge, In, K, Li, Mg, Mn, Na, Ni, Pb, Rb, Sb, Sc, Se, Sn, Sr, Te, Ti, Tl, V, Zn (that is, all main group elements except the noble gases, and the first row of transition metals) and the abbreviations Ac, Ade, Bn, Bu, iBu, tBu, Bz, BOC, Cyt, CE, DBAM, DMAM, DMTr, Et, Gua, Me, Ms, MOC, MOM, MMTr, Ph, Pr, iPr, Tf, Thy, Tol, Tr, Ts, TBDMS, TBDPS, TMS, TMTr, Ura, Z. It can handle two levels of parentheses; e.g. P[OCH(CH_3)_2]_3. When cht detects any duplicate (overlapping) bonds in the drawing file, it will prepend an exclamation mark to the calculated (and likely wrong) sum formula.","Process Name":"cht","Link":"https:\/\/linux.die.net\/man\/1\/cht"}},{"Process":{"Description":"The command chvt N makes \/dev\/ttyN the foreground terminal. (The corresponding screen is created if it did not exist yet. To get rid of unused VTs, use deallocvt(1).) The key combination (Ctrl-)LeftAlt-F N (with N in the range 1-12) usually has a similar effect.","Process Name":"chvt","Link":"https:\/\/linux.die.net\/man\/1\/chvt"}},{"Process":{"Description":"ci stores new revisions into RCS files. Each pathname matching an RCS suffix is taken to be an RCS file. All others are assumed to be working files containing new revisions. ci deposits the contents of each working file into the corresponding RCS file. If only a working file is given, ci tries to find the corresponding RCS file in an RCS subdirectory and then in the working file's directory. For more details, see FILE NAMING below. For ci to work, the caller's login must be on the access list, except if the access list is empty or the caller is the superuser or the owner of the file. To append a new revision to an existing branch, the tip revision on that branch must be locked by the caller. Otherwise, only a new branch can be created. This restriction is not enforced for the owner of the file if non-strict locking is used (see rcs(1)). A lock held by someone else can be broken with the rcs command. Unless the -f option is given, ci checks whether the revision to be deposited differs from the preceding one. If not, instead of creating a new revision ci reverts to the preceding one. To revert, ordinary ci removes the working file and any lock; ci -l keeps and ci -u removes any lock, and then they both generate a new working file much as if co -l or co -u had been applied to the preceding revision. When reverting, any -n and -s options apply to the preceding revision. For each revision deposited, ci prompts for a log message. The log message should summarize the change and must be terminated by end-of-file or by a line containing . by itself. If several files are checked in ci asks whether to reuse the previous log message. If the standard input is not a terminal, ci suppresses the prompt and uses the same log message for all files. See also -m. If the RCS file does not exist, ci creates it and deposits the contents of the working file as the initial revision (default number: 1.1). The access list is initialized to empty. Instead of the log message, ci requests descriptive text (see -t below). The number rev of the deposited revision can be given by any of the options -f, -i, -I, -j, -k, -l, -M, -q, -r, or -u. rev can be symbolic, numeric, or mixed. Symbolic names in rev must already be defined; see the -n and -N options for assigning names during checkin. If rev is $, ci determines the revision number from keyword values in the working file. If rev begins with a period, then the default branch (normally the trunk) is prepended to it. If rev is a branch number followed by a period, then the latest revision on that branch is used. If rev is a revision number, it must be higher than the latest one on the branch to which rev belongs, or must start a new branch. If rev is a branch rather than a revision number, the new revision is appended to that branch. The level number is obtained by incrementing the tip revision number of that branch. If rev indicates a non-existing branch, that branch is created with the initial revision numbered rev.1. If rev is omitted, ci tries to derive the new revision number from the caller's last lock. If the caller has locked the tip revision of a branch, the new revision is appended to that branch. The new revision number is obtained by incrementing the tip revision number. If the caller locked a non-tip revision, a new branch is started at that revision by incrementing the highest branch number at that revision. The default initial branch and level numbers are 1. If rev is omitted and the caller has no lock, but owns the file and locking is not set to strict, then the revision is appended to the default branch (normally the trunk; see the -b option of rcs(1)). Exception: On the trunk, revisions can be appended to the end, but not inserted.","Process Name":"ci","Link":"https:\/\/linux.die.net\/man\/1\/ci"}},{"Process":{"Description":"The cifsiostat command displays statistics about read and write operations on CIFS filesystems. The interval parameter specifies the amount of time in seconds between each report. The first report contains statistics for the time since system startup (boot). Each subsequent report contains statistics collected during the interval since the previous report. A report consists of an CIFS header row followed by a line of statistics for each CIFS filesystem that is mounted. The count parameter can be specified in conjunction with the interval parameter. If the count parameter is specified, the value of count determines the number of reports generated at interval seconds apart. If the interval parameter is specified without the count parameter, the cifsiostat command generates reports continuously.","Process Name":"cifsiostat","Link":"https:\/\/linux.die.net\/man\/1\/cifsiostat"}},{"Process":{"Description":"cilc is the Mono CIL-to-C binding generator. It takes a CIL assembly as input and generates a directory of C sources which, when compiled, provide a C interface to the classes contained within that assembly by means of a shared object library. Generated sources make use of the Mono embedding API. Thus a complete Mono development environment must be available on the development system and a complete Mono runtime environment must be available on target systems.","Process Name":"cilc","Link":"https:\/\/linux.die.net\/man\/1\/cilc"}},{"Process":{"Description":"This manual page documents briefly the cimconv command. cimconv is a tool for the centerim instant messaging client. It can be used to convert contact lists and history from other IM clients. So far the client parameter can be one of these: licq, kxicq2, gnomeicu, micq, trillian.","Process Name":"cimconv","Link":"https:\/\/linux.die.net\/man\/1\/cimconv"}},{"Process":{"Description":"The cimmof command is the command line interface to the Managed Object Format (MOF) Compiler. The MOF Compiler is a utility that compiles MOF files (using the MOF format defined by the DMTF CIM Specification) into CIM classes and instances that are stored in the CIM Repository. The cimmof command can be used to compile MOF files at any time after installation. If no input file is specified, stdin is used as the input. The MOF Compiler requires that the input MOF files be in the current directory or that a fully qualified path be given. To simplify the specification of multiple MOF files in the cimmof command line, the MOF Compiler allows compiling from files containing a list of MOF files using the include pragma (as shown below). #pragma include (\"application.mof\") #pragma include (\"server.mof\") MOF files using the include pragma must be in the current directory or in a directory specified by the -I command line option. The -n option can be used to specify a R namespace in which the CIM classes and instances will be compiled. If this option is not specified, the default R namespace is root\/cimv2 (with the exception of provider registration schemas). For provider registration schemas, if the -n option is not specified, the default R namespace is root\/PG_InterOp. If -n option is specified, the R namespace specified must be root\/PG_InterOp, otherwise, the error message \"The requested operation is not supported.\" is returned. For provider MOFs, the R namespace specified must match one of the namespaces specified in the PG_ProviderCapabilities class schema definition. Options The cimmof command recognizes the following options: -aE Allow Experimental Schema changes. -aEV Allow both Experimental and Version Schema changes. -aV Allow both Major and Down Revision Schema changes. -E Syntax check only -h, --help Display command usage information. -I path Specify the path to included MOF files. This path may be relative or absolute. If the input MOF file has include pragmas and the included files do not reside in the current directory, the directive must be used to specify a path to them on the cimmof command line. -n Override the default CIM Repository namespace. The namespace specified must be a valid CIM namespace name. For the definition of a valid CIM namespace name, refer to the Administrator's Guide. For provider registration schemas, the namepace specified must be root\/PG_InterOp. --namespace Override the default CIM Repository namespace. The namespace specified must be a valid CIM namespace name. For the definition of a valid CIM namespace name, refer to the Administrator's Guide. For provider registration schemas, the namepace specified must be root\/PG_InterOp. --trace Trace to file (default to stdout) -uc Allow update of an existing class definition. --version Display CIM Server version. -w Suppress warning messages. When compiling the MOF files, if there are CIM elements (such as classes, instances, properties, or methods) defined in the MOF files which already exist in the CIM Repository, the cimmof command returns warning messages. The -w option can be used to suppress these warning messages. --xml Output XML only, to stdout. Do not update repository.","Process Name":"cimmof","Link":"https:\/\/linux.die.net\/man\/1\/cimmof"}},{"Process":{"Description":"The cimmofl command is a command line interface to the Managed Object Format (MOF) Compiler. The MOF Compiler is a utility that compiles MOF files (using the MOF format defined by the DMTF CIM Specification) into CIM classes and instances that are stored in the CIM Repository. The cimmofl command can be used to compile MOF files at any time after installation. If no input file is specified, stdin is used as the input. The MOF Compiler requires that the input MOF files be in the current directory or that a fully qualified path be given. To simplify the specification of multiple MOF files in the cimmofl command line, the MOF Compiler allows compiling from files containing a list of MOF files using the include pragma (as shown below). #pragma include (\"application.mof\") #pragma include (\"server.mof\") MOF files using the include pragma must be in the current directory or in a directory specified by the -I command line option. The -n option can be used to specify a R namespace in which the CIM classes and instances will be compiled. If this option is not specified, the default R namespace is root\/cimv2 (with the exception of provider registration schemas). For provider registration schemas, if the -n option is not specified, the default R namespace is root\/PG_InterOp. If -n option is specified, the R namespace specified must be root\/PG_InterOp, otherwise, the error message \"The requested operation is not supported.\" is returned. For provider MOFs, the R namespace specified must match one of the namespaces specified in the PG_ProviderCapabilities class schema definition. Options The cimmofl command recognizes the following options: -aE Allow Experimental Schema changes. -aEV Allow both Experimental and Version Schema changes. -aV Allow both Major and Down Revision Schema changes. --CIMRepository Specify the repository path. -E Syntax check only -h, --help Display command usage information. -I Specify the path to included MOF files. This path may be relative or absolute. If the input MOF file has include pragmas and the included files do not reside in the current directory, the directive must be used to specify a path to them on the cimmofl command line. -n Override the default CIM Repository namespace. The namespace specified must be a valid CIM namespace name. For the definition of a valid CIM namespace name, refer to the Administrator's Guide. For provider registration schemas, the namepace specified must be root\/PG_InterOp. --namespace Override the default CIM Repository namespace. The namespace specified must be a valid CIM namespace name. For the definition of a valid CIM namespace name, refer to the Administrator's Guide. For provider registration schemas, the namepace specified must be root\/PG_InterOp. -R Specify the repository path. -N Specify the repository name - defaults to \"repository\" -M Repository mode [XML, BIN] - defaults to \"XML\" --trace Trace to file (default to stdout). -uc Allow update of an existing class definition. --version Display CIM Server version. -w Suppress warning messages. When compiling the MOF files, if there are CIM elements (such as classes, instances, properties, or methods) defined in the MOF files which already exist in the CIM Repository, the cimmofl command returns warning messages. The -w option can be used to suppress these warning messages. --xml Output XML only, to stdout. Do not update repository.","Process Name":"cimmofl","Link":"https:\/\/linux.die.net\/man\/1\/cimmofl"}},{"Process":{"Description":"The cimprovider command provides a command line interface to disable, enable, unregister, group and list registered CIM providers. If a CIM provider is disabled, the CIM Server rejects any requests to the provider. If a CIM provider is enabled, the CIM Server forwards requests to the provider. And if a CIM provider is unregistered, the CIM Server will no longer have any information about the provider. In order to use the cimprovider command, cimserver has to be running and the specified provider module (a grouping of providers in the same shared library) or provider has to be registered with WBEM Services. The first form of cimprovider disables the specified provider module. When a specified provider module is in the disabled state, any new requests to the providers that are contained in the specified provider module will be rejected. The second form of cimprovider enables the providers that are contained in the specified provider module. The providers that are contained in the specified provider module are now ready to accept new requests. The third form of cimprovider removes (un-registers) the specified provider module and all of its contained providers or the specified provider in the specified provider module. Once removed, a provider or provider module must be re-registered (typically by loading its registration schema via the cimmof command). The fourth form of cimprovider sets the provider module group. If the provider module is active, provider module is disabled first, group is set and enabled again. If group name is CIMServer, provider module is loaded into CIMServer process depending on UserContext value. Specify empty string to remove from grouping. The last form of cimprovider lists all the registered provider modules and module status or all the providers in the specified provider module. To list all providers in all modules, issue a cimprovider -l command, followed by cimprovider -l -m for each listed module. The following provider module status values are defined: OK The provider module is enabled. Stopping The provider module is in the process of being disabled. Stopped The provider module is disabled. Degraded A failure was detected in an out-of-process provider module. Disable and re-enable the provider module to ensure any active indication subscriptions are being served. Error An invalid registration instance was encountered upon CIM Server initialization. Specifying no options with the cimprovider command displays the command usage. Options The cimprovider command recognizes the following options: -d Disables the specified CIM provider R module . If user(s) try to disable a module that is already disabled, an error message is returned and no action is taken. -e Enables the specified CIM provider R module . If user(s) try to enable a module that is already enabled or try to enable a module that is disabling, an error message is returned and no action is taken. -h, --help Display command usage information. -l Displays all the registered provider modules. -m module Specifies the provider module for the operation. -p provider Specifies the provider for the operation. -r Removes the specified provider module and all of its contained providers. If provider is specified, removes the specified provider in the specified provider module (not affecting any other providers in that module). -g Sets the CIM provider module group. Specify empty string to remove from grouping. If the provider module is active, provider module is disabled first, group is set and enabled again. All provider modules with the same group name are loaded into a single agent process except when overridden by specific UserContext values. If group name is CIMServer , provider module is loaded into CIMServer process depending on UserContext value. -s Displays the status of provider modules. -f Displays the full status of provider modules with group name. --version Display CIM Server version number.","Process Name":"cimprovider","Link":"https:\/\/linux.die.net\/man\/1\/cimprovider"}},{"Process":{"Description":"The cimsub command provides a command line interface to manage CIM indication subscriptions on the local CIM Server. The first form of cimsub lists all or selected indication subscriptions, filters, or handlers, and displays the requested information about the instance(s). The second form of cimsub enables the specified subscription, i.e. the subscription instance is modified to set the value of the SubscriptionState property to Enabled. When a subscription is enabled, the CIM Server attempts to process the subscription if at least one provider is available to serve the subscription. The third form of cimsub disables the specified subscription, i.e. the subscription instance is modified to set the value of the SubscriptionState property to Disabled. When a subscription is disabled, the CIM Server does not attempt to process the subscription, regardless of whether any providers are available to serve the subscription. The fourth form of cimsub removes the specified subscription, filter, handler, or subscription and its referenced filter and handler, i.e. each instance is deleted from the repository, and the CIM Server will no longer have any information about the instance(s). A filter or handler may not be removed if it is referenced by any other subscription. The fifth form of cimsub command creates the specified Subscription, filter, or handler. i.e each instance is created into the repository and CIM Server will get the new information about the created Instance(s). Options The cimsub command recognizes the following options: -l List and display information about all or selected indication subscriptions (\"s\"), filters (\"f\"), or handlers (\"h\"). It is an error to specify the -F option with -lh. It is an error to specify the -H option with -lf. With the -lf or -lh option, fnamespace or hnamespace takes precedence over the -n namespace option if both are specified. -e Enable the specified subscription (set the SubscriptionState to Enabled). -d Disable the specified subscription (set the SubscriptionState to Disabled). -r Remove the specified subscription (\"s\"), filter (\"f\"), handler (\"h\"), or subscription and its referenced filter and handler (\"a\") (delete the instance(s) from the repository). The -F and -H options are required with -rs or -ra. It is an error to specify the -F option with -rh. It is an error to specify the -H option with -rf. With the -rf or -rh option, fnamespace or hnamespace takes precedence over the -n namespace option if both are specified. -c Create the specified subscription (\"s\"), filter (\"f\") or handler (\"h\"). The -F filter and -H handler options are required with -cs. The Filtername and -Q Query are required with -cf. The Handler Name is required with -ch.With the -cf or -ch option, fnamespace or hnamespace takes precedence over the -n namespace option if both are specified. -v Include verbose information in the information displayed for each listed instance. -n namespace Specify the namespace. For the -l forms of the command, if no namespace is specified, instances in all namespaces are listed. For all other forms of the command, if no namespace is specified, the command operates on an instance in the namespace root\/PG_InterOp. -F [fnamespace:]filtername Specify the Filter Name. Omission of the filter namespace specifies that it is the same as that of the subscription. -H [hnamespace:][classname.]handlername Specify the Handler Name. Omission of the handler namespace specifies that it is the same as that of the subscription. Omission of the handler classname specifies that it is CIM_ListenerDestinationCIMXML. -Q Specify the Query Expression for the Filter. -L Specify the Query Language for the Filter. -N To Use SourceNamespaces property, Specify multiple SourceNamespaces with coma separated or append comma to the single SourceNamespace. By default SourceNamespace property is populated if the single SourceNamespace is specified. -D Specify Destination of a CIM_IndicationHandlerCIMXML Handler. Required option for CIM_IndicationHandlerCIMXML or CIM_ListenerDestinationCIMXML Handler. -M Specify Mailto of a PG_ListenerDestinationEmail Handler. -C Specify Mailcc of a PG_ListenerDestinationEmail Handler. -S Specify Subject of a PG_ListenerDestinationEmail Handler. -T Specify Target Host of PG_IndicationHandlerSNMPMapper Handler. Required option for SNMPMapper handler. -U Specify Security Name of PG_IndicationHandlerSNMPMapper Handler. -P Specify Port Number of a PG_IndicationHandlerSNMPMapper Handler.(default 162) -V Specify SNMPVersion of a PG_IndicationHandlerSNMPMapper Handler. 2 : SNMPv1 Trap 3 : SNMPv2C Trap 4 : SNMPv2C Inform 5 : SNMPv3 Trap 6 : SNMPv3 Inform Required option for SNMPMapper Handler -E Specify Engine ID of a PG_IndicationHandlerSNMPMapper Handler. --help Display command usage information. --version Display the CIM Server version.","Process Name":"cimsub","Link":"https:\/\/linux.die.net\/man\/1\/cimsub"}},{"Process":{"Description":"","Process Name":"cinder-manage","Link":"https:\/\/linux.die.net\/man\/1\/cinder-manage"}},{"Process":{"Description":"The ciphers command converts textual OpenSSL cipher lists into ordered SSL cipher preference lists. It can be used as a test tool to determine the appropriate cipherlist.","Process Name":"ciphers","Link":"https:\/\/linux.die.net\/man\/1\/ciphers"}},{"Process":{"Description":"ciptool is used to set up, maintain, and inspect the CIP configuration of the Bluetooth subsystem in the Linux kernel.","Process Name":"ciptool","Link":"https:\/\/linux.die.net\/man\/1\/ciptool"}},{"Process":{"Description":"dot draws directed graphs. It works well on DAGs and other graphs that can be drawn as hierarchies. It reads attributed graph files and writes drawings. By default, the output format dot is the input file with layout coordinates appended. neato draws undirected graphs using ''spring'' models (see Kamada and Kawai, Information Processing Letters 31:1, April 1989). Input files must be formatted in the dot attributed graph language. By default, the output of neato is the input graph with layout coordinates appended. twopi draws graphs using a radial layout (see G. Wills, Symposium on Graph Drawing GD'97, September, 1997). Basically, one node is chosen as the center and put at the origin. The remaining nodes are placed on a sequence of concentric circles centered about the origin, each a fixed radial distance from the previous circle. All nodes distance 1 from the center are placed on the first circle; all nodes distance 1 from a node on the first circle are placed on the second circle; and so forth. circo draws graphs using a circular layout (see Six and Tollis, GD '99 and ALENEX '99, and Kaufmann and Wiese, GD '02.) The tool identifies biconnected components and draws the nodes of the component on a circle. The block-cutpoint tree is then laid out using a recursive radial algorithm. Edge crossings within a circle are minimized by placing as many edges on the circle's perimeter as possible. In particular, if the component is outerplanar, the component will have a planar layout. If a node belongs to multiple non-trivial biconnected components, the layout puts the node in one of them. By default, this is the first non-trivial component found in the search from the root component. fdp draws undirected graphs using a ''spring'' model. It relies on a force-directed approach in the spirit of Fruchterman and Reingold (cf. Software-Practice & Experience 21(11), 1991, pp. 1129-1164). sfdp also draws undirected graphs using the ''spring'' model described above, but it uses a multi-scale approach to produce layouts of large graphs in a reasonably short time.","Process Name":"circo","Link":"https:\/\/linux.die.net\/man\/1\/circo"}},{"Process":{"Description":"Animates a number of 3D electronic components.","Process Name":"circuit","Link":"https:\/\/linux.die.net\/man\/1\/circuit"}},{"Process":{"Description":"This command accompanies vpnc. It decrypts the obfuscated pre-shared key from *.pcf-configuration files, which must be specified on the command line. The result will be printed to STDOUT.","Process Name":"cisco-decrypt","Link":"https:\/\/linux.die.net\/man\/1\/cisco-decrypt"}},{"Process":{"Description":"This program is part of Netpbm(1). cistopbm reads a Compuserve RLE file as input. and produces a PBM image as output.","Process Name":"cistopbm","Link":"https:\/\/linux.die.net\/man\/1\/cistopbm"}},{"Process":{"Description":"This is a simple encoder for bitonal files. Argument inputfile is the name of a PBM or bitonal TIFF file containing a single document image. This program produces a DjVuBitonal file named outputdjvufile. The default compression process is lossless: decoding the DjVuBitonal file at full resolution will produce an image exactly identical to the input file. Lossy compression is enabled by options -losslevel, -lossy, or -clean.","Process Name":"cjb2","Link":"https:\/\/linux.die.net\/man\/1\/cjb2"}},{"Process":{"Description":"cjpeg compresses the named image file, or the standard input if no file is named, and produces a JPEG\/JFIF file on the standard output. The currently supported input file formats are: PPM (PBMPLUS color format), PGM (PBMPLUS gray-scale format), BMP, Targa, and RLE (Utah Raster Toolkit format). (RLE is supported only if the URT library is available.)","Process Name":"cjpeg","Link":"https:\/\/linux.die.net\/man\/1\/cjpeg"}},{"Process":{"Description":"The --help prints out a usage message to standard output. -b, --bits <bits> Number of bits in the key to create (default: 1024) -f, --filename <file name> Filename of the key file. -t, --type <type> Type of key (rsa or dsa). -C, --comment <comment> Provide a new comment. -N, --newpass <pass phrase> Provide new passphrase. -P, --pass <pass phrase> Provide old passphrase. -l, --fingerprint Show fingerprint of key file. -p, --changepass Change passphrase of private key file. -q, --quiet Be quiet. -y, --showpub Read private key file and print public key. --version Display version number only.","Process Name":"ckeygen","Link":"https:\/\/linux.die.net\/man\/1\/ckeygen"}},{"Process":{"Description":"ckpasswd is the basic password authenticator for nnrpd, suitable for being run from an auth stanza in readers.conf(5). See readers.conf(5) for more information on how to configure an nnrpd authenticator. ckpasswd accepts a username and password from nnrpd and tells nnrpd(8) whether that's the correct password for that username. By default, when given no arguments, it checks the password against the password field returned by getpwnam(3). Note that these days most systems no longer make real passwords available via getpwnam(3) (some still do if and only if the program calling getpwnam(3) is running as root). Note that ckpasswd expects all passwords to be stored encrypted by the system crypt(3) function and calls crypt(3) on the supplied password before comparing it to the expected password.","Process Name":"ckpasswd","Link":"https:\/\/linux.die.net\/man\/1\/ckpasswd"}},{"Process":{"Description":"cksfv is a tool for verifying CRC32 checksums of files. CRC32 checksums are used to verify that files are not corrupted. The algorithm is cryptographically crippled so it can not be used for security purposes. md5sum (1) or sha1sum (1) are much better tools for checksuming files. cksfv should only be used for compatibility with other systems. cksfv has two operation modes: checksum creation and checksum verification In checksum creation mode cksfv outputs CRC32 checksums of files to to stdout, normally redirected to an .sfv file. In checksum verification mode cksfv reads filenames from an sfv file, and compares the recorded checksum values against recomputed checksums of files.","Process Name":"cksfv","Link":"https:\/\/linux.die.net\/man\/1\/cksfv"}},{"Process":{"Description":"Print CRC checksum and byte counts of each FILE. --help display this help and exit --version output version information and exit","Process Name":"cksum","Link":"https:\/\/linux.die.net\/man\/1\/cksum"}},{"Process":{"Description":"Dump and decode Directory Server replication change log","Process Name":"cl-dump","Link":"https:\/\/linux.die.net\/man\/1\/cl-dump"}},{"Process":{"Description":"cl_status is used to check the status of the High-Availability Linux subsystem.","Process Name":"cl_status","Link":"https:\/\/linux.die.net\/man\/1\/cl_status"}},{"Process":{"Description":"--help -h Show help --version -V Show version --info -i Print information about bytecode --printsrc -p Print bytecode source --trace <level> Set bytecode trace level 0..7 (default 7) --no-trace-showsource Don't show source line during tracing file file to test","Process Name":"clambc","Link":"https:\/\/linux.die.net\/man\/1\/clambc"}},{"Process":{"Description":"clamconf displays the values of configuration options.","Process Name":"clamconf","Link":"https:\/\/linux.die.net\/man\/1\/clamconf"}},{"Process":{"Description":"clamdscan is a clamd client which may be used as a clamscan replacement. It accepts all the options implemented in clamscan but most of them will be ignored because its scanning abilities only depend on clamd.","Process Name":"clamdscan","Link":"https:\/\/linux.die.net\/man\/1\/clamdscan"}},{"Process":{"Description":"clamdtop is a tool to monitor one or multiple clamd(s). It has a (color) ncurses interface, that shows the jobs in clamd's queue, memory usage, and information about the loaded signature database. You can specify on the command-line to which clamd(s) it should connect to. By default it will attempt to connect to the local clamd as defined in clamd.conf.","Process Name":"clamdtop","Link":"https:\/\/linux.die.net\/man\/1\/clamdtop"}},{"Process":{"Description":"This manual page documents briefly the ClamFS user-space file system. clamfs is a program that mount anti-virus protected file system into existing directory tree. Features * User-space file system (no kernel patches, recompilation, etc.) * Configuration stored in XML files * FUSE (and libfuse) used as file system back-end * Scan files using ClamAV * ScanCache (LRU with time-based and out-of-memory expiration) speeds up file access * Sends mail to administrator when detect virus Idea ClamFS is completely user-space anti-virus solution for Linux. It uses libfuse and Linux kernel module to provide file system. ClamAV is used as anti-virus scanner. Normally program (or library) uses glibc open() call to obtain file descriptor. Glibc calls kernel VFS to open file regardless of file system used. If file is on ClamFS file system open call from VFS is directed to user-space by FUSE. ClamFS calls libfuse to communicate with FUSE and through it with VFS. Internals ClamFS is spitted into four parts: * libfuse bindings -- used to communicate with FUSE (and with VFS through it), * ScanCache -- store (per file) results of anti-virus scanning to speed up future open() requests * ScanQueue -- queue files for scanning * clamd \/ libclamav bindings -- communicate with anti-virus scanner","Process Name":"clamfs","Link":"https:\/\/linux.die.net\/man\/1\/clamfs"}},{"Process":{"Description":"clamscan is a command line anti-virus scanner.","Process Name":"clamscan","Link":"https:\/\/linux.die.net\/man\/1\/clamscan"}},{"Process":{"Description":"clamz is a little command-line program to download MP3 files from Amazon.com's music store. It is intended to serve as a substitute for Amazon's official MP3 Downloader, which is not free software (and therefore is only available in binary form for a limited set of platforms.) clamz can be used to download either individual songs or complete albums that you have purchased from Amazon. In order to use clamz, you must first enable the ''MP3 dowloader mode'' by visiting the following URL: http:\/\/www.amazon.com\/gp\/dmusic\/after_download_manager_install.html?AMDVersion=1.0.9 If you are outside the US, replace 'amazon.com' with the corresponding domain for your country (e.g., amazon.co.uk for the UK, or amazon.fr for France.) Amazon does not allow anyone to purchase MP3 files from outside their country of residence. To download the contents of an AMZ file into the current directory, just run clamz some-file-name.amz More advanced options are listed below. Options -o name-format, --output= name-format Set the name of the downloaded file(s). This may contain any of several variables which are derived from the input AMZ file; see FORMAT VARIABLES below. Note that the format string should be enclosed in single quotes, to stop the shell from expanding variables itself. -d directory-format, --output-dir= directory-format Set the directory where the downloaded files should be placed. (If this directory does not exist, it will be created.) This may also contain format variables. -r, --resume Resume downloading a partially-downloaded album. (By default, if you are downloading a file named foo.mp3, and the file foo.mp3 already exists in the destination directory, clamz will rename the new file to foo.mp3.1 to avoid overwriting the old file. If the -r option is used, clamz will instead assume that the first part of the file has already been downloaded, and will resume downloading from where it left off.) -i, --info Rather than downloading anything, just display detailed information about the given AMZ file(s) to standard output. -x, --xml Rather than downloading anything, print the raw, decrypted XML data from the AMZ file to standard output. -v, --verbose Display detailed information while downloading. -q, --quiet Turn off the normal progress display; display only error messages. --forbid-chars= characters Do not allow the given characters to be used in output filenames. Note that control characters and slashes may never be used in filenames. --allow-chars= characters Opposite of the above; remove the given characters from the set of disallowed characters. --allow-uppercase Allow uppercase letters in filenames. --forbid-uppercase Do not allow uppercase letters in filenames. --utf8-filenames Use UTF-8 when writing filenames (the default behavior is to use UTF-8 if the system locale says so, otherwise ASCII.) --ascii-filenames Use only ASCII characters in filenames. --help Print out a summary of options. --version Print out version information. Format Variables As part of a name-format or directory-format option, you may include references to environment variables (e.g., $HOME) or to the 'xdg-user-dirs' configuration variables (e.g., $XDG_MUSIC_DIR or $XDG_DESKTOP_DIR.) In addition, the following special variables are defined for each track, based on the information provided in the AMZ file, and subject to the above configuration options (--forbid-chars, --forbid-uppercase, etc.) ${title}, ${creator}, ${tracknum}, ${discnum}, ${genre}, ${asin} Title, creator, track number, disc number, genre, and ASIN (Amazon Standard Identification Number) of each individual track. ${album}, ${album_artist}, ${album_asin} Title, primary artist, and ASIN of the album the track comes from. (This information is available both for single-track and full-album downloads.) ${suffix} Suffix of the output file (currently only 'mp3'.) ${amz_title}, ${amz_creator}, ${amz_genre}, ${amz_asin} These variables formerly contained metadata for the AMZ file as a whole; current AMZ files do not contain this information. Using these variables is not recommended. Similar to shell variable expansion, you can also use the following conditional expressions: ${var:-string} Expands to the value of variable var if it is defined and non-empty; otherwise, expands to string (which may itself contain variable references.) ${var:+string} Expands to string if the variable var is defined and non-empty; otherwise, expands to an empty string. Note that when you include format variables in a command-line argument, you will usually need to enclose the argument in single quotes ('...'), or insert a backslash before the '$', to prevent the shell from trying to expand the variables itself.","Process Name":"clamz","Link":"https:\/\/linux.die.net\/man\/1\/clamz"}},{"Process":{"Description":"clang is a C and Objective-C compiler which encompasses preprocessing, parsing, optimization, code generation, assembly, and linking. Depending on which high-level mode setting is passed, Clang will stop before doing a full link. While Clang is highly integrated, it is important to understand the stages of compilation, to understand how to invoke it. These stages are: Driver The clang executable is actually a small driver which controls the overall execution of other tools such as the compiler, assembler and linker. Typically you do not need to interact with the driver, but you transparently use it to run the other tools. Preprocessing This stage handles tokenization of the input source file, macro expansion, #include expansion and handling of other preprocessor directives. The output of this stage is typically called a \".i\" (for C) or \".mi\" (for Objective-C) file. Parsing and Semantic Analysis This stage parses the input file, translating preprocessor tokens into a parse tree. Once in the form of a parser tree, it applies semantic analysis to compute types for expressions as well and determine whether the code is well formed. This stage is responsible for generating most of the compiler warnings as well as parse errors. The output of this stage is an \"Abstract Syntax Tree\" ( AST ). Code Generation and Optimization This stage translates an AST into low-level intermediate code (known as \" LLVM IR \") and ultimately to machine code. This phase is responsible for optimizing the generated code and handling target-specfic code generation. The output of this stage is typically called a \".s\" file or \"assembly\" file. Clang also supports the use of an integrated assembler, in which the code generator produces object files directly. This avoids the overhead of generating the \".s\" file and of calling the target assembler. Assembler This stage runs the target assembler to translate the output of the compiler into a target object file. The output of this stage is typically called a \".o\" file or \"object\" file. Linker This stage runs the target linker to merge multiple object files into an executable or dynamic library. The output of this stage is typically called an \"a.out\", \".dylib\" or \".so\" file. The Clang compiler supports a large number of options to control each of these stages. In addition to compilation of code, Clang also supports other tools: Clang Static Analyzer The Clang Static Analyzer is a tool that scans source code to try to find bugs though code analysis. This tool uses many parts of Clang and is built into the same driver.","Process Name":"clang","Link":"https:\/\/linux.die.net\/man\/1\/clang"}},{"Process":{"Description":"You specify the required instance-variables of a desired class by a control-file. Classgen then creates a new package for you which contains the \"new()\"-method, accessor- and manipulator-methods for all instance-variables of that class. All instance-variables are blessed into an anonymous hash {}. classgen adds a basic, simple perldoc skelton to support your documentation for new classes. For a better overview version 3.03 splits the package into a .pm and a .gs file. The .gs file contains all standard get-\/set-methods and will be reviewed only on occasion. The .pm will focus on the specific methods only. Printouts will become much shorter and your overview will increase. Input format of the control file The control file is an ASCII-file which must contain the 3 sections with specific keywords in the specified order: header:\nvariables: classgen will scan for these 2 sections and act accordingly. classgen will insist on using exactly these section-headers. It will die, if you don't. \u2022 header: In this section you can put any Perl statement you need. classgen will simply copy this section to the beginning of your output file. You should at least include package Your_Class_Name_here; use strict; \u2022 variables: In this section you specify all your required instance-variables. classgen will determine the required type from the first character ($,@ or %) and will create all required variable-related methods for you. - For your convenience and to nurture self-documentation of your new class you can add some descriptive comments after the variable, just using a #. Methods generated classgen generates accessor- and manipulator-methods as required by the variables type. The order of methods is: \u2022 new-method \u2022 specifc methods (to be extended by the user) \u2022 accessor methods \u2022 manipulator methods \u2022 (perldoc-skelton) (to be extended by the user) The specific() method is intended for copy&paste operation to easily append the generated class with more specific class-methods by the user. A variable-name consists of <type><var_name>, with <type>=($,@,%). Via <type> methods are generated as: <method name><var_name> Run perldoc Attribute.pm for an overview of all created methods. Missing Methods for the created Objects A serialize() method should be added. So far my personal lack of knowledge in this subject prevented me from supplying you with one. Conway examines some possibilities, which you can find at CPAN . Other Objects involved classgen will use instances from the \u2022 New.pm - Object \u2022 Attributs.pm - Object \u2022 Section.pm - Object \u2022 Comment.pm - just a package, not an object. Cf. perldoc Comment.pm Subroutines used by classgen Internal methods: \u2022 check_of_sections: to ensure every needed section is there \u2022 clean_up: to remove \\t and other whitespaces \u2022 find_section: use this to use correct keywords \u2022 get_sections: to retrieve sections from input file \u2022 init_attributes : to generate Attribute-instances for all variables \u2022 init_new: to initialize the New-instance \u2022 write_begining : to write the first few lines into the new class \u2022 write_end : code at the end of the new class \u2022 write_methods: to generate all required methods for each variable \u2022 write_new: to write the new-function of the generated class How to use classgen In version 3.03 an archive functionality has been introduced. Before classgen creates the .pm and the .gs files from the control file, the old files are copied into the \/archive directory with an increasing numeriacl starting index. So your previous edits are not lost. \u2022 FIRST RUN: Simply create a control file somewhere and create the output file where appropriate (e.g. classgen \/controls\/example \/Objects\/Example.pm). \u2022 ADDITIONAL RUNS: Classgen copies the current versions of .pm and .gs into the \/archive directory. Next, it overwrites the current .pm and .gs as indicated by your control file. \u2022 ADDING METHODS: Edit your newly created package (e.g. Example.pm). Use the dummy-method at the end to create all further required methods of your class. All accessor- and manipulator-methods should already be there (I hope :-) \u2022 REMOVING METHODS: Cut them with your editor from the newly created package (e.g. Example.pm) or put comments at every line of the undesired subroutine. Perhaps you want to use them later again? Then cut them first and paste them to an intermediate-file. \u2022 RESTORING REMOVED METHODS: You have several choices: re-run classgen to create a safe copy of your package (e.g. Example_new.pm); copy&paste the missing part. Or re-use the intermediate-file from the previous hint; retrieving it from one of the files in the \/archive directory etc. How to produce undesired results (and how to avoid them) Version 3.03: Problems from overwriting have been removed. Your edits are copied to the \/archive directory first. How to handle inheritance of classes Derive two classes with classgen. In the header-section of the derived class you can add the required Perl-statements already in the control-file. Make sure to include the base-class in an @ISA-statement and adapt the call to inherit_from() right after the blessing in the new() method of your derived class. See also examples\/inheritance\/README Open issues I would like to know how classgen performs compared to other approaches to object-oriented-implementations. How can I find out how much memory my version actually consumes? - Please, gimme a hint ... Instance-variables could also be blessed into [] or $. Initially I planned to offer these choices, too. It turned out to be more convenient to omit this for a start. Your benefit is that you can do less things wrong. As a by-product the control file becomes more handy ;-) Background \/ Motivation J.Rumbaughs \"Object Oriented Modelling\" showed me a very promissing way to tackle challenging software-projects. The idea is to identify relevant classes (objects), their relationship (associations), their dynamic behaviour (statediagrams) and their flow of data (functional model) - all on paper. Once done and tested by various scenarios a very good specification has been grown. Then there are ways to implement these objects both in object-oriented (e.g. C ++ , Smalltalk, Eiffel) and NON-object-oriented languages (e.g. C, Fortran, Algol) or databases. - What a challange for Perl! I tried Damian Conways excellent book \"Object Oriented Perl\", which has been published in fall 1999. I learned again, that there still \"... are always more than one way to do it\" when it comes to implementing objects in Perl: \u2022 do it by hand: introduce a new instance-variable and write all required methods to acces or change this variable yourself. \u2022 use the AUTOLOAD-mechanism to provide all required get- and set-methods at run-time. \u2022 use class-templates, like Class::Struct or Class::MethodMaker. Clearly, the first choice is the least desireable one. It is prone to typing errors and one spends more time on same methods again rather than on methods unique to the class (as I would prefer). The other methods have some advantages and some disadvantages. The least tasty for me was that they tend to produce \"obscure\" code, at least in examples I looked at. That is, code where I had to spend a lot of thinking to understand what it does. Rather, I'd prefer code which I can understand at just one glance (idealy ;-). Therefor classgen creates almost every get- and set-method you can think of. Should you need it, it is already there. Should you dislike it, simply throw methods away. Or even better, just do not care about their existence at all. Stay focused on what your class should be doing, not on the instance-variables. There is another inportant reason for my approach. I feel OOP-code appears to be more 'cleaner' when it can be distributed over separated, individual files. Some of the approaches mentioned above tend to create and modify objects WITHIN another program. I think that shouldn't be done. Have a look at my 'peanuts' example. Future Plans classgen lets me create new classes within minutes rather than days. I myself am quite satisfied with the current performance. classgen was created more or less on-the-fly. Perhaps I will redesign it by a more consequent OMT-approach lateron. But before doing so I'd need more input about the good's and bad's from you to make up my mind. The missing parts. The nice-to-have things and so on. If you like classgen and want it to have more useful functions please let me know ;-)","Process Name":"classgen","Link":"https:\/\/linux.die.net\/man\/1\/classgen"}},{"Process":{"Description":"Claws Mail is an email client (and news reader) based on GTK+, running on the X Window System, and aiming for: * Quick response * Graceful, and sophisticated interface * Easy configuration, intuitive operation * Abundant features Currently implemented features include: o POP3 support o IMAP4rev1 support o integrated NetNews client o unlimited multiple account handling o thread display o filtering o MIME (attachments) o built-in image view o X-Face and (colour) Face support o multiple MH folder support o mbox importing \/ exporting o external editor support o message queueing o automatic mail checking o draft message function o template function o line-wrapping o clickable URI o XML-based address book o LDAP, vCard, and JPilot support o newly arrived and unread message management o Mew\/Wanderlust-like key bind o printing o APOP authentication support o SMTP AUTH support o SSL support (POP3, SMTP, IMAP4rev1) o IPv6 support o drag & drop (partially implemented) o GnuPG support o automake + autoconf support o internationalization of messages by gettext o support of many code sets, including UTF-8 (Unicode) (when using libjconv) o support for plugins This list is not complete.","Process Name":"claws-mail","Link":"https:\/\/linux.die.net\/man\/1\/claws-mail"}},{"Process":{"Description":"This is a thin wrapper round clbuild for use with common lisp controller. You can use this to install and update clbuild enabled packages. To use first do (as root) clc-clbuild setup, this will download and setup clbuild. Then you can use the normal clbuild commands. These will be ran as the user cl-builder and will setup a clbuild environment in \/usr\/share\/common-lisp\/clbuild\/ For example running (as root again) clc-clbuild install cl-ppcre will download and install cl-ppcre from the appropriate libraries. Please see clc-clbuild help for more information.","Process Name":"clc-clbuild","Link":"https:\/\/linux.die.net\/man\/1\/clc-clbuild"}},{"Process":{"Description":"This is a thin wrapper round clbuild for use with common lisp controller. In effect it will just run clbuild lisp this is not working at the moment due to a patch we need to have in clbuild","Process Name":"clc-lisp","Link":"https:\/\/linux.die.net\/man\/1\/clc-lisp"}},{"Process":{"Description":"Registers a user defined system into the common lisp controller system. This means that any Common Lisp implementation will know how to find the system. The system file itself will be symlinked from the ~\/.clc\/systems directory and the system will be recorded in the ~\/.clc\/user-packages.db file. No removal of old fasl files during upgrades of Common Lisp Implementations will be done, nor will the fasls of different implementations will be separated by clc , it is expected that the users who require this will create a full Debian package or create a symlink in \/usr\/share\/common-lisp\/systems\/ to the system file.","Process Name":"clc-register-user-package","Link":"https:\/\/linux.die.net\/man\/1\/clc-register-user-package"}},{"Process":{"Description":"This is a thin wrapper round clbuild for use with common lisp controller. In effect it will just run clbuild slime this is not working at the moment due to a patch we need to have in clbuild","Process Name":"clc-slime","Link":"https:\/\/linux.die.net\/man\/1\/clc-slime"}},{"Process":{"Description":"Registers a user defined system into the common lisp controller system. This means that any Common Lisp implementation will know how to find the system. The system file itself will be symlinked from the ~\/.clc\/systems directory and the system will be recorded in the ~\/.clc\/user-packages.db file. No removal of old fasl files during upgrades of Common Lisp Implementations will be done, nor will the fasls of different implementations will be separated by clc , it is expected that the users who require this will create a full Debian package or create a symlink in \/usr\/share\/common-lisp\/systems\/ to the system file.","Process Name":"clc-unregister-user-package","Link":"https:\/\/linux.die.net\/man\/1\/clc-unregister-user-package"}},{"Process":{"Description":"cleanarch deletes old keys from a DNSSEC-Tools key archive. Key \"age\" and archives are determined by options and arguments. Command line options and arguments allow selection of archives, keys to delete, amount of output to provide. The options are divided into three groups: archive selection, key selection, and output format. Complete information on options is provided in the OPTIONS section. cleanarch takes a single argument (as distinguished from an option.) This argument may be either a keyrec file or a rollrec file. If the file is a keyrec file, the archive directory for its zone keyrecs are added to the list of archives to clean. If the file is a rollrec file, keyrec files for its zones are searched for the zones' archive directory, and those directories are added to the list of archives to clean. If a zone does not have an archive directory explicitly defined, then the DNSSEC-Tools default will be cleaned. The archives specified by this argument may be modified by archive-selection options. The archive-selection options combine with the keyrec or rollrec file to select a set of archive directories to clean. (Some options can take the place of the file argument.) The key-selection options allow the set of keys to be deleted to contain an entire archive, a particular zone's keys, or all the keys prior to a certain date. The output-format options sets how much output will be given. Without any options selected, the names of keys will be printed as they are deleted. If the -verbose option is given, then the directories selected for searching and the keys selected for deletion will be printed. If the -dirlist option is given, then the directories selected for searching will be printed and no other action will be taken. If the -list option is given, then the keys selected for deletion will be printed and no other action will be taken. cleanarch only cleans the archive directories; the keyrec files are left intact. The cleankrf command should be used in conjunction with cleanarch in order to have a consistent environment.","Process Name":"cleanarch","Link":"https:\/\/linux.die.net\/man\/1\/cleanarch"}},{"Process":{"Description":"This command frees the shared memory segments and semaphores help by the user that calls this command. This command is normally not needed, but if MPI programs exit abnormally, the program may be unable to free the System V IPCs that it held (this is a feature of System V IPCs). In that case, cleanipcs can be used to recover the IPCs that are no longer needed. Note that this command releases all IPCs held by the user. This is the correct behavior for most users, but may cause problems for users of other programs or systems that rely on the persistence of System V IPCs.","Process Name":"cleanipcs","Link":"https:\/\/linux.die.net\/man\/1\/cleanipcs"}},{"Process":{"Description":"cleankrf cleans old data out of a set of DNSSEC-Tools keyrec files. The old data are obsolete signing sets, orphaned keys, and obsolete keys. Obsolete signing sets are set keyrecs unreferenced by a zone keyrec. Revoked signing sets are considered obsolete by cleankrf. Orphaned keys are KSK and ZSK key keyrecs unreferenced by a set keyrec. Obsolete keys are key keyrecs with a keyrec_type of kskobs or zskobs. cleankrf's exit code is the count of orphaned and obsolete keyrecs found.","Process Name":"cleankrf","Link":"https:\/\/linux.die.net\/man\/1\/cleankrf"}},{"Process":{"Description":"The cleanlinks program searches the directory tree descended from the current directory for symbolic links whose targets do not exist, and removes them. It then removes all empty directories in that directory tree. cleanlinks is useful for cleaning up a shadow link tree created with lndir(1) after files have been removed from the real directory.","Process Name":"cleanlinks","Link":"https:\/\/linux.die.net\/man\/1\/cleanlinks"}},{"Process":{"Description":"cleanup_digikamdb will cleanup and optimize the digiKam database file. This will, in most cases, lead to a smaller database file size and an increased access speed, because unneeded elements are removed from the database and data is optimized. The program will make sure that no instance of digiKam is running, because it is more safe to have no database access during the optimization process. It then will read the digiKam configuration file and determine the database location. In a final step the database will be optimized by invoking the sqlite command 'VACUUM;' on it. If more then one database is found in this location, cleanup_digikamdb will optimize every database found in this path and below. For further explanation, see the following description of the VACUUM command from the sqlite3 website: When an object (table, index, or trigger) is dropped from the database, it leaves behind empty space. This empty space will be reused the next time new information is added to the database. But in the meantime, the database file might be larger than strictly necessary. Also, frequent inserts, updates, and deletes can cause the information in the database to become fragmented - scrattered out all across the database file rather than clustered together in one place. The VACUUM command cleans the main database by copying its contents to a temporary database file and reloading the original database file from the copy. This eliminates free pages, aligns table data to be contiguous, and otherwise cleans up the database file structure.","Process Name":"cleanup_digikamdb","Link":"https:\/\/linux.die.net\/man\/1\/cleanup_digikamdb"}},{"Process":{"Description":"clear clears your screen if this is possible. It looks in the environment for the terminal type and then in the terminfo database to figure out how to clear the screen. clear ignores any command-line parameters that may be present.","Process Name":"clear","Link":"https:\/\/linux.die.net\/man\/1\/clear"}},{"Process":{"Description":"CLEX is an interactive full-screen file manager. Refer to the on-line help for more information about usage.","Process Name":"clex","Link":"https:\/\/linux.die.net\/man\/1\/clex"}},{"Process":{"Description":"clide is a program that allows you to colorize and add various display altering attributes to text based on search patterns and expressions. Currently the focus is on creating ANSI color and style escape codes for use in terminal displays.","Process Name":"clide","Link":"https:\/\/linux.die.net\/man\/1\/clide"}},{"Process":{"Description":"Clip, adapted from Daeron Meyer's ginsu module, allows clipping an OOGL object against planes, spheres, or cylinders from the UNIX command line. Its input can come from a file or standard input; output is written to standard output. Options specify a function of space position; the output is the portion of the object where the function is greater or less than some given value, or the portion lying between two values. Alternatively, an object can be sliced into equally-spaced strips. Objects may be of any dimension (but see the BUGS section). Options are: -g value-or-point -l value-or-point Select the portion of the object where the function is greater than ( -g) or less than ( -l) the given value. If both are specified, the result is the portion of the object satisfying both conditions. If, rather than a single number, the argument to -l or -g is a point (a series of x,y,z,... values separated by commas, with no embedded blanks), then the clipping surface is one chosen to pass through that point. -v axisx,y,z,... Specifies a direction in space. For planar clipping (the default), it's the plane normal direction; the clipping function is the inner product between the direction vector and the point on the object. For cylindrical clipping, -v specifies the direction of the cylinder's axis; the clipping function is the distance from the axis. -sph centerx,y,z,... Clip against spheres centered on x,y,z,.... The clipping function is the distance from the given center. Coordinates must be separated by commas without intervening spaces. -cyl centerx,y,z,... Clip against cylinders with an axis passing through centerx,y,z,..., with axis direction given by the -v option. The clipping function is the distance from the axis. -s nslices[, fraction] Clip an object into a series of nslices ribbons spanning its entire extent -- the range of function-values over the object. Part of each ribbon is omitted; the fraction, default .5, sets the width of the visible part of a ribbon compared to the ribbon period. There are a total of ( nslices+ fraction-1) ribbon periods across the object, so e.g. -s 2,.5 slices the object into equal thirds, omitting the middle third. The output OOGL object is a LIST of OFFs, one per ribbon. -e Don't emit a clipped OOGL object, just print two numbers, listing the minimum and maximum function values for the object. If -g or -l clipping options are specified, the object is clipped before determining the function range. If none of the object remains, clip prints \"0 0\".","Process Name":"clip","Link":"https:\/\/linux.die.net\/man\/1\/clip"}},{"Process":{"Description":"Invokes the Common Lisp[1] interpreter and compiler\\. When called without arguments, executes the read\\-eval\\-print loop[2], in which expressions are in turn READ[3] from the standard input, EVAL[4]uated by the lisp interpreter, and their results are PRINT[5]ed to the standard output\\. Invoked with \\-c, compiles the specified lisp files to a platform\\-independent bytecode which can be executed more efficiently\\.","Process Name":"clisp","Link":"https:\/\/linux.die.net\/man\/1\/clisp"}},{"Process":{"Description":"Makes an application from clean (.icl and .dcl) file(s). The main_module_name should be specified without (.icl) extension. If '-o application_name' is not used, the name of the application is 'a.out'. (and is stored in the working directory)","Process Name":"clm","Link":"https:\/\/linux.die.net\/man\/1\/clm"}},{"Process":{"Description":"cln-config is a tool that is used to determine the compiler and linker flags that should be used to compile and link programs that use CLN.","Process Name":"cln-config","Link":"https:\/\/linux.die.net\/man\/1\/cln-config"}},{"Process":{"Description":"Count, or compute differences of, physical lines of source code in the given files (may be archives such as compressed tarballs or zip files) and\/or recursively below the given directories. It is written entirely in Perl, using only modules from the standard distribution.","Process Name":"cloc","Link":"https:\/\/linux.die.net\/man\/1\/cloc"}},{"Process":{"Description":"clogin is an expect(1) script to automate the process of logging into a Cisco router, catalyst switch, Extreme switch, Juniper ERX\/E-series, Procket Networks, or Redback router. There are complementary scripts for Alteon, Avocent (Cyclades), Bay Networks (nortel), ADC-kentrox EZ-T3 mux, Foundry, HP Procurve Switches and Cisco AGMs, Hitachi Routers, Juniper Networks, MRV optical switch, Netscreen firewalls, Netscaler, Riverstone, Netopia, and Lucent TNT, named alogin, avologin, blogin, elogin, flogin, fnlogin, hlogin, htlogin, jlogin, mrvlogin, nlogin, nslogin, rivlogin, tlogin, and tntlogin, respectively. clogin reads the .cloginrc file for its configuration, then connects and logs into each of the routers specified on the command line in the order listed. Command-line options exist to override some of the directives found in the .cloginrc configuration file. The command-line options are as follows: -S Save the configuration on exit, if the device prompts at logout time. This only has affect when used with -s. -V Prints package name and version strings. -c Command to be run on each router list on the command-line. Multiple commands maybe listed by separating them with semi-colons (;). The argument should be quoted to avoid shell expansion. -d Enable expect debugging. -E Specifies a variable to pass through to scripts (-s). For example, the command-line option -Efoo=bar will produce a global variable by the name Efoo with the initial value \"bar\". -e Specify a password to be supplied when gaining enable privileges on the router(s). Also see the password directive of the .cloginrc file. -f Specifies an alternate configuration file. The default is $HOME\/.cloginrc. -p Specifies a password associated with the user specified by the -u option, user directive of the .cloginrc file, or the Unix username of the user. -s The filename of an expect(1) script which will be sourced after the login is successful and is expected to return control to clogin, with the connection to the router intact, when it is done. Note that clogin disables log_user of expect(1)when -s is used. Example script(s) can be found in share\/rancid\/*.exp. -t Alters the timeout interval; the period that clogin waits for an individual command to return a prompt or the login process to produce a prompt or failure. The argument is in seconds. -u Specifies the username used when prompted. The command-line option overrides any user directive found in .cloginrc. The default is the current Unix username. -v Specifies a vty password, that which is prompted for upon connection to the router. This overrides the vty password of the .cloginrc file's password directive. -w Specifies the username used if prompted when gaining enable privileges. The command-line option overrides any user or enauser directives found in .cloginrc. The default is the current Unix username. -x Similar to the -c option; -x specifies a file with commands to run on each of the routers. The commands must not expect additional input, such as 'copy rcp startup-config' does. For example: show version\nshow logging -y Specifies the encryption algorithm for use with the ssh(1) -c option. The default encryption type is often not supported. See the ssh(1) man page for details. The default is 3des.","Process Name":"clogin","Link":"https:\/\/linux.die.net\/man\/1\/clogin"}},{"Process":{"Description":"The cloudlife program draws a cellular automaton based on Conway's Life, except that cells have a maximum age, and only one pixel per cell is drawn every tick.","Process Name":"cloudlife","Link":"https:\/\/linux.die.net\/man\/1\/cloudlife"}},{"Process":{"Description":"clpbar is a simple tool to process a stream of data and print a display for the user on stderr showing (a) the amount of data passed, (b) the throughput of the data transfer, and, if the total size of the data stream is known, (c) estimated time remaining, percent complete, and a progress bar. clpbar was originally written for the purpose of estimating the amount of time needed to transfer large amounts (many, many gigabytes) of data across a network. (Usually in an SSH\/tar pipe.)","Process Name":"clpbar","Link":"https:\/\/linux.die.net\/man\/1\/clpbar"}},{"Process":{"Description":"clubak formats text from standard input containing lines of the form \"node:output\". It is fully backward compatible with dshbak(1) but provides additonal features. For instance, clubak always displays its results sorted by node\/nodeset. You do not need to use clubak when using clush(1) as all output formatting features are already included in. It is provided for other usages, like post-processing results of the form \"node:output\". Like clush(1), clubak uses the ClusterShell.MsgTree module of the ClusterShell library (see pydoc ClusterShell.MsgTree).","Process Name":"clubak","Link":"https:\/\/linux.die.net\/man\/1\/clubak"}},{"Process":{"Description":"clush is a program for executing commands in parallel on a cluster and for gathering their results. clush executes commands interactively or can be used within shell scripts and other applications. It is a partial front-end to the ClusterShell library that ensures a light, unified and robust parallel command execution framework. Thus, it allows traditional shell scripts to benefit from some of the library features. clush currently makes use of the Ssh worker of ClusterShell that only requires ssh(1) (OpenSSH SSH client).","Process Name":"clush","Link":"https:\/\/linux.die.net\/man\/1\/clush"}},{"Process":{"Description":"clusterdb is a utility for reclustering tables in a PostgreSQL database. It finds tables that have previously been clustered, and clusters them again on the same index that was last used. Tables that have never been clustered are not affected. clusterdb is a wrapper around the SQL command CLUSTER [cluster(7)]. There is no effective difference between clustering databases via this utility and via other methods for accessing the server.","Process Name":"clusterdb","Link":"https:\/\/linux.die.net\/man\/1\/clusterdb"}},{"Process":{"Description":"cm2rem.tcl reads the Sun calendar manager data file and converts it into a Remind script. Note that cm2rem.tcl can convert only version 3 calendar manager files. If you are using version 4 files, there should be a system utility to convert them to version 3 files.","Process Name":"cm2rem","Link":"https:\/\/linux.die.net\/man\/1\/cm2rem"}},{"Process":{"Description":"The \"cmake\" executable is the CMake command-line interface. It may be used to configure projects in scripts. Project configuration settings may be specified on the command line with the -D option. The -i option will cause cmake to interactively prompt for such settings. CMake is a cross-platform build system generator. Projects specify their build process with platform-independent CMake listfiles included in each directory of a source tree with the name CMakeLists.txt. Users build a project by using CMake to generate a build system for a native tool on their platform.","Process Name":"cmake","Link":"https:\/\/linux.die.net\/man\/1\/cmake"}},{"Process":{"Description":"The \"cmake-gui\" executable is the CMake GUI. Project configuration settings may be specified interactively. Brief instructions are provided at the bottom of the window when the program is running. CMake is a cross-platform build system generator. Projects specify their build process with platform-independent CMake listfiles included in each directory of a source tree with the name CMakeLists.txt. Users build a project by using CMake to generate a build system for a native tool on their platform.","Process Name":"cmake-gui","Link":"https:\/\/linux.die.net\/man\/1\/cmake-gui"}},{"Process":{"Description":"The \"cmake\" executable is the CMake command-line interface. It may be used to configure projects in scripts. Project configuration settings may be specified on the command line with the -D option. The -i option will cause cmake to interactively prompt for such settings. CMake is a cross-platform build system generator. Projects specify their build process with platform-independent CMake listfiles included in each directory of a source tree with the name CMakeLists.txt. Users build a project by using CMake to generate a build system for a native tool on their platform.","Process Name":"cmake28","Link":"https:\/\/linux.die.net\/man\/1\/cmake28"}},{"Process":{"Description":"The \"cmake-gui\" executable is the CMake GUI. Project configuration settings may be specified interactively. Brief instructions are provided at the bottom of the window when the program is running. CMake is a cross-platform build system generator. Projects specify their build process with platform-independent CMake listfiles included in each directory of a source tree with the name CMakeLists.txt. Users build a project by using CMake to generate a build system for a native tool on their platform.","Process Name":"cmake28-gui","Link":"https:\/\/linux.die.net\/man\/1\/cmake28-gui"}},{"Process":{"Description":"","Process Name":"cmake28commands","Link":"https:\/\/linux.die.net\/man\/1\/cmake28commands"}},{"Process":{"Description":"","Process Name":"cmake28compat","Link":"https:\/\/linux.die.net\/man\/1\/cmake28compat"}},{"Process":{"Description":"The \"cmake\" executable is the CMake command-line interface. It may be used to configure projects in scripts. Project configuration settings may be specified on the command line with the -D option. The -i option will cause cmake to interactively prompt for such settings. CMake is a cross-platform build system generator. Projects specify their build process with platform-independent CMake listfiles included in each directory of a source tree with the name CMakeLists.txt. Users build a project by using CMake to generate a build system for a native tool on their platform.","Process Name":"cmake28modules","Link":"https:\/\/linux.die.net\/man\/1\/cmake28modules"}},{"Process":{"Description":"The \"cmake\" executable is the CMake command-line interface. It may be used to configure projects in scripts. Project configuration settings may be specified on the command line with the -D option. The -i option will cause cmake to interactively prompt for such settings. CMake is a cross-platform build system generator. Projects specify their build process with platform-independent CMake listfiles included in each directory of a source tree with the name CMakeLists.txt. Users build a project by using CMake to generate a build system for a native tool on their platform.","Process Name":"cmake28policies","Link":"https:\/\/linux.die.net\/man\/1\/cmake28policies"}},{"Process":{"Description":"","Process Name":"cmake28props","Link":"https:\/\/linux.die.net\/man\/1\/cmake28props"}},{"Process":{"Description":"","Process Name":"cmake28vars","Link":"https:\/\/linux.die.net\/man\/1\/cmake28vars"}},{"Process":{"Description":"","Process Name":"cmakecommands","Link":"https:\/\/linux.die.net\/man\/1\/cmakecommands"}},{"Process":{"Description":"","Process Name":"cmakecompat","Link":"https:\/\/linux.die.net\/man\/1\/cmakecompat"}},{"Process":{"Description":"The \"cmake\" executable is the CMake command-line interface. It may be used to configure projects in scripts. Project configuration settings may be specified on the command line with the -D option. The -i option will cause cmake to interactively prompt for such settings. CMake is a cross-platform build system generator. Projects specify their build process with platform-independent CMake listfiles included in each directory of a source tree with the name CMakeLists.txt. Users build a project by using CMake to generate a build system for a native tool on their platform.","Process Name":"cmakemodules","Link":"https:\/\/linux.die.net\/man\/1\/cmakemodules"}},{"Process":{"Description":"The \"cmake\" executable is the CMake command-line interface. It may be used to configure projects in scripts. Project configuration settings may be specified on the command line with the -D option. The -i option will cause cmake to interactively prompt for such settings. CMake is a cross-platform build system generator. Projects specify their build process with platform-independent CMake listfiles included in each directory of a source tree with the name CMakeLists.txt. Users build a project by using CMake to generate a build system for a native tool on their platform.","Process Name":"cmakepolicies","Link":"https:\/\/linux.die.net\/man\/1\/cmakepolicies"}},{"Process":{"Description":"","Process Name":"cmakeprops","Link":"https:\/\/linux.die.net\/man\/1\/cmakeprops"}},{"Process":{"Description":"","Process Name":"cmakevars","Link":"https:\/\/linux.die.net\/man\/1\/cmakevars"}},{"Process":{"Description":"cmdtest black box tests Unix command line tools. Given some test scripts, their inputs, and expected outputs, it verifies that the command line produces the expected output. If not, it reports problems, and shows the differences. Each test case foo consists of the following files: foo.script a script to run the test (this is required) foo.stdin the file fed to standard input foo.stdout the expected output to the standard output foo.stderr the expected output to the standard error foo.exit the expected exit code foo.setup a shell script to run before the test foo.teardown a shell script to run after test Usually, a single test is not enough. All tests are put into the same directory, and they may share some setup and teardown code: setup-once a shell script to run once, before any tests setup a shell script to run before each test teardown a shell script to run after each test teardown-once a shell script to run once, after all tests cmdtest is given the name of the directory with all the tests, or several such directories, and it does the following: \u2022 execute setup-once \u2022 for each test case (unique prefix foo): - execute setup - execute foo.setup - execute the command, by running foo.script, and redirecting standard input to come from foo.stdin, and capturing standard output and error and exit codes - execute foo.teardown - execute teardown - report result of test: does exit code match foo.exit, standard output match foo.stdout, and standard error match foo.stderr? \u2022 execute teardown-once Except for foo.script, all of these files are optional. If a setup or teardown script is missing, it is simply not executed. If one of the standard input, output, or error files is missing, it is treated as if it were empty. If the exit code file is missing, it is treated as if it specified an exit code of zero. The shell scripts may use the following environment variables: DATADIR a temporary directory where files may be created by the test TESTNAME name of the current test (will be empty for setup-once and teardown-once) SRCDIR directory from which cmdtest was launched","Process Name":"cmdtest","Link":"https:\/\/linux.die.net\/man\/1\/cmdtest"}},{"Process":{"Description":"Compare two files byte by byte. -b --print-bytes Print differing bytes. -i SKIP --ignore-initial= SKIP Skip the first SKIP bytes of input. -i SKIP1:SKIP2 --ignore-initial= SKIP1:SKIP2 Skip the first SKIP1 bytes of FILE1 and the first SKIP2 bytes of FILE2. -l --verbose Output byte numbers and values of all differing bytes. -n LIMIT --bytes= LIMIT Compare at most LIMIT bytes. -s --quiet --silent Output nothing; yield exit status only. -v --version Output version info. --help Output this help. SKIP1 and SKIP2 are the number of bytes to skip in each file. SKIP values may be followed by the following multiplicative suffixes: kB 1000, K 1024, MB 1,000,000, M 1,048,576, GB 1,000,000,000, G 1,073,741,824, and so on for T, P, E, Z, Y. If a FILE is '-' or missing, read standard input.","Process Name":"cmp","Link":"https:\/\/linux.die.net\/man\/1\/cmp"}},{"Process":{"Description":"The cms command handles S\/MIME v3.1 mail. It can encrypt, decrypt, sign and verify, compress and uncompress S\/MIME messages.","Process Name":"cms","Link":"https:\/\/linux.die.net\/man\/1\/cms"}},{"Process":{"Description":"cmus is a small ncurses based music player. It supports various output methods by output-plugins. It has got completely configurable keybindings and it can be controlled from the outside via cmus-remote(1).","Process Name":"cmus","Link":"https:\/\/linux.die.net\/man\/1\/cmus"}},{"Process":{"Description":"Add FILE\/DIR\/PLAYLIST to playlist, library (-l) or play queue (-q). If no arguments are given cmus-remote reads raw commands from stdin (one command per line). Raw commands are cmus' command mode commands. These same commands are used in configuration files and key bindings. cmus(1) contains full list of commands. For consistency also searching is supported: -C \/text. When -C is given all command line arguments are treated as raw commands.","Process Name":"cmus-remote","Link":"https:\/\/linux.die.net\/man\/1\/cmus-remote"}},{"Process":{"Description":"This program is part of Netpbm(1). cmuwmtopbm reads a CMU window manager bitmap as input. and produces a PBM image as output.","Process Name":"cmuwmtopbm","Link":"https:\/\/linux.die.net\/man\/1\/cmuwmtopbm"}},{"Process":{"Description":"The program cnee can record and replay an X session. cnee also has the ability to distribute events to multiple displays. cnee gets copies of X protocol data from the X server. These are either printed to file ( record mode) or replayed and synchronised ( replay mode). During record and replay cnee can distribute the record\/replayed events to multiple displays.","Process Name":"cnee","Link":"https:\/\/linux.die.net\/man\/1\/cnee"}},{"Process":{"Description":"Cntlm is an NTLM\/NTLMv2 authenticating HTTP proxy. It takes the address of your proxy or proxies (host1..N and port1..N) and opens a listening socket, forwarding each request to the parent proxy (moving in a circular list if the active parent stops working). Along the way, a connection to the parent is created anew and authenticated or, if available, previously cached connection is reused to achieve higher efficiency and faster responses. When the chain is set up, cntlm should be used as a proxy in your applications. Cntlm also integrates transparent TCP\/IP port forwarding (tunneling) through the parent (incl. authentication). Each tunnel opens a new listening socket on the defined local port and forwards all connections to the given host:port behind the secondary proxy. Manual page explains how to setup cntlm properly using configuration file or command-line arguments. Cntlm works similarly to NTLMAPS, plus full NTLM support, a bucket of new features and none of its shortcomings and inefficiencies. It adds support for real keep-alive (on both sides) and it caches all authenticated connections for reuse in subsequent requests. It can be restarted without TIME_WAIT delay, uses just a fraction of memory compared to NTLMAPS and by orders of magnitude less CPU. Each thread is completely independent and one cannot block another. Cntlm has many security\/privacy features like NTLMv2 support and password protection - it is possible to substitute password hashes (which can be obtained using -H) for the actual password or to enter the password interactively. If plaintext password is used, it is automatically hashed during the startup and all its traces are removed from the process memory. In addition to lower usage of system resources, cntlm achieves higher throughput on a given link. By caching authenticated connections, it acts as an HTTP accelerator; This way, the 5-way auth handshake for each connection is transparently eliminated, providing direct access most of the time. NTLMAPS doesn't authenticate in parallel with the request - instead, it first connects, sends a probe and disconnects. No sooner than that it connects again and initiates NTLM handshake. Cntlm also doesn't read the whole request including HTTP body into memory, in fact, no traffic is generated except for the exchange of headers until the client <-> server connection is fully negotiated. Only then are the request and response bodies forwarded, directly between client and server sockets. This way, cntlm avoids most of the TCP\/IP overhead of similar proxies. Along with the fact that cntlm is written in optimized C, it achieves up to fifteen times faster responses. The slower the line, the more impact cntlm has on download speeds. An example of cntlm compared to NTLMAPS under the same conditions: cntlm gave avg 76 kB\/s with peak CPU usage of 0.3% whereas with NTLMAPS it was avg 48 kB\/s with peak CPU at 98% (Pentium M 1.8 GHz). The extreme difference in resource usage is one of many important benefits for laptop use. Peak memory consumption (several complex sites, 50 paralell connections\/threads; values are in KiB):    VSZ   RSS CMD\n  3204  1436 .\/cntlm -f -c .\/cntlm.conf -P pid\n411604  6264 \/usr\/share\/ntlmaps\/main.py -c \/etc\/ntlmaps\/server.cfg Inherent part of the development is profiling and memory management screening using Valgrind. The source distribution contains a file called valgrind.txt, where you can see the report confirming zero leaks, no access to unallocated memory, no usage of uninitialized data - all tracked down to each CPU instruction emulated in Valgrind's virtual CPU during a typical production lifetime of the proxy.","Process Name":"cntlm","Link":"https:\/\/linux.die.net\/man\/1\/cntlm"}},{"Process":{"Description":"co retrieves a revision from each RCS file and stores it into the corresponding working file. Pathnames matching an RCS suffix denote RCS files; all others denote working files. Names are paired as explained in ci(1). Revisions of an RCS file can be checked out locked or unlocked. Locking a revision prevents overlapping updates. A revision checked out for reading or processing (e.g., compiling) need not be locked. A revision checked out for editing and later checkin must normally be locked. Checkout with locking fails if the revision to be checked out is currently locked by another user. (A lock can be broken with rcs(1).) Checkout with locking also requires the caller to be on the access list of the RCS file, unless he is the owner of the file or the superuser, or the access list is empty. Checkout without locking is not subject to accesslist restrictions, and is not affected by the presence of locks. A revision is selected by options for revision or branch number, checkin date\/time, author, or state. When the selection options are applied in combination, co retrieves the latest revision that satisfies all of them. If none of the selection options is specified, co retrieves the latest revision on the default branch (normally the trunk, see the -b option of rcs(1)). A revision or branch number can be attached to any of the options -f, -I, -l, -M, -p, -q, -r, or -u. The options -d (date), -s (state), and -w (author) retrieve from a single branch, the selected branch, which is either specified by one of -f, ..., -u, or the default branch. A co command applied to an RCS file with no revisions creates a zero-length working file. co always performs keyword substitution (see below).","Process Name":"co","Link":"https:\/\/linux.die.net\/man\/1\/co"}},{"Process":{"Description":"Cobbler manages provisioning using a tiered concept of Distributions, Profiles, Systems, and (optionally) Images and Repositories. Distributions contain information about what kernel and initrd are used, plus metadata (required kernel parameters, etc). Profiles associate a Distribution with a kickstart file and optionally customize the metadata further. Systems associate a MAC , IP , and other networking details with a profile and optionally customize the metadata further. Repositories contain yum mirror information. Using cobbler to mirror repositories is an optional feature, though provisioning and package management share a lot in common. Images are a catch-all concept for things that do not play nicely in the \"distribution\" category. Most users will not need these records initially and these are described later in the document. The main advantage of cobbler is that it glues together many disjoint technologies and concepts and abstracts the user from the need to understand them. It allows the systems administrator to concentrate on what he needs to do, and not how it is done. This manpage will focus on the cobbler command line tool for use in configuring cobbler. There is also mention of the Cobbler WebUI which is usable for day-to-day operation of Cobbler once installed\/configured. Docs on the API and XMLRPC components are available online at http:\/\/cobbler.github.com. Most users will be interested in the Web UI and should set it up, though the command line is needed for initial configuration -- in particular \"cobbler check\" and \"cobbler import\", as well as the repo mirroring features. All of these are described later in the documentation.","Process Name":"cobbler","Link":"https:\/\/linux.die.net\/man\/1\/cobbler"}},{"Process":{"Description":"Running cobbler-register on a system will create a cobbler system record for that system on a remote cobbler server. No changes will be made on the system itself.","Process Name":"cobbler-register","Link":"https:\/\/linux.die.net\/man\/1\/cobbler-register"}},{"Process":{"Description":"codeblocks launches the Code::Blocks IDE. Its various command-line arguments are listed below.","Process Name":"codeblocks","Link":"https:\/\/linux.die.net\/man\/1\/codeblocks"}},{"Process":{"Description":"codesnippets allows you to use codesnippets outside Code::Blocks, as a stand-alone application.","Process Name":"codesnippets","Link":"https:\/\/linux.die.net\/man\/1\/codesnippets"}},{"Process":{"Description":"","Process Name":"coherence","Link":"https:\/\/linux.die.net\/man\/1\/coherence"}},{"Process":{"Description":"Col filters out reverse (and half reverse) line feeds so the output is in the correct order with only forward and half forward line feeds, and replaces white-space characters with tabs where possible. This can be useful in processing the output of nroff(1) and tbl(1). Col reads from standard input and writes to standard output. The options are as follows:       -b'       Do not output any backspaces, printing only the last characterwritten to each column position. -f' Forward half line feeds are permitted (''fine'' mode). Normally characters printed on a half line boundary are printed on the following line. -p' Force unknown control sequences to be passed through unchanged. Normally, col will filter out any control sequences from the input other than those recognized and interpreted by itself, which are listed below. -x' Output multiple spaces instead of tabs. -lnum Buffer at least num lines in memory. By default, 128 lines are buffered. The control sequences for carriage motion that col understands and their decimal values are listed in the following table: ESC-7' reverse line feed (escape then 7) ESC-8' half reverse line feed (escape then 8) ESC-9' half forward line feed (escape then 9) backspace' moves back one column (8); ignored in the first column carriage return (13) newline' forward line feed (10); also does carriage return shift in' shift to normal character set (15) shift out' shift to alternate character set (14) space' moves forward one column (32) tab' moves forward to next tab stop (9) vertical tab' reverse line feed (11) All unrecognized control characters and escape sequences are discarded. Col keeps track of the character set as characters are read and makes sure the character set is correct when they are output. If the input attempts to back up to the last flushed line, col will display a warning message.","Process Name":"col","Link":"https:\/\/linux.die.net\/man\/1\/col"}},{"Process":{"Description":"Colcrt provides virtual half-line and reverse line feed sequences for terminals without such capability, and on which overstriking is destructive. Half-line characters and underlining (changed to dashing '-') are placed on new lines in between the normal output lines. Available options:       -'        Suppress all underlining.  This option is especially usefulfor previewing allboxed tables from tbl(1). -2' Causes all half-lines to be printed, effectively double spacing the output. Normally, a minimal space output format is used which will suppress empty lines. The program never suppresses two consecutive empty lines, however. The -2 option is useful for sending output to the line printer when the output contains superscripts and subscripts which would otherwise be invisible.","Process Name":"colcrt","Link":"https:\/\/linux.die.net\/man\/1\/colcrt"}},{"Process":{"Description":"collateindex.pl creates index data for DocBook XML or SGML files.","Process Name":"collateindex.pl","Link":"https:\/\/linux.die.net\/man\/1\/collateindex.pl"}},{"Process":{"Description":"collectd is a daemon that receives system statistics and makes them available in a number of ways. The main daemon itself doesn't have any real functionality apart from loading, querying and submitting to plugins. For a description of available plugins please see \" PLUGINS \" below.","Process Name":"collectd","Link":"https:\/\/linux.die.net\/man\/1\/collectd"}},{"Process":{"Description":"This small program is the glue between collectd and nagios. collectd collects various performance statistics which it provides via the \"unixsock plugin\", see collectd-unixsock(5). This program is called by Nagios, connects to the UNIX socket and reads the values from collectd. It then returns OKAY , WARNING or CRITICAL depending on the values and the ranges provided by Nagios.","Process Name":"collectd-nagios","Link":"https:\/\/linux.die.net\/man\/1\/collectd-nagios"}},{"Process":{"Description":"collectdmon is a small \"wrapper\" daemon which starts and monitors the collectd daemon. If collectd terminates it will automatically be restarted, unless collectdmon was told to shut it down.","Process Name":"collectdmon","Link":"https:\/\/linux.die.net\/man\/1\/collectdmon"}},{"Process":{"Description":"The collectl utility is a system monitoring tool that records or displays specific operating system data for one or more sets of subsystems. Any set of the subsystems, such as CPU, Disks, Memory or Sockets can be included in or excluded from data collection. Data can either be displayed back to the terminal, or stored in either a compressed or uncompressed data file. The data files themselves can either be in raw format (essentially a direct copy from the associated \/proc structures) or in a space separated plottable format such that it can be easily plotted using tools such as gnuplot or excel. Data files can be read and manipulated from the command line, or through use of command scripts. Upon startup, collectl.conf is read, which sets a number of default parameters and switch values. Collectl searches for this file first in \/etc, then in the directory the collectl execuable lives in (typically \/usr\/sbin) and finally the current directory. These locations can be overriden with the -C switch. Unless you're doing something really special, this file need never be touched, the only exception perhaps being when choosing to run collectl as a service and you wish to change it's default behavior which is set by the DaemonCommand entry.","Process Name":"collectl","Link":"https:\/\/linux.die.net\/man\/1\/collectl"}},{"Process":{"Description":"This utility shall only expand command arguments. It is used when a command is needed, as in the then condition of an if command, but nothing is to be done by the command.","Process Name":"colon","Link":"https:\/\/linux.die.net\/man\/1\/colon"}},{"Process":{"Description":"colordiff is a wrapper for diff and produces the same output as diff but with coloured syntax highlighting at the command line to improve readability. The output is similar to how a diff-generated patch might appear in Vim or Emacs with the appropriate syntax highlighting options enabled. The colour schemes can be read from a central configuration file or from a local user ~\/.colordiffrc file. colordiff makes use of ANSI colours and as such will only work when ANSI colours can be used - typical examples are xterms and Eterms, as well as console sessions. colordiff has been tested on various flavours of Linux and under OpenBSD, but should be broadly portable to other systems.","Process Name":"colordiff","Link":"https:\/\/linux.die.net\/man\/1\/colordiff"}},{"Process":{"Description":"colorit is a script for markuping text input and sending a result to pager ( less -r by default) or stdout. Markuping rules are described in a configuration file which is filtered by preprocessor (the default is m4 ). The define HOME equal to your home directory is set for the preprocessor using -D option supported by both m4 and cpp. This script can be used for colorizing the text by ANSI escape sequences, or making a simple text conversions and may be used as a pager instead of less by dict or other programs. In particular, log files, dict, cc, make, or diff output can easily be colorized and viewed.","Process Name":"colorit","Link":"https:\/\/linux.die.net\/man\/1\/colorit"}},{"Process":{"Description":"This manual page documents briefly the colorize utility. colorize is a short (no, it's not short anymore :) perl script to colorize your logs. You can even use syslog-ng to redirect all logs to the script and colorize them on the fly!","Process Name":"colorize","Link":"https:\/\/linux.die.net\/man\/1\/colorize"}},{"Process":{"Description":"Colrm removes selected columns from a file. Input is taken from standard input. Output is sent to standard output. If called with one parameter the columns of each line will be removed starting with the specified column. If called with two parameters the columns from the first column to the last column will be removed. Column numbering starts with column 1.","Process Name":"colrm","Link":"https:\/\/linux.die.net\/man\/1\/colrm"}},{"Process":{"Description":"The column utility formats its input into multiple columns. Rows are filled before columns. Input is taken from file operands, or, by default, from the standard input. Empty lines are ignored. The options are as follows:       -c'        Output is formatted for a display columns wide. -s' Specify a set of characters to be used to delimit columns for the -t option. -t' Determine the number of columns the input contains and create a table. Columns are delimited with whitespace, by default, or with the characters supplied using the -s option. Useful for pretty-printing displays. -x' Fill columns before filling rows. Column exits 0 on success, >0 if an error occurred.","Process Name":"column","Link":"https:\/\/linux.die.net\/man\/1\/column"}},{"Process":{"Description":"USAGE combig.pl [OPTIONS] BIGRAM INPUT PARAMETERS \u2022 BIGRAM Specify a file of bigram counts created by NSP programs count.pl. The entries in BIGRAM will be formatted as follows: word1<>word2<>n11 n1p np1 Here, word1 is followed by word2 n11 times. word1 occurs as the 1st word in total n1p bigrams and word2 occurs as the 2nd word in np1 bigrams. \u2022 OPTIONS --help Displays this message. --version Displays the version information. OUTPUT combig.pl produces a count of the number of times two words make up a bigram in either order, whereas count.pl produces counts for a single fixed ordering. In other words, combig.pl combines the counts of bigrams that are composed of the same words but in reverse order. While the BIGRAM shows pairs of words forming bigrams, output of combig will show the pairs of words that are co-occurrences or that co-occur irrespective of their order. e.g. if bigrams         word1<>word2<>n11 n1p np1\nand\n        word2<>word1<>m11 m1p mp1 are found in BIGRAM file, then combig.pl treats these as a single unordered bigram word1<>word2<>n11+m11 n1p+mp1 np1+m1p where the new bigram will show a combined contingency table in which the order of words doesn't matter.                         word2           ~word2\n        ___________________________________________________________\n         word1  |       n11+m11         n12+m21     | n1p+mp1\n                |                                   |\n        ~word1  |       n21+m12         n22+m22-n   | n2p+mp2-n\n                ___________________________________________________\n                        np1+m1p         np2+m2p-n   |  n\nhere the entry \u2022 (word1,word2)=n11+m11 shows the number of bigrams having both word1 and word2 in either order i.e. word1<>word2 + word2<>word1 \u2022 (word1,~word2)=n12+m21 shows the number of bigrams having word1 but not word2 at either position i.e. word1<>~word2 + ~word2<>word1 \u2022 (~word1,word2)=n21+m12 shows the number of bigrams having word2 but not word1 at either position i.e. ~word1<>word2 + word2<>~word1 \u2022 (~word1,~word2)=n22+m22 shows the number of bigrams not having word1 nor word2 at either position i.e. ~word1<>~word2 + ~word2<>~word1 - n where n=total number of bigrams The mathematical proof of how the cell counts in the above contingency table are counted is explained in section Proof. When a bigram appears in only one order i.e. word1<>word2<>n11 n1p np1 appears but word2<>word1<>m11 m1p mp1 does not, then the combined bigram will be same as the original bigram that appears. Or in other words, word1<>word2<>n11 n1p np1 is displayed as it is. PROOF OF CORRECTNESS A bigram word1<>word2<>n11 n1p np1 represents a contingency table           word2         ~word2\n        --------------------------------------\nword1   n11     |       n12     |       n1p\n                |               |\n~word1  n21     |       n22     |       n2p\n        --------------------------------------\n        np1     |       np2     |       n while a bigram word2<>word1<>m11 m1p mp1 represents a contingency table           word1         ~word1\n        --------------------------------------\nword2   m11     |       m12     |       m1p\n                |               |\n~word2  m21     |       m22     |       m2p\n        --------------------------------------\n        mp1     |       mp2     |       n Here, n11+n12+n21+n22 = n Also, m11+m12+m21+m22 = n combig.pl combines bigram counts into a single order independant word pair word1<>word2<>n11+m11 n12+m21 n21+m12 And the corresponding contingency table will be shown as                 word2           ~word2\n        -----------------------------------------\nword1   n11+m11   |     n12+m21   |     n1p+mp1\n                  |               |\n~word1  n21+m12   |     n22+m22-n |     n2p+mp2\n        -----------------------------------------\n        np1+m1p   |     np2+m2p   |     n The first cell (n11+m11) shows the #bigrams having word1 and word2 (irrespective of their positions) i.e. word1<>word2 or word2<>word1 which is n11+m11. The second cell (n12+m21) shows the #bigrams having word1 but not word2 at any position i.e. word1<>~word2 or ~word2<>word1 which is n12+m21. The third cell (n21+m12) shows the #bigrams having word2 but not word1 at any position i.e. ~word1<>word2 or word2<>~word1 which is n21+m12. The fourth cell (m22+n22-n) shows the #bigrams not having word1 nor word2 at any position which = n - (n11+m11) - (n12+m21) - (n21+m12)\n\n= n - (n11+n12+n21) - (m11+m12+m21)\n\n= n - (n-n22) - (n-m22)\n\n= n22 + m22 - n Alternative proof - n22 = m11 + m12 + m21 + X      (a)\n\nm22 = n11 + n12 + n21 + X      (b) where X = #bigrams not having either word1 or word2. as both n22 and m22 have some terms in common which show the bigrams not having either word1 or word2. But, m11+m12+m21 = n - m22 Substituting this in eqn (a) n22 = n - m22 + X Or X = n22 + m22 - n Or add (a) and (b) to get n22+m22 = (n11+m11) + (n12+m21) + (n21+m12) + 2X rearranging terms, n22+m22 = (n11+n12+n21) + (m11+m12+m21) + 2X but n11+n12+n21 = n - n22 and\n\nm11+m12+m21 = n - m22 Hence, n22+m22 = (n-n22) + (n-m22) + 2X\n\n2(n22+m22-n) = 2X Or (n22+m22-n) = X which is the fourth cell count. Viewing Bigrams as Graphs In bigrams, the order of words is important. Bigram word1<>word2 shows that word2 follows word1. Bigrams can be viewed as a directed graph where a bigram word1<>word2 will represent a directed edge e from initial vertex word1 to terminal vertex word2(word1->word2). In this case, n11, which is the number of times bigram word1<>word2 occurs, becomes the weight of the directed edge word1->word2. n1p, which is the number of bigrams having word1 at 1st position, becomes the out degree of vertex word1 and np1, which is the number of bigrams having word2 at 2nd position, becomes the in degree of vertex word2 combig.pl creates a new list of word pairs from these bigrams such that the order of words can be ignored. Viewed another way, it converts the directed graph of given bigrams to an undirected graph showing new word pairs. A pair say word1<>word2<>n11 n1p np1 shown in the output of combig can be viewed as an undirected edge joining word1 and word2 having weight n11. If we count the degree of vertex word1 it will be n1p and degree of vertex word2 will be np1.","Process Name":"combig.pl","Link":"https:\/\/linux.die.net\/man\/1\/combig.pl"}},{"Process":{"Description":"combine combines the lines in two files. Depending on the boolean operation specified, the contents will be combined in different ways: and Outputs lines that are in file1 if they are also present in file2. not Outputs lines that are in file1 but not in file2. or Outputs lines that are in file1 or file2. xor Outputs lines that are in either file1 or file2, but not in both files. \"-\" can be specified for either file to read stdin for that file. The input files need not be sorted, and the lines are output in the order they occur in file1 (followed by the order they occur in file2 for the two \"or\" operations). Bear in mind that this means that the operations are not commutative; \"a and b\" will not necessarily be the same as \"b and a\". To obtain commutative behavior sort and uniq the result. Note that this program can be installed as \"_\" to allow for the syntactic sugar shown in the latter half of the synopsis (similar to the test\/[ command). It is not currently installed as \"_\" by default, but you can alias it to that if you like.","Process Name":"combine","Link":"https:\/\/linux.die.net\/man\/1\/combine"}},{"Process":{"Description":"combinediff creates a unified diff that expresses the sum of two diffs. The diff files must be listed in the order that they are to be applied. For best results, the diffs must have at least three lines of context. Since combinediff doesn't have the advantage of being able to look at the files that are to be modified, it has stricter requirements on the input format than patch(1) does. The output of GNU diff will be okay, even with extensions, but if you intend to use a hand-edited patch it might be wise to clean up the offsets and counts using recountdiff(1) first. Note, however, that the two patches must be in strict incremental order. In other words, the second patch must be relative to the state of the original set of files after the first patch was applied. The diffs may be in context format. The output, however, will be in unified format.","Process Name":"combinediff","Link":"https:\/\/linux.die.net\/man\/1\/combinediff"}},{"Process":{"Description":"Comix is a user-friendly, customizable image viewer. It is specifically designed to handle comic books, but also serves as a generic viewer. It reads images in ZIP, RAR or tar archives (also gzip or bzip2 compressed) as well as plain image files.","Process Name":"comix","Link":"https:\/\/linux.die.net\/man\/1\/comix"}},{"Process":{"Description":"Compare sorted files FILE1 and FILE2 line by line. With no options, produce three-column output. Column one contains lines unique to FILE1, column two contains lines unique to FILE2, and column three contains lines common to both files. -1 suppress column 1 (lines unique to FILE1) -2 suppress column 2 (lines unique to FILE2) -3 suppress column 3 (lines that appear in both files) --check-order check that the input is correctly sorted, even if all input lines are pairable --nocheck-order do not check that the input is correctly sorted --output-delimiter= STR separate columns with STR --help display this help and exit --version output version information and exit Note, comparisons honor the rules specified by 'LC_COLLATE'.","Process Name":"comm","Link":"https:\/\/linux.die.net\/man\/1\/comm"}},{"Process":{"Description":"","Process Name":"command","Link":"https:\/\/linux.die.net\/man\/1\/command"}},{"Process":{"Description":"","Process Name":"commandline","Link":"https:\/\/linux.die.net\/man\/1\/commandline"}},{"Process":{"Description":"","Process Name":"comment_myspace","Link":"https:\/\/linux.die.net\/man\/1\/comment_myspace"}},{"Process":{"Description":"Image Settings: -authenticate value decrypt image with this password -channel type apply option to select image channels -colorspace type alternate image colorspace -compose operator set image composite operator -compress type type of pixel compression when writing the image -decipher filename convert cipher pixels to plain pixels -define format:option define one or more image format options -density geometry horizontal and vertical density of the image -depth value image depth -encipher filename convert plain pixels to cipher pixels -extract geometry extract area from image -format \"string\" output formatted image characteristics -fuzz distance colors within this distance are considered equal -identify identify the format and characteristics of the image -interlace type type of image interlacing scheme -highlight-color color emphasize pixel differences with this color -limit type value pixel cache resource limit -lowlight-color color de-emphasize pixel differences with this color -metric type measure differences between images with this metric -monitor monitor progress -profile filename add, delete, or apply an image profile -quality value JPEG\/MIFF\/PNG compression level -quiet suppress all warning messages -quantize colorspace reduce colors in this colorspace -regard-warnings pay attention to warning messages -sampling-factor geometry horizontal and vertical sampling factor -seed value seed a new sequence of pseudo-random numbers -set attribute value set an image attribute -size geometry width and height of image -transparent-color color transparent color -type type image type -verbose print detailed information about the image -virtual-pixel method virtual pixel access method Miscellaneous Options: -debug events display copious debugging information -help print program options -log format format of debugging information -list type print a list of supported option arguments -version print version information By default, the image format of 'file' is determined by its magic number. To specify a particular image format, precede the filename with an image format name and a colon (i.e. ps:image) or specify the image type as the filename suffix (i.e. image.ps). Specify 'file' as '-' for standard input or output.","Process Name":"compare","Link":"https:\/\/linux.die.net\/man\/1\/compare"}},{"Process":{"Description":"The SuSE Secure Compartment was designed to allow safe execution of priviliged and\/or untrusted executables and services. It has got all features possible included, which can be used to minimize the risk of a trojanized or vulnerable program\/service.","Process Name":"compartment","Link":"https:\/\/linux.die.net\/man\/1\/compartment"}},{"Process":{"Description":"This draws a compass, with all elements spinning about randomly, for that ''lost and nauseous'' feeling.","Process Name":"compass","Link":"https:\/\/linux.die.net\/man\/1\/compass"}},{"Process":{"Description":"this plugin force the rpm internal digest to be compatible with rpm 4.4.x","Process Name":"compat_digest.plug","Link":"https:\/\/linux.die.net\/man\/1\/compat_digest.plug"}},{"Process":{"Description":"compface is a filter for generating highly compressed representations of 48x48x1 face image files. uncompface is an inverse filter which performs an inverse transformation with no loss of data. The algorithm used is highly tuned for its purpose and achieves better than a five to one compression ratio on average. Arguments operate identically for both programmes. The first argument, if present, causes input to be taken from the named file instead of from standard input unless it is the string ''-''. A second argument, if present, causes output to go to the named file instead of to standard output unless it is the string ''-''. Subsequent pairs of arguments may be used to specify further pairs of input and output files. The input format for compface (and the output format for uncompface) is 48 lines each of 3 sixteen bit hexadecimal integers, comma terminated in C initialiser style. The output format of compface (and the input format for uncompface) is some number of lines made up of a space followed by printable characters (in the range ''!'' to ''~'' inclusive). The first line contains 72 characters and following lines contain 79 characters except that the last line may be short. This version of compface has been patched to also be able to handle normal XBM images. uncompface will produce XBM output only if the -X switch is applied. The amount of compression obtained varies between face image files but the output of compface averages less than 200 characters. The average number of output lines is three.","Process Name":"compface","Link":"https:\/\/linux.die.net\/man\/1\/compface"}},{"Process":{"Description":"","Process Name":"compgen","Link":"https:\/\/linux.die.net\/man\/1\/compgen"}},{"Process":{"Description":"compile_encoding compiles an input XML encmap file into a binary encoded file usable by XML::Parser.","Process Name":"compile_encoding","Link":"https:\/\/linux.die.net\/man\/1\/compile_encoding"}},{"Process":{"Description":"Compile_et converts a table listing error-code names and associated messages into a C source file suitable for use with the com_err(3) library. The source file name must end with a suffix of ''.et''; the file consists of a declaration supplying the name (up to four characters long) of the error-code table: error_table name followed by up to 256 entries of the form: error_code name, \" string \" and a final end to indicate the end of the table. The name of the table is used to construct the name of a subroutine initialize_XXXX_error_table which must be called in order for the com_err library to recognize the error table. The various error codes defined are assigned sequentially increasing numbers (starting with a large number computed as a hash function of the name of the table); thus for compatibility it is suggested that new codes be added only to the end of an existing table, and that no codes be removed from tables. The names defined in the table are placed into a C header file with preprocessor directives defining them as integer constants of up to 32 bits in magnitude. A C source file is also generated which should be compiled and linked with the object files which reference these error codes; it contains the text of the messages and the initialization subroutine. Both C files have names derived from that of the original source file, with the ''.et'' suffix replaced by ''.c'' and ''.h''. A ''#'' in the source file is treated as a comment character, and all remaining text to the end of the source line will be ignored.","Process Name":"compile_et","Link":"https:\/\/linux.die.net\/man\/1\/compile_et"}},{"Process":{"Description":"Used to convert grammars in Perl 6 syntax into Perl 5 modules.","Process Name":"compile_p6grammar.pl","Link":"https:\/\/linux.die.net\/man\/1\/compile_p6grammar.pl"}},{"Process":{"Description":"Experimental software. This script tries to \"compile\" a chef program into a Perl program. It includes the modules Acme::Chef::* into the generated code. It does so by searching your @INC for the module files and inserting the module code into the source code of the executable. Hence, the generated executable should be somewhat more portable. This script should run in any environment that can execute chef programs using the chef.pl interpreter except maybe on VMS and MacOS. Anything else: See Acme::Chef.","Process Name":"compilechef","Link":"https:\/\/linux.die.net\/man\/1\/compilechef"}},{"Process":{"Description":"","Process Name":"complete","Link":"https:\/\/linux.die.net\/man\/1\/complete"}},{"Process":{"Description":"","Process Name":"compopt","Link":"https:\/\/linux.die.net\/man\/1\/compopt"}},{"Process":{"Description":"Image Settings: -affine matrix affine transform matrix -authenticate value decrypt image with this password -blue-primary point chromaticity blue primary point -channel type apply option to select image channels -colorspace type alternate image colorspace -comment string annotate image with comment -compose operator composite operator -compress type type of pixel compression when writing the image -decipher filename convert cipher pixels to plain pixels -define format:option define one or more image format options -depth value image depth -density geometry horizontal and vertical density of the image -display server get image or font from this X server -dispose method layer disposal method -dither method apply error diffusion to image -encipher filename convert plain pixels to cipher pixels -encoding type text encoding type -endian type endianness (MSB or LSB) of the image -filter type use this filter when resizing an image -font name render text with this font -format \"string\" output formatted image characteristics -gravity type which direction to gravitate towards -green-primary point chromaticity green primary point -interlace type type of image interlacing scheme -interpolate method pixel color interpolation method -label string assign a label to an image -limit type value pixel cache resource limit -matte store matte channel if the image has one -monitor monitor progress -page geometry size and location of an image canvas (setting) -pointsize value font point size -quality value JPEG\/MIFF\/PNG compression level -quiet suppress all warning messages -red-primary point chromaticity red primary point -regard-warnings pay attention to warning messages -sampling-factor geometry horizontal and vertical sampling factor -scene value image scene number -seed value seed a new sequence of pseudo-random numbers -size geometry width and height of image -support factor resize support: > 1.0 is blurry, < 1.0 is sharp -transparent-color color transparent color -treedepth value color tree depth -tile repeat composite operation across and down image -units type the units of image resolution -verbose print detailed information about the image -virtual-pixel method virtual pixel access method -white-point point chromaticity white point Image Operators: -blend geometry blend images -colors value preferred number of colors in the image -displace geometry shift image pixels defined by a displacement map -dissolve value dissolve the two images a given percent -extract geometry extract area from image -geometry geometry location of the composite image -identify identify the format and characteristics of the image -monochrome transform image to black and white -negate replace every pixel with its complementary color -profile filename add ICM or IPTC information profile to image -quantize colorspace reduce colors in this colorspace -repage geometry size and location of an image canvas (operator) -rotate degrees apply Paeth rotation to the image -resize geometry resize the image -sharpen geometry sharpen the image -stegano offset hide watermark within an image -stereo combine two image to create a stereo anaglyph -strip strip image of all profiles and comments -thumbnail geometry create a thumbnail of the image -transform affine transform image -type type image type -unsharp geometry sharpen the image -watermark geometry percent brightness and saturation of a watermark -write filename write images to this file Miscellaneous Options: -debug events display copious debugging information -help print program options -log format format of debugging information -list type print a list of supported option arguments -version print version information By default, the image format of 'file' is determined by its magic number. To specify a particular image format, precede the filename with an image format name and a colon (i.e. ps:image) or specify the image type as the filename suffix (i.e. image.ps). Specify 'file' as '-' for standard input or output.","Process Name":"composite","Link":"https:\/\/linux.die.net\/man\/1\/composite"}},{"Process":{"Description":"Compress reduces the size of the named files using adaptive Lempel-Ziv coding. Whenever possible, each file is replaced by one with the extension .Z, while keeping the same ownership modes, access and modification times. If no files are specified, the standard input is compressed to the standard output. Compress will only attempt to compress regular files. In particular, it will ignore symbolic links. If a file has multiple hard links, compress will refuse to compress it unless the -f flag is given. If -f is not given and compress is run in the foreground, the user is prompted as to whether an existing file should be overwritten. Compressed files can be restored to their original form using uncompress or zcat. uncompress takes a list of files on its command line and replaces each file whose name ends with .Z and which begins with the correct magic number with an uncompressed file without the .Z. The uncompressed file will have the mode, ownership and timestamps of the compressed file. The -c option makes compress\/uncompress write to the standard output; no files are changed. zcat is identical to uncompress -c. zcat uncompresses either a list of files on the command line or its standard input and writes the uncompressed data on standard output. zcat will uncompress files that have the correct magic number whether they have a .Z suffix or not. If the -r flag is specified, compress will operate recursively. If any of the file names specified on the command line are directories, compress will descend into the directory and compress all the files it finds there. The -V flag tells each of these programs to print its version and patchlevel, along with any preprocessor flags specified during compilation, on stderr before doing any compression or uncompression. Compress uses the modified Lempel-Ziv algorithm popularized in \"A Technique for High Performance Data Compression\", Terry A. Welch, IEEE Computer, vol. 17, no. 6 (June 1984), pp. 8-19. Common substrings in the file are first replaced by 9-bit codes 257 and up. When code 512 is reached, the algorithm switches to 10-bit codes and continues to use more bits until the limit specified by the -b flag is reached (default 16). Bits must be between 9 and 16. The default can be changed in the source to allow compress to be run on a smaller machine. After the bits limit is attained, compress periodically checks the compression ratio. If it is increasing, compress continues to use the existing code dictionary. However, if the compression ratio decreases, compress discards the table of substrings and rebuilds it from scratch. This allows the algorithm to adapt to the next \"block\" of the file. Note that the -b flag is omitted for uncompress, since the bits parameter specified during compression is encoded within the output, along with a magic number to ensure that neither decompression of random data nor recompression of compressed data is attempted. The amount of compression obtained depends on the size of the input, the number of bits per code, and the distribution of common substrings. Typically, text such as source code or English is reduced by 50-60%. Compression is generally much better than that achieved by Huffman coding (as used in pack), or adaptive Huffman coding (compact), and takes less time to compute. Under the -v option, a message is printed yielding the percentage of reduction for each file compressed. Exit status is normally 0; if the last file is larger after (attempted) compression, the status is 2; if an error occurs, exit status is 1.","Process Name":"compress","Link":"https:\/\/linux.die.net\/man\/1\/compress"}},{"Process":{"Description":"compton is a compositor based on Dana Jansens' version of xcompmgr (which itself was written by Keith Packard). It includes many improvements over the original xcompmgr, including window frame opacity, inactive window transparency, and shadows on argb windows.","Process Name":"compton","Link":"https:\/\/linux.die.net\/man\/1\/compton"}},{"Process":{"Description":"The cona program corrects file names in a directory, it removes german umlauts and white spaces.","Process Name":"cona","Link":"https:\/\/linux.die.net\/man\/1\/cona"}},{"Process":{"Description":"conch is a SSHv2 client for logging into a remote machine and executing commands. It provides encrypted and secure communications across a possibly insecure network. Arbitrary TCP\/IP ports can also be forwarded over the secure connection. conch connects and logs into hostname (as user or the current username). The user must prove her\/his identity through a public-key or a password. Alternatively, if a connection is already open to a server, a new shell can be opened over the connection without having to reauthenticate. If command is specified, command is executed instead of a shell. If the -s option is given, command is treated as an SSHv2 subsystem name. Authentication Conch supports the public-key, keyboard-interactive, and password authentications. The public-key method allows the RSA or DSA algorithm to be used. The client uses his\/her private key, $HOME\/.ssh\/id_rsa or $HOME\/.ssh\/id_dsa to sign the session identifier, known only by the client and server. The server checks that the matching public key is valid for the user, and that the signature is correct. If public-key authentication fails, conch can authenticate by sending an encrypted password over the connection. Connection sharing conch has the ability to multiplex multiple shells, commands and TCP\/IP ports over the same secure connection. To disable multiplexing for a connection, use the -I flag. The -K option determines how the client connects to the remote host. It is a comma-separated list of the methods to use, in order of preference. The two connection methods are 'unix' (for connecting over a multiplexed connection) and 'direct' (to connect directly). To disable connecting over a multiplexed connection, do not include 'unix' in the preference list. As an example of how connection sharing works, to speed up CVS over SSH: conch --noshell --fork -l cvs_user cvs_host set CVS_RSH=conch Now, when CVS connects to cvs_host as cvs_user, instead of making a new connection to the server, conch will add a new channel to the existing connection. This saves the cost of repeatedly negotiating the cryptography and authentication. The options are as follows:       -A'        Enables authentication agent forwarding. -a' Disables authentication agent forwarding (default). -C' Enable compression. -c cipher_spec Selects encryption algorithms to be used for this connection, as a comma-separated list of ciphers in order of preference. The list that conch supports is (in order of default preference): aes256-ctr, aes256-cbc, aes192-ctr, aes192-cbc, aes128-ctr, aes128-cbc, cast128-ctr, cast128-cbc, blowfish-ctr, blowfish, idea-ctr, idea-cbc, 3des-ctr, 3des-cbc. -e ch | ^ch | none Sets the escape character for sessions with a PTY (default: '~'). The escape character is only recognized at the beginning of a line (after a newline). The escape character followed by a dot ('.') closes the connection; followed by ^Z suspends the connection; and followed by the escape character sends the escape character once. Setting the character to ''none'' disables any escapes. -f' Fork to background after authentication. -I' Do not allow connection sharing over this connection. -i identity_spec The file from which the identity (private key) for RSA or DSA authentication is read. The defaults are $HOME\/.ssh\/id_rsa and $HOME\/.ssh\/id_dsa. It is possible to use this option more than once to use more than one private key. -K connection_spec Selects methods for connection to the server, as a comma-separated list of methods in order of preference. See Connection sharing for more information. -L port:host:hostport Specifies that the given port on the client host is to be forwarded to the given host and port on the remote side. This allocates a socket to listen to port on the local side, and when connections are made to that socket, they are forwarded over the secure channel and a connection is made to host port hostport from the remote machine. Only root can forward privieged ports. -l user Log in using this username. -m mac_spec Selects MAC (message authentication code) algorithms, as a comma-separated list in order of preference. The list that conch supports is (in order of preference): hmac-sha1, hmac-md5. -N' Do not execute a shell or command. -n' Redirect input from \/dev\/null. -o openssh_option Ignored OpenSSH options. -p port The port to connect to on the server. -R port:host:hostport Specifies that the given port on the remote host is to be forwarded to the given host and port on the local side. This allocates a socket to listen to port on the remote side, and when connections are made to that socket, they are forwarded over the secure channel and a connection is made to host port hostport from the client host. Only root can forward privieged ports. -s' Reconnect to the server if the connection is lost. -s' Invoke command (mandatory) as a SSHv2 subsystem. -T' Do not allocate a TTY. -t' Allocate a TTY even if command is given. -V' Display version number only. -v' Log to stderr. -x' Disable X11 connection forwarding (default).","Process Name":"conch","Link":"https:\/\/linux.die.net\/man\/1\/conch"}},{"Process":{"Description":"The GNU build system distinguishes three types of machines, the 'build' machine on which the compilers are run, the 'host' machine on which the package being built will run, and, exclusively when you build a compiler, assembler etc., the 'target' machine, for which the compiler being built will produce code. This script will guess the type of the 'build' machine. Output the configuration name of the system 'config.guess' is run on. Operation modes: -h, --help print this help, then exit -t, --time-stamp print date of last modification, then exit -v, --version print version number, then exit","Process Name":"config.guess","Link":"https:\/\/linux.die.net\/man\/1\/config.guess"}},{"Process":{"Description":"..-. Canonicalize a configuration name. Operation modes: -h, --help print this help, then exit -t, --time-stamp print date of last modification, then exit -v, --version print version number, then exit","Process Name":"config.sub","Link":"https:\/\/linux.die.net\/man\/1\/config.sub"}},{"Process":{"Description":"You use the config:alarms command to enable or disable alarm notification, and to resolve active alarms.","Process Name":"config_alarms","Link":"https:\/\/linux.die.net\/man\/1\/config_alarms"}},{"Process":{"Description":"Use the config:cache command to enable caching for HTTP based protocols. Also use this command to configure general caching parameters, including storage, freshness, bypass, and alternates. When you execute the config:cache command, you must use one of the options described below.","Process Name":"config_cache","Link":"https:\/\/linux.die.net\/man\/1\/config_cache"}},{"Process":{"Description":"You use the config:clock command specify system date, time, and timezone settings. A full restart of proxy software occurs when these changes take place. This command requires root privileges, see config:root command.","Process Name":"config_clock","Link":"https:\/\/linux.die.net\/man\/1\/config_clock"}},{"Process":{"Description":"The \"config_data\" tool provides a command-line interface to the configuration of Perl modules. By \"configuration\", we mean something akin to \"user preferences\" or \"local settings\". This is a formalization and abstraction of the systems that people like Andreas Koenig (\"CPAN::Config\"), Jon Swartz (\"HTML::Mason::Config\"), Andy Wardley (\"Template::Config\"), and Larry Wall (perl's own Config.pm) have developed independently. The configuration system emplyed here was developed in the context of \"Module::Build\". Under this system, configuration information for a module \"Foo\", for example, is stored in a module called \"Foo::ConfigData\") (I would have called it \"Foo::Config\", but that was taken by all those other systems mentioned in the previous paragraph...). These \"...::ConfigData\" modules contain the configuration data, as well as publically accessible methods for querying and setting (yes, actually re-writing) the configuration data. The \"config_data\" script (whose docs you are currently reading) is merely a front-end for those methods. If you wish, you may create alternate front-ends. The two types of data that may be stored are called \"config\" values and \"feature\" values. A \"config\" value may be any perl scalar, including references to complex data structures. It must, however, be serializable using \"Data::Dumper\". A \"feature\" is a boolean (1 or 0) value.","Process Name":"config_data","Link":"https:\/\/linux.die.net\/man\/1\/config_data"}},{"Process":{"Description":"Use the config:dns command to configure DNS proxying and DNS lookups. When you execute the config:dns command, you must use one of the options described below.","Process Name":"config_dns","Link":"https:\/\/linux.die.net\/man\/1\/config_dns"}},{"Process":{"Description":"The config:get command enables you to display a variable and its value from the records.config file.","Process Name":"config_get","Link":"https:\/\/linux.die.net\/man\/1\/config_get"}},{"Process":{"Description":"The config:hard-restart command restarts your management, monitoring, and proxy software. The config:hard-restart command takes no options.","Process Name":"config_hard-restart","Link":"https:\/\/linux.die.net\/man\/1\/config_hard-restart"}},{"Process":{"Description":"Use the config:hostdb command to configure the host database. You configure such parameters as foreground, background, and lookup timeouts. When you execute the config:hostdb command, you must use one of the options described below.","Process Name":"config_hostdb","Link":"https:\/\/linux.die.net\/man\/1\/config_hostdb"}},{"Process":{"Description":"The config:http command enables you to configure HTTP caching through a variety of options. You must use one of the options described below.","Process Name":"config_http","Link":"https:\/\/linux.die.net\/man\/1\/config_http"}},{"Process":{"Description":"The config:icp command enables you to configure ICP caching through a variety of options. Use the config:icp command with options to configure mode, port numbers, timeouts, and to enable ICP multicast. You must use one of the options described below.","Process Name":"config_icp","Link":"https:\/\/linux.die.net\/man\/1\/config_icp"}},{"Process":{"Description":"Use the config:logging command to enable and configure logging. Use the config:logging command to specify format, type, storage, splitting, rolling, and other parameters. When you execute the config:logging command, you must use one of the options described below.","Process Name":"config_logging","Link":"https:\/\/linux.die.net\/man\/1\/config_logging"}},{"Process":{"Description":"The config:name command enables you to set the name of your proxy system.","Process Name":"config_name","Link":"https:\/\/linux.die.net\/man\/1\/config_name"}},{"Process":{"Description":"The config:network command enables you to configure network settings such as hostname, defaultrouter, search-domain, dns, and interface settings. You must use one of the options described below.","Process Name":"config_network","Link":"https:\/\/linux.die.net\/man\/1\/config_network"}},{"Process":{"Description":"You use the config:parent command to enable HTTP parent caching, specify a parent cache, and indicate where the proxy should locate the parent.config file, which contains specific parent cache configuration information.","Process Name":"config_parent","Link":"https:\/\/linux.die.net\/man\/1\/config_parent"}},{"Process":{"Description":"Use the config:port-tunnels command to specify a port for the proxy to use for tunneling.","Process Name":"config_port-tunnels","Link":"https:\/\/linux.die.net\/man\/1\/config_port-tunnels"}},{"Process":{"Description":"You use the config:remap command to specify from which location (URL) the proxy should retrieve and install your remap.config file.","Process Name":"config_remap","Link":"https:\/\/linux.die.net\/man\/1\/config_remap"}},{"Process":{"Description":"The config:reset-stats command resets all statistics. (See the \"See Also\" section below for a list of commands that display statistics.) The config:reset- stats command takes no options.","Process Name":"config_reset-stats","Link":"https:\/\/linux.die.net\/man\/1\/config_reset-stats"}},{"Process":{"Description":"The config:restart command restarts your proxy software. When you execute the config:restart command, you may use one of the options described below. cluster Specifies cluster-wide restart.","Process Name":"config_restart","Link":"https:\/\/linux.die.net\/man\/1\/config_restart"}},{"Process":{"Description":"If you want to become the root user type the config:root command. You will be prompted to enter the root password. The config:root command takes no options.","Process Name":"config_root","Link":"https:\/\/linux.die.net\/man\/1\/config_root"}},{"Process":{"Description":"When you enable scheduled update, the proxy can automatically update certain objects in the local cache at a specified time. When you execute the config:scheduled-update command, you must use one of the options described below.","Process Name":"config_scheduled-update","Link":"https:\/\/linux.die.net\/man\/1\/config_scheduled-update"}},{"Process":{"Description":"The config:security command enables you to indicate from what URL Traffic Server should retrieve the configuration file (IP_allow) that controls client access to the Traffic Server proxy cache. The command also enables you to indicate from what URL Traffic Server should retrieve the configuration file (mgmt_allow) that controls remote host access to the Traffic Manager UI. Finally, the config:security command enables you to indicate from what URL Traffic Server should retrieve the configuration file (admin) (password) that controls administrator access to Traffic Manager activities.","Process Name":"config_security","Link":"https:\/\/linux.die.net\/man\/1\/config_security"}},{"Process":{"Description":"The config:set command enables you to set the value of a variable from the records.config file.","Process Name":"config_set","Link":"https:\/\/linux.die.net\/man\/1\/config_set"}},{"Process":{"Description":"You use the config:socks command to enable the SOCKS option and configure SOCKS parameters, including default servers, version, and port values.","Process Name":"config_socks","Link":"https:\/\/linux.die.net\/man\/1\/config_socks"}},{"Process":{"Description":"The config:ssl command enables you to configure SSL protocol settings through a variety of options. You must use one of the options described below.","Process Name":"config_ssl","Link":"https:\/\/linux.die.net\/man\/1\/config_ssl"}},{"Process":{"Description":"The config:start command starts your proxy software. The config:start command takes no options.","Process Name":"config_start","Link":"https:\/\/linux.die.net\/man\/1\/config_start"}},{"Process":{"Description":"","Process Name":"config_status_demo.pl","Link":"https:\/\/linux.die.net\/man\/1\/config_status_demo.pl"}},{"Process":{"Description":"The config:stop command stops your proxy software. The config:stop command takes no options.","Process Name":"config_stop","Link":"https:\/\/linux.die.net\/man\/1\/config_stop"}},{"Process":{"Description":"You use the config:upgrade command to perform over-the-wire software upgrades. This command supports three types of upgrades: silent, interactive, and reinstall. When you use this command, you must specify an upgrade server URL, and optionally specify a username and password. This command requires root privileges. Use the config:root command to switch to root user.","Process Name":"config_upgrade","Link":"https:\/\/linux.die.net\/man\/1\/config_upgrade"}},{"Process":{"Description":"You use the config:virtual-ip command to enable and configure virtual IP addressing, including IP address, device, and subinterface.","Process Name":"config_virtual-ip","Link":"https:\/\/linux.die.net\/man\/1\/config_virtual-ip"}},{"Process":{"Description":"Image Settings: -monitor monitor progress -quiet suppress all warning messages -regard-warnings pay attention to warning messages -seed value seed a new sequence of pseudo-random numbers -verbose print detailed information about the image Miscellaneous Options: -debug events display copious debugging information -help print program options -log format format of debugging information -list type print a list of supported option arguments -version print version information In additiion, define any key value pairs required by your script. For example, conjure -size 100x100 -color blue -foo bar script.msl","Process Name":"conjure","Link":"https:\/\/linux.die.net\/man\/1\/conjure"}},{"Process":{"Description":"Conky is a system monitor for X originally based on torsmo. Since its inception, Conky has changed significantly from its predecessor, while maintaining simplicity and configurability. Conky can display just about anything, either on your root desktop or in its own window. Not only does Conky have many built-in objects, it can also display just about any piece of information by using scripts and other external programs. Conky has more than 250 built in objects, including support for a plethora of OS stats (uname, uptime, CPU usage, mem usage, disk usage, \"top\" like process stats, and network monitoring, just to name a few), built in IMAP and POP3 support, built in support for many popular music players (MPD, XMMS2, BMPx, Audacious), and much much more. Conky can display this info either as text, or using simple progress bars and graph widgets, with different fonts and colours. We are always looking for help, whether its reporting bugs, writing patches, or writing docs. Please use the facilities at SourceForge to make bug reports, feature requests, and submit patches, or stop by #conky on irc.freenode.net if you have questions or want to contribute. Thanks for your interest in Conky.","Process Name":"conky","Link":"https:\/\/linux.die.net\/man\/1\/conky"}},{"Process":{"Description":"conman is a program for connecting to remote consoles being managed by conmand. Console names can be separated by spaces and\/or commas. Globbing is used by default to match console names against the configuration, but regular expression matching can be enabled with the ' -r' option. conman supports three modes of console access: monitor (read-only), interactive (read-write), and broadcast (write-only). If neither the '-m' (monitor) nor '-b' (broadcast) options are specified, the console session is opened in interactive mode.","Process Name":"conman","Link":"https:\/\/linux.die.net\/man\/1\/conman"}},{"Process":{"Description":"Console is used to manipulate console terminals remotely or to poll running conserver(8) daemons for status information. In the first form above, console asks the user's password before granting interactive access to a console (on a non-trusted system), since such a session may provide single-user access. If the server's autocompletion feature is enabled, only as much of the console name as is required to identify it uniquely to the server is required. For non-interactive options, console outputs only the requested information and exits. Console knows only of a primary conserver host (see the -M option below), to which it initially connects. In a multi-server environment, the primary server may refer the client to a different server handling the requested console, or it will provide a list of all servers if required (as when console is invoked with the -r option). Console then opens connections to the appropriate server(s). It is not necessary for the user of console to know which server manages which consoles, as long as console knows a valid primary server and all available consoles are listed in the primary server's configuration file.","Process Name":"console","Link":"https:\/\/linux.die.net\/man\/1\/console"}},{"Process":{"Description":"consoletype prints the type of console connected to standard input, and checks whether the console connected to standard input is the current foreground virtual console. With no arguments, it prints vt if console is a virtual terminal (\/dev\/tty* or \/dev\/console device if not on a serial console), serial if standard input is a serial console (\/dev\/console or \/dev\/ttyS*) and pty if standard input is a pseudo terminal.","Process Name":"consoletype","Link":"https:\/\/linux.die.net\/man\/1\/consoletype"}},{"Process":{"Description":"Conspy allows the user to take control of a Linux virtual console. The user can see what is displayed on the console and their keystrokes are sent to it. To exit from conspy press the escape key three times in quick succession.","Process Name":"conspy","Link":"https:\/\/linux.die.net\/man\/1\/conspy"}},{"Process":{"Description":"Run the pdfTeX typesetter on file, usually creating file.pdf. If the file argument has no extension, \".tex\" will be appended to it. Instead of a filename, a set of pdfTeX commands can be given, the first of which must start with a backslash. With a &format argument pdfTeX uses a different set of precompiled commands, contained in format.fmt; it is usually better to use the -fmt format option instead. pdfTeX is a version of TeX, with the e-TeX extensions, that can create PDF files as well as DVI files. In DVI mode, pdfTeX can be used as a complete replacement for the TeX engine. The typical use of pdfTeX is with a pregenerated formats for which PDF output has been enabled. The pdftex command uses the equivalent of the plain TeX format, and the pdflatex command uses the equivalent of the LaTeX format. To generate formats, use the -ini switch. The pdfinitex and pdfvirtex commands are pdfTeX's analogues to the initex and virtex commands. In this installation, if the links exist, they are symbolic links to the pdftex executable. In PDF mode, pdfTeX can natively handle the PDF, JPG, JBIG2, and PNG graphics formats. pdfTeX cannot include PostScript or Encapsulated PostScript (EPS) graphics files; first convert them to PDF using epstopdf(1). pdfTeX's handling of its command-line arguments is similar to that of of the other TeX programs in the web2c implementation.","Process Name":"cont-de","Link":"https:\/\/linux.die.net\/man\/1\/cont-de"}},{"Process":{"Description":"Run the pdfTeX typesetter on file, usually creating file.pdf. If the file argument has no extension, \".tex\" will be appended to it. Instead of a filename, a set of pdfTeX commands can be given, the first of which must start with a backslash. With a &format argument pdfTeX uses a different set of precompiled commands, contained in format.fmt; it is usually better to use the -fmt format option instead. pdfTeX is a version of TeX, with the e-TeX extensions, that can create PDF files as well as DVI files. In DVI mode, pdfTeX can be used as a complete replacement for the TeX engine. The typical use of pdfTeX is with a pregenerated formats for which PDF output has been enabled. The pdftex command uses the equivalent of the plain TeX format, and the pdflatex command uses the equivalent of the LaTeX format. To generate formats, use the -ini switch. The pdfinitex and pdfvirtex commands are pdfTeX's analogues to the initex and virtex commands. In this installation, if the links exist, they are symbolic links to the pdftex executable. In PDF mode, pdfTeX can natively handle the PDF, JPG, JBIG2, and PNG graphics formats. pdfTeX cannot include PostScript or Encapsulated PostScript (EPS) graphics files; first convert them to PDF using epstopdf(1). pdfTeX's handling of its command-line arguments is similar to that of of the other TeX programs in the web2c implementation.","Process Name":"cont-en","Link":"https:\/\/linux.die.net\/man\/1\/cont-en"}},{"Process":{"Description":"Run the pdfTeX typesetter on file, usually creating file.pdf. If the file argument has no extension, \".tex\" will be appended to it. Instead of a filename, a set of pdfTeX commands can be given, the first of which must start with a backslash. With a &format argument pdfTeX uses a different set of precompiled commands, contained in format.fmt; it is usually better to use the -fmt format option instead. pdfTeX is a version of TeX, with the e-TeX extensions, that can create PDF files as well as DVI files. In DVI mode, pdfTeX can be used as a complete replacement for the TeX engine. The typical use of pdfTeX is with a pregenerated formats for which PDF output has been enabled. The pdftex command uses the equivalent of the plain TeX format, and the pdflatex command uses the equivalent of the LaTeX format. To generate formats, use the -ini switch. The pdfinitex and pdfvirtex commands are pdfTeX's analogues to the initex and virtex commands. In this installation, if the links exist, they are symbolic links to the pdftex executable. In PDF mode, pdfTeX can natively handle the PDF, JPG, JBIG2, and PNG graphics formats. pdfTeX cannot include PostScript or Encapsulated PostScript (EPS) graphics files; first convert them to PDF using epstopdf(1). pdfTeX's handling of its command-line arguments is similar to that of of the other TeX programs in the web2c implementation.","Process Name":"cont-nl","Link":"https:\/\/linux.die.net\/man\/1\/cont-nl"}},{"Process":{"Description":"contacts is a small, lightweight addressbook that features advanced vCard field type handling and is designed for use on hand-held devices, such as the Nokia 770 or the Sharp Zaurus series of PDAs. Usage: contacts [ OPTION...] Help Options: -?, --help Show help options --help-all Show all help options --help-gtk Show GTK+ Options Application Options: -p, --plug Socket ID of an XEmbed socket to plug into --display= DISPLAY X display to use","Process Name":"contacts","Link":"https:\/\/linux.die.net\/man\/1\/contacts"}},{"Process":{"Description":"","Process Name":"contains","Link":"https:\/\/linux.die.net\/man\/1\/contains"}},{"Process":{"Description":"","Process Name":"continue","Link":"https:\/\/linux.die.net\/man\/1\/continue"}},{"Process":{"Description":"control_rancid is a sh(1) script to parse a group's router.db(5), run rancid for each of the devices, possibly re-run rancid for devices that failed collection, e-mail diffs, and e-mail error reports. The command-line options are as follows: -V Prints package name and version strings. -m mail_rcpt Specify the recipient of diff mail, which is normally rancid-<group>. The argument may be a single address, multiple comma separated addresses, or -m may be specified multiple times. -r device_name Specify the name, as it appears in the router.db, of a particular device to collect and generate diffs for. The device must be marked \"up\". The -r option alters the subject line of the diff mail. It will begin with <group name>\/<device name> rather than just the group name alone. control_rancid is normally (and best) run via rancid-run(1) which provides a locking mechanism on a group basis and saves output in a log file for each group.","Process Name":"control_rancid","Link":"https:\/\/linux.die.net\/man\/1\/control_rancid"}},{"Process":{"Description":"convcal is part of the grace software package, an application for two-dimensional data visualization. convcal converts dates from and to various formats. The following date formats are supported (hour, minutes and seconds are always optional): iso 1999-12-31T23:59:59.999 european 31\/12\/1999 23:59:59.999 or 31\/12\/99 23:59:59.999 us 12\/31\/1999 23:59:59.999 or 12\/31\/99 23:59:59.999 days 123456.789 seconds 123456.789","Process Name":"convcal","Link":"https:\/\/linux.die.net\/man\/1\/convcal"}},{"Process":{"Description":"convdate translates the date\/time strings given on the command line, outputting the results one to a line. The input can either be a date in some format that parsedate(3) can parse or the number of seconds since epoch (if -c is given). The output is either ctime(3) results, the number of seconds since epoch, or a Usenet Date: header, depending on the options given.","Process Name":"convdate","Link":"https:\/\/linux.die.net\/man\/1\/convdate"}},{"Process":{"Description":"Image Settings: -adjoin join images into a single multi-image file -affine matrix affine transform matrix -antialias remove pixel-aliasing -authenticate value decrypt image with this password -background color background color -bias value add bias when convolving an image -black-point-compensation use black point compensation -blue-primary point chromaticity blue primary point -bordercolor color border color -caption string assign a caption to an image -cdl filename color correct with a color decision list -channel type apply option to select image channels -colors value preferred number of colors in the image -colorspace type alternate image colorspace -comment string annotate image with comment -compose operator set image composite operator -compress type type of pixel compression when writing the image -decipher filename convert cipher pixels to plain pixels -define format:option define one or more image format options -delay value display the next image after pausing -density geometry horizontal and vertical density of the image -depth value image depth -display server get image or font from this X server -dispose method layer disposal method -dither method apply error diffusion to image -encipher filename convert plain pixels to cipher pixels -encoding type text encoding type -endian type endianness (MSB or LSB) of the image -family name render text with this font family -fill color color to use when filling a graphic primitive -filter type use this filter when resizing an image -flatten flatten a sequence of images -font name render text with this font -format \"string\" output formatted image characteristics -fuzz distance colors within this distance are considered equal -gravity type horizontal and vertical text placement -green-primary point chromaticity green primary point -intent type type of rendering intent when managing the image color -interlace type type of image interlacing scheme -interpolate method pixel color interpolation method -label string assign a label to an image -limit type value pixel cache resource limit -loop iterations add Netscape loop extension to your GIF animation -mask filename associate a mask with the image -matte store matte channel if the image has one -mattecolor color frame color -monitor monitor progress -orient type image orientation -origin geometry image origin -page geometry size and location of an image canvas (setting) -ping efficiently determine image attributes -pointsize value font point size -preview type image preview type -quality value JPEG\/MIFF\/PNG compression level -quiet suppress all warning messages -red-primary point chromaticity red primary point -regard-warnings pay attention to warning messages -sampling-factor geometry horizontal and vertical sampling factor -scene value image scene number -seed value seed a new sequence of pseudo-random numbers -size geometry width and height of image -stretch type render text with this font stretch -stroke color graphic primitive stroke color -strokewidth value graphic primitive stroke width -style type render text with this font style -support factor resize support: > 1.0 is blurry, < 1.0 is sharp -texture filename name of texture to tile onto the image background -tile-offset geometry tile offset -treedepth value color tree depth -transparent-color color transparent color -undercolor color annotation bounding box color -units type the units of image resolution -verbose print detailed information about the image -view FlashPix viewing transforms -virtual-pixel method virtual pixel access method -weight type render text with this font weight -white-point point chromaticity white point Image Operators: -adaptive-blur geometry adaptively blur pixels; decrease effect near edges -adaptive-resize geometry adaptively resize image with data dependent triangulation -adaptive-sharpen geometry adaptively sharpen pixels; increase effect near edges -annotate geometry text annotate the image with text -auto-orient automatically orient image -black-threshold value force all pixels below the threshold into black -blur geometry reduce image noise and reduce detail levels -border geometry surround image with a border of color -charcoal radius simulate a charcoal drawing -chop geometry remove pixels from the image interior -clip clip along the first path from the 8BIM profile -clip-mask filename associate a clip mask with the image -clip-path id clip along a named path from the 8BIM profile -colorize value colorize the image with the fill color -contrast enhance or reduce the image contrast -contrast-stretch geometry improve contrast by 'stretching' the intensity range -convolve coefficients apply a convolution kernel to the image -cycle amount cycle the image colormap -despeckle reduce the speckles within an image -draw string annotate the image with a graphic primitive -edge radius apply a filter to detect edges in the image -emboss radius emboss an image -enhance apply a digital filter to enhance a noisy image -equalize perform histogram equalization to an image -evaluate operator value evaluate an arithmetic, relational, or logical expression -extent geometry set the image size -extract geometry extract area from image -fft implements the discrete Fourier transform (DFT) -flip flip image vertically -floodfill geometry color floodfill the image with color -flop flop image horizontally -frame geometry surround image with an ornamental border -function name apply a function to the image -gamma value level of gamma correction -gaussian-blur geometry reduce image noise and reduce detail levels -geometry geometry perferred size or location of the image -identify identify the format and characteristics of the image -ift implements the inverse discrete Fourier transform (DFT) -implode amount implode image pixels about the center -lat geometry local adaptive thresholding -layers method optimize or compare image layers -level value adjust the level of image contrast -linear-stretch geometry improve contrast by 'stretching with saturation' the intensity range -median radius apply a median filter to the image -modulate value vary the brightness, saturation, and hue -monochrome transform image to black and white -motion-blur geometry simulate motion blur -negate replace every pixel with its complementary color -noise radius add or reduce noise in an image -normalize transform image to span the full range of colors -opaque color change this color to the fill color -ordered-dither NxN add a noise pattern to the image with specific amplitudes -paint radius simulate an oil painting -polaroid angle simulate a Polaroid picture -posterize levels reduce the image to a limited number of color levels -print string interpret string and print to console -profile filename add, delete, or apply an image profile -quantize colorspace reduce colors in this colorspace -radial-blur angle radial blur the image -raise value lighten\/darken image edges to create a 3-D effect -random-threshold low,high random threshold the image -recolor matrix translate, scale, shear, or rotate image colors -region geometry apply options to a portion of the image -render render vector graphics -repage geometry size and location of an image canvas -resample geometry change the resolution of an image -resize geometry resize the image -roll geometry roll an image vertically or horizontally -rotate degrees apply Paeth rotation to the image -sample geometry scale image with pixel sampling -scale geometry scale the image -segment values segment an image -selective-blur geometry selectively blur pixels within a contrast threshold -sepia-tone threshold simulate a sepia-toned photo -set property value set an image property -shade degrees shade the image using a distant light source -shadow geometry simulate an image shadow -sharpen geometry sharpen the image -shave geometry shave pixels from the image edges -shear geometry slide one edge of the image along the X or Y axis -sigmoidal-contrast geometry lightness rescaling using sigmoidal contrast enhancement -sketch geometry simulate a pencil sketch -solarize threshold negate all pixels above the threshold level -splice geometry splice the background color into the image -spread amount displace image pixels by a random amount -strip strip image of all profiles and comments -swirl degrees swirl image pixels about the center -threshold value threshold the image -thumbnail geometry create a thumbnail of the image -tile filename tile image when filling a graphic primitive -tint value tint the image with the fill color -transform affine transform image -transparent color make this color transparent within the image -transpose flip image vertically and rotate 90 degrees -transverse flop image horizontally and rotate 270 degrees -trim trim image edges -type type image type -unique-colors discard all but one of any pixel color -unsharp geometry sharpen the image -vignette geometry soften the edges of the image in vignette style -wave geometry alter an image along a sine wave -white-threshold value force all pixels above the threshold into white Image Sequence Operators: -affinity filename transform image colors to match this set of colors -append append an image sequence -average average an image sequence -clut apply a color lookup table to the image -coalesce merge a sequence of images -combine combine a sequence of images -composite composite image -crop geometry cut out a rectangular region of the image -deconstruct break down an image sequence into constituent parts -flatten flatten a sequence of images -fx expression apply mathematical expression to an image channel(s) -hald-clut apply a Hald color lookup table to the image -morph value morph an image sequence -mosaic create a mosaic from an image sequence -process arguments process the image with a custom image filter -separate separate an image channel into a grayscale image -write filename write images to this file Image Stack Operators: -clone index clone an image -delete index delete the image from the image sequence -insert index insert last image into the image sequence -swap indexes swap two images in the image sequence Miscellaneous Options: -debug events display copious debugging information -help print program options -log format format of debugging information -list type print a list of supported option arguments -version print version information By default, the image format of 'file' is determined by its magic number. To specify a particular image format, precede the filename with an image format name and a colon (i.e. ps:image) or specify the image type as the filename suffix (i.e. image.ps). Specify 'file' as '-' for standard input or output.","Process Name":"convert","Link":"https:\/\/linux.die.net\/man\/1\/convert"}},{"Process":{"Description":"convertar operates on input and output files of different Trust Anchor Repository ( TAR ) formats. convertar decides what type of file format is being referred to by a \"type:filename\" specification. Options can be passed to the convertar module by using a '\/' delimited specifier in the type identifier. Such as \"type\/option=value\/other=othervalue:filename\". See below for a list of different input and output formats that convertar understands by default and what options they take. See the Net::DNS::SEC::Tools::TrustAnchor module and its documentation for writing new plugins to allow convertar to understand other TAR formats.","Process Name":"convertar","Link":"https:\/\/linux.die.net\/man\/1\/convertar"}},{"Process":{"Description":"convertior accepts a stringified IOR and a new hostname, then outputs a new IOR for the combination.","Process Name":"convertior","Link":"https:\/\/linux.die.net\/man\/1\/convertior"}},{"Process":{"Description":"convert the standard format binary fontfile name fontfile to the codepage format required by restorefont(1). The converted font is written to vgafontfile. A binary font file of any number of characters up to 256 can be used, although at least defining the first 128 characters is a good idea. The fontheight must be in the range 1 - 32. The fontfile file consist of fontheight bytes stored sequentially (top to bottom) for each character in your font, starting with the character with code 0. The format of vgafontfile is that required by restorefont(1) and described there. This utility is part of svgalib and can be found in the utils\/ subdirectory of the original svgalib distribution. However, it is not installed by default, s.t. it is unclear where you can find it if your svgalib was install linux distribution. In case of any such problem, simply get an svgalib distribution from the net. You don't need to install it. Just make in the utils\/ subdirecty. As of this writing, svgalib-1.2.12.tar.gz is the latest version and can be retrieved by ftp from sunsite.unc.edu at \/pub\/Linux\/libs\/graphics and tsx-11.mit.edu at \/pub\/linux\/sources\/libs which will most probably be mirrored by a site close to you.","Process Name":"convfont","Link":"https:\/\/linux.die.net\/man\/1\/convfont"}},{"Process":{"Description":"convmv is meant to help convert a single filename, a directory tree and the contained files or a whole filesystem into a different encoding. It just converts the filenames, not the content of the files. A special feature of convmv is that it also takes care of symlinks, also converts the symlink target pointer in case the symlink target is being converted, too. All this comes in very handy when one wants to switch over from old 8-bit locales to UTF-8 locales. It is also possible to convert directories to UTF-8 which are already partly UTF-8 encoded. convmv is able to detect if certain files are UTF-8 encoded and will skip them by default. To turn this smartness off use the \"--nosmart\" switch. Filesystem issues Almost all POSIX filesystems do not care about how filenames are encoded, here are some exceptions: HFS+ on OS X \/ Darwin Linux and (most?) other Unix-like operating systems use the so called normalization form C ( NFC ) for its UTF-8 encoding by default but do not enforce this. Darwin, the base of the Macintosh OS enforces normalization form D ( NFD ), where a few characters are encoded in a different way. On OS X it's not possible to create NFC UTF-8 filenames because this is prevented at filesystem layer. On HFS+ filenames are internally stored in UTF-16 and when converted back to UTF-8 , for the underlying BSD system to be handable, NFD is created. See http:\/\/developer.apple.com\/qa\/qa2001\/qa1173.html for defails. I think it was a very bad idea and breaks many things under OS X which expect a normal POSIX conforming system. Anywhere else convmv is able to convert files from NFC to NFD or vice versa which makes interoperability with such systems a lot easier. JFS If people mount JFS partitions with iocharset=utf8, there is a similar problem, because JFS is designed to store filenames internally in UTF-16 , too; that is because Linux' JFS is really JFS2 , which was a rewrite of JFS for OS\/2 . JFS partitions should always be mounted with iocharset=iso8859-1, which is also the default with recent 2.6.6 kernels. If this is not done, JFS does not behave like a POSIX filesystem and it might happen that certain files cannot be created at all, for example filenames in ISO-8859-1 encoding. Only when interoperation with OS\/2 is needed iocharset should be set according to your used locale charmap. NFS4 Despite other POSIX filesystems RFC3530 ( NFS 4) mandates UTF-8 but also says: \"The nfs4_cs_prep profile does not specify a normalization form. A later revision of this specification may specify a particular normalization form.\" In other words, if you want to use NFS4 you might find the conversion and normalization features of convmv quite useful. FAT\/VFAT and NTFS NTFS and VFAT (for long filenames) use UTF-16 internally to store filenames. You should not need to convert filenames if you mount one of those filesystems. Use appropriate mount options instead! How to undo double UTF-8 (or other) encoded filenames Sometimes it might happen that you \"double-encoded\" certain filenames, for example the file names already were UTF-8 encoded and you accidently did another conversion from some charset to UTF-8 . You can simply undo that by converting that the other way round. The from-charset has to be UTF-8 and the to-charset has to be the from-charset you previously accidently used. If you use the \"--fixdouble\" option convmv will make sure that only files will be processed that will still be UTF-8 encoded after conversion and it will leave non-UTF-8 files untouched. You should check to get the correct results by doing the conversion without \"--notest\" before, also the \"--qfrom\" option might be helpful, because the double utf-8 file names might screw up your terminal if they are being printed - they often contain control sequences which do funny things with your terminal window. If you are not sure about the charset which was accidently converted from, using \"--qfrom\" is a good way to fiddle out the required encoding without destroying the file names finally. How to repair Samba files When in the smb.conf (of Samba 2.x) there hasn't been set a correct \"character set\" variable, files which are created from Win* clients are being created in the client's codepage, e.g. cp850 for western european languages. As a result of that the files which contain non-ASCII characters are screwed up if you \"ls\" them on the Unix server. If you change the \"character set\" variable afterwards to iso8859-1, newly created files are okay, but the old files are still screwed up in the Windows encoding. In this case convmv can also be used to convert the old Samba-shared files from cp850 to iso8859-1. By the way: Samba 3.x finally maps to UTF-8 filenames by default, so also when you migrate from Samba 2 to Samba 3 you might have to convert your file names. Netatalk interoperability issues When Netatalk is being switched to UTF-8 which is supported in version 2 then it is NOT sufficient to rename the file names. There needs to be done more. See http:\/\/netatalk.sourceforge.net\/2.0\/htmldocs\/upgrade.html#volumes-and-filenames and the uniconv utility of Netatalk for details.","Process Name":"convmv","Link":"https:\/\/linux.die.net\/man\/1\/convmv"}},{"Process":{"Description":"cooc creates a 256 by 256 one channel co-occurrence matrix of the box determined by the parameters (xp, yp; xs, ys) within the image file. The matrix is written onto the Vasari image file matrix. The displacement vector is determined by (dx, dy). The user must ensure that there is enough border pixels around the box within im dictated by the displacement vector (dx,dy) or else the program fails. All entries of the co-occurrence matrix are double normalised to the number of pairs involved. This function is a direct implementation of the paper: Haralick R. M., Shanmugan K. and Dinstein I., 'Textural features for image classification', IEEE Transactions on Systems, Man, and Cybernetics, Vol. SMC-3, No 6, Nov. 1973, pp 610-621. If flag sym is 1, the created co-occurrence matrix is symmetric that is dispacement vectors (dx, dy), (-dx, -dy) create exactly the same matrix. If sym is 0, the created co-occurrence matrix is not symmetric that is dispacement vectors (dx, dy), (-dx, -dy) create different matrices. Input image should be one band unsigned char image. cooc_features calculates and prints at the standard error output features of the cooccurrence matrix matrix.","Process Name":"cooc","Link":"https:\/\/linux.die.net\/man\/1\/cooc"}},{"Process":{"Description":"cooc creates a 256 by 256 one channel co-occurrence matrix of the box determined by the parameters (xp, yp; xs, ys) within the image file. The matrix is written onto the Vasari image file matrix. The displacement vector is determined by (dx, dy). The user must ensure that there is enough border pixels around the box within im dictated by the displacement vector (dx,dy) or else the program fails. All entries of the co-occurrence matrix are double normalised to the number of pairs involved. This function is a direct implementation of the paper: Haralick R. M., Shanmugan K. and Dinstein I., 'Textural features for image classification', IEEE Transactions on Systems, Man, and Cybernetics, Vol. SMC-3, No 6, Nov. 1973, pp 610-621. If flag sym is 1, the created co-occurrence matrix is symmetric that is dispacement vectors (dx, dy), (-dx, -dy) create exactly the same matrix. If sym is 0, the created co-occurrence matrix is not symmetric that is dispacement vectors (dx, dy), (-dx, -dy) create different matrices. Input image should be one band unsigned char image. cooc_features calculates and prints at the standard error output features of the cooccurrence matrix matrix.","Process Name":"cooc_features","Link":"https:\/\/linux.die.net\/man\/1\/cooc_features"}},{"Process":{"Description":"The bibutils program set inter-converts between various bibliography formats using Library of Congress [1] 's Metadata Object Description Schema (MODS) [2] version 3.1. For example, one can convert RIS-format files to Bibtex by doing two transformations: RIS->MODS->Bibtex.","Process Name":"copac2xml","Link":"https:\/\/linux.die.net\/man\/1\/copac2xml"}},{"Process":{"Description":"copydatabase - Perform a document-by-document copy of one or more Xapian databases","Process Name":"copydatabase","Link":"https:\/\/linux.die.net\/man\/1\/copydatabase"}},{"Process":{"Description":"mirrordir is a set of useful utilities for manipulating and mirroring directories. Included is also the command pslogin - an alternative to ssh(1), and forward(1) for forwarding arbitrary TCP socket connections over encrypted secure channels. mirrordir copies files that are different between the directories control and mirror to the directory mirror. Files whose modification times or sizes differ are copied. File permissions, ownerships, modification times, access times (only if --access-times is used), sticky bits, and device types are duplicated. Symlinks are duplicated without any translation. Symlink modification and access times (of the symlink itself, not the file it points to) are not preserved. Hard linked files are merely copied. Creation times cannot be set with Unix as far as I can see. mirrordir is a DANGEROUS command because files or directories that exist in mirror that don't exist in control are deleted. If control is entirely empty, then all files and directories in mirror will be deleted. If mirror is entirely empty, then all files and directories in control will be copied. In short, mirrordir forces mirror to be an exact replica of the directory tree control in every possible detail suitable for purposes of timed backup. It naturally descends into subdirectories to all their depths. mirrordir tries to be as efficient as possible by making the minimal set of changes necessary to mirror the directory. Access time duplication is not usually required and creates unnecessary load. Hence it is given as an option. The directory control is left untouched. If --restore-access is given then access times are reset to their original with each read. If the the --strict-locking option is on, files in control that are copied are locked for 'shared reading'. This will ensure, if another process is busy writing to that file, that the file is not copied in its incomplete or corrupted state. Usually mirrordir will not exit, but will give error messages to stderr to report any problems, and then will continue. The directory mirror or dest must exist, even if it is empty. Before erasing all the files in a directory, mirrordir checks for the file *--keep-me (where * is zero or one characters). If this file is present it will abort with an error message. Hence such a file can be created in all directories that you are fearful of being recursively erased. copydir is equivalent to mirrordir -ck --no-erase-directories ... (although -c implies -k anyway), so copydir is very much like a rigorous version of cp(1) where filenames can also be URLs, and only outdated files are replaced. Use copydir instead of mirrordir for most file transfers. Only use mirrordir, when you really want to delete things. recursdir is a further program that does nothing but descend into the directories on the command line. It is equivalent to mirrordir --recurs-mode ... It was born after the -C option was added, and can be used as a more rigorous version of find(1) and can also pack all the files it finds into a tar file. pslogin is yet a further program which has almost nothing to do with the previous three. It envokes a secure login session using secure-mcserv. It is equivalent to mirrordir --login-mode --secure ... pslogin should be called logindir. See --login-mode below. forward is yet a further program which has almost nothing to do with first three. It can do forwarding of arbitrary services over a secure channel. See forward(1) for details. The importance of this package is that you can use URL's instead of normal filenames, and hence manipulate files over a network. The URL types currently supported are ftp:\/\/ and mc:\/\/ (http:\/\/ is not a filesystem and therefore is not supported). mc:\/\/ is the Midnight Commander filesystem and is served by the secure-mcserv daemon. It has the advantage of serving cryptographically strong secure file transfers and logins. You can also use glob expressions in filenames for the recursdir and copydir commands. These will be recursively expanded.","Process Name":"copydir","Link":"https:\/\/linux.die.net\/man\/1\/copydir"}},{"Process":{"Description":"Simulates coral growth, albeit somewhat slowly.","Process Name":"coral","Link":"https:\/\/linux.die.net\/man\/1\/coral"}},{"Process":{"Description":"See Module::CoreList for one.","Process Name":"corelist","Link":"https:\/\/linux.die.net\/man\/1\/corelist"}},{"Process":{"Description":"The couchdb command runs the Apache CouchDB server. Erlang is called with: -sasl errlog_type error +K true +A 4 Erlang inherits the environment of this command. You can override these options using the environment: ERL_AFLAGS, ERL_FLAGS, ERL_ZFLAGS See erl(1) for more information about the environment variables. The exit status is 0 for success or 1 for failure.","Process Name":"couchdb","Link":"https:\/\/linux.die.net\/man\/1\/couchdb"}},{"Process":{"Description":"This script will take a list of JSON-encoded files and publish them to a CouchDB database. The paths of the filenames will be used as document ids in CouchDB, and slashes in the path will be escaped properly. This will let you upload documents that have ids with '\/'s in them (like '_design\/docs').","Process Name":"couchdb-push","Link":"https:\/\/linux.die.net\/man\/1\/couchdb-push"}},{"Process":{"Description":"The couchjs command runs the Apache CouchDB JavaScript interpreter. The exit status is 0 for success or 1 for failure.","Process Name":"couchjs","Link":"https:\/\/linux.die.net\/man\/1\/couchjs"}},{"Process":{"Description":"Lynx changed its name to Cougar during May 2002 in order to avoid name conflict with the famous text-mode Web browser. Cougar is a hierarchical layout extractor. It builds a netlist of interconnections from a symbolic layout view. The input argument is the name of the symbolic layout cell to be extracted, using as input format the one selected by the mbk_in_ph(1) environment variable. If output is present, the resulting netlist will be given this name. If no output is given, then input will also be the generated netlist name. The output format is specified by the mbk_out_lo(1) environment variable. As most of the Alliance cad tools, cougar uses mbk(1) environment variables. mbk_cata_lib(1), mbk_work_lib(1), mbk_in_ph(1), mbk_out_lo(1), RDS_TECHNO_NAME(1). Cougar computes capacitances attached to the signals if the -ac option is set. At the moment, the value of these capacitances is computed for a typical one micron technology, and cannot be changed by the user through a technology file. The extracted netlist can be simulated for performance evaluation. The typical capacitances are given below in 10e-18 farad \/ lamda^2 : POLY 100 ALU1 50 ALU2 25","Process Name":"cougar","Link":"https:\/\/linux.die.net\/man\/1\/cougar"}},{"Process":{"Description":"","Process Name":"count","Link":"https:\/\/linux.die.net\/man\/1\/count"}},{"Process":{"Description":"See perldoc README .pod","Process Name":"count.pl","Link":"https:\/\/linux.die.net\/man\/1\/count.pl"}},{"Process":{"Description":"count2huge.pl convert the output of the count.pl to huge-count.pl for the same input text and options. The reason we do this is because for the vector relatedness measure of UMLS-Similarity, it requires the bigrams which starts with the same term are grouped together. When the bigrams are sorted, it could optimize the processing time. For details, please see the vector-input.pl of UMLS-Similarity. See perldoc count2huge.pl","Process Name":"count2huge.pl","Link":"https:\/\/linux.die.net\/man\/1\/count2huge.pl"}},{"Process":{"Description":"Report coverage statistics in a variety of formats. The summary option produces a short textual summary. Other reports are available by using the report option. The following reports are currently available: text                  - detailed textual summary\nhtml                  - detailed HTML reports","Process Name":"cover","Link":"https:\/\/linux.die.net\/man\/1\/cover"}},{"Process":{"Description":"Covered is a Verilog code coverage analysis tool that can be useful for determining how well a diagnostic test suite is covering the design under test. Covered reads in the Verilog design files and a VCD or LXT2 formatted dumpfile from a diagnostic run and generates a database file called a Coverage Description Database (CDD) file, using the score command. Covered's score command can alternatively be used to generate a CDD file and a Verilog module for using Covered as a VPI module in a testbench which can obtain coverage information in parallel with simulation (see USING COVERED AS A VPI MODULE). The resulting CDD file can be merged with other CDD files from the same design to create accummulated coverage, using the merge command. Once a CDD file is created, the user can use Covered to generate various human-readable coverage reports in an ASCII format or use Covered's GUI to interactively look at coverage results, using the report command. If uncovered coverage points are found that the user wants to exclude from coverage, this can be handled with either the command-line exclude command or within the GUI. When multiple CDD files are created from the same design, the user may obtain a coverage ranking of those CDD files to determine an ideal order for regression testing as well as understand which CDD files can be excluded from regressions due to their inability to hit new coverage points. Additionally, as part of Covered's score command, race condition possibilities are found in the design files and can be either ignored, flagged as warnings or flagged as errors. By specifying race conditions as errors, Covered can also be used as a race condition checker.","Process Name":"covered","Link":"https:\/\/linux.die.net\/man\/1\/covered"}},{"Process":{"Description":"","Process Name":"coverperl","Link":"https:\/\/linux.die.net\/man\/1\/coverperl"}},{"Process":{"Description":"make.coverpg is called from faxspool(1) to generate a cover page for the just-processed fax. It has to create a proper G3 file (e.g. via pbm2g3(1) or hp2hig3(1) or ghostscript(1)) and output that on stdout. If the program doesn't exist, or can't be executed, the fax simply won't get a coverpage (so, if you don't want a fax coverpage, do not install it...) make.coverpg can put anything it wants on the page, but note that there are certain legal requirements in certain countries about the contents that *have* to be on the cover page, for example, the fax phone number of the sender and the recepient, the number of pages, or similar things. make.coverpg gets the informations about the fax to be sent from the command line, in the order listed above. If the environment variable normal_res is set to something non-empty, faxspool requests that make.coverpg creates a cover page in normal resolution (98 lpi). Default is fine resolution (196 lpi). NO make.coverpg program is installed by default, since everyones needs differ too wildly. Some sample coverpage programs are provided in the mgetty source tree, in the samples\/ subdirectory (coverpg.pbm shows how to do it with \"pbmtext|pbm2g3\", coverpg.ps shows how I do it with ghostscript). In this directory, you can also find two shell scripts (fax and faxmemo) that will take advantage of one more esoteric feature of my coverpage programs: if called with the option \"-m <memo-file>\", the sample programs will put a text file \"<memo-file>\" on the cover page (used for short notes or such). To make use of it, faxspool is called with the option '-C \"make.coverpg -m <memo-file>\"' (the double quotes are needed!).","Process Name":"coverpg","Link":"https:\/\/linux.die.net\/man\/1\/coverpg"}},{"Process":{"Description":"Cowsay generates an ASCII picture of a cow saying something provided by the user. If run with no arguments, it accepts standard input, word-wraps the message given at about 40 columns, and prints the cow saying the given message on standard output. To aid in the use of arbitrary messages with arbitrary whitespace, use the -n option. If it is specified, the given message will not be word-wrapped. This is possibly useful if you want to make the cow think or speak in figlet(6). If -n is specified, there must not be any command-line arguments left after all the switches have been processed. The -W specifies roughly (where the message should be wrapped. The default is equivalent to -W 40 i.e. wrap words at or before the 40th column. If any command-line arguments are left over after all switches have been processed, they become the cow's message. The program will not accept standard input for a message in this case. There are several provided modes which change the appearance of the cow depending on its particular emotional\/physical state. The -b option initiates Borg mode; -d causes the cow to appear dead; -g invokes greedy mode; -p causes a state of paranoia to come over the cow; -s makes the cow appear thoroughly stoned; -t yields a tired cow; -w is somewhat the opposite of -t, and initiates wired mode; -y brings on the cow's youthful appearance. The user may specify the -e option to select the appearance of the cow's eyes, in which case the first two characters of the argument string eye_string will be used. The default eyes are 'oo'. The tongue is similarly configurable through -T and tongue_string; it must be two characters and does not appear by default. However, it does appear in the 'dead' and 'stoned' modes. Any configuration done by -e and -T will be lost if one of the provided modes is used. The -f option specifies a particular cow picture file (''cowfile'') to use. If the cowfile spec contains '\/' then it will be interpreted as a path relative to the current directory. Otherwise, cowsay will search the path specified in the COWPATH environment variable. To list all cowfiles on the current COWPATH, invoke with the -l switch. If the program is invoked as cowthink then the cow will think its message instead of saying it.","Process Name":"cowsay","Link":"https:\/\/linux.die.net\/man\/1\/cowsay"}},{"Process":{"Description":"Cowsay generates an ASCII picture of a cow saying something provided by the user. If run with no arguments, it accepts standard input, word-wraps the message given at about 40 columns, and prints the cow saying the given message on standard output. To aid in the use of arbitrary messages with arbitrary whitespace, use the -n option. If it is specified, the given message will not be word-wrapped. This is possibly useful if you want to make the cow think or speak in figlet(6). If -n is specified, there must not be any command-line arguments left after all the switches have been processed. The -W specifies roughly (where the message should be wrapped. The default is equivalent to -W 40 i.e. wrap words at or before the 40th column. If any command-line arguments are left over after all switches have been processed, they become the cow's message. The program will not accept standard input for a message in this case. There are several provided modes which change the appearance of the cow depending on its particular emotional\/physical state. The -b option initiates Borg mode; -d causes the cow to appear dead; -g invokes greedy mode; -p causes a state of paranoia to come over the cow; -s makes the cow appear thoroughly stoned; -t yields a tired cow; -w is somewhat the opposite of -t, and initiates wired mode; -y brings on the cow's youthful appearance. The user may specify the -e option to select the appearance of the cow's eyes, in which case the first two characters of the argument string eye_string will be used. The default eyes are 'oo'. The tongue is similarly configurable through -T and tongue_string; it must be two characters and does not appear by default. However, it does appear in the 'dead' and 'stoned' modes. Any configuration done by -e and -T will be lost if one of the provided modes is used. The -f option specifies a particular cow picture file (''cowfile'') to use. If the cowfile spec contains '\/' then it will be interpreted as a path relative to the current directory. Otherwise, cowsay will search the path specified in the COWPATH environment variable. To list all cowfiles on the current COWPATH, invoke with the -l switch. If the program is invoked as cowthink then the cow will think its message instead of saying it.","Process Name":"cowthink","Link":"https:\/\/linux.die.net\/man\/1\/cowthink"}},{"Process":{"Description":"Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY. Mandatory arguments to long options are mandatory for short options too. -a, --archive same as -dR --preserve= all --backup[= CONTROL] make a backup of each existing destination file -b like --backup but does not accept an argument --copy-contents copy contents of special files when recursive -d same as --no-dereference --preserve= links -f, --force if an existing destination file cannot be opened, remove it and try again (redundant if the -n option is used) -i, --interactive prompt before overwrite (overrides a previous -n option) -H follow command-line symbolic links in SOURCE -l, --link link files instead of copying -L, --dereference always follow symbolic links in SOURCE -n, --no-clobber do not overwrite an existing file (overrides a previous -i option) -P, --no-dereference never follow symbolic links in SOURCE -p same as --preserve= mode,ownership,timestamps --preserve[= ATTR_LIST] preserve the specified attributes (default: mode,ownership,timestamps), if possible additional attributes: context, links, xattr, all -c same as --preserve= context --no-preserve= ATTR_LIST don't preserve the specified attributes --parents use full source file name under DIRECTORY -R, -r, --recursive copy directories recursively --reflink[= WHEN] control clone\/CoW copies. See below. --remove-destination remove each existing destination file before attempting to open it (contrast with --force) --sparse= WHEN control creation of sparse files. See below. --strip-trailing-slashes remove any trailing slashes from each SOURCE argument -s, --symbolic-link make symbolic links instead of copying -S, --suffix= SUFFIX override the usual backup suffix -t, --target-directory= DIRECTORY copy all SOURCE arguments into DIRECTORY -T, --no-target-directory treat DEST as a normal file -u, --update copy only when the SOURCE file is newer than the destination file or when the destination file is missing -v, --verbose explain what is being done -x, --one-file-system stay on this file system -Z, --context= CONTEXT set security context of copy to CONTEXT --help display this help and exit --version output version information and exit By default, sparse SOURCE files are detected by a crude heuristic and the corresponding DEST file is made sparse as well. That is the behavior selected by --sparse=auto. Specify --sparse=always to create a sparse DEST file whenever the SOURCE file contains a long enough sequence of zero bytes. Use --sparse=never to inhibit creation of sparse files. When --reflink[=always] is specified, perform a lightweight copy, where the data blocks are copied only when modified. If this is not possible the copy fails, or if --reflink=auto is specified, fall back to a standard copy. The backup suffix is '~', unless set with --suffix or SIMPLE_BACKUP_SUFFIX. The version control method may be selected via the --backup option or through the VERSION_CONTROL environment variable. Here are the values: none, off never make backups (even if --backup is given) numbered, t make numbered backups existing, nil numbered if numbered backups exist, simple otherwise simple, never always make simple backups As a special case, cp makes a backup of SOURCE when the force and backup options are given and SOURCE and DEST are the same name for an existing, regular file.","Process Name":"cp","Link":"https:\/\/linux.die.net\/man\/1\/cp"}},{"Process":{"Description":"The \"cpack\" executable is the CMake packaging program. CMake-generated build trees created for projects that use the INSTALL_* commands have packaging support. This program will generate the package. CMake is a cross-platform build system generator. Projects specify their build process with platform-independent CMake listfiles included in each directory of a source tree with the name CMakeLists.txt. Users build a project by using CMake to generate a build system for a native tool on their platform.","Process Name":"cpack","Link":"https:\/\/linux.die.net\/man\/1\/cpack"}},{"Process":{"Description":"The \"cpack\" executable is the CMake packaging program. CMake-generated build trees created for projects that use the INSTALL_* commands have packaging support. This program will generate the package. CMake is a cross-platform build system generator. Projects specify their build process with platform-independent CMake listfiles included in each directory of a source tree with the name CMakeLists.txt. Users build a project by using CMake to generate a build system for a native tool on their platform.","Process Name":"cpack28","Link":"https:\/\/linux.die.net\/man\/1\/cpack28"}},{"Process":{"Description":"Program cpaldjvu is a DjVuDocument encoder for images containing few colors. It performs best on images containing large solid color areas such as screen dumps. Compression ratios on such images can be much higher than those achieved by GIF or PNG compression. This program works by first reducing the number of distinct colors to a small specified value using a simple color quantization algorithm. The dominant color is encoded into the background layer. The other colors are encoded into the foreground layer.","Process Name":"cpaldjvu","Link":"https:\/\/linux.die.net\/man\/1\/cpaldjvu"}},{"Process":{"Description":"This script provides a command interface (not a shell) to CPAN . At the moment it uses CPAN .pm to do the work, but it is not a one-shot command runner for CPAN .pm. Meta Options These options are mutually exclusive, and the script processes them in this order: [hvCAar]. Once the script finds one, it ignores the others, and then exits after it finishes the task. The script ignores any other command line options. -a Creates the CPAN .pm autobundle with CPAN::Shell->autobundle. -A module [ module ... ] Shows the primary maintainers for the specified modules -C module [ module ... ] Show the \"Changes\" files for the specified modules -D module [ module ... ] Show the module details. This prints one line for each out-of-date module (meaning, modules locally installed but have newer versions on CPAN ). Each line has three columns: module name, local version, and CPAN version. -L author [ author ... ] List the modules by the specified authors. -h Prints a help message. -O Show the out-of-date modules. -r Recompiles dynamically loaded modules with CPAN::Shell->recompile. -v Print the script version and CPAN .pm version. Module options These options are mutually exclusive, and the script processes them in alphabetical order. It only processes the first one it finds. c Runs a 'make clean' in the specified module's directories. f Forces the specified action, when it normally would have failed. i Installed the specified modules. m Makes the specified modules. t Runs a 'make test' on the specified modules. Examples # print a help message\ncpan -h\n\n# print the version numbers\ncpan -v\n\n# create an autobundle\ncpan -a\n\n# recompile modules\ncpan -r\n\n# install modules ( sole -i is optional )\ncpan -i Netscape::Booksmarks Business::ISBN\n\n# force install modules ( must use -i )\ncpan -fi CGI::Minimal URI","Process Name":"cpan","Link":"https:\/\/linux.die.net\/man\/1\/cpan"}},{"Process":{"Description":"This script will create distributions of \"CPAN\" modules of the format you specify, including its prerequisites. These packages can then be installed using the corresponding package manager for the format. Note, you can also do this interactively from the default shell, \"CPANPLUS::Shell::Default\". See the \"CPANPLUS::Dist\" documentation, as well as the documentation of your format of choice for any format specific documentation.","Process Name":"cpan2dist","Link":"https:\/\/linux.die.net\/man\/1\/cpan2dist"}},{"Process":{"Description":"","Process Name":"cpancover","Link":"https:\/\/linux.die.net\/man\/1\/cpancover"}},{"Process":{"Description":"This script launches the CPANPLUS utility to perform various operations from the command line. If it's invoked without arguments, an interactive shell is executed by default. Optionally, it can take a single-letter switch and one or more argument, to perform the associated action on each arguments. A summary of the available commands is listed below; \"cpanp -h\" provides a detailed list. h                   # help information\nv                   # version information\n\na AUTHOR ...        # search by author(s)\nm MODULE ...        # search by module(s)\nf MODULE ...        # list all releases of a module\n\ni MODULE ...        # install module(s)\nt MODULE ...        # test module(s)\nu MODULE ...        # uninstall module(s)\nd MODULE ...        # download module(s)\nl MODULE ...        # display detailed information about module(s)\nr MODULE ...        # display README files of module(s)\nc MODULE ...        # check for module report(s) from cpan-testers\nz MODULE ...        # extract module(s) and open command prompt in it\n\nx                   # reload CPAN indices\n\no [ MODULE ... ]    # list installed module(s) that aren't up to date\nb                   # write a bundle file for your configuration Each command may be followed by one or more options. If preceded by \"no\", the corresponding option will be set to 0, otherwise it's set to 1. Example: To skip a module's tests, cpanp -i --skiptest MODULE ... Valid options for most commands are \"cpantest\", \"debug\", \"flush\", \"force\", \"prereqs\", \"storable\", \"verbose\", \"md5\", \"signature\", and \"skiptest\"; the 'd' command also accepts \"fetchdir\". Please consult CPANPLUS::Configure for an explanation to their meanings. Example: To download a module's tarball to the current directory, cpanp -d --fetchdir=. MODULE ... Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"cpanp","Link":"https:\/\/linux.die.net\/man\/1\/cpanp"}},{"Process":{"Description":"This utility lets you create and verify SIGNATURE files.","Process Name":"cpansign","Link":"https:\/\/linux.die.net\/man\/1\/cpansign"}},{"Process":{"Description":"cpanspec will generate a spec file to build a rpm from a CPAN-style Perl module distribution.","Process Name":"cpanspec","Link":"https:\/\/linux.die.net\/man\/1\/cpanspec"}},{"Process":{"Description":"cpantest uniformly posts package test results in support of the cpan-testers project. See http:\/\/www.cpantesters.org\/ for details.","Process Name":"cpantest","Link":"https:\/\/linux.die.net\/man\/1\/cpantest"}},{"Process":{"Description":"\"cpants_lint.pl\" checks the Kwalitee of CPAN distributions. More exact, it checks how a given tarball will be rated on \"http:\/\/cpants.perl.org\", without needing to upload it first. \"cpants_lint.pl\" is also used by \"cpants.perl.org\" itself to check all dists on CPAN . For more information on Kwalitee, and the whole of CPANTS , see \"http:\/\/cpants.perl.org\" and \/ or \"Module::CPANTS::Analyse\".","Process Name":"cpants_lint.pl","Link":"https:\/\/linux.die.net\/man\/1\/cpants_lint.pl"}},{"Process":{"Description":"cpdic copies user dictionary from-dic to to-dic to rewrite dictionary directory file -- dics.dir -- . If the user dictionary directory does not exist, cpdic creates it and creates dics.dir under it. If the -i option is specified, the system dictionary will be copied. If the -u option is specified, another user dictionary will be copied.","Process Name":"cpdic","Link":"https:\/\/linux.die.net\/man\/1\/cpdic"}},{"Process":{"Description":"The cpdup utility makes an exact mirror copy of the source in the destination, creating and deleting files and directories as necessary. utimes, hardlinks, softlinks, devices, permissions, and flags are mirrored. By default, cpdup asks for confirmation if any file or directory needs to be removed from the destination and does not copy files which it believes to have already been synchronized (by observing that the source and destination files' sizes and mtimes match). cpdup does not cross mount points in either the source or the destination. As a safety measure, cpdup refuses to replace a destination directory with a file. The following options are available:       -C'      If the source or target is a remote host, request that thessh(1) session be compressed.  This is the same as -F -C. -v[ v[ v]] Set verboseness. By default cpdup does not report its progress except when asking for confirmation. A single -v will only report modifications made to the destination. -vv will report directories as they are being traversed as well as modifications made to the destination. -vvv will cause all files and directories to be reported whether or not modifications are made. -d' Print directories as they are being traversed. Useful to watch the progress; this typically produces much less output than -vv. -u' Causes the output generated by -v and -d to be unbuffered. This can be useful for obtaining prompt progress updates through a pipe. -I' will cause cpdup to print a summary at the end with performance counters. -f' Forces file updates to occur even if the files appear to be the same. If the -H option is used, this option will force a byte for byte comparison between the original file and the file in the hardlink path, even if all the stat info matches, but will still use a hardlink if they match. -F ssh-arg Pass ssh-arg to ssh. For example ''-F -p222''. Note the lack of a space. -s0 Disable the disallow-file-replaces-directory safety feature. This safety feature is enabled by default to prevent user mistakes from blowing away everything accidentally. -i0 Do not request confirmation when removing something. -j0 Do not try to recreate CHR or BLK devices. -l' Line buffer verbose output. -q' Quiet operation. -o' Do not remove any files, just overwrite\/add. -m' Generate and maintain an MD5 checkfile called .MD5.CHECKSUMS in each directory on the source and do an MD5 check on each file of the destination when the destination appears to be the same as the source. If the check fails, the source is recopied to the destination. When you specify a destination directory, the MD5 checkfile is only updated as needed and may not be updated even if modifications are made to a source file. If you do not specify a destination directory the cpdup command forcefully regenerates the MD5 checkfile for every file in the source. -M file Works the same as -m but allows you to specify the name of the MD5 checkfile. -H path cpdup will create a hardlink from a file found under path to the target instead of copying the source to the target if the file found via path is identical to the source. Note that a remote host specification should not be used for this option's path, but the path will be relative to the target machine. This allows one to use cpdup to create incremental backups of a filesystem. Create a direct 'level 0' backup, and then specify the level 0 backup path with this option when creating an incremental backup to a different target directory. This method works so long as the filesystem does not hit a hardlink limit. If the system does hit a hardlink limit, cpdup will generate a warning and copy the file instead. Note that cpdup must record file paths for any hardlinked file while operating and therefore uses a great deal more memory when dealing with hardlinks or hardlink-based backups. Example use: cpdup -i0 -s0 -I -H \/backup\/home.l0 \/home \/backup\/home.l1 WARNING: If this option is used cpdup must record the paths for all files it encounters while it operates and it is possible that you may run the process out of memory. The file found via the hardlink path will be byte-by-byte compared with the source if the -V or -f option is also used, otherwise only the stat info is checked to determine whether it matches the source. -V' This forces the contents of regular files to be verified, even if the files appear to the be the same. Whereas the -f (force) option forces a copy regardless, this option will avoid rewriting the target if everything matches and the contents are verified to be the same. -VV This works the same as -V but ignores mtime entirely, making it suitable for comparing HAMMER master and slave filesystems or copies made without mtime retention. -S' This places cpdup into slave mode and is used to initiate the slave protocol on a remote machine. This option is not intended to be used by humans. -R' Place the slave into read-only mode. Can only be used when the source is remote. Useful for unattended backups via SSH keys. -k' Generate and maintain a FSMID checkfile called .FSMID.CHECK in each directory on the target. cpdup will check the FSMID for each source file or directory against the checkfile on the target and will not copy the file or recurse through the directory when a match occurs. Any source file or directory with the same name as the checkfile will be ignored. The FSMID will be re-checked after the copy has been completed and cpdup will loop on that directory or file until it is sure it has an exact copy. Warning: FSMID is not always supported by a filesystem and may not be synchronized if a crash occurs. will simulate an FSMID when it is otherwise not supported by the filesystem, and users should be aware that simulated FSMIDs may change state in such cases even if the underlying hierarchy does not due to cache flushes. Additionally, the FSMID may not reflect changes made to remote filesystems by other hosts. For example, using these options with NFS mounted sources will not work well. -K file Works the same as -k but allows you to specify the name of the FSMID checkfile. -x' Causes cpdup to use the exclusion file .cpignore in each directory on the source to determine which files to ignore. When this option is used, the exclusion filename itself is automatically excluded from the copy. If this option is not used then the filename .cpignore is not considered special and will be copied along with everything else. -X file Works the same as -x but allows you to specify the name of the exclusion file. This file is automatically excluded from the copy. Only one exclusion file may be specified.","Process Name":"cpdup","Link":"https:\/\/linux.die.net\/man\/1\/cpdup"}},{"Process":{"Description":"GNU cpio is a tool for creating and extracting archives, or copying files from one place to another. It handles a number of cpio formats as well as reading and writing tar files. Following archive formats are supported: binary, old ASCII, new ASCII, crc, HPUX binary, HPUX old ASCII, old tar, and POSIX.1 tar. The tar format is provided for compatibility with the tar program. By default, cpio creates binary format archives, for compatibility with older cpio programs. When extracting from archives, cpio automatically recognizes which kind of archive it is reading and can read archives created on machines with a different byte-order.","Process Name":"cpio","Link":"https:\/\/linux.die.net\/man\/1\/cpio"}},{"Process":{"Description":"Cpmchattr changes the file attributes for files on CP\/M disks.","Process Name":"cpmchattr","Link":"https:\/\/linux.die.net\/man\/1\/cpmchattr"}},{"Process":{"Description":"Cpmchmod changes the file mode for files on CP\/M files.","Process Name":"cpmchmod","Link":"https:\/\/linux.die.net\/man\/1\/cpmchmod"}},{"Process":{"Description":"cpmcp copies one or more files to or from a CP\/M disk. When copying multiple files, the last argument must be a drive or directory. The drive letter does not matter because the device is specified by the image, it is only used to specify which direction you want to copy. The user number is specified after the drive letter, if omitted user 0 is used. You can use * and ? in CP\/M file names, which have the same meaning in sh(1) file name patterns.","Process Name":"cpmcp","Link":"https:\/\/linux.die.net\/man\/1\/cpmcp"}},{"Process":{"Description":"Cpmls lists the sorted contents of the directory.","Process Name":"cpmls","Link":"https:\/\/linux.die.net\/man\/1\/cpmls"}},{"Process":{"Description":"cpmrm removes files from CP\/M disks.","Process Name":"cpmrm","Link":"https:\/\/linux.die.net\/man\/1\/cpmrm"}},{"Process":{"Description":"The C preprocessor, often known as cpp, is a macro processor that is used automatically by the C compiler to transform your program before compilation. It is called a macro processor because it allows you to define macros, which are brief abbreviations for longer constructs. The C preprocessor is intended to be used only with C, C ++ , and Objective-C source code. In the past, it has been abused as a general text processor. It will choke on input which does not obey C's lexical rules. For example, apostrophes will be interpreted as the beginning of character constants, and cause errors. Also, you cannot rely on it preserving characteristics of the input which are not significant to C-family languages. If a Makefile is preprocessed, all the hard tabs will be removed, and the Makefile will not work. Having said that, you can often get away with using cpp on things which are not C. Other Algol-ish programming languages are often safe (Pascal, Ada, etc.) So is assembly, with caution. -traditional-cpp mode preserves more white space, and is otherwise more permissive. Many of the problems can be avoided by writing C or C ++ style comments instead of native language comments, and keeping macros simple. Wherever possible, you should use a preprocessor geared to the language you are writing in. Modern versions of the GNU assembler have macro facilities. Most high level programming languages have their own conditional compilation and inclusion mechanism. If all else fails, try a true general text processor, such as GNU M4. C preprocessors vary in some details. This manual discusses the GNU C preprocessor, which provides a small superset of the features of ISO Standard C. In its default mode, the GNU C preprocessor does not do a few things required by the standard. These are features which are rarely, if ever, used, and may cause surprising changes to the meaning of a program which does not expect them. To get strict ISO Standard C, you should use the -std=c89 or -std=c99 options, depending on which version of the standard you want. To get all the mandatory diagnostics, you must also use -pedantic. This manual describes the behavior of the ISO preprocessor. To minimize gratuitous differences, where the ISO preprocessor's behavior does not conflict with traditional semantics, the traditional preprocessor should behave the same way. The various differences that do exist are detailed in the section Traditional Mode. For clarity, unless noted otherwise, references to CPP in this manual refer to GNU CPP .","Process Name":"cpp","Link":"https:\/\/linux.die.net\/man\/1\/cpp"}},{"Process":{"Description":"Cppcheck is a command-line tool that tries to detect bugs that your C\/C++ compiler doesn't see. It is versatile, and can check non-standard code including various compiler extensions, inline assembly code, etc. Its internal preprocessor can handle includes, macros, and several preprocessor commands. While Cppcheck is highly configurable, you can start using it just by giving it a path to the source code.","Process Name":"cppcheck","Link":"https:\/\/linux.die.net\/man\/1\/cppcheck"}},{"Process":{"Description":"is a liberalised re-implementation of cpp (1) , the C pre-processor, in and for Haskell. Why re-implement cpp? Rightly or wrongly, the C pre-processor is widely used in Haskell source code. It enables conditional compilation for different compilers, different versions of the same compiler, and different OS platforms. It is also occasionally used for its macro language, which can enable certain forms of platform-specific detail-filling, such as the tedious boilerplate generation of instance definitions and FFI declarations. However, there are two problems with cpp, aside from the obvious aesthetic ones: For some Haskell systems, notably Hugs on Windows, a true cpp is not available by default. Even for the other Haskell systems, the common cpp provided by the gcc 3.x series is changing subtly in ways that are incompatible with Haskell's syntax. There have always been problems with, for instance, string gaps, and prime characters in identifiers. These problems are only going to get worse. So, it seemed right to attempt to provide an alternative to cpp, both more compatible with Haskell, and itself written in Haskell so that it can be distributed with compilers. is pretty-much feature-complete, and compatible with the -traditional style of cpp. It has two modes: conditional compilation only ( --nomacro), and full macro-expansion (default). In --nomacro mode, performs only conditional compilation actions, i.e. #include's, #if's, and #ifdef's are processed according to text-replacement definitions (both command-line and internal), but no parameterised macro expansion is performed. In full compatibility mode (the default), textual replacements and macro expansions are also processed in the remaining body of non-cpp text. Working Features: #ifdef simple conditional compilation #if the full boolean language of defined(), &&, ||, ==, etc. #elif chained conditionals #define in-line definitions (text replacements and macros) #undef in-line revocation of definitions #include file inclusion #line line number directives \\\\n line continuations within all # directives \/**\/ token catenation within a macro definition ## ANSI-style token catenation # ANSI-style token stringisation __FILE__ special text replacement for DIY error messages __LINE__ special text replacement for DIY error messages __DATE__ special text replacement __TIME__ special text replacement Macro expansion is recursive. Redefinition of a macro name does not generate a warning. Macros can be defined on the command-line with -D just like textual replacements. Macro names are permitted to be Haskell identifiers e.g. with the prime ga and backtick ' characters, which is slightly looser than in C, but they still may not include operator symbols. Numbering of lines in the output is preserved so that any later processor can give meaningful error messages. When a file is #include'd, inserts #line directives for the same reason. Numbering should be correct even in the presence of line continuations. If you don't want #line directives in the final output, use the --noline option. Any syntax errors in cpp directives gives a message to stderr and halts the program. Failure to find a #include'd file produces a warning to stderr, but processing continues. You can give any number of filenames on the command-line. The results are catenated on standard output. -Dsym define a textual replacement (default value is 1) -Dsym=val define a textual replacement with a specific value -Ipath add a directory to the search path for #include's -Ofile specify a file for output (default is stdout) --nomacro only process #ifdef's and #include's, do not expand macros --noline remove #line droppings from the output --strip convert C-style comments to whitespace, even outside cpp directives --hashes recognise the ANSI # stringise operator, and ## for token catenation, within macros --text treat the input as plain text, not Haskell code --layout preserve newlines within macro expansions --unlit remove literate-style comments --version report version number of cpphs and stop There are NO textual replacements defined by default. (Normal cpp usually has definitions for machine, OS, etc. These could easily be added to the cpphs source code if you wish.) The search path is searched in order of the -I options, except that the directory of the calling file, then the current directory, are always searched first. Again, there is no default search path (and again, this could easily be changed).","Process Name":"cpphs","Link":"https:\/\/linux.die.net\/man\/1\/cpphs"}},{"Process":{"Description":"cppunit-config is a tool that is used to configure to determine the compiler and linker flags that should be used to compile and link programs that use cppunit. It is also used internally to the .m4 macros for GNU autoconf that are included with cppunit.","Process Name":"cppunit-config","Link":"https:\/\/linux.die.net\/man\/1\/cppunit-config"}},{"Process":{"Description":"This program is capable op monitoring HP (Compaq) array controllers. Both the devices controller by the cpqarray driver and devices controlled by the cciss driver are supported. The program monitors the health status for each logical volume and reports if it changes. When used on a cciss controller it reports all events on the controller chain. The default is to log to the syslog facility, but the program has an option to send traps to a monitoring system.","Process Name":"cpqarrayd","Link":"https:\/\/linux.die.net\/man\/1\/cpqarrayd"}},{"Process":{"Description":"Cproto generates function prototypes for functions defined in the specified C source files to the standard output. The function definitions may be in the old style or ANSI C style. Optionally, cproto also outputs declarations for variables defined in the files. If no file argument is given, cproto reads its input from the standard input. By giving a command line option, cproto will also convert function definitions in the specified files from the old style to the ANSI C style. The original source files along with files specified by #include \"file\" directives appearing in the source code will be overwritten with the converted code. If no file names are given on the command line, then the program reads the source code from the standard input and outputs the converted source to the standard output. If any comments appear in the parameter declarations for a function definition, such as in the example, main (argc, argv)\nint argc;       \/* number of arguments *\/\nchar *argv[];   \/* arguments *\/\n{\n} then the converted function definition will have the form int\nmain (\n    int argc,       \/* number of arguments *\/\n    char *argv[]   \/* arguments *\/\n)\n{\n} Otherwise, the converted function definition will look like int\nmain (int argc, char *argv[])\n{\n} Cproto can optionally convert function definitions from the ANSI style to the old style. In this mode, the program also converts function declarators and prototypes that appear outside function bodies. This is not a complete ANSI C to old C conversion. The program does not change anything within function bodies. Cproto can optionally generate source in lint-library format. This is useful in environments where the lint utility is used to supplement prototype checking of your program.","Process Name":"cproto","Link":"https:\/\/linux.die.net\/man\/1\/cproto"}},{"Process":{"Description":"A small tool which prints out cpufreq information helpful to developers and interested users.","Process Name":"cpufreq-info","Link":"https:\/\/linux.die.net\/man\/1\/cpufreq-info"}},{"Process":{"Description":"cpufreq-set allows you to modify cpufreq settings without having to type e.g. \"\/sys\/devices\/system\/cpu\/cpu0\/cpufreq\/scaling_set_speed\" all the time.","Process Name":"cpufreq-set","Link":"https:\/\/linux.die.net\/man\/1\/cpufreq-set"}},{"Process":{"Description":"cpuid dumps detailed information about the CPU(s) gathered from the CPUID instruction, and also determines the exact model of CPU(s) from that information. It dumps all information available from the CPUID instruction. The exact collection of information available varies between manufacturers and processors. The following information is available consistently on all modern CPUs:     vendor_id\n    version information (1\/eax)\n    miscellaneous (1\/ebx)\n    feature information (1\/ecx) It also produces synthetic fields based on information from multiple CPUID functions. Currently, the synthetic fields are the exact model of each CPU (but see LIMITATIONS below) as (synth); the multiprocessing characteristics including the number of cores per chip (c) and the number of hyperthreads per core (t) as (multi-processing synth); and a decoding of the APIC physical ID as (APIC synth). The determination of the model is based on the following information:     version information (1\/eax), processor type\n    version information (1\/eax), family\n    version information (1\/eax), model\n    version information (1\/eax), stepping id\n    version information (1\/eax), extended family\n    version information (1\/eax), extended model\n    feature information (1\/ecx), virtual machine extensions\n    brand id (1\/ebx)\n    brand (0x80000004)\n    cache and TLB information (2)\n    deterministic cache parameters (4\/eax), extra processor cores\n    AMD extended brand id (0x80000001\/ebx)\n    AMD extended processor signature (0x80000001\/eax)\n    Transmeta processor revision ID (0x80860001\/ebx & ecx) The determination of the multiprocessing characteristics and decoding of APIC physical ID is based on the following information:     feature information (1\/edx), hyper-threading \/ multi-core supported\n    miscellaneous (1\/ebx), cpu count\n    deterministic cache parameters (4\/eax), extra processor cores on this die\n    x2APIC features \/ processor topology (0xb)\n    AMD feature flags (0x80000001\/ecx)\n    AMD Logical CPU cores (0x80000008\/ecx), number of logical CPU cores - 1 In addition, a simpler and coarser determination of the CPU is performed using only the information listed above under version information (1\/eax). It is provided as (simple synth) under version information (1\/eax). However, it tends to be unable to distinguish between various modern CPUs.","Process Name":"cpuid","Link":"https:\/\/linux.die.net\/man\/1\/cpuid"}},{"Process":{"Description":"cpupower is a collection of tools to examine and tune power saving related features of your processor. The manpages of the commands (cpupower-<command>(1)) provide detailed descriptions of supported features. Run cpupower help to get an overview of supported commands.","Process Name":"cpupower","Link":"https:\/\/linux.die.net\/man\/1\/cpupower"}},{"Process":{"Description":"A small tool which prints out cpufreq information helpful to developers and interested users.","Process Name":"cpupower-frequency-info","Link":"https:\/\/linux.die.net\/man\/1\/cpupower-frequency-info"}},{"Process":{"Description":"cpupower frequency-set allows you to modify cpufreq settings without having to type e.g. \"\/sys\/devices\/system\/cpu\/cpu0\/cpufreq\/scaling_set_speed\" all the time.","Process Name":"cpupower-frequency-set","Link":"https:\/\/linux.die.net\/man\/1\/cpupower-frequency-set"}},{"Process":{"Description":"cpupower info shows kernel configurations or processor hardware registers affecting processor power saving policies. Some options are platform wide, some affect single cores. By default values of core zero are displayed only. cpupower --cpu all cpuinfo will show the settings of all cores, see cpupower(1) how to choose specific cores.","Process Name":"cpupower-info","Link":"https:\/\/linux.die.net\/man\/1\/cpupower-info"}},{"Process":{"Description":"cpupower-monitor reports processor topology, frequency and idle power state statistics. Either command is forked and statistics are printed upon its completion, or statistics are printed periodically. cpupower-monitor implements independent processor sleep state and frequency counters. Some are retrieved from kernel statistics, some are directly reading out hardware registers. Use -l to get an overview which are supported on your system.","Process Name":"cpupower-monitor","Link":"https:\/\/linux.die.net\/man\/1\/cpupower-monitor"}},{"Process":{"Description":"cpupower set sets kernel configurations or directly accesses hardware registers affecting processor power saving policies. Some options are platform wide, some affect single cores. By default values are applied on all cores. How to modify single core configurations is described in the cpupower(1) manpage in the --cpu option section. Whether an option affects the whole system or can be applied to individual cores is described in the Options sections. Use cpupower info to read out current settings and whether they are supported on the system at all.","Process Name":"cpupower-set","Link":"https:\/\/linux.die.net\/man\/1\/cpupower-set"}},{"Process":{"Description":"create_cvsignore is used to create a preliminary .cvsignore in the current directory. It does this based on certain contents it finds in Makefile.am No lines will be removed from any existing .cvsignore. If there is not already a .cvsignore file, it will be added to the cvs repository. Note that you must have a Makefile.am in the current directory for this tool to work. This utility is part of the KDE Software Development Kit.","Process Name":"create_cvsignore","Link":"https:\/\/linux.die.net\/man\/1\/create_cvsignore"}},{"Process":{"Description":"create_makefile creates the Makefile.in and Makefile in a subdirectory containing a Makefile.am. This script saves time compared to re-running configure completely Note that you must supply the path to the desired Makefile Makefile.am (though the final \/Makefile may be omitted). This script may be run from the toplevel directory (the one containing configure) or from one of it's subdirectories. If the source directory is different from the build directory (see the environment variables below), it will be assumed that the Makefile.am and Makefile.in belong beneath the source directory and that the Makefile belongs beneath the build directory. This utility is part of the KDE Software Development Kit.","Process Name":"create_makefile","Link":"https:\/\/linux.die.net\/man\/1\/create_makefile"}},{"Process":{"Description":"create_makefiles recreates all Makefiles in dir and its (recursed) subdirectories from the corresponding Makefile.am templates. This script must be run from the toplevel directory (the one containing configure). This script saves time compared to re-running configure completely. If the source directory is different from the build directory (see the environment variables below), it will be assumed that each Makefile.am and Makefile.in belongs beneath the source directory and that each Makefile belongs beneath the build directory. This utility is part of the KDE Software Development Kit.","Process Name":"create_makefiles","Link":"https:\/\/linux.die.net\/man\/1\/create_makefiles"}},{"Process":{"Description":"createdb creates a new PostgreSQL database. Normally, the database user who executes this command becomes the owner of the new database. However a different owner can be specified via the -O option, if the executing user has appropriate privileges. createdb is a wrapper around the SQL command CREATE DATABASE [create_database(7)]. There is no effective difference between creating databases via this utility and via other methods for accessing the server.","Process Name":"createdb","Link":"https:\/\/linux.die.net\/man\/1\/createdb"}},{"Process":{"Description":"This program create a database for working with Postgis geometries. Using this program any user with createdb permissions can create a spatial enabled database.","Process Name":"createdb.postgis","Link":"https:\/\/linux.die.net\/man\/1\/createdb.postgis"}},{"Process":{"Description":"createlang is a utility for adding a new programming language to a PostgreSQL database. createlang is just a wrapper around the CREATE LANGUAGE [create_language(7)] command.","Process Name":"createlang","Link":"https:\/\/linux.die.net\/man\/1\/createlang"}},{"Process":{"Description":"The program suggests passwords using a random number generator.","Process Name":"createp","Link":"https:\/\/linux.die.net\/man\/1\/createp"}},{"Process":{"Description":"createuser creates a new PostgreSQL user (or more precisely, a role). Only superusers and users with CREATEROLE privilege can create new users, so createuser must be invoked by someone who can connect as a superuser or a user with CREATEROLE privilege. If you wish to create a new superuser, you must connect as a superuser, not merely with CREATEROLE privilege. Being a superuser implies the ability to bypass all access permission checks within the database, so superuserdom should not be granted lightly. createuser is a wrapper around the SQL command CREATE ROLE [create_role(7)]. There is no effective difference between creating users via this utility and via other methods for accessing the server.","Process Name":"createuser","Link":"https:\/\/linux.die.net\/man\/1\/createuser"}},{"Process":{"Description":"addr2line translates addresses into file names and line numbers. Given an address in an executable or an offset in a section of a relocatable object, it uses the debugging information to figure out which file name and line number are associated with it. The executable or relocatable object to use is specified with the -e option. The default is the file a.out. The section in the relocatable object to use is specified with the -j option. addr2line has two modes of operation. In the first, hexadecimal addresses are specified on the command line, and addr2line displays the file name and line number for each address. In the second, addr2line reads hexadecimal addresses from standard input, and prints the file name and line number for each address on standard output. In this mode, addr2line may be used in a pipe to convert dynamically chosen addresses. The format of the output is FILENAME:LINENO . The file name and line number for each input address is printed on separate lines. If the -f option is used, then each FILENAME:LINENO line is preceded by FUNCTIONNAME which is the name of the function containing the address. If the -i option is used and the code at the given address is present there because of inlining by the compiler then the { FUNCTIONNAME } FILENAME:LINENO information for the inlining function will be displayed afterwards. This continues recursively until there is no more inlining to report. If the -a option is used then the output is prefixed by the input address. If the -p option is used then the output for each input address is displayed on one, possibly quite long, line. If -p is not used then the output is broken up into multiple lines, based on the paragraphs above. If the file name or function name can not be determined, addr2line will print two question marks in their place. If the line number can not be determined, addr2line will print 0.","Process Name":"cris-linux-gnu-addr2line","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-addr2line"}},{"Process":{"Description":"The GNU ar program creates, modifies, and extracts from archives. An archive is a single file holding a collection of other files in a structure that makes it possible to retrieve the original individual files (called members of the archive). The original files' contents, mode (permissions), timestamp, owner, and group are preserved in the archive, and can be restored on extraction. GNU ar can maintain archives whose members have names of any length; however, depending on how ar is configured on your system, a limit on member-name length may be imposed for compatibility with archive formats maintained with other tools. If it exists, the limit is often 15 characters (typical of formats related to a.out) or 16 characters (typical of formats related to coff). ar is considered a binary utility because archives of this sort are most often used as libraries holding commonly needed subroutines. ar creates an index to the symbols defined in relocatable object modules in the archive when you specify the modifier s. Once created, this index is updated in the archive whenever ar makes a change to its contents (save for the q update operation). An archive with such an index speeds up linking to the library, and allows routines in the library to call each other without regard to their placement in the archive. You may use nm -s or nm --print-armap to list this index table. If an archive lacks the table, another form of ar called ranlib can be used to add just the table. GNU ar can optionally create a thin archive, which contains a symbol index and references to the original copies of the member files of the archives. Such an archive is useful for building libraries for use within a local build, where the relocatable objects are expected to remain available, and copying the contents of each object would only waste time and space. Thin archives are also flattened, so that adding one or more archives to a thin archive will add the elements of the nested archive individually. The paths to the elements of the archive are stored relative to the archive itself. GNU ar is designed to be compatible with two different facilities. You can control its activity using command-line options, like the different varieties of ar on Unix systems; or, if you specify the single command-line option -M, you can control it with a script supplied via standard input, like the MRI \"librarian\" program.","Process Name":"cris-linux-gnu-ar","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-ar"}},{"Process":{"Description":"GNU as is really a family of assemblers. If you use (or have used) the GNU assembler on one architecture, you should find a fairly similar environment when you use it on another architecture. Each version has much in common with the others, including object file formats, most assembler directives (often called pseudo-ops) and assembler syntax. as is primarily intended to assemble the output of the GNU C compiler \"gcc\" for use by the linker \"ld\". Nevertheless, we've tried to make as assemble correctly everything that other assemblers for the same machine would assemble. Any exceptions are documented explicitly. This doesn't mean as always uses the same syntax as another assembler for the same architecture; for example, we know of several incompatible versions of 680x0 assembly language syntax. Each time you run as it assembles exactly one source program. The source program is made up of one or more files. (The standard input is also a file.) You give as a command line that has zero or more input file names. The input files are read (from left file name to right). A command line argument (in any position) that has no special meaning is taken to be an input file name. If you give as no file names it attempts to read one input file from the as standard input, which is normally your terminal. You may have to type ctl-D to tell as there is no more program to assemble. Use -- if you need to explicitly name the standard input file in your command line. If the source is empty, as produces a small, empty object file. as may write warnings and error messages to the standard error file (usually your terminal). This should not happen when a compiler runs as automatically. Warnings report an assumption made so that as could keep assembling a flawed program; errors report a grave problem that stops the assembly. If you are invoking as via the GNU C compiler, you can use the -Wa option to pass arguments through to the assembler. The assembler arguments must be separated from each other (and the -Wa) by commas. For example: gcc -c -g -O -Wa,-alh,-L file.c This passes two options to the assembler: -alh (emit a listing to standard output with high-level and assembly source) and -L (retain local symbols in the symbol table). Usually you do not need to use this -Wa mechanism, since many compiler command-line options are automatically passed to the assembler by the compiler. (You can call the GNU compiler driver with the -v option to see precisely what options it passes to each compilation pass, including the assembler.)","Process Name":"cris-linux-gnu-as","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-as"}},{"Process":{"Description":"The C ++ and Java languages provide function overloading, which means that you can write many functions with the same name, providing that each function takes parameters of different types. In order to be able to distinguish these similarly named functions C ++ and Java encode them into a low-level assembler name which uniquely identifies each different version. This process is known as mangling. The c++filt [1] program does the inverse mapping: it decodes (demangles) low-level names into user-level names so that they can be read. Every alphanumeric word (consisting of letters, digits, underscores, dollars, or periods) seen in the input is a potential mangled name. If the name decodes into a C ++ name, the C ++ name replaces the low-level name in the output, otherwise the original word is output. In this way you can pass an entire assembler source file, containing mangled names, through c++filt and see the same source file containing demangled names. You can also use c++filt to decipher individual symbols by passing them on the command line: c++filt <symbol> If no symbol arguments are given, c++filt reads symbol names from the standard input instead. All the results are printed on the standard output. The difference between reading names from the command line versus reading names from the standard input is that command line arguments are expected to be just mangled names and no checking is performed to separate them from surrounding text. Thus for example: c++filt -n _Z1fv will work and demangle the name to \"f()\" whereas: c++filt -n _Z1fv, will not work. (Note the extra comma at the end of the mangled name which makes it invalid). This command however will work: echo _Z1fv, | c++filt -n and will display \"f(),\", i.e., the demangled name followed by a trailing comma. This behaviour is because when the names are read from the standard input it is expected that they might be part of an assembler source file where there might be extra, extraneous characters trailing after a mangled name. For example: .type   _Z1fv, @function","Process Name":"cris-linux-gnu-c++filt","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-c++filt"}},{"Process":{"Description":"The C preprocessor, often known as cpp, is a macro processor that is used automatically by the C compiler to transform your program before compilation. It is called a macro processor because it allows you to define macros, which are brief abbreviations for longer constructs. The C preprocessor is intended to be used only with C, C ++ , and Objective-C source code. In the past, it has been abused as a general text processor. It will choke on input which does not obey C's lexical rules. For example, apostrophes will be interpreted as the beginning of character constants, and cause errors. Also, you cannot rely on it preserving characteristics of the input which are not significant to C-family languages. If a Makefile is preprocessed, all the hard tabs will be removed, and the Makefile will not work. Having said that, you can often get away with using cpp on things which are not C. Other Algol-ish programming languages are often safe (Pascal, Ada, etc.) So is assembly, with caution. -traditional-cpp mode preserves more white space, and is otherwise more permissive. Many of the problems can be avoided by writing C or C ++ style comments instead of native language comments, and keeping macros simple. Wherever possible, you should use a preprocessor geared to the language you are writing in. Modern versions of the GNU assembler have macro facilities. Most high level programming languages have their own conditional compilation and inclusion mechanism. If all else fails, try a true general text processor, such as GNU M4. C preprocessors vary in some details. This manual discusses the GNU C preprocessor, which provides a small superset of the features of ISO Standard C. In its default mode, the GNU C preprocessor does not do a few things required by the standard. These are features which are rarely, if ever, used, and may cause surprising changes to the meaning of a program which does not expect them. To get strict ISO Standard C, you should use the -std=c90, -std=c99 or -std=c11 options, depending on which version of the standard you want. To get all the mandatory diagnostics, you must also use -pedantic. This manual describes the behavior of the ISO preprocessor. To minimize gratuitous differences, where the ISO preprocessor's behavior does not conflict with traditional semantics, the traditional preprocessor should behave the same way. The various differences that do exist are detailed in the section Traditional Mode. For clarity, unless noted otherwise, references to CPP in this manual refer to GNU CPP .","Process Name":"cris-linux-gnu-cpp","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-cpp"}},{"Process":{"Description":"dlltool reads its inputs, which can come from the -d and -b options as well as object files specified on the command line. It then processes these inputs and if the -e option has been specified it creates a exports file. If the -l option has been specified it creates a library file and if the -z option has been specified it creates a def file. Any or all of the -e, -l and -z options can be present in one invocation of dlltool. When creating a DLL , along with the source for the DLL , it is necessary to have three other files. dlltool can help with the creation of these files. The first file is a .def file which specifies which functions are exported from the DLL , which functions the DLL imports, and so on. This is a text file and can be created by hand, or dlltool can be used to create it using the -z option. In this case dlltool will scan the object files specified on its command line looking for those functions which have been specially marked as being exported and put entries for them in the .def file it creates. In order to mark a function as being exported from a DLL , it needs to have an -export:<name_of_function> entry in the .drectve section of the object file. This can be done in C by using the asm() operator: asm (\".section .drectve\");\nasm (\".ascii \\\"-export:my_func\\\"\");\n\nint my_func (void) { ... } The second file needed for DLL creation is an exports file. This file is linked with the object files that make up the body of the DLL and it handles the interface between the DLL and the outside world. This is a binary file and it can be created by giving the -e option to dlltool when it is creating or reading in a .def file. The third file needed for DLL creation is the library file that programs will link with in order to access the functions in the DLL (an 'import library'). This file can be created by giving the -l option to dlltool when it is creating or reading in a .def file. If the -y option is specified, dlltool generates a delay-import library that can be used instead of the normal import library to allow a program to link to the dll only as soon as an imported function is called for the first time. The resulting executable will need to be linked to the static delayimp library containing __delayLoadHelper2(), which in turn will import LoadLibraryA and GetProcAddress from kernel32. dlltool builds the library file by hand, but it builds the exports file by creating temporary files containing assembler statements and then assembling these. The -S command line option can be used to specify the path to the assembler that dlltool will use, and the -f option can be used to pass specific flags to that assembler. The -n can be used to prevent dlltool from deleting these temporary assembler files when it is done, and if -n is specified twice then this will prevent dlltool from deleting the temporary object files it used to build the library. Here is an example of creating a DLL from a source file dll.c and also creating a program (from an object file called program.o) that uses that DLL: gcc -c dll.c\ndlltool -e exports.o -l dll.lib dll.o\ngcc dll.o exports.o -o dll.dll\ngcc program.o dll.lib -o program dlltool may also be used to query an existing import library to determine the name of the DLL to which it is associated. See the description of the -I or --identify option.","Process Name":"cris-linux-gnu-dlltool","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-dlltool"}},{"Process":{"Description":"elfedit updates the ELF header of ELF files which have the matching ELF machine and file types. The options control how and which fields in the ELF header should be updated. elffile... are the ELF files to be updated. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files.","Process Name":"cris-linux-gnu-elfedit","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-elfedit"}},{"Process":{"Description":"When you invoke GCC , it normally does preprocessing, compilation, assembly and linking. The \"overall options\" allow you to stop this process at an intermediate stage. For example, the -c option says not to run the linker. Then the output consists of object files output by the assembler. Other options are passed on to one stage of processing. Some options control the preprocessor and others the compiler itself. Yet other options control the assembler and linker; most of these are not documented here, since you rarely need to use any of them. Most of the command-line options that you can use with GCC are useful for C programs; when an option is only useful with another language (usually C ++ ), the explanation says so explicitly. If the description for a particular option does not mention a source language, you can use that option with all supported languages. The gcc program accepts options and file names as operands. Many options have multi-letter names; therefore multiple single-letter options may not be grouped: -dv is very different from -d -v. You can mix options and other arguments. For the most part, the order you use doesn't matter. Order does matter when you use several options of the same kind; for example, if you specify -L more than once, the directories are searched in the order specified. Also, the placement of the -l option is significant. Many options have long names starting with -f or with -W---for example, -fmove-loop-invariants, -Wformat and so on. Most of these have both positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"cris-linux-gnu-gcc","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-gcc"}},{"Process":{"Description":"gcov is a test coverage program. Use it in concert with GCC to analyze your programs to help create more efficient, faster running code and to discover untested parts of your program. You can use gcov as a profiling tool to help discover where your optimization efforts will best affect your code. You can also use gcov along with the other profiling tool, gprof, to assess which parts of your code use the greatest amount of computing time. Profiling tools help you analyze your code's performance. Using a profiler such as gcov or gprof, you can find out some basic performance statistics, such as: \u2022 how often each line of code executes \u2022 what lines of code are actually executed \u2022 how much computing time each section of code uses Once you know these things about how your code works when compiled, you can look at each module to see which modules should be optimized. gcov helps you determine where to work on optimization. Software developers also use coverage testing in concert with testsuites, to make sure software is actually good enough for a release. Testsuites can verify that a program works as expected; a coverage program tests to see how much of the program is exercised by the testsuite. Developers can then determine what kinds of test cases need to be added to the testsuites to create both better testing and a better final product. You should compile your code without optimization if you plan to use gcov because the optimization, by combining some lines of code into one function, may not give you as much information as you need to look for 'hot spots' where the code is using a great deal of computer time. Likewise, because gcov accumulates statistics by line (at the lowest resolution), it works best with a programming style that places only one statement on each line. If you use complicated macros that expand to loops or to other control structures, the statistics are less helpful---they only report on the line where the macro call appears. If your complex macros behave like functions, you can replace them with inline functions to solve this problem. gcov creates a logfile called sourcefile.gcov which indicates how many times each line of a source file sourcefile.c has executed. You can use these logfiles along with gprof to aid in fine-tuning the performance of your programs. gprof gives timing information you can use along with the information you get from gcov. gcov works only on code compiled with GCC . It is not compatible with any other profiling or test coverage mechanism.","Process Name":"cris-linux-gnu-gcov","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-gcov"}},{"Process":{"Description":"\"gprof\" produces an execution profile of C, Pascal, or Fortran77 programs. The effect of called routines is incorporated in the profile of each caller. The profile data is taken from the call graph profile file (gmon.out default) which is created by programs that are compiled with the -pg option of \"cc\", \"pc\", and \"f77\". The -pg option also links in versions of the library routines that are compiled for profiling. \"Gprof\" reads the given object file (the default is \"a.out\") and establishes the relation between its symbol table and the call graph profile from gmon.out. If more than one profile file is specified, the \"gprof\" output shows the sum of the profile information in the given profile files. \"Gprof\" calculates the amount of time spent in each routine. Next, these times are propagated along the edges of the call graph. Cycles are discovered, and calls into a cycle are made to share the time of the cycle. Several forms of output are available from the analysis. The flat profile shows how much time your program spent in each function, and how many times that function was called. If you simply want to know which functions burn most of the cycles, it is stated concisely here. The call graph shows, for each function, which functions called it, which other functions it called, and how many times. There is also an estimate of how much time was spent in the subroutines of each function. This can suggest places where you might try to eliminate function calls that use a lot of time. The annotated source listing is a copy of the program's source code, labeled with the number of times each line of the program was executed.","Process Name":"cris-linux-gnu-gprof","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-gprof"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"cris-linux-gnu-ld","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-ld"}},{"Process":{"Description":"nlmconv converts the relocatable i386 object file infile into the NetWare Loadable Module outfile, optionally reading headerfile for NLM header information. For instructions on writing the NLM command file language used in header files, see the linkers section, NLMLINK in particular, of the NLM Development and Tools Overview, which is part of the NLM Software Developer's Kit (\" NLM SDK \"), available from Novell, Inc. nlmconv uses the GNU Binary File Descriptor library to read infile; nlmconv can perform a link step. In other words, you can list more than one object file for input if you list them in the definitions file (rather than simply specifying one input file on the command line). In this case, nlmconv calls the linker for you.","Process Name":"cris-linux-gnu-nlmconv","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-nlmconv"}},{"Process":{"Description":"GNU nm lists the symbols from object files objfile.... If no object files are listed as arguments, nm assumes the file a.out. For each symbol, nm shows: \u2022 The symbol value, in the radix selected by options (see below), or hexadecimal by default. \u2022 The symbol type. At least the following types are used; others are, as well, depending on the object file format. If lowercase, the symbol is usually local; if uppercase, the symbol is global (external). There are however a few lowercase symbols that are shown for special global symbols (\"u\", \"v\" and \"w\"). \"A\" The symbol's value is absolute, and will not be changed by further linking. \"B\" \"b\" The symbol is in the uninitialized data section (known as BSS ). \"C\" The symbol is common. Common symbols are uninitialized data. When linking, multiple common symbols may appear with the same name. If the symbol is defined anywhere, the common symbols are treated as undefined references. \"D\" \"d\" The symbol is in the initialized data section. \"G\" \"g\" The symbol is in an initialized data section for small objects. Some object file formats permit more efficient access to small data objects, such as a global int variable as opposed to a large global array. \"i\" For PE format files this indicates that the symbol is in a section specific to the implementation of DLLs. For ELF format files this indicates that the symbol is an indirect function. This is a GNU extension to the standard set of ELF symbol types. It indicates a symbol which if referenced by a relocation does not evaluate to its address, but instead must be invoked at runtime. The runtime execution will then return the value to be used in the relocation. \"N\" The symbol is a debugging symbol. \"p\" The symbols is in a stack unwind section. \"R\" \"r\" The symbol is in a read only data section. \"S\" \"s\" The symbol is in an uninitialized data section for small objects. \"T\" \"t\" The symbol is in the text (code) section. \"U\" The symbol is undefined. \"u\" The symbol is a unique global symbol. This is a GNU extension to the standard set of ELF symbol bindings. For such a symbol the dynamic linker will make sure that in the entire process there is just one symbol with this name and type in use. \"V\" \"v\" The symbol is a weak object. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the weak symbol becomes zero with no error. On some systems, uppercase indicates that a default value has been specified. \"W\" \"w\" The symbol is a weak symbol that has not been specifically tagged as a weak object symbol. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the symbol is determined in a system-specific manner without error. On some systems, uppercase indicates that a default value has been specified. \"-\" The symbol is a stabs symbol in an a.out object file. In this case, the next values printed are the stabs other field, the stabs desc field, and the stab type. Stabs symbols are used to hold debugging information. \"?\" The symbol type is unknown, or object file format specific. \u2022 The symbol name.","Process Name":"cris-linux-gnu-nm","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-nm"}},{"Process":{"Description":"The GNU objcopy utility copies the contents of an object file to another. objcopy uses the GNU BFD Library to read and write the object files. It can write the destination object file in a format different from that of the source object file. The exact behavior of objcopy is controlled by command-line options. Note that objcopy should be able to copy a fully linked file between any two formats. However, copying a relocatable object file between any two formats may not work as expected. objcopy creates temporary files to do its translations and deletes them afterward. objcopy uses BFD to do all its translation work; it has access to all the formats described in BFD and thus is able to recognize most formats without being told explicitly. objcopy can be used to generate S-records by using an output target of srec (e.g., use -O srec). objcopy can be used to generate a raw binary file by using an output target of binary (e.g., use -O binary). When objcopy generates a raw binary file, it will essentially produce a memory dump of the contents of the input object file. All symbols and relocation information will be discarded. The memory dump will start at the load address of the lowest section copied into the output file. When generating an S-record or a raw binary file, it may be helpful to use -S to remove sections containing debugging information. In some cases -R will be useful to remove sections which contain information that is not needed by the binary file. Note---objcopy is not able to change the endianness of its input files. If the input format has an endianness (some formats do not), objcopy can only copy the inputs into file formats that have the same endianness or which have no endianness (e.g., srec). (However, see the --reverse-bytes option.)","Process Name":"cris-linux-gnu-objcopy","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-objcopy"}},{"Process":{"Description":"objdump displays information about one or more object files. The options control what particular information to display. This information is mostly useful to programmers who are working on the compilation tools, as opposed to programmers who just want their program to compile and work. objfile... are the object files to be examined. When you specify archives, objdump shows information on each of the member object files.","Process Name":"cris-linux-gnu-objdump","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-objdump"}},{"Process":{"Description":"ranlib generates an index to the contents of an archive and stores it in the archive. The index lists each symbol defined by a member of an archive that is a relocatable object file. You may use nm -s or nm --print-armap to list this index. An archive with such an index speeds up linking to the library and allows routines in the library to call each other without regard to their placement in the archive. The GNU ranlib program is another form of GNU ar; running ranlib is completely equivalent to executing ar -s.","Process Name":"cris-linux-gnu-ranlib","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-ranlib"}},{"Process":{"Description":"readelf displays information about one or more ELF format object files. The options control what particular information to display. elffile... are the object files to be examined. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files. This program performs a similar function to objdump but it goes into more detail and it exists independently of the BFD library, so if there is a bug in BFD then readelf will not be affected.","Process Name":"cris-linux-gnu-readelf","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-readelf"}},{"Process":{"Description":"The GNU size utility lists the section sizes---and the total size---for each of the object or archive files objfile in its argument list. By default, one line of output is generated for each object file or each module in an archive. objfile... are the object files to be examined. If none are specified, the file \"a.out\" will be used.","Process Name":"cris-linux-gnu-size","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-size"}},{"Process":{"Description":"For each file given, GNU strings prints the printable character sequences that are at least 4 characters long (or the number given with the options below) and are followed by an unprintable character. By default, it only prints the strings from the initialized and loaded sections of object files; for other types of files, it prints the strings from the whole file. strings is mainly useful for determining the contents of non-text files.","Process Name":"cris-linux-gnu-strings","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-strings"}},{"Process":{"Description":"GNU strip discards all symbols from object files objfile. The list of object files may include archives. At least one object file must be given. strip modifies the files named in its argument, rather than writing modified copies under different names.","Process Name":"cris-linux-gnu-strip","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-strip"}},{"Process":{"Description":"windmc reads message definitions from an input file (.mc) and translate them into a set of output files. The output files may be of four kinds: \"h\" A C header file containing the message definitions. \"rc\" A resource file compilable by the windres tool. \"bin\" One or more binary files containing the resource data for a specific message language. \"dbg\" A C include file that maps message id's to their symbolic name. The exact description of these different formats is available in documentation from Microsoft. When windmc converts from the \"mc\" format to the \"bin\" format, \"rc\", \"h\", and optional \"dbg\" it is acting like the Windows Message Compiler.","Process Name":"cris-linux-gnu-windmc","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-windmc"}},{"Process":{"Description":"windres reads resources from an input file and copies them into an output file. Either file may be in one of three formats: \"rc\" A text format read by the Resource Compiler. \"res\" A binary format generated by the Resource Compiler. \"coff\" A COFF object or executable. The exact description of these different formats is available in documentation from Microsoft. When windres converts from the \"rc\" format to the \"res\" format, it is acting like the Windows Resource Compiler. When windres converts from the \"res\" format to the \"coff\" format, it is acting like the Windows \"CVTRES\" program. When windres generates an \"rc\" file, the output is similar but not identical to the format expected for the input. When an input \"rc\" file refers to an external filename, an output \"rc\" file will instead include the file contents. If the input or output format is not specified, windres will guess based on the file name, or, for the input file, the file contents. A file with an extension of .rc will be treated as an \"rc\" file, a file with an extension of .res will be treated as a \"res\" file, and a file with an extension of .o or .exe will be treated as a \"coff\" file. If no output file is specified, windres will print the resources in \"rc\" format to standard output. The normal use is for you to write an \"rc\" file, use windres to convert it to a COFF object file, and then link the COFF file into your application. This will make the resources described in the \"rc\" file available to Windows.","Process Name":"cris-linux-gnu-windres","Link":"https:\/\/linux.die.net\/man\/1\/cris-linux-gnu-windres"}},{"Process":{"Description":"The critical program displays a self-organizing critical system that gradually emerges from chaos. critical performs a simulation on a two-dimensional array of integers. The array is initialized to random values. On each iteration, it draws a line to the array position with the greatest value. It then replaces that location and the eight neighboring locations with randomly-selected values. The lines are initially random, but over time a chaotic self-organizing system evolves: areas of the screen which happen to have lower values are less likely to be updated to new values, and so the line tends to avoid those areas. Eventually, the histogram of changes approaches the power-law curve typical of such systems. The simplest documented self-organizing system is the one-dimensional equivalent of critical. I heard about this algorithm second-hand: apparently there was an article in Scientific American describing it sometime in 1997.","Process Name":"critical","Link":"https:\/\/linux.die.net\/man\/1\/critical"}},{"Process":{"Description":"The crl command processes CRL files in DER or PEM format.","Process Name":"crl","Link":"https:\/\/linux.die.net\/man\/1\/crl"}},{"Process":{"Description":"The crl2pkcs7 command takes an optional CRL and one or more certificates and converts them into a PKCS#7 degenerate \"certificates only\" structure.","Process Name":"crl2pkcs7","Link":"https:\/\/linux.die.net\/man\/1\/crl2pkcs7"}},{"Process":{"Description":"cronolog is a simple program that reads log messages from its input and writes them to a set of output files, the names of which are constructed using template and the current date and time. The template uses the same format specifiers as the Unix date(1) command (which are the same as the standard C strftime library function). Before writing a message cronolog checks the time to see whether the current log file is still valid and if not it closes the current file, expands the template using the current date and time to generate a new file name, opens the new file (creating missing directories on the path of the new log file as needed unless the program is compiled with -DDONT_CREATE_SUBDIRS) and calculates the time at which the new file will become invalid. cronolog is intended to be used in conjunction with a Web server, such as Apache to split the access log into daily or monthly logs. For example the Apache configuration directives: TransferLog \"|\/usr\/sbin\/cronolog \/www\/logs\/%Y\/%m\/%d\/access.log\" ErrorLog \"|\/usr\/sbin\/cronolog \/www\/logs\/%Y\/%m\/%d\/errors.log\" would instruct Apache to pipe its access and error log messages into separate copies of cronolog, which would create new log files each day in a directory hierarchy structured by date, i.e. on 31 December 1996 messages would be written to \/www\/logs\/1996\/12\/31\/access.log \/www\/logs\/1996\/12\/31\/errors.log after midnight the files \/www\/logs\/1997\/01\/01\/access.log \/www\/logs\/1997\/01\/01\/errors.log would be used, with the directories 1997, 1997\/01 and 1997\/01\/01 being created if they did not already exist. (Note that prior to version 1.2 Apache did not allow a program to be specified as the argument of the ErrorLog directive.)","Process Name":"cronolog","Link":"https:\/\/linux.die.net\/man\/1\/cronolog"}},{"Process":{"Description":"cronosplit is a simple program that reads lines from a set of input log files, which must be in Common Log Format and write each lines to an output files, the name of which is constructed using the template specified and timestamp from the the line. The template uses the same format specifiers as the Unix date(1) command (which are the same as the standard C strftime library function).","Process Name":"cronosplit","Link":"https:\/\/linux.die.net\/man\/1\/cronosplit"}},{"Process":{"Description":"Crontab is the program used to install, remove or list the tables used to drive the cron(8) daemon. Each user can have their own crontab, and though these are files in \/var\/spool\/ , they are not intended to be edited directly. For SELinux in mls mode can be even more crontabs - for each range. For more see selinux(8). The cron jobs could be allow or disallow for different users. For classical crontab there exists cron.allow and cron.deny files. If cron.allow file exists, then you must be listed therein in order to be allowed to use this command. If the cron.allow file does not exist but the cron.deny file does exist, then you must not be listed in the cron.deny file in order to use this command. If neither of these files exists, only the super user will be allowed to use this command. The second option is using PAM authentication, where you set up users, which could or couldn't use crontab and also system cron jobs from \/etc\/cron.d\/. The temporary directory could be set in enviroment variables. If it's not set by user than \/tmp is used.","Process Name":"crontab","Link":"https:\/\/linux.die.net\/man\/1\/crontab"}},{"Process":{"Description":"","Process Name":"cross","Link":"https:\/\/linux.die.net\/man\/1\/cross"}},{"Process":{"Description":"addr2line translates addresses into file names and line numbers. Given an address in an executable or an offset in a section of a relocatable object, it uses the debugging information to figure out which file name and line number are associated with it. The executable or relocatable object to use is specified with the -e option. The default is the file a.out. The section in the relocatable object to use is specified with the -j option. addr2line has two modes of operation. In the first, hexadecimal addresses are specified on the command line, and addr2line displays the file name and line number for each address. In the second, addr2line reads hexadecimal addresses from standard input, and prints the file name and line number for each address on standard output. In this mode, addr2line may be used in a pipe to convert dynamically chosen addresses. The format of the output is FILENAME:LINENO . The file name and line number for each input address is printed on separate lines. If the -f option is used, then each FILENAME:LINENO line is preceded by FUNCTIONNAME which is the name of the function containing the address. If the -i option is used and the code at the given address is present there because of inlining by the compiler then the { FUNCTIONNAME } FILENAME:LINENO information for the inlining function will be displayed afterwards. This continues recursively until there is no more inlining to report. If the -a option is used then the output is prefixed by the input address. If the -p option is used then the output for each input address is displayed on one, possibly quite long, line. If -p is not used then the output is broken up into multiple lines, based on the paragraphs above. If the file name or function name can not be determined, addr2line will print two question marks in their place. If the line number can not be determined, addr2line will print 0.","Process Name":"cross-addr2line","Link":"https:\/\/linux.die.net\/man\/1\/cross-addr2line"}},{"Process":{"Description":"The GNU ar program creates, modifies, and extracts from archives. An archive is a single file holding a collection of other files in a structure that makes it possible to retrieve the original individual files (called members of the archive). The original files' contents, mode (permissions), timestamp, owner, and group are preserved in the archive, and can be restored on extraction. GNU ar can maintain archives whose members have names of any length; however, depending on how ar is configured on your system, a limit on member-name length may be imposed for compatibility with archive formats maintained with other tools. If it exists, the limit is often 15 characters (typical of formats related to a.out) or 16 characters (typical of formats related to coff). ar is considered a binary utility because archives of this sort are most often used as libraries holding commonly needed subroutines. ar creates an index to the symbols defined in relocatable object modules in the archive when you specify the modifier s. Once created, this index is updated in the archive whenever ar makes a change to its contents (save for the q update operation). An archive with such an index speeds up linking to the library, and allows routines in the library to call each other without regard to their placement in the archive. You may use nm -s or nm --print-armap to list this index table. If an archive lacks the table, another form of ar called ranlib can be used to add just the table. GNU ar can optionally create a thin archive, which contains a symbol index and references to the original copies of the member files of the archives. Such an archive is useful for building libraries for use within a local build, where the relocatable objects are expected to remain available, and copying the contents of each object would only waste time and space. Thin archives are also flattened, so that adding one or more archives to a thin archive will add the elements of the nested archive individually. The paths to the elements of the archive are stored relative to the archive itself. GNU ar is designed to be compatible with two different facilities. You can control its activity using command-line options, like the different varieties of ar on Unix systems; or, if you specify the single command-line option -M, you can control it with a script supplied via standard input, like the MRI \"librarian\" program.","Process Name":"cross-ar","Link":"https:\/\/linux.die.net\/man\/1\/cross-ar"}},{"Process":{"Description":"GNU as is really a family of assemblers. If you use (or have used) the GNU assembler on one architecture, you should find a fairly similar environment when you use it on another architecture. Each version has much in common with the others, including object file formats, most assembler directives (often called pseudo-ops) and assembler syntax. as is primarily intended to assemble the output of the GNU C compiler \"gcc\" for use by the linker \"ld\". Nevertheless, we've tried to make as assemble correctly everything that other assemblers for the same machine would assemble. Any exceptions are documented explicitly. This doesn't mean as always uses the same syntax as another assembler for the same architecture; for example, we know of several incompatible versions of 680x0 assembly language syntax. Each time you run as it assembles exactly one source program. The source program is made up of one or more files. (The standard input is also a file.) You give as a command line that has zero or more input file names. The input files are read (from left file name to right). A command line argument (in any position) that has no special meaning is taken to be an input file name. If you give as no file names it attempts to read one input file from the as standard input, which is normally your terminal. You may have to type ctl-D to tell as there is no more program to assemble. Use -- if you need to explicitly name the standard input file in your command line. If the source is empty, as produces a small, empty object file. as may write warnings and error messages to the standard error file (usually your terminal). This should not happen when a compiler runs as automatically. Warnings report an assumption made so that as could keep assembling a flawed program; errors report a grave problem that stops the assembly. If you are invoking as via the GNU C compiler, you can use the -Wa option to pass arguments through to the assembler. The assembler arguments must be separated from each other (and the -Wa) by commas. For example: gcc -c -g -O -Wa,-alh,-L file.c This passes two options to the assembler: -alh (emit a listing to standard output with high-level and assembly source) and -L (retain local symbols in the symbol table). Usually you do not need to use this -Wa mechanism, since many compiler command-line options are automatically passed to the assembler by the compiler. (You can call the GNU compiler driver with the -v option to see precisely what options it passes to each compilation pass, including the assembler.)","Process Name":"cross-as","Link":"https:\/\/linux.die.net\/man\/1\/cross-as"}},{"Process":{"Description":"The C ++ and Java languages provide function overloading, which means that you can write many functions with the same name, providing that each function takes parameters of different types. In order to be able to distinguish these similarly named functions C ++ and Java encode them into a low-level assembler name which uniquely identifies each different version. This process is known as mangling. The c++filt [1] program does the inverse mapping: it decodes (demangles) low-level names into user-level names so that they can be read. Every alphanumeric word (consisting of letters, digits, underscores, dollars, or periods) seen in the input is a potential mangled name. If the name decodes into a C ++ name, the C ++ name replaces the low-level name in the output, otherwise the original word is output. In this way you can pass an entire assembler source file, containing mangled names, through c++filt and see the same source file containing demangled names. You can also use c++filt to decipher individual symbols by passing them on the command line: c++filt <symbol> If no symbol arguments are given, c++filt reads symbol names from the standard input instead. All the results are printed on the standard output. The difference between reading names from the command line versus reading names from the standard input is that command line arguments are expected to be just mangled names and no checking is performed to separate them from surrounding text. Thus for example: c++filt -n _Z1fv will work and demangle the name to \"f()\" whereas: c++filt -n _Z1fv, will not work. (Note the extra comma at the end of the mangled name which makes it invalid). This command however will work: echo _Z1fv, | c++filt -n and will display \"f(),\", i.e., the demangled name followed by a trailing comma. This behaviour is because when the names are read from the standard input it is expected that they might be part of an assembler source file where there might be extra, extraneous characters trailing after a mangled name. For example: .type   _Z1fv, @function","Process Name":"cross-c++filt","Link":"https:\/\/linux.die.net\/man\/1\/cross-c++filt"}},{"Process":{"Description":"The C preprocessor, often known as cpp, is a macro processor that is used automatically by the C compiler to transform your program before compilation. It is called a macro processor because it allows you to define macros, which are brief abbreviations for longer constructs. The C preprocessor is intended to be used only with C, C ++ , and Objective-C source code. In the past, it has been abused as a general text processor. It will choke on input which does not obey C's lexical rules. For example, apostrophes will be interpreted as the beginning of character constants, and cause errors. Also, you cannot rely on it preserving characteristics of the input which are not significant to C-family languages. If a Makefile is preprocessed, all the hard tabs will be removed, and the Makefile will not work. Having said that, you can often get away with using cpp on things which are not C. Other Algol-ish programming languages are often safe (Pascal, Ada, etc.) So is assembly, with caution. -traditional-cpp mode preserves more white space, and is otherwise more permissive. Many of the problems can be avoided by writing C or C ++ style comments instead of native language comments, and keeping macros simple. Wherever possible, you should use a preprocessor geared to the language you are writing in. Modern versions of the GNU assembler have macro facilities. Most high level programming languages have their own conditional compilation and inclusion mechanism. If all else fails, try a true general text processor, such as GNU M4. C preprocessors vary in some details. This manual discusses the GNU C preprocessor, which provides a small superset of the features of ISO Standard C. In its default mode, the GNU C preprocessor does not do a few things required by the standard. These are features which are rarely, if ever, used, and may cause surprising changes to the meaning of a program which does not expect them. To get strict ISO Standard C, you should use the -std=c90, -std=c99 or -std=c11 options, depending on which version of the standard you want. To get all the mandatory diagnostics, you must also use -pedantic. This manual describes the behavior of the ISO preprocessor. To minimize gratuitous differences, where the ISO preprocessor's behavior does not conflict with traditional semantics, the traditional preprocessor should behave the same way. The various differences that do exist are detailed in the section Traditional Mode. For clarity, unless noted otherwise, references to CPP in this manual refer to GNU CPP .","Process Name":"cross-cpp","Link":"https:\/\/linux.die.net\/man\/1\/cross-cpp"}},{"Process":{"Description":"dlltool reads its inputs, which can come from the -d and -b options as well as object files specified on the command line. It then processes these inputs and if the -e option has been specified it creates a exports file. If the -l option has been specified it creates a library file and if the -z option has been specified it creates a def file. Any or all of the -e, -l and -z options can be present in one invocation of dlltool. When creating a DLL , along with the source for the DLL , it is necessary to have three other files. dlltool can help with the creation of these files. The first file is a .def file which specifies which functions are exported from the DLL , which functions the DLL imports, and so on. This is a text file and can be created by hand, or dlltool can be used to create it using the -z option. In this case dlltool will scan the object files specified on its command line looking for those functions which have been specially marked as being exported and put entries for them in the .def file it creates. In order to mark a function as being exported from a DLL , it needs to have an -export:<name_of_function> entry in the .drectve section of the object file. This can be done in C by using the asm() operator: asm (\".section .drectve\");\nasm (\".ascii \\\"-export:my_func\\\"\");\n\nint my_func (void) { ... } The second file needed for DLL creation is an exports file. This file is linked with the object files that make up the body of the DLL and it handles the interface between the DLL and the outside world. This is a binary file and it can be created by giving the -e option to dlltool when it is creating or reading in a .def file. The third file needed for DLL creation is the library file that programs will link with in order to access the functions in the DLL (an 'import library'). This file can be created by giving the -l option to dlltool when it is creating or reading in a .def file. If the -y option is specified, dlltool generates a delay-import library that can be used instead of the normal import library to allow a program to link to the dll only as soon as an imported function is called for the first time. The resulting executable will need to be linked to the static delayimp library containing __delayLoadHelper2(), which in turn will import LoadLibraryA and GetProcAddress from kernel32. dlltool builds the library file by hand, but it builds the exports file by creating temporary files containing assembler statements and then assembling these. The -S command line option can be used to specify the path to the assembler that dlltool will use, and the -f option can be used to pass specific flags to that assembler. The -n can be used to prevent dlltool from deleting these temporary assembler files when it is done, and if -n is specified twice then this will prevent dlltool from deleting the temporary object files it used to build the library. Here is an example of creating a DLL from a source file dll.c and also creating a program (from an object file called program.o) that uses that DLL: gcc -c dll.c\ndlltool -e exports.o -l dll.lib dll.o\ngcc dll.o exports.o -o dll.dll\ngcc program.o dll.lib -o program dlltool may also be used to query an existing import library to determine the name of the DLL to which it is associated. See the description of the -I or --identify option.","Process Name":"cross-dlltool","Link":"https:\/\/linux.die.net\/man\/1\/cross-dlltool"}},{"Process":{"Description":"elfedit updates the ELF header of ELF files which have the matching ELF machine and file types. The options control how and which fields in the ELF header should be updated. elffile... are the ELF files to be updated. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files.","Process Name":"cross-elfedit","Link":"https:\/\/linux.die.net\/man\/1\/cross-elfedit"}},{"Process":{"Description":"When you invoke GCC , it normally does preprocessing, compilation, assembly and linking. The \"overall options\" allow you to stop this process at an intermediate stage. For example, the -c option says not to run the linker. Then the output consists of object files output by the assembler. Other options are passed on to one stage of processing. Some options control the preprocessor and others the compiler itself. Yet other options control the assembler and linker; most of these are not documented here, since you rarely need to use any of them. Most of the command-line options that you can use with GCC are useful for C programs; when an option is only useful with another language (usually C ++ ), the explanation says so explicitly. If the description for a particular option does not mention a source language, you can use that option with all supported languages. The gcc program accepts options and file names as operands. Many options have multi-letter names; therefore multiple single-letter options may not be grouped: -dv is very different from -d -v. You can mix options and other arguments. For the most part, the order you use doesn't matter. Order does matter when you use several options of the same kind; for example, if you specify -L more than once, the directories are searched in the order specified. Also, the placement of the -l option is significant. Many options have long names starting with -f or with -W---for example, -fmove-loop-invariants, -Wformat and so on. Most of these have both positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"cross-gcc","Link":"https:\/\/linux.die.net\/man\/1\/cross-gcc"}},{"Process":{"Description":"gcov is a test coverage program. Use it in concert with GCC to analyze your programs to help create more efficient, faster running code and to discover untested parts of your program. You can use gcov as a profiling tool to help discover where your optimization efforts will best affect your code. You can also use gcov along with the other profiling tool, gprof, to assess which parts of your code use the greatest amount of computing time. Profiling tools help you analyze your code's performance. Using a profiler such as gcov or gprof, you can find out some basic performance statistics, such as: \u2022 how often each line of code executes \u2022 what lines of code are actually executed \u2022 how much computing time each section of code uses Once you know these things about how your code works when compiled, you can look at each module to see which modules should be optimized. gcov helps you determine where to work on optimization. Software developers also use coverage testing in concert with testsuites, to make sure software is actually good enough for a release. Testsuites can verify that a program works as expected; a coverage program tests to see how much of the program is exercised by the testsuite. Developers can then determine what kinds of test cases need to be added to the testsuites to create both better testing and a better final product. You should compile your code without optimization if you plan to use gcov because the optimization, by combining some lines of code into one function, may not give you as much information as you need to look for 'hot spots' where the code is using a great deal of computer time. Likewise, because gcov accumulates statistics by line (at the lowest resolution), it works best with a programming style that places only one statement on each line. If you use complicated macros that expand to loops or to other control structures, the statistics are less helpful---they only report on the line where the macro call appears. If your complex macros behave like functions, you can replace them with inline functions to solve this problem. gcov creates a logfile called sourcefile.gcov which indicates how many times each line of a source file sourcefile.c has executed. You can use these logfiles along with gprof to aid in fine-tuning the performance of your programs. gprof gives timing information you can use along with the information you get from gcov. gcov works only on code compiled with GCC . It is not compatible with any other profiling or test coverage mechanism.","Process Name":"cross-gcov","Link":"https:\/\/linux.die.net\/man\/1\/cross-gcov"}},{"Process":{"Description":"\"gprof\" produces an execution profile of C, Pascal, or Fortran77 programs. The effect of called routines is incorporated in the profile of each caller. The profile data is taken from the call graph profile file (gmon.out default) which is created by programs that are compiled with the -pg option of \"cc\", \"pc\", and \"f77\". The -pg option also links in versions of the library routines that are compiled for profiling. \"Gprof\" reads the given object file (the default is \"a.out\") and establishes the relation between its symbol table and the call graph profile from gmon.out. If more than one profile file is specified, the \"gprof\" output shows the sum of the profile information in the given profile files. \"Gprof\" calculates the amount of time spent in each routine. Next, these times are propagated along the edges of the call graph. Cycles are discovered, and calls into a cycle are made to share the time of the cycle. Several forms of output are available from the analysis. The flat profile shows how much time your program spent in each function, and how many times that function was called. If you simply want to know which functions burn most of the cycles, it is stated concisely here. The call graph shows, for each function, which functions called it, which other functions it called, and how many times. There is also an estimate of how much time was spent in the subroutines of each function. This can suggest places where you might try to eliminate function calls that use a lot of time. The annotated source listing is a copy of the program's source code, labeled with the number of times each line of the program was executed.","Process Name":"cross-gprof","Link":"https:\/\/linux.die.net\/man\/1\/cross-gprof"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"cross-ld","Link":"https:\/\/linux.die.net\/man\/1\/cross-ld"}},{"Process":{"Description":"nlmconv converts the relocatable i386 object file infile into the NetWare Loadable Module outfile, optionally reading headerfile for NLM header information. For instructions on writing the NLM command file language used in header files, see the linkers section, NLMLINK in particular, of the NLM Development and Tools Overview, which is part of the NLM Software Developer's Kit (\" NLM SDK \"), available from Novell, Inc. nlmconv uses the GNU Binary File Descriptor library to read infile; nlmconv can perform a link step. In other words, you can list more than one object file for input if you list them in the definitions file (rather than simply specifying one input file on the command line). In this case, nlmconv calls the linker for you.","Process Name":"cross-nlmconv","Link":"https:\/\/linux.die.net\/man\/1\/cross-nlmconv"}},{"Process":{"Description":"GNU nm lists the symbols from object files objfile.... If no object files are listed as arguments, nm assumes the file a.out. For each symbol, nm shows: \u2022 The symbol value, in the radix selected by options (see below), or hexadecimal by default. \u2022 The symbol type. At least the following types are used; others are, as well, depending on the object file format. If lowercase, the symbol is usually local; if uppercase, the symbol is global (external). There are however a few lowercase symbols that are shown for special global symbols (\"u\", \"v\" and \"w\"). \"A\" The symbol's value is absolute, and will not be changed by further linking. \"B\" \"b\" The symbol is in the uninitialized data section (known as BSS ). \"C\" The symbol is common. Common symbols are uninitialized data. When linking, multiple common symbols may appear with the same name. If the symbol is defined anywhere, the common symbols are treated as undefined references. \"D\" \"d\" The symbol is in the initialized data section. \"G\" \"g\" The symbol is in an initialized data section for small objects. Some object file formats permit more efficient access to small data objects, such as a global int variable as opposed to a large global array. \"i\" For PE format files this indicates that the symbol is in a section specific to the implementation of DLLs. For ELF format files this indicates that the symbol is an indirect function. This is a GNU extension to the standard set of ELF symbol types. It indicates a symbol which if referenced by a relocation does not evaluate to its address, but instead must be invoked at runtime. The runtime execution will then return the value to be used in the relocation. \"N\" The symbol is a debugging symbol. \"p\" The symbols is in a stack unwind section. \"R\" \"r\" The symbol is in a read only data section. \"S\" \"s\" The symbol is in an uninitialized data section for small objects. \"T\" \"t\" The symbol is in the text (code) section. \"U\" The symbol is undefined. \"u\" The symbol is a unique global symbol. This is a GNU extension to the standard set of ELF symbol bindings. For such a symbol the dynamic linker will make sure that in the entire process there is just one symbol with this name and type in use. \"V\" \"v\" The symbol is a weak object. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the weak symbol becomes zero with no error. On some systems, uppercase indicates that a default value has been specified. \"W\" \"w\" The symbol is a weak symbol that has not been specifically tagged as a weak object symbol. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the symbol is determined in a system-specific manner without error. On some systems, uppercase indicates that a default value has been specified. \"-\" The symbol is a stabs symbol in an a.out object file. In this case, the next values printed are the stabs other field, the stabs desc field, and the stab type. Stabs symbols are used to hold debugging information. \"?\" The symbol type is unknown, or object file format specific. \u2022 The symbol name.","Process Name":"cross-nm","Link":"https:\/\/linux.die.net\/man\/1\/cross-nm"}},{"Process":{"Description":"The GNU objcopy utility copies the contents of an object file to another. objcopy uses the GNU BFD Library to read and write the object files. It can write the destination object file in a format different from that of the source object file. The exact behavior of objcopy is controlled by command-line options. Note that objcopy should be able to copy a fully linked file between any two formats. However, copying a relocatable object file between any two formats may not work as expected. objcopy creates temporary files to do its translations and deletes them afterward. objcopy uses BFD to do all its translation work; it has access to all the formats described in BFD and thus is able to recognize most formats without being told explicitly. objcopy can be used to generate S-records by using an output target of srec (e.g., use -O srec). objcopy can be used to generate a raw binary file by using an output target of binary (e.g., use -O binary). When objcopy generates a raw binary file, it will essentially produce a memory dump of the contents of the input object file. All symbols and relocation information will be discarded. The memory dump will start at the load address of the lowest section copied into the output file. When generating an S-record or a raw binary file, it may be helpful to use -S to remove sections containing debugging information. In some cases -R will be useful to remove sections which contain information that is not needed by the binary file. Note---objcopy is not able to change the endianness of its input files. If the input format has an endianness (some formats do not), objcopy can only copy the inputs into file formats that have the same endianness or which have no endianness (e.g., srec). (However, see the --reverse-bytes option.)","Process Name":"cross-objcopy","Link":"https:\/\/linux.die.net\/man\/1\/cross-objcopy"}},{"Process":{"Description":"objdump displays information about one or more object files. The options control what particular information to display. This information is mostly useful to programmers who are working on the compilation tools, as opposed to programmers who just want their program to compile and work. objfile... are the object files to be examined. When you specify archives, objdump shows information on each of the member object files.","Process Name":"cross-objdump","Link":"https:\/\/linux.die.net\/man\/1\/cross-objdump"}},{"Process":{"Description":"ranlib generates an index to the contents of an archive and stores it in the archive. The index lists each symbol defined by a member of an archive that is a relocatable object file. You may use nm -s or nm --print-armap to list this index. An archive with such an index speeds up linking to the library and allows routines in the library to call each other without regard to their placement in the archive. The GNU ranlib program is another form of GNU ar; running ranlib is completely equivalent to executing ar -s.","Process Name":"cross-ranlib","Link":"https:\/\/linux.die.net\/man\/1\/cross-ranlib"}},{"Process":{"Description":"readelf displays information about one or more ELF format object files. The options control what particular information to display. elffile... are the object files to be examined. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files. This program performs a similar function to objdump but it goes into more detail and it exists independently of the BFD library, so if there is a bug in BFD then readelf will not be affected.","Process Name":"cross-readelf","Link":"https:\/\/linux.die.net\/man\/1\/cross-readelf"}},{"Process":{"Description":"The GNU size utility lists the section sizes---and the total size---for each of the object or archive files objfile in its argument list. By default, one line of output is generated for each object file or each module in an archive. objfile... are the object files to be examined. If none are specified, the file \"a.out\" will be used.","Process Name":"cross-size","Link":"https:\/\/linux.die.net\/man\/1\/cross-size"}},{"Process":{"Description":"For each file given, GNU strings prints the printable character sequences that are at least 4 characters long (or the number given with the options below) and are followed by an unprintable character. By default, it only prints the strings from the initialized and loaded sections of object files; for other types of files, it prints the strings from the whole file. strings is mainly useful for determining the contents of non-text files.","Process Name":"cross-strings","Link":"https:\/\/linux.die.net\/man\/1\/cross-strings"}},{"Process":{"Description":"GNU strip discards all symbols from object files objfile. The list of object files may include archives. At least one object file must be given. strip modifies the files named in its argument, rather than writing modified copies under different names.","Process Name":"cross-strip","Link":"https:\/\/linux.die.net\/man\/1\/cross-strip"}},{"Process":{"Description":"windmc reads message definitions from an input file (.mc) and translate them into a set of output files. The output files may be of four kinds: \"h\" A C header file containing the message definitions. \"rc\" A resource file compilable by the windres tool. \"bin\" One or more binary files containing the resource data for a specific message language. \"dbg\" A C include file that maps message id's to their symbolic name. The exact description of these different formats is available in documentation from Microsoft. When windmc converts from the \"mc\" format to the \"bin\" format, \"rc\", \"h\", and optional \"dbg\" it is acting like the Windows Message Compiler.","Process Name":"cross-windmc","Link":"https:\/\/linux.die.net\/man\/1\/cross-windmc"}},{"Process":{"Description":"windres reads resources from an input file and copies them into an output file. Either file may be in one of three formats: \"rc\" A text format read by the Resource Compiler. \"res\" A binary format generated by the Resource Compiler. \"coff\" A COFF object or executable. The exact description of these different formats is available in documentation from Microsoft. When windres converts from the \"rc\" format to the \"res\" format, it is acting like the Windows Resource Compiler. When windres converts from the \"res\" format to the \"coff\" format, it is acting like the Windows \"CVTRES\" program. When windres generates an \"rc\" file, the output is similar but not identical to the format expected for the input. When an input \"rc\" file refers to an external filename, an output \"rc\" file will instead include the file contents. If the input or output format is not specified, windres will guess based on the file name, or, for the input file, the file contents. A file with an extension of .rc will be treated as an \"rc\" file, a file with an extension of .res will be treated as a \"res\" file, and a file with an extension of .o or .exe will be treated as a \"coff\" file. If no output file is specified, windres will print the resources in \"rc\" format to standard output. The normal use is for you to write an \"rc\" file, use windres to convert it to a COFF object file, and then link the COFF file into your application. This will make the resources described in the \"rc\" file available to Windows.","Process Name":"cross-windres","Link":"https:\/\/linux.die.net\/man\/1\/cross-windres"}},{"Process":{"Description":"crossroads is the main control program of Crossroads, a balancing and fail over utility. The Crossroads utility consists of two binaries: crossroads and crossroads-daemon. The latter is controlled by the front end crossroads. Recognized states in crossroads tell are: o available or up when a backend is working; o unavailable when a back end is temporarily down, but can be woken up; o down when a back end is permanently down (e.g., for maintenance). Crossroads itself will never activate a backend in this state. o waking when the live status of a back end is being checked.","Process Name":"crossroads","Link":"https:\/\/linux.die.net\/man\/1\/crossroads"}},{"Process":{"Description":"crossroads-mgr is a self-contained web server, written in Perl, providing a web interface to control Crossroads. The web interface allows one to: o View the states of back ends, and their usage; o Set the states of back ends (e.g., to take a back end out of service). The web interface cannot be used to fully configure Crossroads (e.g., to add new back ends). This must be done via the configuration file crossroads.conf. Note furthermore that crossroads status and crossroads tell are commandline tools that achieve the same functionality as crossroads-mgr.","Process Name":"crossroads-mgr","Link":"https:\/\/linux.die.net\/man\/1\/crossroads-mgr"}},{"Process":{"Description":"cryptoflex-tool is used to manipulate PKCS data structures on Schlumberger Cryptoflex smart cards. Users can create, list and read PINs and keys stored on the smart card. User PIN authentication is performed for those operations that require it.","Process Name":"cryptoflex-tool","Link":"https:\/\/linux.die.net\/man\/1\/cryptoflex-tool"}},{"Process":{"Description":"Moving polygons, similar to a kaleidescope (more like a kaleidescope than the hack called 'kaleid,' actually.)","Process Name":"crystal","Link":"https:\/\/linux.die.net\/man\/1\/crystal"}},{"Process":{"Description":"Cs2cs performs transformation between the source and destination cartographic coordinate system on a set of input points. The coordinate system transformation can include translation between projected and geographic coordinates as well as the application of datum shifts. The following control parameters can appear in any order: -I method to specify inverse translation, convert from +to coordinate system to the primary coordinate system defined. -ta A specifies a character employed as the first character to denote a control line to be passed through without processing. This option applicable to ascii input only. (# is the default value). -e string String is an arbitrary string to be output if an error is detected during data transformations. The default value is: *\\t*. Note that if the -b, -i or -o options are employed, an error is returned as HUGE_VAL value for both return values. -E causes the input coordinates to be copied to the output line prior to printing the converted values. -l [p|P|=|e|u|d] id List projection identifiers with -l, -lp or -lP (expanded) that can be selected with +proj. -l=id gives expanded description of projection id. List ellipsoid identifiers with -le, that can be selected with +ellps, -lu list of cartesian to meter conversion factors that can be selected with +units or -ld list of datums that can be selected with +datum. -r This options reverses the order of the expected input from longitude-latitude or x-y to latitude-longitude or y-x. -s This options reverses the order of the output from x-y or longitude-latitude to y-x or latitude-longitude. -f format Format is a printf format string to control the form of the output values. For inverse projections, the output will be in degrees when this option is employed. If a format is specified for inverse projection the output data will be in decimal degrees. The default format is \"%.2f\" for forward projection and DMS for inverse. -[w|W] n N is the number of significant fractional digits to employ for seconds output (when the option is not specified, -w3 is assumed). When -W is employed the fields will be constant width and with leading zeroes. -v causes a listing of cartographic control parameters tested for and used by the program to be printed prior to input data. The +args run-line arguments are associated with cartographic parameters and usage varies with projection and for a complete description see Cartographic Projection Procedures for the UNIX Environment-A User's Manual ) and supplementary documentation for Release 4. The cs2cs program requires two coordinate system definitions. The first (or primary is defined based on all projection parameters not appearing after the +to argument. All projection parameters appearing after the +to argument are considered the definition of the second coordinate system. If there is no second coordinate system defined, a geographic coordinate system based on the datum and ellipsoid of the source coordinate system is assumed. Note that the source and destination coordinate system can both be projections, both be geographic, or one of each and may have the same or different datums. Additional projection control parameters may be contained in two auxiliary control files: the first is optionally referenced with the +init=file:id and the second is always processed after the name of the projection has been established from either the run-line or the contents of +init file. The environment parameter PROJ_LIB establishes the default directory for a file reference without an absolute path. This is also used for supporting files like datum shift files. One or more files (processed in left to right order) specify the source of data to be transformed. A - will specify the location of processing standard input. If no files are specified, the input is assumed to be from stdin. For input data the two data values must be in the first two white space separated fields and when both input and output are ASCII all trailing portions of the input line are appended to the output line. Input geographic data (longitude and latitude) must be in DMS format and input cartesian data must be in units consistent with the ellipsoid major axis or sphere radius units. Output geographic coordinates will be in DMS (if the -w switch is not employed) and precise to 0.001\" with trailing, zero-valued minute-second fields deleted.","Process Name":"cs2cs","Link":"https:\/\/linux.die.net\/man\/1\/cs2cs"}},{"Process":{"Description":"cscope is an interactive, screen-oriented tool that allows the user to browse through C source files for specified elements of code. By default, cscope examines the C (.c and .h), lex (.l), and yacc (.y) source files in the current directory. cscope may also be invoked for source files named on the command line. In either case, cscope searches the standard directories for #include files that it does not find in the current directory. cscope uses a symbol cross-reference, called cscope.out by default, to locate functions, function calls, macros, variables, and preprocessor symbols in the files. cscope builds the symbol cross-reference the first time it is used on the source files for the program being browsed. On a subsequent invocation, cscope rebuilds the cross-reference only if a source file has changed or the list of source files is different. When the cross-reference is rebuilt, the data for the unchanged files are copied from the old cross-reference, which makes rebuilding faster than the initial build.","Process Name":"cscope","Link":"https:\/\/linux.die.net\/man\/1\/cscope"}},{"Process":{"Description":"This program creates a DjVuDocument file outputdjvufile from separated data files sepfiles. It can read separated data from the standard input when given a single dash instead of the separated data file names. This feature is intended for pre-processing programs that push separated data into csepdjvu via a pipe. Each separated data file represents one or more page images. When the program arguments specify multiple pages, all the pages are encoded and saved as a bundled multi-page document. When the program arguments specify a single page, the page is encoded and saved as a single page file.","Process Name":"csepdjvu","Link":"https:\/\/linux.die.net\/man\/1\/csepdjvu"}},{"Process":{"Description":"tcsh is an enhanced but completely compatible version of the Berkeley UNIX C shell, csh(1). It is a command language interpreter usable both as an interactive login shell and a shell script command processor. It includes a command-line editor (see The command-line editor), programmable word completion (see Completion and listing), spelling correction (see Spelling correction), a history mechanism (see History substitution), job control (see Jobs) and a C-like syntax. The NEW FEATURES section describes major enhancements of tcsh over csh(1). Throughout this manual, features of tcsh not found in most csh(1) implementations (specifically, the 4.4BSD csh) are labeled with '(+)', and features which are present in csh(1) but not usually documented are labeled with '(u)'. Argument list processing If the first argument (argument 0) to the shell is '-' then it is a login shell. A login shell can be also specified by invoking the shell with the -l flag as the only argument. The rest of the flag arguments are interpreted as follows: -b Forces a ''break'' from option processing, causing any further shell arguments to be treated as non-option arguments. The remaining arguments will not be interpreted as shell options. This may be used to pass options to a shell script without confusion or possible subterfuge. The shell will not run a set-user ID script without this option. -c Commands are read from the following argument (which must be present, and must be a single argument), stored in the command shell variable for reference, and executed. Any remaining arguments are placed in the argv shell variable. -d The shell loads the directory stack from ~\/.cshdirs as described under Startup and shutdown, whether or not it is a login shell. (+) -D name[= value] Sets the environment variable name to value. (Domain\/OS only) (+) -e The shell exits if any invoked command terminates abnormally or yields a non-zero exit status. -f The shell does not load any resource or startup files, or perform any command hashing, and thus starts faster. -F The shell uses fork(2) instead of vfork(2) to spawn processes. (+) -i The shell is interactive and prompts for its top-level input, even if it appears to not be a terminal. Shells are interactive without this option if their inputs and outputs are terminals. -l The shell is a login shell. Applicable only if -l is the only flag specified. -m The shell loads ~\/.tcshrc even if it does not belong to the effective user. Newer versions of su(1) can pass -m to the shell. (+) -n The shell parses commands but does not execute them. This aids in debugging shell scripts. -q The shell accepts SIGQUIT (see Signal handling) and behaves when it is used under a debugger. Job control is disabled. (u) -s Command input is taken from the standard input. -t The shell reads and executes a single line of input. A '\\' may be used to escape the newline at the end of this line and continue onto another line. -v Sets the verbose shell variable, so that command input is echoed after history substitution. -x Sets the echo shell variable, so that commands are echoed immediately before execution. -V Sets the verbose shell variable even before executing ~\/.tcshrc. -X Is to -x as -V is to -v. --help Print a help message on the standard output and exit. (+) --version Print the version\/platform\/compilation options on the standard output and exit. This information is also contained in the version shell variable. (+) After processing of flag arguments, if arguments remain but none of the -c, -i, -s, or -t options were given, the first argument is taken as the name of a file of commands, or ''script'', to be executed. The shell opens this file and saves its name for possible resubstitution by '$0'. Because many systems use either the standard version 6 or version 7 shells whose shell scripts are not compatible with this shell, the shell uses such a 'standard' shell to execute a script whose first character is not a '#', i.e., that does not start with a comment. Remaining arguments are placed in the argv shell variable. Startup and shutdown A login shell begins by executing commands from the system files \/etc\/csh.cshrc and \/etc\/csh.login. It then executes commands from files in the user's home directory: first ~\/.tcshrc (+) or, if ~\/.tcshrc is not found, ~\/.cshrc, then ~\/.history (or the value of the histfile shell variable), then ~\/.login, and finally ~\/.cshdirs (or the value of the dirsfile shell variable) (+). The shell may read \/etc\/csh.login before instead of after \/etc\/csh.cshrc, and ~\/.login before instead of after ~\/.tcshrc or ~\/.cshrc and ~\/.history, if so compiled; see the version shell variable. (+) Non-login shells read only \/etc\/csh.cshrc and ~\/.tcshrc or ~\/.cshrc on startup. For examples of startup files, please consult http:\/\/tcshrc.sourceforge.net. Commands like stty(1) and tset(1), which need be run only once per login, usually go in one's ~\/.login file. Users who need to use the same set of files with both csh(1) and tcsh can have only a ~\/.cshrc which checks for the existence of the tcsh shell variable (q.v.) before using tcsh-specific commands, or can have both a ~\/.cshrc and a ~\/.tcshrc which sources (see the builtin command) ~\/.cshrc. The rest of this manual uses '~\/.tcshrc' to mean '~\/.tcshrc or, if ~\/.tcshrc is not found, ~\/.cshrc'. In the normal case, the shell begins reading commands from the terminal, prompting with '> '. (Processing of arguments and the use of the shell to process files containing command scripts are described later.) The shell repeatedly reads a line of command input, breaks it into words, places it on the command history list, parses it and executes each command in the line. One can log out by typing '^D' on an empty line, 'logout' or 'login' or via the shell's autologout mechanism (see the autologout shell variable). When a login shell terminates it sets the logout shell variable to 'normal' or 'automatic' as appropriate, then executes commands from the files \/etc\/csh.logout and ~\/.logout. The shell may drop DTR on logout if so compiled; see the version shell variable. The names of the system login and logout files vary from system to system for compatibility with different csh(1) variants; see FILES. Editing We first describe The command-line editor. The Completion and listing and Spelling correction sections describe two sets of functionality that are implemented as editor commands but which deserve their own treatment. Finally, Editor commands lists and describes the editor commands specific to the shell and their default bindings. The command-line editor (+) Command-line input can be edited using key sequences much like those used in GNU Emacs or vi(1). The editor is active only when the edit shell variable is set, which it is by default in interactive shells. The bindkey builtin can display and change key bindings. Emacs-style key bindings are used by default (unless the shell was compiled otherwise; see the version shell variable), but bindkey can change the key bindings to vi-style bindings en masse. The shell always binds the arrow keys (as defined in the TERMCAP environment variable) to down down-history up up-history left backward-char right forward-char unless doing so would alter another single-character binding. One can set the arrow key escape sequences to the empty string with settc to prevent these bindings. The ANSI\/VT100 sequences for arrow keys are always bound. Other key bindings are, for the most part, what Emacs and vi(1) users would expect and can easily be displayed by bindkey, so there is no need to list them here. Likewise, bindkey can list the editor commands with a short description of each. Note that editor commands do not have the same notion of a ''word'' as does the shell. The editor delimits words with any non-alphanumeric characters not in the shell variable wordchars, while the shell recognizes only whitespace and some of the characters with special meanings to it, listed under Lexical structure. Completion and listing (+) The shell is often able to complete words when given a unique abbreviation. Type part of a word (for example 'ls \/usr\/lost') and hit the tab key to run the complete-word editor command. The shell completes the filename '\/usr\/lost' to '\/usr\/lost+found\/', replacing the incomplete word with the complete word in the input buffer. (Note the terminal '\/'; completion adds a '\/' to the end of completed directories and a space to the end of other completed words, to speed typing and provide a visual indicator of successful completion. The addsuffix shell variable can be unset to prevent this.) If no match is found (perhaps '\/usr\/lost+found' doesn't exist), the terminal bell rings. If the word is already complete (perhaps there is a '\/usr\/lost' on your system, or perhaps you were thinking too far ahead and typed the whole thing) a '\/' or space is added to the end if it isn't already there. Completion works anywhere in the line, not at just the end; completed text pushes the rest of the line to the right. Completion in the middle of a word often results in leftover characters to the right of the cursor that need to be deleted. Commands and variables can be completed in much the same way. For example, typing 'em[tab]' would complete 'em' to 'emacs' if emacs were the only command on your system beginning with 'em'. Completion can find a command in any directory in path or if given a full pathname. Typing 'echo $ar[tab]' would complete '$ar' to '$argv' if no other variable began with 'ar'. The shell parses the input buffer to determine whether the word you want to complete should be completed as a filename, command or variable. The first word in the buffer and the first word following ';', '|', '|&', '&&' or '||' is considered to be a command. A word beginning with '$' is considered to be a variable. Anything else is a filename. An empty line is 'completed' as a filename. You can list the possible completions of a word at any time by typing '^D' to run the delete-char-or-list-or-eof editor command. The shell lists the possible completions using the ls-F builtin (q.v.) and reprints the prompt and unfinished command line, for example: > ls \/usr\/l[^D] lbin\/ lib\/ local\/ lost+found\/ > ls \/usr\/l If the autolist shell variable is set, the shell lists the remaining choices (if any) whenever completion fails: > set autolist > nm \/usr\/lib\/libt[tab] libtermcap.a@ libtermlib.a@ > nm \/usr\/lib\/libterm If autolist is set to 'ambiguous', choices are listed only when completion fails and adds no new characters to the word being completed. A filename to be completed can contain variables, your own or others' home directories abbreviated with '~' (see Filename substitution) and directory stack entries abbreviated with '=' (see Directory stack substitution). For example, > ls ~k[^D] kahn kas kellogg > ls ~ke[tab] > ls ~kellogg\/ or > set local = \/usr\/local > ls $lo[tab] > ls $local\/[^D] bin\/ etc\/ lib\/ man\/ src\/ > ls $local\/ Note that variables can also be expanded explicitly with the expand-variables editor command. delete-char-or-list-or-eof lists at only the end of the line; in the middle of a line it deletes the character under the cursor and on an empty line it logs one out or, if ignoreeof is set, does nothing. 'M-^D', bound to the editor command list-choices, lists completion possibilities anywhere on a line, and list-choices (or any one of the related editor commands that do or don't delete, list and\/or log out, listed under delete-char-or-list-or-eof) can be bound to '^D' with the bindkey builtin command if so desired. The complete-word-fwd and complete-word-back editor commands (not bound to any keys by default) can be used to cycle up and down through the list of possible completions, replacing the current word with the next or previous word in the list. The shell variable fignore can be set to a list of suffixes to be ignored by completion. Consider the following: > ls Makefile condiments.h~ main.o side.c README main.c meal side.o condiments.h main.c~ > set fignore = (.o \\~) > emacs ma[^D] main.c main.c~ main.o > emacs ma[tab] > emacs main.c 'main.c~' and 'main.o' are ignored by completion (but not listing), because they end in suffixes in fignore. Note that a '\\' was needed in front of '~' to prevent it from being expanded to home as described under Filename substitution. fignore is ignored if only one completion is possible. If the complete shell variable is set to 'enhance', completion 1) ignores case and 2) considers periods, hyphens and underscores ('.', '-' and '_') to be word separators and hyphens and underscores to be equivalent. If you had the following files comp.lang.c comp.lang.perl comp.std.c++ comp.lang.c++ comp.std.c and typed 'mail -f c.l.c[tab]', it would be completed to 'mail -f comp.lang.c', and ^D would list 'comp.lang.c' and 'comp.lang.c++'. 'mail -f c..c++[^D]' would list 'comp.lang.c++' and 'comp.std.c++'. Typing 'rm a--file[^D]' in the following directory A_silly_file a-hyphenated-file another_silly_file would list all three files, because case is ignored and hyphens and underscores are equivalent. Periods, however, are not equivalent to hyphens or underscores. Completion and listing are affected by several other shell variables: recexact can be set to complete on the shortest possible unique match, even if more typing might result in a longer match: > ls fodder foo food foonly > set recexact > rm fo[tab] just beeps, because 'fo' could expand to 'fod' or 'foo', but if we type another 'o', > rm foo[tab] > rm foo the completion completes on 'foo', even though 'food' and 'foonly' also match. autoexpand can be set to run the expand-history editor command before each completion attempt, autocorrect can be set to spelling-correct the word to be completed (see Spelling correction) before each completion attempt and correct can be set to complete commands automatically after one hits 'return'. matchbeep can be set to make completion beep or not beep in a variety of situations, and nobeep can be set to never beep at all. nostat can be set to a list of directories and\/or patterns that match directories to prevent the completion mechanism from stat(2)ing those directories. listmax and listmaxrows can be set to limit the number of items and rows (respectively) that are listed without asking first. recognize_only_executables can be set to make the shell list only executables when listing commands, but it is quite slow. Finally, the complete builtin command can be used to tell the shell how to complete words other than filenames, commands and variables. Completion and listing do not work on glob-patterns (see Filename substitution), but the list-glob and expand-glob editor commands perform equivalent functions for glob-patterns. Spelling correction (+) The shell can sometimes correct the spelling of filenames, commands and variable names as well as completing and listing them. Individual words can be spelling-corrected with the spell-word editor command (usually bound to M-s and M-S) and the entire input buffer with spell-line (usually bound to M-$). The correct shell variable can be set to 'cmd' to correct the command name or 'all' to correct the entire line each time return is typed, and autocorrect can be set to correct the word to be completed before each completion attempt. When spelling correction is invoked in any of these ways and the shell thinks that any part of the command line is misspelled, it prompts with the corrected line: > set correct = cmd > lz \/usr\/bin CORRECT>ls \/usr\/bin (y|n|e|a)? One can answer 'y' or space to execute the corrected line, 'e' to leave the uncorrected command in the input buffer, 'a' to abort the command as if '^C' had been hit, and anything else to execute the original line unchanged. Spelling correction recognizes user-defined completions (see the complete builtin command). If an input word in a position for which a completion is defined resembles a word in the completion list, spelling correction registers a misspelling and suggests the latter word as a correction. However, if the input word does not match any of the possible completions for that position, spelling correction does not register a misspelling. Like completion, spelling correction works anywhere in the line, pushing the rest of the line to the right and possibly leaving extra characters to the right of the cursor. Beware: spelling correction is not guaranteed to work the way one intends, and is provided mostly as an experimental feature. Suggestions and improvements are welcome. Editor commands (+) 'bindkey' lists key bindings and 'bindkey -l' lists and briefly describes editor commands. Only new or especially interesting editor commands are described here. See emacs(1) and vi(1) for descriptions of each editor's key bindings. The character or characters to which each command is bound by default is given in parentheses. '^character' means a control character and 'M-character' a meta character, typed as escape-character on terminals without a meta key. Case counts, but commands that are bound to letters by default are bound to both lower- and uppercase letters for convenience. complete-word (tab) Completes a word as described under Completion and listing. complete-word-back (not bound) Like complete-word-fwd, but steps up from the end of the list. complete-word-fwd (not bound) Replaces the current word with the first word in the list of possible completions. May be repeated to step down through the list. At the end of the list, beeps and reverts to the incomplete word. complete-word-raw (^X-tab) Like complete-word, but ignores user-defined completions. copy-prev-word (M-^_) Copies the previous word in the current line into the input buffer. See also insert-last-word. dabbrev-expand (M-\/) Expands the current word to the most recent preceding one for which the current is a leading substring, wrapping around the history list (once) if necessary. Repeating dabbrev-expand without any intervening typing changes to the next previous word etc., skipping identical matches much like history-search-backward does. delete-char (not bound) Deletes the character under the cursor. See also delete-char-or-list-or-eof. delete-char-or-eof (not bound) Does delete-char if there is a character under the cursor or end-of-file on an empty line. See also delete-char-or-list-or-eof. delete-char-or-list (not bound) Does delete-char if there is a character under the cursor or list-choices at the end of the line. See also delete-char-or-list-or-eof. delete-char-or-list-or-eof (^D) Does delete-char if there is a character under the cursor, list-choices at the end of the line or end-of-file on an empty line. See also those three commands, each of which does only a single action, and delete-char-or-eof, delete-char-or-list and list-or-eof, each of which does a different two out of the three. down-history (down-arrow, ^N) Like up-history, but steps down, stopping at the original input line. end-of-file (not bound) Signals an end of file, causing the shell to exit unless the ignoreeof shell variable (q.v.) is set to prevent this. See also delete-char-or-list-or-eof. expand-history (M-space) Expands history substitutions in the current word. See History substitution. See also magic-space, toggle-literal-history and the autoexpand shell variable. expand-glob (^X-*) Expands the glob-pattern to the left of the cursor. See Filename substitution. expand-line (not bound) Like expand-history, but expands history substitutions in each word in the input buffer, expand-variables (^X-$) Expands the variable to the left of the cursor. See Variable substitution. history-search-backward (M-p, M-P) Searches backwards through the history list for a command beginning with the current contents of the input buffer up to the cursor and copies it into the input buffer. The search string may be a glob-pattern (see Filename substitution) containing '*', '?', '[]' or '{}'. up-history and down-history will proceed from the appropriate point in the history list. Emacs mode only. See also history-search-forward and i-search-back. history-search-forward (M-n, M-N) Like history-search-backward, but searches forward. i-search-back (not bound) Searches backward like history-search-backward, copies the first match into the input buffer with the cursor positioned at the end of the pattern, and prompts with 'bck: ' and the first match. Additional characters may be typed to extend the search, i-search-back may be typed to continue searching with the same pattern, wrapping around the history list if necessary, ( i-search-back must be bound to a single character for this to work) or one of the following special characters may be typed: ^W Appends the rest of the word under the cursor to the search pattern. delete (or any character bound to backward-delete-char) Undoes the effect of the last character typed and deletes a character from the search pattern if appropriate. ^G If the previous search was successful, aborts the entire search. If not, goes back to the last successful search. escape Ends the search, leaving the current line in the input buffer. Any other character not bound to self-insert-command terminates the search, leaving the current line in the input buffer, and is then interpreted as normal input. In particular, a carriage return causes the current line to be executed. Emacs mode only. See also i-search-fwd and history-search-backward. i-search-fwd (not bound) Like i-search-back, but searches forward. insert-last-word (M-_) Inserts the last word of the previous input line ('!$') into the input buffer. See also copy-prev-word. list-choices (M-^D) Lists completion possibilities as described under Completion and listing. See also delete-char-or-list-or-eof and list-choices-raw. list-choices-raw (^X-^D) Like list-choices, but ignores user-defined completions. list-glob (^X-g, ^X-G) Lists (via the ls-F builtin) matches to the glob-pattern (see Filename substitution) to the left of the cursor. list-or-eof (not bound) Does list-choices or end-of-file on an empty line. See also delete-char-or-list-or-eof. magic-space (not bound) Expands history substitutions in the current line, like expand-history, and inserts a space. magic-space is designed to be bound to the space bar, but is not bound by default. normalize-command (^X-?) Searches for the current word in PATH and, if it is found, replaces it with the full path to the executable. Special characters are quoted. Aliases are expanded and quoted but commands within aliases are not. This command is useful with commands that take commands as arguments, e.g., 'dbx' and 'sh -x'. normalize-path (^X-n, ^X-N) Expands the current word as described under the 'expand' setting of the symlinks shell variable. overwrite-mode (unbound) Toggles between input and overwrite modes. run-fg-editor (M-^Z) Saves the current input line and looks for a stopped job with a name equal to the last component of the file name part of the EDITOR or VISUAL environment variables, or, if neither is set, 'ed' or 'vi'. If such a job is found, it is restarted as if 'fg % job' had been typed. This is used to toggle back and forth between an editor and the shell easily. Some people bind this command to '^Z' so they can do this even more easily. run-help (M-h, M-H) Searches for documentation on the current command, using the same notion of 'current command' as the completion routines, and prints it. There is no way to use a pager; run-help is designed for short help files. If the special alias helpcommand is defined, it is run with the command name as a sole argument. Else, documentation should be in a file named command.help, command.1, command.6, command.8 or command, which should be in one of the directories listed in the HPATH environment variable. If there is more than one help file only the first is printed. self-insert-command (text characters) In insert mode (the default), inserts the typed character into the input line after the character under the cursor. In overwrite mode, replaces the character under the cursor with the typed character. The input mode is normally preserved between lines, but the inputmode shell variable can be set to 'insert' or 'overwrite' to put the editor in that mode at the beginning of each line. See also overwrite-mode. sequence-lead-in (arrow prefix, meta prefix, ^X) Indicates that the following characters are part of a multi-key sequence. Binding a command to a multi-key sequence really creates two bindings: the first character to sequence-lead-in and the whole sequence to the command. All sequences beginning with a character bound to sequence-lead-in are effectively bound to undefined-key unless bound to another command. spell-line (M-$) Attempts to correct the spelling of each word in the input buffer, like spell-word, but ignores words whose first character is one of '-', '!', '^' or '%', or which contain '\\', '*' or '?', to avoid problems with switches, substitutions and the like. See Spelling correction. spell-word (M-s, M-S) Attempts to correct the spelling of the current word as described under Spelling correction. Checks each component of a word which appears to be a pathname. toggle-literal-history (M-r, M-R) Expands or 'unexpands' history substitutions in the input buffer. See also expand-history and the autoexpand shell variable. undefined-key (any unbound key) Beeps. up-history (up-arrow, ^P) Copies the previous entry in the history list into the input buffer. If histlit is set, uses the literal form of the entry. May be repeated to step up through the history list, stopping at the top. vi-search-back (?) Prompts with '?' for a search string (which may be a glob-pattern, as with history-search-backward), searches for it and copies it into the input buffer. The bell rings if no match is found. Hitting return ends the search and leaves the last match in the input buffer. Hitting escape ends the search and executes the match. vi mode only. vi-search-fwd (\/) Like vi-search-back, but searches forward. which-command (M-?) Does a which (see the description of the builtin command) on the first word of the input buffer. yank-pop (M-y) When executed immediately after a yank or another yank-pop, replaces the yanked string with the next previous string from the killring. This also has the effect of rotating the killring, such that this string will be considered the most recently killed by a later yank command. Repeating yank-pop will cycle through the killring any number of times. Lexical structure The shell splits input lines into words at blanks and tabs. The special characters '&', '|', ';', '<', '>', '(', and ')' and the doubled characters '&&', '||', '<<' and '>>' are always separate words, whether or not they are surrounded by whitespace. When the shell's input is not a terminal, the character '#' is taken to begin a comment. Each '#' and the rest of the input line on which it appears is discarded before further parsing. A special character (including a blank or tab) may be prevented from having its special meaning, and possibly made part of another word, by preceding it with a backslash ('\\') or enclosing it in single ('''), double ('\"') or backward (''') quotes. When not otherwise quoted a newline preceded by a '\\' is equivalent to a blank, but inside quotes this sequence results in a newline. Furthermore, all Substitutions (see below) except History substitution can be prevented by enclosing the strings (or parts of strings) in which they appear with single quotes or by quoting the crucial character(s) (e.g., '$' or ''' for Variable substitution or Command substitution respectively) with '\\'. (Alias substitution is no exception: quoting in any way any character of a word for which an alias has been defined prevents substitution of the alias. The usual way of quoting an alias is to precede it with a backslash.) History substitution is prevented by backslashes but not by single quotes. Strings quoted with double or backward quotes undergo Variable substitution and Command substitution, but other substitutions are prevented. Text inside single or double quotes becomes a single word (or part of one). Metacharacters in these strings, including blanks and tabs, do not form separate words. Only in one special case (see Command substitution below) can a double-quoted string yield parts of more than one word; single-quoted strings never do. Backward quotes are special: they signal Command substitution (q.v.), which may result in more than one word. Quoting complex strings, particularly strings which themselves contain quoting characters, can be confusing. Remember that quotes need not be used as they are in human writing! It may be easier to quote not an entire string, but only those parts of the string which need quoting, using different types of quoting to do so if appropriate. The backslash_quote shell variable (q.v.) can be set to make backslashes always quote '\\', ''', and '\"'. (+) This may make complex quoting tasks easier, but it can cause syntax errors in csh(1) scripts. Substitutions We now describe the various transformations the shell performs on the input in the order in which they occur. We note in passing the data structures involved and the commands and variables which affect them. Remember that substitutions can be prevented by quoting as described under Lexical structure. History substitution Each command, or ''event'', input from the terminal is saved in the history list. The previous command is always saved, and the history shell variable can be set to a number to save that many commands. The histdup shell variable can be set to not save duplicate events or consecutive duplicate events. Saved commands are numbered sequentially from 1 and stamped with the time. It is not usually necessary to use event numbers, but the current event number can be made part of the prompt by placing an '!' in the prompt shell variable. The shell actually saves history in expanded and literal (unexpanded) forms. If the histlit shell variable is set, commands that display and store history use the literal form. The history builtin command can print, store in a file, restore and clear the history list at any time, and the savehist and histfile shell variables can be can be set to store the history list automatically on logout and restore it on login. History substitutions introduce words from the history list into the input stream, making it easy to repeat commands, repeat arguments of a previous command in the current command, or fix spelling mistakes in the previous command with little typing and a high degree of confidence. History substitutions begin with the character '!'. They may begin anywhere in the input stream, but they do not nest. The '!' may be preceded by a '\\' to prevent its special meaning; for convenience, a '!' is passed unchanged when it is followed by a blank, tab, newline, '=' or '('. History substitutions also occur when an input line begins with '^'. This special abbreviation will be described later. The characters used to signal history substitution ('!' and '^') can be changed by setting the histchars shell variable. Any input line which contains a history substitution is printed before it is executed. A history substitution may have an ''event specification'', which indicates the event from which words are to be taken, a ''word designator'', which selects particular words from the chosen event, and\/or a ''modifier'', which manipulates the selected words. An event specification can be n A number, referring to a particular event -n An offset, referring to the event n before the current event # The current event. This should be used carefully in csh(1), where there is no check for recursion. tcsh allows 10 levels of recursion. (+) ! The previous event (equivalent to '-1') s The most recent event whose first word begins with the string s ?s? The most recent event which contains the string s. The second '?' can be omitted if it is immediately followed by a newline. For example, consider this bit of someone's history list: 9 8:30 nroff -man wumpus.man 10 8:31 cp wumpus.man wumpus.man.old 11 8:36 vi wumpus.man 12 8:37 diff wumpus.man.old wumpus.man The commands are shown with their event numbers and time stamps. The current event, which we haven't typed in yet, is event 13. '!11' and '!-2' refer to event 11. '!!' refers to the previous event, 12. '!!' can be abbreviated '!' if it is followed by ':' (':' is described below). '!n' refers to event 9, which begins with 'n'. '!?old?' also refers to event 12, which contains 'old'. Without word designators or modifiers history references simply expand to the entire event, so we might type '!cp' to redo the copy command or '!!|more' if the 'diff' output scrolled off the top of the screen. History references may be insulated from the surrounding text with braces if necessary. For example, '!vdoc' would look for a command beginning with 'vdoc', and, in this example, not find one, but '!{v}doc' would expand unambiguously to 'vi wumpus.mandoc'. Even in braces, history substitutions do not nest. (+) While csh(1) expands, for example, '!3d' to event 3 with the letter 'd' appended to it, tcsh expands it to the last event beginning with '3d'; only completely numeric arguments are treated as event numbers. This makes it possible to recall events beginning with numbers. To expand '!3d' as in csh(1) say '!{3}d'. To select words from an event we can follow the event specification by a ':' and a designator for the desired words. The words of an input line are numbered from 0, the first (usually command) word being 0, the second word (first argument) being 1, etc. The basic word designators are: 0 The first (command) word n The nth argument ^ The first argument, equivalent to '1' $ The last argument % The word matched by an ?s? search x-y A range of words -y Equivalent to '0-y' * Equivalent to '^-$', but returns nothing if the event contains only 1 word x* Equivalent to 'x-$' x- Equivalent to 'x*', but omitting the last word ('$') Selected words are inserted into the command line separated by single blanks. For example, the 'diff' command in the previous example might have been typed as 'diff !!:1.old !!:1' (using ':1' to select the first argument from the previous event) or 'diff !-2:2 !-2:1' to select and swap the arguments from the 'cp' command. If we didn't care about the order of the 'diff' we might have said 'diff !-2:1-2' or simply 'diff !-2:*'. The 'cp' command might have been written 'cp wumpus.man !#:1.old', using '#' to refer to the current event. '!n:- hurkle.man' would reuse the first two words from the 'nroff' command to say 'nroff -man hurkle.man'. The ':' separating the event specification from the word designator can be omitted if the argument selector begins with a '^', '$', '*', '%' or '-'. For example, our 'diff' command might have been 'diff !!^.old !!^' or, equivalently, 'diff !!$.old !!$'. However, if '!!' is abbreviated '!', an argument selector beginning with '-' will be interpreted as an event specification. A history reference may have a word designator but no event specification. It then references the previous command. Continuing our 'diff' example, we could have said simply 'diff !^.old !^' or, to get the arguments in the opposite order, just 'diff !*'. The word or words in a history reference can be edited, or ''modified'', by following it with one or more modifiers, each preceded by a ':': h Remove a trailing pathname component, leaving the head. t Remove all leading pathname components, leaving the tail. r Remove a filename extension '.xxx', leaving the root name. e Remove all but the extension. u Uppercase the first lowercase letter. l Lowercase the first uppercase letter. s\/l\/r\/ Substitute l for r. l is simply a string like r, not a regular expression as in the eponymous ed(1) command. Any character may be used as the delimiter in place of '\/'; a '\\' can be used to quote the delimiter expect '(', ')', '|' and '>' inside l and r. The character '&' in the r is replaced by l; '\\' also quotes '&'. If l is empty (''''), the l from a previous substitution or the s from a previous search or event number in event specification is used. The trailing delimiter may be omitted if it is immediately followed by a newline. & Repeat the previous substitution. g Apply the following modifier once to each word. a (+) Apply the following modifier as many times as possible to a single word. 'a' and 'g' can be used together to apply a modifier globally. With the 's' modifier, only the patterns contained in the original word are substituted, not patterns that contain any substitution result. p Print the new command line but do not execute it. q Quote the substituted words, preventing further substitutions. x Like q, but break into words at blanks, tabs and newlines. Modifiers are applied to only the first modifiable word (unless 'g' is used). It is an error for no word to be modifiable. For example, the 'diff' command might have been written as 'diff wumpus.man.old !#^:r', using ':r' to remove '.old' from the first argument on the same line ('!#^'). We could say 'echo hello out there', then 'echo !*:u' to capitalize 'hello', 'echo !*:au' to say it out loud, or 'echo !*:agu' to really shout. We might follow 'mail -s \"I forgot my password\" rot' with '!:s\/rot\/root' to correct the spelling of 'root' (but see Spelling correction for a different approach). There is a special abbreviation for substitutions. '^', when it is the first character on an input line, is equivalent to '!:s^'. Thus we might have said '^rot^root' to make the spelling correction in the previous example. This is the only history substitution which does not explicitly begin with '!'. (+) In csh as such, only one modifier may be applied to each history or variable expansion. In tcsh, more than one may be used, for example % mv wumpus.man \/usr\/man\/man1\/wumpus.1 % man !$:t:r man wumpus In csh, the result would be 'wumpus.1:r'. A substitution followed by a colon may need to be insulated from it with braces: > mv a.out \/usr\/games\/wumpus > setenv PATH !$:h:$PATH Bad ! modifier: $. > setenv PATH !{-2$:h}:$PATH setenv PATH \/usr\/games:\/bin:\/usr\/bin:. The first attempt would succeed in csh but fails in tcsh, because tcsh expects another modifier after the second colon rather than '$'. Finally, history can be accessed through the editor as well as through the substitutions just described. The up- and down-history, history-search-backward and -forward, i-search-back and -fwd, vi-search-back and -fwd, copy-prev-word and insert-last-word editor commands search for events in the history list and copy them into the input buffer. The toggle-literal-history editor command switches between the expanded and literal forms of history lines in the input buffer. expand-history and expand-line expand history substitutions in the current word and in the entire input buffer respectively. Alias substitution The shell maintains a list of aliases which can be set, unset and printed by the alias and unalias commands. After a command line is parsed into simple commands (see Commands) the first word of each command, left-to-right, is checked to see if it has an alias. If so, the first word is replaced by the alias. If the alias contains a history reference, it undergoes History substitution (q.v.) as though the original command were the previous input line. If the alias does not contain a history reference, the argument list is left untouched. Thus if the alias for 'ls' were 'ls -l' the command 'ls \/usr' would become 'ls -l \/usr', the argument list here being undisturbed. If the alias for 'lookup' were 'grep !^ \/etc\/passwd' then 'lookup bill' would become 'grep bill \/etc\/passwd'. Aliases can be used to introduce parser metasyntax. For example, 'alias print 'pr \\!* | lpr'' defines a ''command'' ('print') which pr(1)s its arguments to the line printer. Alias substitution is repeated until the first word of the command has no alias. If an alias substitution does not change the first word (as in the previous example) it is flagged to prevent a loop. Other loops are detected and cause an error. Some aliases are referred to by the shell; see Special aliases. Variable substitution The shell maintains a list of variables, each of which has as value a list of zero or more words. The values of shell variables can be displayed and changed with the set and unset commands. The system maintains its own list of ''environment'' variables. These can be displayed and changed with printenv, setenv and unsetenv. (+) Variables may be made read-only with 'set -r' (q.v.) Read-only variables may not be modified or unset; attempting to do so will cause an error. Once made read-only, a variable cannot be made writable, so 'set -r' should be used with caution. Environment variables cannot be made read-only. Some variables are set by the shell or referred to by it. For instance, the argv variable is an image of the shell's argument list, and words of this variable's value are referred to in special ways. Some of the variables referred to by the shell are toggles; the shell does not care what their value is, only whether they are set or not. For instance, the verbose variable is a toggle which causes command input to be echoed. The -v command line option sets this variable. Special shell variables lists all variables which are referred to by the shell. Other operations treat variables numerically. The '@' command permits numeric calculations to be performed and the result assigned to a variable. Variable values are, however, always represented as (zero or more) strings. For the purposes of numeric operations, the null string is considered to be zero, and the second and subsequent words of multi-word values are ignored. After the input line is aliased and parsed, and before each command is executed, variable substitution is performed keyed by '$' characters. This expansion can be prevented by preceding the '$' with a '\\' except within '\"'s where it always occurs, and within '''s where it never occurs. Strings quoted by ''' are interpreted later (see Command substitution below) so '$' substitution does not occur there until later, if at all. A '$' is passed unchanged if followed by a blank, tab, or end-of-line. Input\/output redirections are recognized before variable expansion, and are variable expanded separately. Otherwise, the command name and entire argument list are expanded together. It is thus possible for the first (command) word (to this point) to generate more than one word, the first of which becomes the command name, and the rest of which become arguments. Unless enclosed in '\"' or given the ':q' modifier the results of variable substitution may eventually be command and filename substituted. Within '\"', a variable whose value consists of multiple words expands to a (portion of a) single word, with the words of the variable's value separated by blanks. When the ':q' modifier is applied to a substitution the variable will expand to multiple words with each word separated by a blank and quoted to prevent later command or filename substitution. The following metasequences are provided for introducing variable values into the shell input. Except as noted, it is an error to reference a variable which is not set. $name ${ name} Substitutes the words of the value of variable name, each separated by a blank. Braces insulate name from following characters which would otherwise be part of it. Shell variables have names consisting of letters and digits starting with a letter. The underscore character is considered a letter. If name is not a shell variable, but is set in the environment, then that value is returned (but some of the other forms given below are not available in this case). $ name[ selector] ${ name[ selector]} Substitutes only the selected words from the value of name. The selector is subjected to '$' substitution and may consist of a single number or two numbers separated by a '-'. The first word of a variable's value is numbered '1'. If the first number of a range is omitted it defaults to '1'. If the last member of a range is omitted it defaults to '$# name'. The selector '*' selects all words. It is not an error for a range to be empty if the second argument is omitted or in range. $0 Substitutes the name of the file from which command input is being read. An error occurs if the name is not known. $ number ${ number} Equivalent to '$argv[ number]'. $* Equivalent to '$argv', which is equivalent to '$argv[*]'. The ':' modifiers described under History substitution, except for ':p', can be applied to the substitutions above. More than one may be used. (+) Braces may be needed to insulate a variable substitution from a literal colon just as with History substitution (q.v.); any modifiers must appear within the braces. The following substitutions can not be modified with ':' modifiers. $?name ${? name} Substitutes the string '1' if name is set, '0' if it is not. $?0 Substitutes '1' if the current input filename is known, '0' if it is not. Always '0' in interactive shells. $# name ${# name} Substitutes the number of words in name. $# Equivalent to '$#argv'. (+) $% name ${% name} Substitutes the number of characters in name. (+) $% number ${% number} Substitutes the number of characters in $argv[ number]. (+) $? Equivalent to '$status'. (+) $$ Substitutes the (decimal) process number of the (parent) shell. $! Substitutes the (decimal) process number of the last background process started by this shell. (+) $_ Substitutes the command line of the last command executed. (+) $< Substitutes a line from the standard input, with no further interpretation thereafter. It can be used to read from the keyboard in a shell script. (+) While csh always quotes $<, as if it were equivalent to '$<:q', tcsh does not. Furthermore, when tcsh is waiting for a line to be typed the user may type an interrupt to interrupt the sequence into which the line is to be substituted, but csh does not allow this. The editor command expand-variables, normally bound to '^X-$', can be used to interactively expand individual variables. Command, filename and directory stack substitution The remaining substitutions are applied selectively to the arguments of builtin commands. This means that portions of expressions which are not evaluated are not subjected to these expansions. For commands which are not internal to the shell, the command name is substituted separately from the argument list. This occurs very late, after input-output redirection is performed, and in a child of the main shell. Command substitution Command substitution is indicated by a command enclosed in '''. The output from such a command is broken into separate words at blanks, tabs and newlines, and null words are discarded. The output is variable and command substituted and put in place of the original string. Command substitutions inside double quotes ('\"') retain blanks and tabs; only newlines force new words. The single final newline does not force a new word in any case. It is thus possible for a command substitution to yield only part of a word, even if the command outputs a complete line. By default, the shell since version 6.12 replaces all newline and carriage return characters in the command by spaces. If this is switched off by unsetting csubstnonl, newlines separate commands as usual. Filename substitution If a word contains any of the characters '*', '?', '[' or '{' or begins with the character '~' it is a candidate for filename substitution, also known as ''globbing''. This word is then regarded as a pattern (''glob-pattern''), and replaced with an alphabetically sorted list of file names which match the pattern. In matching filenames, the character '.' at the beginning of a filename or immediately following a '\/', as well as the character '\/' must be matched explicitly. The character '*' matches any string of characters, including the null string. The character '?' matches any single character. The sequence '[...]' matches any one of the characters enclosed. Within '[...]', a pair of characters separated by '-' matches any character lexically between the two. (+) Some glob-patterns can be negated: The sequence '[^...]' matches any single character not specified by the characters and\/or ranges of characters in the braces. An entire glob-pattern can also be negated with '^': > echo * bang crash crunch ouch > echo ^cr* bang ouch Glob-patterns which do not use '?', '*', or '[]' or which use '{}' or '~' (below) are not negated correctly. The metanotation 'a{b,c,d}e' is a shorthand for 'abe ace ade'. Left-to-right order is preserved: '\/usr\/source\/s1\/{oldls,ls}.c' expands to '\/usr\/source\/s1\/oldls.c \/usr\/source\/s1\/ls.c'. The results of matches are sorted separately at a low level to preserve this order: '..\/{memo,*box}' might expand to '..\/memo ..\/box ..\/mbox'. (Note that 'memo' was not sorted with the results of matching '*box'.) It is not an error when this construct expands to files which do not exist, but it is possible to get an error from a command to which the expanded list is passed. This construct may be nested. As a special case the words '{', '}' and '{}' are passed undisturbed. The character '~' at the beginning of a filename refers to home directories. Standing alone, i.e., '~', it expands to the invoker's home directory as reflected in the value of the home shell variable. When followed by a name consisting of letters, digits and '-' characters the shell searches for a user with that name and substitutes their home directory; thus '~ken' might expand to '\/usr\/ken' and '~ken\/chmach' to '\/usr\/ken\/chmach'. If the character '~' is followed by a character other than a letter or '\/' or appears elsewhere than at the beginning of a word, it is left undisturbed. A command like 'setenv MANPATH \/usr\/man:\/usr\/local\/man:~\/lib\/man' does not, therefore, do home directory substitution as one might hope. It is an error for a glob-pattern containing '*', '?', '[' or '~', with or without '^', not to match any files. However, only one pattern in a list of glob-patterns must match a file (so that, e.g., 'rm *.a *.c *.o' would fail only if there were no files in the current directory ending in '.a', '.c', or '.o'), and if the nonomatch shell variable is set a pattern (or list of patterns) which matches nothing is left unchanged rather than causing an error. The noglob shell variable can be set to prevent filename substitution, and the expand-glob editor command, normally bound to '^X-*', can be used to interactively expand individual filename substitutions. Directory stack substitution (+) The directory stack is a list of directories, numbered from zero, used by the pushd, popd and dirs builtin commands (q.v.). dirs can print, store in a file, restore and clear the directory stack at any time, and the savedirs and dirsfile shell variables can be set to store the directory stack automatically on logout and restore it on login. The dirstack shell variable can be examined to see the directory stack and set to put arbitrary directories into the directory stack. The character '=' followed by one or more digits expands to an entry in the directory stack. The special case '=-' expands to the last directory in the stack. For example, > dirs -v 0 \/usr\/bin 1 \/usr\/spool\/uucp 2 \/usr\/accts\/sys > echo =1 \/usr\/spool\/uucp > echo =0\/calendar \/usr\/bin\/calendar > echo =- \/usr\/accts\/sys The noglob and nonomatch shell variables and the expand-glob editor command apply to directory stack as well as filename substitutions. Other substitutions (+) There are several more transformations involving filenames, not strictly related to the above but mentioned here for completeness. Any filename may be expanded to a full path when the symlinks variable (q.v.) is set to 'expand'. Quoting prevents this expansion, and the normalize-path editor command does it on demand. The normalize-command editor command expands commands in PATH into full paths on demand. Finally, cd and pushd interpret '-' as the old working directory (equivalent to the shell variable owd). This is not a substitution at all, but an abbreviation recognized by only those commands. Nonetheless, it too can be prevented by quoting. Commands The next three sections describe how the shell executes commands and deals with their input and output. Simple commands, pipelines and sequences A simple command is a sequence of words, the first of which specifies the command to be executed. A series of simple commands joined by '|' characters forms a pipeline. The output of each command in a pipeline is connected to the input of the next. Simple commands and pipelines may be joined into sequences with ';', and will be executed sequentially. Commands and pipelines can also be joined into sequences with '||' or '&&', indicating, as in the C language, that the second is to be executed only if the first fails or succeeds respectively. A simple command, pipeline or sequence may be placed in parentheses, '()', to form a simple command, which may in turn be a component of a pipeline or sequence. A command, pipeline or sequence can be executed without waiting for it to terminate by following it with an '&'. Builtin and non-builtin command execution Builtin commands are executed within the shell. If any component of a pipeline except the last is a builtin command, the pipeline is executed in a subshell. Parenthesized commands are always executed in a subshell. (cd; pwd); pwd thus prints the home directory, leaving you where you were (printing this after the home directory), while cd; pwd leaves you in the home directory. Parenthesized commands are most often used to prevent cd from affecting the current shell. When a command to be executed is found not to be a builtin command the shell attempts to execute the command via execve(2). Each word in the variable path names a directory in which the shell will look for the command. If the shell is not given a -f option, the shell hashes the names in these directories into an internal table so that it will try an execve(2) in only a directory where there is a possibility that the command resides there. This greatly speeds command location when a large number of directories are present in the search path. This hashing mechanism is not used: 1. If hashing is turned explicitly off via unhash. 2. If the shell was given a -f argument. 3. For each directory component of path which does not begin with a '\/'. 4. If the command contains a '\/'. In the above four cases the shell concatenates each component of the path vector with the given command name to form a path name of a file which it then attempts to execute it. If execution is successful, the search stops. If the file has execute permissions but is not an executable to the system (i.e., it is neither an executable binary nor a script that specifies its interpreter), then it is assumed to be a file containing shell commands and a new shell is spawned to read it. The shell special alias may be set to specify an interpreter other than the shell itself. On systems which do not understand the '#!' script interpreter convention the shell may be compiled to emulate it; see the version shell variable. If so, the shell checks the first line of the file to see if it is of the form '#!interpreter arg ...'. If it is, the shell starts interpreter with the given args and feeds the file to it on standard input. Input\/output The standard input and standard output of a command may be redirected with the following syntax: < name Open file name (which is first variable, command and filename expanded) as the standard input. << word Read the shell input up to a line which is identical to word. word is not subjected to variable, filename or command substitution, and each input line is compared to word before any substitutions are done on this input line. Unless a quoting '\\', '\"', '' or ''' appears in word variable and command substitution is performed on the intervening lines, allowing '\\' to quote '$', '\\' and '''. Commands which are substituted have all blanks, tabs, and newlines preserved, except for the final newline which is dropped. The resultant text is placed in an anonymous temporary file which is given to the command as standard input. > name >! name >& name >&! name The file name is used as standard output. If the file does not exist then it is created; if the file exists, it is truncated, its previous contents being lost. If the shell variable noclobber is set, then the file must not exist or be a character special file (e.g., a terminal or '\/dev\/null') or an error results. This helps prevent accidental destruction of files. In this case the '!' forms can be used to suppress this check. The forms involving '&' route the diagnostic output into the specified file as well as the standard output. name is expanded in the same way as '<' input filenames are. >> name >>& name >>! name >>&! name Like '>', but appends output to the end of name. If the shell variable noclobber is set, then it is an error for the file not to exist, unless one of the '!' forms is given. A command receives the environment in which the shell was invoked as modified by the input-output parameters and the presence of the command in a pipeline. Thus, unlike some previous shells, commands run from a file of shell commands have no access to the text of the commands by default; rather they receive the original standard input of the shell. The '<<' mechanism should be used to present inline data. This permits shell command scripts to function as components of pipelines and allows the shell to block read its input. Note that the default standard input for a command run detached is not the empty file \/dev\/null, but the original standard input of the shell. If this is a terminal and if the process attempts to read from the terminal, then the process will block and the user will be notified (see Jobs). Diagnostic output may be directed through a pipe with the standard output. Simply use the form '|&' rather than just '|'. The shell cannot presently redirect diagnostic output without also redirecting standard output, but '(command > output-file) >& error-file' is often an acceptable workaround. Either output-file or error-file may be '\/dev\/tty' to send output to the terminal. Features Having described how the shell accepts, parses and executes command lines, we now turn to a variety of its useful features. Control flow The shell contains a number of commands which can be used to regulate the flow of control in command files (shell scripts) and (in limited but useful ways) from terminal input. These commands all operate by forcing the shell to reread or skip in its input and, due to the implementation, restrict the placement of some of the commands. The foreach, switch, and while statements, as well as the if-then-else form of the if statement, require that the major keywords appear in a single simple command on an input line as shown below. If the shell's input is not seekable, the shell buffers up input whenever a loop is being read and performs seeks in this internal buffer to accomplish the rereading implied by the loop. (To the extent that this allows, backward gotos will succeed on non-seekable inputs.) Expressions The if, while and exit builtin commands use expressions with a common syntax. The expressions can include any of the operators described in the next three sections. Note that the @ builtin command (q.v.) has its own separate syntax. Logical, arithmetical and comparison operators These operators are similar to those of C and have the same precedence. They include || && | ^ & == != =~ !~ <= >= < > << >> + - * \/ % ! ~ ( ) Here the precedence increases to the right, '==' '!=' '=~' and '!~', '<=' '>=' '<' and '>', '<<' and '>>', '+' and '-', '*' '\/' and '%' being, in groups, at the same level. When multiple operators which have same precedence are used in one expression, calculation must be done from operator of right side. The '==' '!=' '=~' and '!~' operators compare their arguments as strings; all others operate on numbers. The operators '=~' and '!~' are like '!=' and '==' except that the right hand side is a glob-pattern (see Filename substitution) against which the left hand operand is matched. This reduces the need for use of the switch builtin command in shell scripts when all that is really needed is pattern matching. Null or missing arguments are considered '0'. The results of all expressions are strings, which represent decimal numbers. It is important to note that no two components of an expression can appear in the same word; except when adjacent to components of expressions which are syntactically significant to the parser ('&' '|' '<' '>' '(' ')') they should be surrounded by spaces. Command exit status Commands can be executed in expressions and their exit status returned by enclosing them in braces ('{}'). Remember that the braces should be separated from the words of the command by spaces. Command executions succeed, returning true, i.e., '1', if the command exits with status 0, otherwise they fail, returning false, i.e., '0'. If more detailed status information is required then the command should be executed outside of an expression and the status shell variable examined. File inquiry operators Some of these operators perform true\/false tests on files and related objects. They are of the form - op file, where op is one of r Read access w Write access x Execute access X Executable in the path or shell builtin, e.g., '-X ls' and '-X ls-F' are generally true, but '-X \/bin\/ls' is not (+) e Existence o Ownership z Zero size s Non-zero size (+) f Plain file d Directory l Symbolic link (+) * b Block special file (+) c Character special file (+) p Named pipe (fifo) (+) * S Socket special file (+) * u Set-user-ID bit is set (+) g Set-group-ID bit is set (+) k Sticky bit is set (+) t file (which must be a digit) is an open file descriptor for a terminal device (+) R Has been migrated (convex only) (+) L Applies subsequent operators in a multiple-operator test to a symbolic link rather than to the file to which the link points (+) * file is command and filename expanded and then tested to see if it has the specified relationship to the real user. If file does not exist or is inaccessible or, for the operators indicated by '*', if the specified file type does not exist on the current system, then all enquiries return false, i.e., '0'. These operators may be combined for conciseness: '-xy file' is equivalent to '-x file && -y file'. (+) For example, '-fx' is true (returns '1') for plain executable files, but not for directories. L may be used in a multiple-operator test to apply subsequent operators to a symbolic link rather than to the file to which the link points. For example, '-lLo' is true for links owned by the invoking user. Lr, Lw and Lx are always true for links and false for non-links. L has a different meaning when it is the last operator in a multiple-operator test; see below. It is possible but not useful, and sometimes misleading, to combine operators which expect file to be a file with operators which do not, (e.g., X and t). Following L with a non-file operator can lead to particularly strange results. Other operators return other information, i.e., not just '0' or '1'. (+) They have the same format as before; op may be one of A Last file access time, as the number of seconds since the epoch A: Like A, but in timestamp format, e.g., 'Fri May 14 16:36:10 1993' M Last file modification time M: Like M, but in timestamp format C Last inode modification time C: Like C, but in timestamp format D Device number I Inode number F Composite file identifier, in the form device:inode L The name of the file pointed to by a symbolic link N Number of (hard) links P Permissions, in octal, without leading zero P: Like P, with leading zero Pmode Equivalent to '-P file & mode', e.g., '-P22 file' returns '22' if file is writable by group and other, '20' if by group only, and '0' if by neither Pmode: Like Pmode, with leading zero U Numeric userid U: Username, or the numeric userid if the username is unknown G Numeric groupid G: Groupname, or the numeric groupid if the groupname is unknown Z Size, in bytes Only one of these operators may appear in a multiple-operator test, and it must be the last. Note that L has a different meaning at the end of and elsewhere in a multiple-operator test. Because '0' is a valid return value for many of these operators, they do not return '0' when they fail: most return '-1', and F returns ':'. If the shell is compiled with POSIX defined (see the version shell variable), the result of a file inquiry is based on the permission bits of the file and not on the result of the access(2) system call. For example, if one tests a file with -w whose permissions would ordinarily allow writing but which is on a file system mounted read-only, the test will succeed in a POSIX shell but fail in a non-POSIX shell. File inquiry operators can also be evaluated with the filetest builtin command (q.v.) (+). Jobs The shell associates a job with each pipeline. It keeps a table of current jobs, printed by the jobs command, and assigns them small integer numbers. When a job is started asynchronously with '&', the shell prints a line which looks like [1] 1234 indicating that the job which was started asynchronously was job number 1 and had one (top-level) process, whose process id was 1234. If you are running a job and wish to do something else you may hit the suspend key (usually '^Z'), which sends a STOP signal to the current job. The shell will then normally indicate that the job has been 'Suspended' and print another prompt. If the listjobs shell variable is set, all jobs will be listed like the jobs builtin command; if it is set to 'long' the listing will be in long format, like 'jobs -l'. You can then manipulate the state of the suspended job. You can put it in the ''background'' with the bg command or run some other commands and eventually bring the job back into the ''foreground'' with fg. (See also the run-fg-editor editor command.) A '^Z' takes effect immediately and is like an interrupt in that pending output and unread input are discarded when it is typed. The wait builtin command causes the shell to wait for all background jobs to complete. The '^]' key sends a delayed suspend signal, which does not generate a STOP signal until a program attempts to read(2) it, to the current job. This can usefully be typed ahead when you have prepared some commands for a job which you wish to stop after it has read them. The '^Y' key performs this function in csh(1); in tcsh, '^Y' is an editing command. (+) A job being run in the background stops if it tries to read from the terminal. Background jobs are normally allowed to produce output, but this can be disabled by giving the command 'stty tostop'. If you set this tty option, then background jobs will stop when they try to produce output like they do when they try to read input. There are several ways to refer to jobs in the shell. The character '%' introduces a job name. If you wish to refer to job number 1, you can name it as '%1'. Just naming a job brings it to the foreground; thus '%1' is a synonym for 'fg %1', bringing job 1 back into the foreground. Similarly, saying '%1 &' resumes job 1 in the background, just like 'bg %1'. A job can also be named by an unambiguous prefix of the string typed in to start it: '%ex' would normally restart a suspended ex(1) job, if there were only one suspended job whose name began with the string 'ex'. It is also possible to say '%?string' to specify a job whose text contains string, if there is only one such job. The shell maintains a notion of the current and previous jobs. In output pertaining to jobs, the current job is marked with a '+' and the previous job with a '-'. The abbreviations '%+', '%', and (by analogy with the syntax of the history mechanism) '%%' all refer to the current job, and '%-' refers to the previous job. The job control mechanism requires that the stty(1) option 'new' be set on some systems. It is an artifact from a 'new' implementation of the tty driver which allows generation of interrupt characters from the keyboard to tell jobs to stop. See stty(1) and the setty builtin command for details on setting options in the new tty driver. Status reporting The shell learns immediately whenever a process changes state. It normally informs you whenever a job becomes blocked so that no further progress is possible, but only right before it prints a prompt. This is done so that it does not otherwise disturb your work. If, however, you set the shell variable notify, the shell will notify you immediately of changes of status in background jobs. There is also a shell command notify which marks a single process so that its status changes will be immediately reported. By default notify marks the current process; simply say 'notify' after starting a background job to mark it. When you try to leave the shell while jobs are stopped, you will be warned that 'There are suspended jobs.' You may use the jobs command to see what they are. If you do this or immediately try to exit again, the shell will not warn you a second time, and the suspended jobs will be terminated. Automatic, periodic and timed events (+) There are various ways to run commands and take other actions automatically at various times in the ''life cycle'' of the shell. They are summarized here, and described in detail under the appropriate Builtin commands, Special shell variables and Special aliases. The sched builtin command puts commands in a scheduled-event list, to be executed by the shell at a given time. The beepcmd, cwdcmd, periodic, precmd, postcmd, and jobcmd Special aliases can be set, respectively, to execute commands when the shell wants to ring the bell, when the working directory changes, every tperiod minutes, before each prompt, before each command gets executed, after each command gets executed, and when a job is started or is brought into the foreground. The autologout shell variable can be set to log out or lock the shell after a given number of minutes of inactivity. The mail shell variable can be set to check for new mail periodically. The printexitvalue shell variable can be set to print the exit status of commands which exit with a status other than zero. The rmstar shell variable can be set to ask the user, when 'rm *' is typed, if that is really what was meant. The time shell variable can be set to execute the time builtin command after the completion of any process that takes more than a given number of CPU seconds. The watch and who shell variables can be set to report when selected users log in or out, and the log builtin command reports on those users at any time. Native Language System support (+) The shell is eight bit clean (if so compiled; see the version shell variable) and thus supports character sets needing this capability. NLS support differs depending on whether or not the shell was compiled to use the system's NLS (again, see version). In either case, 7-bit ASCII is the default character code (e.g., the classification of which characters are printable) and sorting, and changing the LANG or LC_CTYPE environment variables causes a check for possible changes in these respects. When using the system's NLS, the setlocale(3) function is called to determine appropriate character code\/classification and sorting (e.g., a 'en_CA.UTF-8' would yield \"UTF-8\" as a character code). This function typically examines the LANG and LC_CTYPE environment variables; refer to the system documentation for further details. When not using the system's NLS, the shell simulates it by assuming that the ISO 8859-1 character set is used whenever either of the LANG and LC_CTYPE variables are set, regardless of their values. Sorting is not affected for the simulated NLS. In addition, with both real and simulated NLS, all printable characters in the range \\200-\\377, i.e., those that have M-char bindings, are automatically rebound to self-insert-command. The corresponding binding for the escape-char sequence, if any, is left alone. These characters are not rebound if the NOREBIND environment variable is set. This may be useful for the simulated NLS or a primitive real NLS which assumes full ISO 8859-1. Otherwise, all M-char bindings in the range \\240-\\377 are effectively undone. Explicitly rebinding the relevant keys with bindkey is of course still possible. Unknown characters (i.e., those that are neither printable nor control characters) are printed in the format \\nnn. If the tty is not in 8 bit mode, other 8 bit characters are printed by converting them to ASCII and using standout mode. The shell never changes the 7\/8 bit mode of the tty and tracks user-initiated changes of 7\/8 bit mode. NLS users (or, for that matter, those who want to use a meta key) may need to explicitly set the tty in 8 bit mode through the appropriate stty(1) command in, e.g., the ~\/.login file. OS variant support (+) A number of new builtin commands are provided to support features in particular operating systems. All are described in detail in the Builtin commands section. On systems that support TCF (aix-ibm370, aix-ps2), getspath and setspath get and set the system execution path, getxvers and setxvers get and set the experimental version prefix and migrate migrates processes between sites. The jobs builtin prints the site on which each job is executing. Under BS2000, bs2cmd executes commands of the underlying BS2000\/OSD operating system. Under Domain\/OS, inlib adds shared libraries to the current environment, rootnode changes the rootnode and ver changes the systype. Under Mach, setpath is equivalent to Mach's setpath(1). Under Masscomp\/RTU and Harris CX\/UX, universe sets the universe. Under Harris CX\/UX, ucb or att runs a command under the specified universe. Under Convex\/OS, warp prints or sets the universe. The VENDOR, OSTYPE and MACHTYPE environment variables indicate respectively the vendor, operating system and machine type (microprocessor class or machine model) of the system on which the shell thinks it is running. These are particularly useful when sharing one's home directory between several types of machines; one can, for example, set path = (~\/bin.$MACHTYPE \/usr\/ucb \/bin \/usr\/bin .) in one's ~\/.login and put executables compiled for each machine in the appropriate directory. The version shell variable indicates what options were chosen when the shell was compiled. Note also the newgrp builtin, the afsuser and echo_style shell variables and the system-dependent locations of the shell's input files (see FILES). Signal handling Login shells ignore interrupts when reading the file ~\/.logout. The shell ignores quit signals unless started with -q. Login shells catch the terminate signal, but non-login shells inherit the terminate behavior from their parents. Other signals have the values which the shell inherited from its parent. In shell scripts, the shell's handling of interrupt and terminate signals can be controlled with onintr, and its handling of hangups can be controlled with hup and nohup. The shell exits on a hangup (see also the logout shell variable). By default, the shell's children do too, but the shell does not send them a hangup when it exits. hup arranges for the shell to send a hangup to a child when it exits, and nohup sets a child to ignore hangups. Terminal management (+) The shell uses three different sets of terminal (''tty'') modes: 'edit', used when editing, 'quote', used when quoting literal characters, and 'execute', used when executing commands. The shell holds some settings in each mode constant, so commands which leave the tty in a confused state do not interfere with the shell. The shell also matches changes in the speed and padding of the tty. The list of tty modes that are kept constant can be examined and modified with the setty builtin. Note that although the editor uses CBREAK mode (or its equivalent), it takes typed-ahead characters anyway. The echotc, settc and telltc commands can be used to manipulate and debug terminal capabilities from the command line. On systems that support SIGWINCH or SIGWINDOW, the shell adapts to window resizing automatically and adjusts the environment variables LINES and COLUMNS if set. If the environment variable TERMCAP contains li# and co# fields, the shell adjusts them to reflect the new window size.","Process Name":"csh","Link":"https:\/\/linux.die.net\/man\/1\/csh"}},{"Process":{"Description":"The csharp is an interactive C# shell that allows the user to enter and evaluate C# statements and expressions from the command line. The regular mcs command line options can be used in this version of the compiler. The gsharp command is a GUI version of the C# interpreter that uses Gtk# and provides an area to attach widgets as well. This version can be attached to other Gtk# applications in a safe way as it injects itself into the main loop of a Gtk# application, avoiding any problems arising from the multi-threaded nature of injecting itself into a target process. This version allows a number of scripts to be specified in the command line.","Process Name":"csharp","Link":"https:\/\/linux.die.net\/man\/1\/csharp"}},{"Process":{"Description":"the cshost(1) prints the list of machines that are allowed to make connections to cannaserver(1M). cannaserver(1M) allows network connections only from programs running on the same machine or machines listed in the file \/etc\/hosts.canna. If \/etc\/hosts.canna does not exist or the file is empty, access is granted to everyone.","Process Name":"cshost","Link":"https:\/\/linux.die.net\/man\/1\/cshost"}},{"Process":{"Description":"Output pieces of FILE separated by PATTERN(s) to files 'xx00', 'xx01', ..., and output byte counts of each piece to standard output. Mandatory arguments to long options are mandatory for short options too. -b, --suffix-format= FORMAT use sprintf FORMAT instead of %02d -f, --prefix= PREFIX use PREFIX instead of 'xx' -k, --keep-files do not remove output files on errors -n, --digits= DIGITS use specified number of digits instead of 2 -s, --quiet, --silent do not print counts of output file sizes -z, --elide-empty-files remove empty output files --help display this help and exit --version output version information and exit Read standard input if FILE is -. Each PATTERN may be: INTEGER copy up to but not including specified line number \/REGEXP\/[OFFSET] copy up to but not including a matching line %REGEXP%[OFFSET] skip to, but not including a matching line {INTEGER} repeat the previous pattern specified number of times {*} repeat the previous pattern as many times as possible A line OFFSET is a required '+' or '-' followed by a positive integer.","Process Name":"csplit","Link":"https:\/\/linux.die.net\/man\/1\/csplit"}},{"Process":{"Description":"The command opens an administration console and an xterm to all specified hosts. Any text typed into the administration console is replicated to all windows. All windows may also be typed into directly. This tool is intended for (but not limited to) cluster administration where the same configuration or commands must be run on each node within the cluster. Performing these commands all at once via this tool ensures all nodes are kept in sync. Connections are opened via ssh so a correctly installed and configured ssh installation is required. If, however, the program is called by \"crsh\" then the rsh protocol is used (and the communications channel is insecure), or by \"ctel\" then telnet is used. Extra caution should be taken when editing system files such as \/etc\/inet\/hosts as lines may not necessarily be in the same order. Assuming line 5 is the same across all servers and modifying that is dangerous. Better to search for the specific line to be changed and double-check before changes are committed. Further Notes Please also see \" KNOWN BUGS \". \u2022 The dotted line on any sub-menu is a tear-off, i.e. click on it and the sub-menu is turned into its own window. \u2022 Unchecking a hostname on the Hosts sub-menu will unplug the host from the cluster control window, so any text typed into the console is not sent to that host. Re-selecting it will plug it back in. \u2022 If your window manager menu bars are obscured by terminal windows see the \"screen_reserve_XXXXX\" options in the csshrc file (see \" FILES \"). \u2022 If the terminals overlap too much see the \"terminal_reserve_XXXXX\" options in the csshrc file (see \" FILES \"). \u2022 If the code is called as crsh instead of cssh (i.e. a symlink called crsh points to the cssh file or the file is renamed) rsh is used as the communications protocol instead of ssh. \u2022 If the code is called as ctel instead of cssh (i.e. a symlink called ctel points to the cssh file or the file is renamed) telnet is used as the communications protocol instead of ssh. \u2022 When using cssh on a large number of systems to connect back to a single system (e.g. you issue a command to the cluster to scp a file from a given location) and when these connections require authentication (i.e. you are going to authenticate with a password), the sshd daemon at that location may refuse connects after the number specified by MaxStartups in sshd_config is exceeded. (If this value is not set, it defaults to 10.) This is expected behavior; sshd uses this mechanism to prevent DoS attacks from unauthenticated sources. Please tune sshd_config and reload the SSH daemon, or consider using the ~\/.ssh\/authorized_keys mechanism for authentication if you encounter this problem. \u2022 If client windows fail to open, try running: \"cssh -e {single host name}\" This will test the mechanisms used to open windows to hosts. This could be due to either the \"-xrm\" terminal option which enables \"AllowSendEvents\" (some terminal do not require this option, other terminals have another method for enabling it - see your terminal documention) or the \"ConnectTimeout\" ssh option (see the configuration option \"-o\" or file \"csshrc\" below to resolve this).","Process Name":"cssh","Link":"https:\/\/linux.die.net\/man\/1\/cssh"}},{"Process":{"Description":"rancid is a perl(1) script which uses the login scripts (see clogin(1)) to login to a device, execute commands to display the configuration, etc, then filters the output for formatting, security, and so on. rancid's product is a file with the name of it's last argument plus the suffix .new. For example, hostname.new. There are complementary scripts for other platforms and\/or manufacturers that are supported by rancid(1). Briefly, these are: agmrancid Cisco Anomaly Guard Module (AGM) arancid Alteon WebOS switches arrancid Arista Networks devices brancid Bay Networks (nortel) cat5rancid Cisco catalyst switches cssrancid Cisco content services switches erancid ADC-kentrox EZ-T3 mux f10rancid Force10 f5rancid F5 BigIPs fnrancid Fortinet Firewalls francid Foundry and HP procurve OEMs of Foundry hrancid HP Procurve Switches htranicd Hitachi Routers jerancid Juniper Networks E-series jrancid Juniper Networks mrancid MRTd mrvrancid MRV optical switches nrancid Netscreen firewalls nsrancid Netscaler nxrancid Cisco Nexus boxes prancid Procket Networks rivrancid Riverstone rrancid Redback srancid SMC switch (some Dell OEMs) trancid Netopia sDSL\/T1 routers tntrancid Lucent TNT xrancid Extreme switches xrrancid Cisco IOS-XR boxes zrancid Zebra routing software The command-line options are as follows: -V Prints package name and version strings. -d Display debugging information. -l Display somewhat less debugging information. -f rancid should interpret the next argument as a filename which contains the output it would normally collect from the device ( hostname) with clogin(1).","Process Name":"cssrancid","Link":"https:\/\/linux.die.net\/man\/1\/cssrancid"}},{"Process":{"Description":"Cstream filters data streams, much like the UNIX tool dd(1). It has a more traditional commandline syntax, support for precise bandwidth limiting and reporting and support for FIFOs. Data limits and throughput rate calculation will work for files > 4 GB. Cstream reads from the standard input and writes to the standard output, if no filenames are given. It will also 'generate' or 'sink' data if desired. Options:       -b num'          Set the block size used for read\/write to num.  Thedefault is 8192 bytes. -B num' Buffer input up to num bytes before writing. The default is the blocksize. It is an error to set this to anything below the blocksize. Useful when writing tapes and simlilar that prefer few large writes of many small. -c num' Concurrent operation. Use a seperate process for outout. This is especially useful in combination with the -B option. 0 = use one process only (default) 1 = read process will buffer 2 = write process will buffer 3 = both processes will buffer. In combination with a large buffer size this will often load your memory heavily, everytime the reader transfers the buffer it collected to the writer. If you use -c 3 and have a buffer size of 128 Megabytes 256 MB of memory will be touched at once. -i num' -o num' Set the file names to use for input or output, respectivly. If the output file name is \"-\", data will just be discarded. If the input file name is \"-\", data will be generated 'out of the void'. If these options aren't given, stdin\/stout will be used. If you need to give -o or -i options and want stdin\/stdout, specify the empty string, like this: cstream -i'' If TCP support has been compiled in (default), hostname:portnumber will try to connect to the specified host at the specified port and :portnumber will open a TCP socket on the local machine and wait for a connection to arrive. SECURITY NOTE: cstream includes no mechanism to restrict the hosts that may connect to this port. Unless your machine has other network filters, anyone will be able to connect. -I string -O string Specify the type of input and output file, respectivly. If string includes 'f', a fifo will be created. If string includes 'a', the file will be assumed to be a opensound-compatible audio device and will be switched to CD-like settings. If string includes 't', a copy of the stream will be sent to file descriptor 3. If string includes 'N', TCP will not be used for that file even if the name has a \":\". -l' Include line count in statistics. -n num' Limit the total amount of data to num. If there is more input available, it will be discarded, cstream will exit after the limit has been reached. If there is less input, the limit will not be reached and no error will be signaled. num may have a trailing 'k', 'm' or 'g' which means Kilobytes, Megabytes or Gigabytes (where Kilo = 1024). This applies to all numeric options. -p filename Write the process id of cstream to filename. If cstream uses a seperate writer process (option -c), this is the pid of the parent (reader) process. -t num' Limit the throughput of the data stream to num bytes\/second. Limiting is done at the input side, you can rely on cstream not accepting more than this rate. If the number you give is positive, cstream accumulates errors and tries to keep the overall rate at the specified value, for the whole session. If you give a negative number, it is an upper limit for each read\/write system call pair. In other words: the negative number will never exceed that limit, the positive number will exceed it to make good for previous underutilization. -T num' Report throughput every num seconds. -v num' Set verbose level to num. By default, it is set to 0, which means no messages are displayed as long as no errors occur. A value of 1 means that total amount of data and throughput will be displayed at the end of program run. A value of 2 means the transfer rate since the end of the first read\/write pair will also be reported (useful when there is an initial delay). A value of 3 means there will also be seperate measurements for read and write. This option is resource-consuming and currently isn't implemented. A value of 4 means that notices about each single read\/write will be displayed. High values include all message types of lower values. -V' Print version number to stdout and exit with 0. filename A single filename as the last argument without an option switch will be used as input file if -i has not been used. SIGUSR1","Process Name":"cstream","Link":"https:\/\/linux.die.net\/man\/1\/cstream"}},{"Process":{"Description":"See: http:\/\/translate.sourceforge.net\/wiki\/toolkit\/csv2po for examples and usage instructions","Process Name":"csv2po","Link":"https:\/\/linux.die.net\/man\/1\/csv2po"}},{"Process":{"Description":"","Process Name":"csv2tbx","Link":"https:\/\/linux.die.net\/man\/1\/csv2tbx"}},{"Process":{"Description":"The csv2yapet utility converts a CSV file src to an YAPET file dst. The expected columns and column order of the CSV file is explained in the section called \"Source File Format\". For each successfully imported source file record a corresponding YAPET password record will be generated in the resulting file. After the successful conversion the file dst can be opened with yapet(1). The file suffix .pet is appended to the output file dst if not provided as part of the file path. The password either entered on the standard input or provided as command line argument using the -p option is used to encrypt the destination file dst and must be used to decrypt the file content when opening the file in yapet(1). csv2yapet will not overwrite existing destination files. If the destination file already exists csv2yapet will abort the conversion. Errors during the conversion process will be displayed unless the -q option is provided. Source File Format The source file src to be converted has to be organized as CSV file. The default character for delimiting the fields is ',' (comma) unless specified differently on the command line providing the -s option (refer to the section called \"OPTIONS\"). Fields containing the field separator character have to be enclosed in double quotes (\"). Double quote characters to be converted literally, has each to be preceded by another double quote character (see the section called \"EXAMPLES\"). The following table will describe the fields of the source CSV file as expected by csv2yapet: Table 1. Source file format See the section called \"EXAMPLES\" for an example CSV file.","Process Name":"csv2yapet","Link":"https:\/\/linux.die.net\/man\/1\/csv2yapet"}},{"Process":{"Description":"This manual page documents briefly the csync2 command. A verbose manual can be found on the csync2 homepage: http:\/\/oss.linbit.com\/csync2\/paper.pdf csync2 is a program for cluster synchronization.","Process Name":"csync2","Link":"https:\/\/linux.die.net\/man\/1\/csync2"}},{"Process":{"Description":"The ct_run program is automatically installed with Erlang\/OTP and Common Test (please see the Installation chapter in the Common Test User's Guide for more information). The program accepts a number of different start flags. Some flags trigger ct_run to start the Common Test application and pass on data to it. Some flags start an Erlang node prepared for running Common Test in a particular mode. There is an interface function that corresponds to this program, called ct:run_test\/1, for starting Common Test from the Erlang shell (or an Erlang program). Please see the ct man page for details. ct_run also accepts Erlang emulator flags. These are used when ct_run calls erl to start the Erlang node (making it possible to e.g. add directories to the code server path, change the cookie on the node, start additional applications, etc). With the optional flag: -erl_args it's possible to divide the options on the ct_run command line into two groups, one that Common Test should process (those preceding -erl_args), and one it should completely ignore and pass on directly to the emulator (those following -erl_args). Options preceding -erl_args that Common Test doesn't recognize, also get passed on to the emulator untouched. By means of -erl_args the user may specify flags with the same name, but with different destinations, on the ct_run command line. If -pa or -pz flags are specified in the Common Test group of options (preceding -erl_args), relative directories will be converted to absolute and re-inserted into the code path by Common Test (to avoid problems loading user modules when Common Test changes working directory during test runs). Common Test will however ignore -pa and -pz flags following -erl_args on the command line. These directories are added to the code path normally (i.e. on specified form) If ct_run is called with option: -help it prints all valid start flags to stdout.","Process Name":"ct_run","Link":"https:\/\/linux.die.net\/man\/1\/ct_run"}},{"Process":{"Description":"The ctags and etags programs (hereinafter collectively referred to as ctags, except where distinguished) generate an index (or \"tag\") file for a variety of language objects found in file(s). This tag file allows these items to be quickly and easily located by a text editor or other utility. A \"tag\" signifies a language object for which an index entry is available (or, alternatively, the index entry created for that object). Alternatively, ctags can generate a cross reference file which lists, in human readable form, information about the various source objects found in a set of language files. Tag index files are supported by numerous editors, which allow the user to locate the object associated with a name appearing in a source file and jump to the file and line which defines the name. Those known about at the time of this release are: vi(1) and its derivatives (e.g. Elvis, Vim, Vile, Lemmy), CRiSP, Emacs, FTE (Folding Text Editor), JED, jEdit, Mined, NEdit (Nirvana Edit), TSE (The SemWare Editor), UltraEdit, WorkSpace, X2, Zeus Ctags is capable of generating different kinds of tags for each of many different languages. For a complete list of supported languages, the names by which they are recognized, and the kinds of tags which are generated for each, see the --list-languages and --list-kinds options.","Process Name":"ctags","Link":"https:\/\/linux.die.net\/man\/1\/ctags"}},{"Process":{"Description":"The ctangle program converts a CWEB source document into a C program that may be compiled in the usual way. The output file includes #line specifications so that debugging can be done in terms of the CWEB source file. The cweave program converts the same CWEB file into a TeX file that may be formatted and printed in the usual way. It takes appropriate care of typographic details like page layout and the use of indentation, italics, boldface, etc., and it supplies extensive cross-index information that it gathers automatically. CWEB allows you to prepare a single document containing all the information that is needed both to produce a compilable C program and to produce a well-formatted document describing the program in as much detail as the writer may desire. The user of CWEB ought to be familiar with TeX as well as C. The command line should have one, two, or three names on it. The first is taken as the CWEB file (and .w is added if there is no extension). If that file cannot be opened, the extension .web is tried instead. (But .w is recommended, since .web usually implies Pascal.) If there is a second name, it is a change file (and .ch is added if there is no extension). The change file overrides parts of the WEB file, as described in the documentation. If there is a third name, it overrides the default name of the output file, which is ordinarily the same as the name of the input file (but on the current directory) with the extension .c or .tex. Options in the command line may be either turned off with - (if they are on by default) or turned on with + (if they are off by default). In fact, the options are processed from left to right, so a sequence like -f +f corresponds to +f (which is the default). The -b option suppresses the banner line that normally appears on your terminal when ctangle or cweave begins. The -h option suppresses the happy message that normally appears if the processing was successful. The -p option suppresses progress reports (starred module numbers) as the processing takes place. If you say -bhp, you get nothing but error messages. The +s option prints statistics about memory usage at the end of a run (assuming that the programs have been compiled with the -DSTAT switch). There are three other options applicable to cweave only: -f means do not force a newline after every statement in the formatted output. -e inhibits the enclosure of C material formatted by cweave in brackets \\PB{...}. Such brackets are normally inserted so that special hooks can be used by cweb-latex and similar programs. -x means omit the index and table of contents.","Process Name":"ctangle","Link":"https:\/\/linux.die.net\/man\/1\/ctangle"}},{"Process":{"Description":"ctdb is a utility to view and manage a ctdb cluster.","Process Name":"ctdb","Link":"https:\/\/linux.die.net\/man\/1\/ctdb"}},{"Process":{"Description":"ctdbd is the main ctdb daemon. ctdbd provides a clustered version of the TDB database with automatic rebuild\/recovery of the databases upon nodefailures. Combined with a cluster filesystem ctdbd provides a full HA environment for services such as clustered Samba and NFS as well as other services. ctdbd provides monitoring of all nodes in the cluster and automatically reconfigures the cluster and recovers upon node failures. ctdbd is the main component in clustered Samba that provides a high-availability load-sharing CIFS server cluster.","Process Name":"ctdbd","Link":"https:\/\/linux.die.net\/man\/1\/ctdbd"}},{"Process":{"Description":"The \"ctest\" executable is the CMake test driver program. CMake-generated build trees created for projects that use the ENABLE_TESTING and ADD_TEST commands have testing support. This program will run the tests and report results.","Process Name":"ctest","Link":"https:\/\/linux.die.net\/man\/1\/ctest"}},{"Process":{"Description":"The \"ctest\" executable is the CMake test driver program. CMake-generated build trees created for projects that use the ENABLE_TESTING and ADD_TEST commands have testing support. This program will run the tests and report results.","Process Name":"ctest28","Link":"https:\/\/linux.die.net\/man\/1\/ctest28"}},{"Process":{"Description":"The CTIE program is used to process multiple change files used with the CWEB system for literate programming. The CWEB programs allow one to apply patches to a source file (the masterfile) by means of a single change file. CTIE can be used to either merge up to 32 change files into a single one or to apply the changes to the masterfile in order to create a new master file. Output of either the new change file or the patched master file goes into outputfile. Any include files (indicated using the CWEB @i command) are fully expanded during CTIE processing, which means that the change files are able to modify the contents of any included files. Also, if this program has been compiled using the kpathsea library, then files are searched for using the CWEBINPUTS environment variable, as described in the kpathsea documentation.","Process Name":"ctie","Link":"https:\/\/linux.die.net\/man\/1\/ctie"}},{"Process":{"Description":"ctopy automates the parts of translating C source code to Python source code that are difficult for a human but easy for a machine. This allows a human programmer to concentrate on the parts of the translation that actually require human attention. ctopy uses regular expressions rather than actually parsing the C, so it is easily confused by syntactically invalid C or even irregular indentation styles. On the other hand, this approach makes it easier to preserve the location of comments. ctopy relies on the fact that C and Python have fundamentally similar constant and expression syntax; the only parts of C expressions that don't map over in a completely trivial way are ?: and comma expressions. These will be passed through unaltered. Exception: passing a comma expression as an argument to a C library function call may confuse ctopy into translating the function call incorrectly. ctopy also relies on the fact that C is normally indented in a way that indicates its block structure. It expects code to be indented in one of the canonical (K&R or Berkeley\/C++) styles. Running it on GNU-style code is not recommended. To avoid problems, run your code through indent(1) before handing it to copy; indent -orig or indent -kr should produce good results. Finally, ctopy relies on the fact that C and Python have several control structures that are both syntactically and semantically similar. It translates if\/else, while, and do\/while (the termination clause in each do\/while will require re-indenting). It doesn't try to translate switch\/case statements. ctopy will translate C for loops that map directly to a Python loop of the form \"for var in range(maxval)\" or \"for var in range(minval, maxval)\", where minval and maxval may be expressions. More complex loops it will pass through unaltered. It recognizes C idioms for looping forever and translates those into \"while True:\". ctopy throws away all type information and removes all variable declarations that don't have initializers. It maps the C '->' operator as well as the C '.' operator to Python '.'; this works because in Python all non-scalars are passed by reference. The C99 boolean constants 'true' and 'false' are mapped to Python 'True' and 'False' ctopy does not translate C structure and union declarations. It cannot tell the member declarations in these from variable declarations, and will try to remove them. Nor will ctopy translate most structure initializations correctly. Important exception: it can translate one-dimensional array initializers containing only scalars into Python tuples. ctopy does translate preprocessor defines and certain simple uses of enum types into Python. It also translates preprocessor macros with arguments into Python functions. Preprocessor macros that expand to syntactically invalid C fragments will be heedlessly translated into broken Python. ctopy knows a large number of mappings between C library headers and Python standard library modules, and applies them. Where possible, it translates C library functions to equivalent Python standard library calls. It emits warnings to standard error when calling conventions don't match. You can give ctopy hints. A hint may be a line in a file pulled in by the -h option, or tokens in a source file comment beginning with '$ctopy'. Available hints are as follows: type <name> Treat <name> as a scalar type, not requiring initialization except by first assignment. class <name> Treat name as a class; variables of this type must be initialized by calling the class's contructor and assigning the returned value to the variable. stringify <name> When this name occurs surrounded by word boundaries, put string quotes around it. Useful for translating enumerated-type values into string literals, which Python has as a first-class type. printflike <name> declarates that a function is like printf; its string arguments containing, if any, should be treated like printf formats and their arg lists replaced with % followed by a tuple containing the args.","Process Name":"ctopy","Link":"https:\/\/linux.die.net\/man\/1\/ctopy"}},{"Process":{"Description":"ctow converts the Canna text-form dictionary file into the Wnn text-form dictionary. If all dictionary files are omitted, this dictionary is input through the standard input. In this case, the Wnn dictionary is output to the standard output. If only one dictionary file is specified, it is regarded as the one of the Japanes Input System. In this case, the Wnn dictionary is output from the standard output.","Process Name":"ctow","Link":"https:\/\/linux.die.net\/man\/1\/ctow"}},{"Process":{"Description":"ctrans is a metafile translator, taking metafile(s), a metafile stored in the NCAR Computer Graphics Metafile (CGM) standard, and interpreting its instructions on the device defined by the GRAPHCAP environment variable. Fonts are stroked according to specifications in the Fontcap file defined by the FONTCAP environment variable. ctrans utilizes Graphcaps by default, see graphcap(5NCARG), while providing optional processing by user provided libraries, if that is required by the device or desired by the user. Thus, ctrans is capable of driving any device for which a Graphcap is available; with programming modifications, ctrans can accommodate any device for which an external library of plotting routines is available. Currently, the following Graphcap independent devices are supported: X11 under release 4 and 5, version 11 of X. ctrans can also translate metacode into the following raster formats: a60, avs, hdf, hppcl, nrif, sun and xwd. The device specifier for these raster formats is the name of the format. For example \"-d xwd\" specifies translation to an xwd formatted raster file. Additionally, a clear text driver, \"-d CTXT\", is available on any terminal. Not all of the aforementioned devices may be supported by your particular configuration of ctrans. For a list of supported devices see the gcaps(1NCARG) command. ctrans will read from the standard input if no metafile name is specified or the the name specified is '-'.","Process Name":"ctrans","Link":"https:\/\/linux.die.net\/man\/1\/ctrans"}},{"Process":{"Description":"This manual page documents briefly the ctunnel command. ctunnel is a command line program for tunneling and\/or proxying TCP or UDP connections via a cryptographic tunnel. ctunnel can be used to secure any existing TCP or UDP based protocol, such as (but not limited to) HTTP, Telnet, FTP, RSH, MySQL, VNC, SSH, XDMCP and NFS. ctunnel can also proxy connections, effectivly bouncing a cryptographic tunnel via any number of intermediary hosts (at a loss of speed of course).","Process Name":"ctunnel","Link":"https:\/\/linux.die.net\/man\/1\/ctunnel"}},{"Process":{"Description":"ctxtools is useful for dealing with ConTeXt source and generated files and for installing new versions of ConTeXt.","Process Name":"ctxtools","Link":"https:\/\/linux.die.net\/man\/1\/ctxtools"}},{"Process":{"Description":"The cu command is used to call up another system and act as a dial in terminal. It can also do simple file transfers with no error checking. cu takes a single argument, besides the options. If the argument is the string \"dir\" cu will make a direct connection to the port. This may only be used by users with write access to the port, as it permits reprogramming the modem. Otherwise, if the argument begins with a digit, it is taken to be a phone number to call. Otherwise, it is taken to be the name of a system to call. The -z or --system option may be used to name a system beginning with a digit, and the -c or --phone option may be used to name a phone number that does not begin with a digit. cu locates a port to use in the UUCP configuration files. If a simple system name is given, it will select a port appropriate for that system. The -p, --port, -l, --line, -s and --speed options may be used to control the port selection. When a connection is made to the remote system, cu forks into two processes. One reads from the port and writes to the terminal, while the other reads from the terminal and writes to the port. cu provides several commands that may be used during the conversation. The commands all begin with an escape character, initially ~ (tilde). The escape character is only recognized at the beginning of a line. To send an escape character to the remote system at the start of a line, it must be entered twice. All commands are either a single character or a word beginning with % (percent sign). cu recognizes the following commands: ~. Terminate the conversation. ~! command Run command in a shell. If command is empty, starts up a shell. ~$ command Run command, sending the standard output to the remote system. ~| command Run command, taking the standard input from the remote system. ~+ command Run command, taking the standard input from the remote system and sending the standard output to the remote system. ~#, ~%break Send a break signal, if possible. ~c directory, ~%cd directory Change the local directory. ~> file Send a file to the remote system. This just dumps the file over the communication line. It is assumed that the remote system is expecting it. ~< Receive a file from the remote system. This prompts for the local file name and for the remote command to execute to begin the file transfer. It continues accepting data until the contents of the eofread variable are seen. ~p from to, ~%put from to Send a file to a remote Unix system. This runs the appropriate commands on the remote system. ~t from to, ~%take from to Retrieve a file from a remote Unix system. This runs the appropriate commands on the remote system. ~s variable value Set a cu variable to the given value. If value is not given, the variable is set to true. ~! variable Set a cu variable to false. ~z Suspend the cu session. This is only supported on some systems. On systems for which ^Z may be used to suspend a job, ~^Z will also suspend the session. ~%nostop Turn off XON\/XOFF handling. ~%stop Turn on XON\/XOFF handling. ~v List all the variables and their values. ~? List all commands. cu also supports several variables. They may be listed with the ~v command, and set with the ~s or ~! commands. escape The escape character. Initially ~ (tilde). delay If this variable is true, cu will delay for a second after recognizing the escape character before printing the name of the local system. The default is true. eol The list of characters which are considered to finish a line. The escape character is only recognized after one of these is seen. The default is carriage return, ^U, ^C, ^O, ^D, ^S, ^Q, ^R. binary Whether to transfer binary data when sending a file. If this is false, then newlines in the file being sent are converted to carriage returns. The default is false. binary-prefix A string used before sending a binary character in a file transfer, if the binary variable is true. The default is ^V. echo-check Whether to check file transfers by examining what the remote system echoes back. This probably doesn't work very well. The default is false. echonl The character to look for after sending each line in a file. The default is carriage return. timeout The timeout to use, in seconds, when looking for a character, either when doing echo checking or when looking for the echonl character. The default is 30. kill The character to use delete a line if the echo check fails. The default is ^U. resend The number of times to resend a line if the echo check continues to fail. The default is 10. eofwrite The string to write after sending a file with the ~> command. The default is ^D. eofread The string to look for when receiving a file with the ~< command. The default is $, which is intended to be a typical shell prompt. verbose Whether to print accumulated information during a file transfer. The default is true.","Process Name":"cu","Link":"https:\/\/linux.die.net\/man\/1\/cu"}},{"Process":{"Description":"Draws a pulsating set of overlapping boxes with ever-chaning blobby patterns undulating across their surfaces.","Process Name":"cubenetic","Link":"https:\/\/linux.die.net\/man\/1\/cubenetic"}},{"Process":{"Description":"Draws a series of rotating 3D boxes that intersect each other and eventually fill space.","Process Name":"cubestorm","Link":"https:\/\/linux.die.net\/man\/1\/cubestorm"}},{"Process":{"Description":"Cue2toc converts cuefile from CUE to TOC format and writes the result to tocfile. If either cuefile or tocfile is omitted or a single dash \"-\" cue2toc reads from standard input and writes to standard ouput respectively. CUE files are text files describing the layout of a CD-Rom and typically carry the extension \".cue\". Cdrdao is a CD-burning application which has its own native TOC format to describe the disc layout. Although cdrdao has direct support for reading CUE files, it is currently limited to data tracks only. So cue2toc's main usefulness lies in converting CUE files containing audio tracks. Output of CD-Text data can be disabled with the -n option. CUE files often come with MP3 files but since cdrdao doesnt support decoding them on the fly they probably must be decoded by other means prior to writing the CD (e.g. using lame). For this reason you can specify a filename with the -w option to be used for all audio tracks instead of the one in the CUE file. Of course this is only really useful if all the tracks are based on the same file. This seems to be the case quite often however. Cue2toc normally displays warning messages for unsupported commands and constructs. The -q option disables these warnings.","Process Name":"cue2toc","Link":"https:\/\/linux.die.net\/man\/1\/cue2toc"}},{"Process":{"Description":"cups-config is the CUPS program configuration utility. It should be used by application developers to determine the necessary command-line options for the compiler and linker, as well as determining installation directories for filters, configuration files, and drivers.","Process Name":"cups-config","Link":"https:\/\/linux.die.net\/man\/1\/cups-config"}},{"Process":{"Description":"cupstestdsc tests the conformance of PostScript files to the Adobe PostScript Language Document Structuring Conventions Specification version 3.0. The results of testing and any other output are sent to the standard output. The second form of the command reads PostScript from the standard input.","Process Name":"cupstestdsc","Link":"https:\/\/linux.die.net\/man\/1\/cupstestdsc"}},{"Process":{"Description":"cupstestppd tests the conformance of PPD files to the Adobe PostScript Printer Description file format specification version 4.3. It can also be used to list the supported options and available fonts in a PPD file. The results of testing and any other output are sent to the standard output. The first form of cupstestppd tests one or more PPD files on the command-line. The second form tests the PPD file provided on the standard input.","Process Name":"cupstestppd","Link":"https:\/\/linux.die.net\/man\/1\/cupstestppd"}},{"Process":{"Description":"curl is a tool to transfer data from or to a server, using one of the supported protocols (HTTP, HTTPS, FTP, FTPS, SCP, SFTP, TFTP, DICT, TELNET, LDAP or FILE). The command is designed to work without user interaction. curl offers a busload of useful tricks like proxy support, user authentication, FTP upload, HTTP post, SSL connections, cookies, file transfer resume and more. As you will see below, the number of features will make your head spin! curl is powered by libcurl for all transfer-related features. See libcurl(3) for details.","Process Name":"curl","Link":"https:\/\/linux.die.net\/man\/1\/curl"}},{"Process":{"Description":"curl-config displays information about the curl and libcurl installation.","Process Name":"curl-config","Link":"https:\/\/linux.die.net\/man\/1\/curl-config"}},{"Process":{"Description":"The program curlftpfs is a tool to mount remote ftp hosts as local directories. It connects to the host FTP server and maps its directory structure to the path directory. curlftpfs is powered by libcurl for all transfer-related features. See libcurl(3) for details. The filesystem part is implemented on top of FUSE. See http:\/\/fuse.sourceforge.net\/ for details.","Process Name":"curlftpfs","Link":"https:\/\/linux.die.net\/man\/1\/curlftpfs"}},{"Process":{"Description":"Print selected parts of lines from each FILE to standard output. Mandatory arguments to long options are mandatory for short options too. -b, --bytes= LIST select only these bytes -c, --characters= LIST select only these characters -d, --delimiter= DELIM use DELIM instead of TAB for field delimiter -f, --fields= LIST select only these fields; also print any line that contains no delimiter character, unless the -s option is specified -n with -b: don't split multibyte characters --complement complement the set of selected bytes, characters or fields -s, --only-delimited do not print lines not containing delimiters --output-delimiter= STRING use STRING as the output delimiter the default is to use the input delimiter --help display this help and exit --version output version information and exit Use one, and only one of -b, -c or -f. Each LIST is made up of one range, or many ranges separated by commas. Selected input is written in the same order that it is read, and is written exactly once. Each range is one of: N N'th byte, character or field, counted from 1 N- from N'th byte, character or field, to end of line N-M from N'th to M'th (included) byte, character or field -M from first to M'th (included) byte, character or field With no FILE, or when FILE is -, read standard input.","Process Name":"cut","Link":"https:\/\/linux.die.net\/man\/1\/cut"}},{"Process":{"Description":"Cutmp3 is a small and fast command line MP3 editor. It lets you select sections of an MP3 interactively or via a timetable and save them to separate files without quality loss. It uses mpg123 for playback and works with VBR files and even with files bigger than 2GB. Other features are configurable silence seeking and ID3 tag seeking, which are useful for concatenated mp3s.","Process Name":"cutmp3","Link":"https:\/\/linux.die.net\/man\/1\/cutmp3"}},{"Process":{"Description":"","Process Name":"cvs","Link":"https:\/\/linux.die.net\/man\/1\/cvs"}},{"Process":{"Description":"Create a new git repository based on the version history stored in a CVS repository. Each CVS commit will be mirrored in the git repository, including such information as date of commit and id of the committer. The output of this program are a \"blobfile\" and a \"dumpfile\", which together can be loaded into a git repository using \"git fast-import\". CVS-REPOS-PATH is the filesystem path of the part of the CVS repository that you want to convert. This path doesn't have to be the top level directory of a CVS repository; it can point at a project within a repository, in which case only that project will be converted. This path or one of its parent directories has to contain a subdirectory called CVSROOT (though the CVSROOT directory can be empty). It is not possible directly to convert a CVS repository to which you only have remote access, but the FAQ describes tools that may be used to create a local copy of a remote CVS repository.","Process Name":"cvs2git","Link":"https:\/\/linux.die.net\/man\/1\/cvs2git"}},{"Process":{"Description":"Create a new Subversion repository based on the version history stored in a CVS repository. Each CVS commit will be mirrored in the Subversion repository, including such information as date of commit and id of the committer. CVS-REPOS-PATH is the filesystem path of the part of the CVS repository that you want to convert. It is not possible to convert a CVS repository to which you only have remote access; see the FAQ for more information. This path doesn't have to be the top level directory of a CVS repository; it can point at a project within a repository, in which case only that project will be converted. This path or one of its parent directories has to contain a subdirectory called CVSROOT (though the CVSROOT directory can be empty). Multiple CVS repositories can be converted into a single Subversion repository in a single run of cvs2svn, but only by using an --options file.","Process Name":"cvs2svn","Link":"https:\/\/linux.die.net\/man\/1\/cvs2svn"}},{"Process":{"Description":"cvsblame opens Konqueror to display the output of cvs annotate of a cvs controlled file. When the mouse is on a revision number shown in the second column, a popup with the respective log message appears. In the popup, a proper mailto: link to the author of a revision can be created as follows: In your home directory, make a file .cvsblame. In that file, enter for each repository you are working with a line like accounts :pserver:gehrmab@cvs.kde.org:\/home\/kde \/home\/bernd\/.kdeaccounts where the accounts file contains a simple list of cvs usernames in the first column and the respective mail address in the second.","Process Name":"cvsblame","Link":"https:\/\/linux.die.net\/man\/1\/cvsblame"}},{"Process":{"Description":"cvscheck prints information about the status of your local CVS checkout without communicating with the server. This means it is extremely fast and does not require a network connection. The given directory and all of its subdirectories will be processed recursively. If no directory is given, the current directory and its recursed subdirectories will be used. Each file with an interesting status will be printed with a status character in front of its name. The status characters are as follows. ? foobar.c The file is not known to CVS M foobar.c The file is definitely locally modified m foobar.c The file might have local changes. You should diff with the server to make sure. C foobar.c The file has a CVS conflict and therefore cannot be committed. U foobar.c This file is in CVS but is missing in your local checkout. T foobar.c This file has an unusual sticky CVS tag. A foobar.c You have done a cvs add for this file, but have not yet committed it. R foobar.c You have done a cvs rm for this file, but have not yet committed it. This utility is part of the KDE Software Development Kit.","Process Name":"cvscheck","Link":"https:\/\/linux.die.net\/man\/1\/cvscheck"}},{"Process":{"Description":"CvsGraph generates images and image maps from CVS\/RCS repository files.","Process Name":"cvsgraph","Link":"https:\/\/linux.die.net\/man\/1\/cvsgraph"}},{"Process":{"Description":"cvslastchange displays the last change committed to CVS for a file. It uses cvs diff and cvs log to do this. cvslastchange works on any CVS branch, not just HEAD. This utility is part of the KDE Software Development Kit.","Process Name":"cvslastchange","Link":"https:\/\/linux.die.net\/man\/1\/cvslastchange"}},{"Process":{"Description":"cvslastlog shows the log associated with the last CVS commit for the given file. It depends on the version of the local file, not the one on the server. This utility is part of the KDE Software Development Kit.","Process Name":"cvslastlog","Link":"https:\/\/linux.die.net\/man\/1\/cvslastlog"}},{"Process":{"Description":"CVSps is a program for generating 'patchset' information from a CVS repository. A patchset in this case is defined as a set of changes made to a collection of files, and all committed at the same time (using a single 'cvs commit' command). This information is valuable to seeing the big picture of the evolution of a cvs project. While cvs tracks revision information, it is often difficult to see what changes were committed","Process Name":"cvsps","Link":"https:\/\/linux.die.net\/man\/1\/cvsps"}},{"Process":{"Description":"cvsrevertlast is used to revert all the files on the command line by one version in CVS. The files will not be committed. This utility is part of the KDE Software Development Kit.","Process Name":"cvsrevertlast","Link":"https:\/\/linux.die.net\/man\/1\/cvsrevertlast"}},{"Process":{"Description":"cvsversion displays the version in CVS of a file, as known by the local checked out directory. No connection is required to the CVS server. It can be used in other scripts, or simply to ask for diffs using cvs diff -r <version> [-r <version>] <file>","Process Name":"cvsversion","Link":"https:\/\/linux.die.net\/man\/1\/cvsversion"}},{"Process":{"Description":"Cvt is a utility for calculating VESA Coordinated Video Timing modes. Given the desired horizontal and vertical resolutions, a modeline adhering to the CVT standard is printed. This modeline can be included in Xorg xorg.conf(5)","Process Name":"cvt","Link":"https:\/\/linux.die.net\/man\/1\/cvt"}},{"Process":{"Description":"Converts a file encoded in a specified or default non-unicode encoding to unicode, or, if the file is already in unicode, converts it to a specified or default non-unicode encoding. The converted text is printed to standard out.","Process Name":"cvtenc","Link":"https:\/\/linux.die.net\/man\/1\/cvtenc"}},{"Process":{"Description":"The ctangle program converts a CWEB source document into a C program that may be compiled in the usual way. The output file includes #line specifications so that debugging can be done in terms of the CWEB source file. The cweave program converts the same CWEB file into a TeX file that may be formatted and printed in the usual way. It takes appropriate care of typographic details like page layout and the use of indentation, italics, boldface, etc., and it supplies extensive cross-index information that it gathers automatically. CWEB allows you to prepare a single document containing all the information that is needed both to produce a compilable C program and to produce a well-formatted document describing the program in as much detail as the writer may desire. The user of CWEB ought to be familiar with TeX as well as C. The command line should have one, two, or three names on it. The first is taken as the CWEB file (and .w is added if there is no extension). If that file cannot be opened, the extension .web is tried instead. (But .w is recommended, since .web usually implies Pascal.) If there is a second name, it is a change file (and .ch is added if there is no extension). The change file overrides parts of the WEB file, as described in the documentation. If there is a third name, it overrides the default name of the output file, which is ordinarily the same as the name of the input file (but on the current directory) with the extension .c or .tex. Options in the command line may be either turned off with - (if they are on by default) or turned on with + (if they are off by default). In fact, the options are processed from left to right, so a sequence like -f +f corresponds to +f (which is the default). The -b option suppresses the banner line that normally appears on your terminal when ctangle or cweave begins. The -h option suppresses the happy message that normally appears if the processing was successful. The -p option suppresses progress reports (starred module numbers) as the processing takes place. If you say -bhp, you get nothing but error messages. The +s option prints statistics about memory usage at the end of a run (assuming that the programs have been compiled with the -DSTAT switch). There are three other options applicable to cweave only: -f means do not force a newline after every statement in the formatted output. -e inhibits the enclosure of C material formatted by cweave in brackets \\PB{...}. Such brackets are normally inserted so that special hooks can be used by cweb-latex and similar programs. -x means omit the index and table of contents.","Process Name":"cweave","Link":"https:\/\/linux.die.net\/man\/1\/cweave"}},{"Process":{"Description":"The ctangle program converts a CWEB source document into a C program that may be compiled in the usual way. The output file includes #line specifications so that debugging can be done in terms of the CWEB source file. The cweave program converts the same CWEB file into a TeX file that may be formatted and printed in the usual way. It takes appropriate care of typographic details like page layout and the use of indentation, italics, boldface, etc., and it supplies extensive cross-index information that it gathers automatically. CWEB allows you to prepare a single document containing all the information that is needed both to produce a compilable C program and to produce a well-formatted document describing the program in as much detail as the writer may desire. The user of CWEB ought to be familiar with TeX as well as C. The command line should have one, two, or three names on it. The first is taken as the CWEB file (and .w is added if there is no extension). If that file cannot be opened, the extension .web is tried instead. (But .w is recommended, since .web usually implies Pascal.) If there is a second name, it is a change file (and .ch is added if there is no extension). The change file overrides parts of the WEB file, as described in the documentation. If there is a third name, it overrides the default name of the output file, which is ordinarily the same as the name of the input file (but on the current directory) with the extension .c or .tex. Options in the command line may be either turned off with - (if they are on by default) or turned on with + (if they are off by default). In fact, the options are processed from left to right, so a sequence like -f +f corresponds to +f (which is the default). The -b option suppresses the banner line that normally appears on your terminal when ctangle or cweave begins. The -h option suppresses the happy message that normally appears if the processing was successful. The -p option suppresses progress reports (starred module numbers) as the processing takes place. If you say -bhp, you get nothing but error messages. The +s option prints statistics about memory usage at the end of a run (assuming that the programs have been compiled with the -DSTAT switch). There are three other options applicable to cweave only: -f means do not force a newline after every statement in the formatted output. -e inhibits the enclosure of C material formatted by cweave in brackets \\PB{...}. Such brackets are normally inserted so that special hooks can be used by cweb-latex and similar programs. -x means omit the index and table of contents.","Process Name":"cweb","Link":"https:\/\/linux.die.net\/man\/1\/cweb"}},{"Process":{"Description":"cxmon is an interactive command-driven file manipulation tool that is inspired by the \"Amiga Monitor\" by Timo Rossi. It has commands and features similar to a machine code monitor\/debugger, but it lacks any functions for running\/tracing code. There are, however, built-in PowerPC, 680x0, 80x86, 6502 and Z80 disassemblers and special support for disassembling MacOS code. By default, cxmon operates on a fixed-size (but adjustable) memory buffer with adresses starting at 0. Type \"h\" to get a list of supported commands. For more information, see the included \"README\" file.","Process Name":"cxmon","Link":"https:\/\/linux.die.net\/man\/1\/cxmon"}},{"Process":{"Description":"The cxpm program can be used to check the format of any XPM (version 1, 2, or 3) file. On error, unlike sxpm, cxpm prints out an error message indicating where the parser choked. This should help finding out what's wrong with an XPM file but do not expect too much from it though. This is not even close from being some kind of lint program for XPM. First, it stops at the first error it encounters - so several fix and retry cycles may be necessary to get your file to parse successfully. Second, cxpm only cares about the format. If, for instance, your pixmap uses too many colors for your system you still may experience difficulties displaying it. Be warned. When no filename is given cxpm reads from the standard input.","Process Name":"cxpm","Link":"https:\/\/linux.die.net\/man\/1\/cxpm"}},{"Process":{"Description":"The cxref utility shall analyze a collection of C-language files and attempt to build a cross-reference table. Information from #define lines shall be included in the symbol table. A sorted listing shall be written to standard output of all symbols (auto, static, and global) in each file separately, or with the -c option, in combination. Each symbol shall contain an asterisk before the declaring reference.","Process Name":"cxref","Link":"https:\/\/linux.die.net\/man\/1\/cxref"}},{"Process":{"Description":"cxxmetric counts lines of code, comment and blank space and calculates various other statistics for each given source file. Source files must be in C or C++. This utility is part of the KDE Software Development Kit.","Process Name":"cxxmetric","Link":"https:\/\/linux.die.net\/man\/1\/cxxmetric"}},{"Process":{"Description":"The cyphesis tools manage the data and configuration files used by cyphesis. The cyaddrules tool uploads Atlas XML rules files into the a running server. If rules already exist in the server, then the default attributes are updated, and if the rule has a script associated with it, then the script is reloaded from file. cyaddrules can either upload the file given on the command line, or if none is specified it will upload the rule file for the ruleset specified in the server config, and any rule files it depends on. cyaddrules is typically used to add additional rules during rule development without having to restart the server. Rules can be loaded directly into a running server using cyaddrules. The cyconfig program provides an easy scriptable way to make permanent changes to the server config from the command line. It works by writing new config options to .cyphesis.vconf in the users home directory, which is read by the server at startup, and overrides the contents of the main config file. For a description of the various options that the server takes, please see cyphesis(1). The cyconvertrules program converts Atlas XML rules files from an older historic format into the format used by cyphesis since version 0.3.1. The cydumprules program dumps rules from the database table used by the server into a set of Atlas XML rules files. Note that this does not produce a set of files containing the same rules as those that were loaded in using cyloadrules because some of the rules in the rulesets that were not top level may have been overloaded, and do not exist in the database table. Typically this command will be used to get an updated copy of the top level ruleset from the server database, and the rest of the files will be ignored. The cyloadrules tool loads Atlas XML rules files into the database table used by the server. The rules table is read by the server at startup, so a server restart is typically required after loading rules. cyloadrules can either load the file given on the command line, or if none is specified it will load the rule file for the ruleset specified in the server config, and any rule files it depends on. Rules can be loaded directly into a running server using cyaddrules. The cypasswd tool is used to administrate the accounts table in the server database.","Process Name":"cyaddrules","Link":"https:\/\/linux.die.net\/man\/1\/cyaddrules"}},{"Process":{"Description":"The cyclient program connects to a cyphesis server and populates the world with data.","Process Name":"cyclient","Link":"https:\/\/linux.die.net\/man\/1\/cyclient"}},{"Process":{"Description":"The cycmd program provides a commandline interface to a cyphesis server.","Process Name":"cycmd","Link":"https:\/\/linux.die.net\/man\/1\/cycmd"}},{"Process":{"Description":"The cyphesis tools manage the data and configuration files used by cyphesis. The cyaddrules tool uploads Atlas XML rules files into the a running server. If rules already exist in the server, then the default attributes are updated, and if the rule has a script associated with it, then the script is reloaded from file. cyaddrules can either upload the file given on the command line, or if none is specified it will upload the rule file for the ruleset specified in the server config, and any rule files it depends on. cyaddrules is typically used to add additional rules during rule development without having to restart the server. Rules can be loaded directly into a running server using cyaddrules. The cyconfig program provides an easy scriptable way to make permanent changes to the server config from the command line. It works by writing new config options to .cyphesis.vconf in the users home directory, which is read by the server at startup, and overrides the contents of the main config file. For a description of the various options that the server takes, please see cyphesis(1). The cyconvertrules program converts Atlas XML rules files from an older historic format into the format used by cyphesis since version 0.3.1. The cydumprules program dumps rules from the database table used by the server into a set of Atlas XML rules files. Note that this does not produce a set of files containing the same rules as those that were loaded in using cyloadrules because some of the rules in the rulesets that were not top level may have been overloaded, and do not exist in the database table. Typically this command will be used to get an updated copy of the top level ruleset from the server database, and the rest of the files will be ignored. The cyloadrules tool loads Atlas XML rules files into the database table used by the server. The rules table is read by the server at startup, so a server restart is typically required after loading rules. cyloadrules can either load the file given on the command line, or if none is specified it will load the rule file for the ruleset specified in the server config, and any rule files it depends on. Rules can be loaded directly into a running server using cyaddrules. The cypasswd tool is used to administrate the accounts table in the server database.","Process Name":"cyconvertrules","Link":"https:\/\/linux.die.net\/man\/1\/cyconvertrules"}},{"Process":{"Description":"The cyphesis tools manage the data and configuration files used by cyphesis. The cyaddrules tool uploads Atlas XML rules files into the a running server. If rules already exist in the server, then the default attributes are updated, and if the rule has a script associated with it, then the script is reloaded from file. cyaddrules can either upload the file given on the command line, or if none is specified it will upload the rule file for the ruleset specified in the server config, and any rule files it depends on. cyaddrules is typically used to add additional rules during rule development without having to restart the server. Rules can be loaded directly into a running server using cyaddrules. The cyconfig program provides an easy scriptable way to make permanent changes to the server config from the command line. It works by writing new config options to .cyphesis.vconf in the users home directory, which is read by the server at startup, and overrides the contents of the main config file. For a description of the various options that the server takes, please see cyphesis(1). The cyconvertrules program converts Atlas XML rules files from an older historic format into the format used by cyphesis since version 0.3.1. The cydumprules program dumps rules from the database table used by the server into a set of Atlas XML rules files. Note that this does not produce a set of files containing the same rules as those that were loaded in using cyloadrules because some of the rules in the rulesets that were not top level may have been overloaded, and do not exist in the database table. Typically this command will be used to get an updated copy of the top level ruleset from the server database, and the rest of the files will be ignored. The cyloadrules tool loads Atlas XML rules files into the database table used by the server. The rules table is read by the server at startup, so a server restart is typically required after loading rules. cyloadrules can either load the file given on the command line, or if none is specified it will load the rule file for the ruleset specified in the server config, and any rule files it depends on. Rules can be loaded directly into a running server using cyaddrules. The cypasswd tool is used to administrate the accounts table in the server database.","Process Name":"cydumprules","Link":"https:\/\/linux.die.net\/man\/1\/cydumprules"}},{"Process":{"Description":"The cyphesis tools manage the data and configuration files used by cyphesis. The cyaddrules tool uploads Atlas XML rules files into the a running server. If rules already exist in the server, then the default attributes are updated, and if the rule has a script associated with it, then the script is reloaded from file. cyaddrules can either upload the file given on the command line, or if none is specified it will upload the rule file for the ruleset specified in the server config, and any rule files it depends on. cyaddrules is typically used to add additional rules during rule development without having to restart the server. Rules can be loaded directly into a running server using cyaddrules. The cyconfig program provides an easy scriptable way to make permanent changes to the server config from the command line. It works by writing new config options to .cyphesis.vconf in the users home directory, which is read by the server at startup, and overrides the contents of the main config file. For a description of the various options that the server takes, please see cyphesis(1). The cyconvertrules program converts Atlas XML rules files from an older historic format into the format used by cyphesis since version 0.3.1. The cydumprules program dumps rules from the database table used by the server into a set of Atlas XML rules files. Note that this does not produce a set of files containing the same rules as those that were loaded in using cyloadrules because some of the rules in the rulesets that were not top level may have been overloaded, and do not exist in the database table. Typically this command will be used to get an updated copy of the top level ruleset from the server database, and the rest of the files will be ignored. The cyloadrules tool loads Atlas XML rules files into the database table used by the server. The rules table is read by the server at startup, so a server restart is typically required after loading rules. cyloadrules can either load the file given on the command line, or if none is specified it will load the rule file for the ruleset specified in the server config, and any rule files it depends on. Rules can be loaded directly into a running server using cyaddrules. The cypasswd tool is used to administrate the accounts table in the server database.","Process Name":"cyloadrules","Link":"https:\/\/linux.die.net\/man\/1\/cyloadrules"}},{"Process":{"Description":"A hack similar to 'greynetic', but less frenetic. The first implementation was by Stephen Linhart; then Ozymandias G. Desiderata wrote a Java applet clone. That clone was discovered by Jamie Zawinski, and ported to C for inclusion here.","Process Name":"cynosure","Link":"https:\/\/linux.die.net\/man\/1\/cynosure"}},{"Process":{"Description":"The cyphesis tools manage the data and configuration files used by cyphesis. The cyaddrules tool uploads Atlas XML rules files into the a running server. If rules already exist in the server, then the default attributes are updated, and if the rule has a script associated with it, then the script is reloaded from file. cyaddrules can either upload the file given on the command line, or if none is specified it will upload the rule file for the ruleset specified in the server config, and any rule files it depends on. cyaddrules is typically used to add additional rules during rule development without having to restart the server. Rules can be loaded directly into a running server using cyaddrules. The cyconfig program provides an easy scriptable way to make permanent changes to the server config from the command line. It works by writing new config options to .cyphesis.vconf in the users home directory, which is read by the server at startup, and overrides the contents of the main config file. For a description of the various options that the server takes, please see cyphesis(1). The cyconvertrules program converts Atlas XML rules files from an older historic format into the format used by cyphesis since version 0.3.1. The cydumprules program dumps rules from the database table used by the server into a set of Atlas XML rules files. Note that this does not produce a set of files containing the same rules as those that were loaded in using cyloadrules because some of the rules in the rulesets that were not top level may have been overloaded, and do not exist in the database table. Typically this command will be used to get an updated copy of the top level ruleset from the server database, and the rest of the files will be ignored. The cyloadrules tool loads Atlas XML rules files into the database table used by the server. The rules table is read by the server at startup, so a server restart is typically required after loading rules. cyloadrules can either load the file given on the command line, or if none is specified it will load the rule file for the ruleset specified in the server config, and any rule files it depends on. Rules can be loaded directly into a running server using cyaddrules. The cypasswd tool is used to administrate the accounts table in the server database.","Process Name":"cypasswd","Link":"https:\/\/linux.die.net\/man\/1\/cypasswd"}},{"Process":{"Description":"The cyphesis program implements the core world server for WorldForge.","Process Name":"cyphesis","Link":"https:\/\/linux.die.net\/man\/1\/cyphesis"}},{"Process":{"Description":"The cyphesis tools manage the data and configuration files used by cyphesis. The cyaddrules tool uploads Atlas XML rules files into the a running server. If rules already exist in the server, then the default attributes are updated, and if the rule has a script associated with it, then the script is reloaded from file. cyaddrules can either upload the file given on the command line, or if none is specified it will upload the rule file for the ruleset specified in the server config, and any rule files it depends on. cyaddrules is typically used to add additional rules during rule development without having to restart the server. Rules can be loaded directly into a running server using cyaddrules. The cyconfig program provides an easy scriptable way to make permanent changes to the server config from the command line. It works by writing new config options to .cyphesis.vconf in the users home directory, which is read by the server at startup, and overrides the contents of the main config file. For a description of the various options that the server takes, please see cyphesis(1). The cyconvertrules program converts Atlas XML rules files from an older historic format into the format used by cyphesis since version 0.3.1. The cydumprules program dumps rules from the database table used by the server into a set of Atlas XML rules files. Note that this does not produce a set of files containing the same rules as those that were loaded in using cyloadrules because some of the rules in the rulesets that were not top level may have been overloaded, and do not exist in the database table. Typically this command will be used to get an updated copy of the top level ruleset from the server database, and the rest of the files will be ignored. The cyloadrules tool loads Atlas XML rules files into the database table used by the server. The rules table is read by the server at startup, so a server restart is typically required after loading rules. cyloadrules can either load the file given on the command line, or if none is specified it will load the rule file for the ruleset specified in the server config, and any rule files it depends on. Rules can be loaded directly into a running server using cyaddrules. The cypasswd tool is used to administrate the accounts table in the server database.","Process Name":"cyphesis-tools","Link":"https:\/\/linux.die.net\/man\/1\/cyphesis-tools"}},{"Process":{"Description":"This module implements cyradm in Perl. It is a shell around Cyrus::IMAP::Admin. Commands are provided in both Tcl-compatible forms and GNU-style long option forms.","Process Name":"cyradm","Link":"https:\/\/linux.die.net\/man\/1\/cyradm"}}]