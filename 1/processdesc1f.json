[{"Process":{"Description":"rancid is a perl(1) script which uses the login scripts (see clogin(1)) to login to a device, execute commands to display the configuration, etc, then filters the output for formatting, security, and so on. rancid's product is a file with the name of it's last argument plus the suffix .new. For example, hostname.new. There are complementary scripts for other platforms and\/or manufacturers that are supported by rancid(1). Briefly, these are: agmrancid Cisco Anomaly Guard Module (AGM) arancid Alteon WebOS switches arrancid Arista Networks devices brancid Bay Networks (nortel) cat5rancid Cisco catalyst switches cssrancid Cisco content services switches erancid ADC-kentrox EZ-T3 mux f10rancid Force10 f5rancid F5 BigIPs fnrancid Fortinet Firewalls francid Foundry and HP procurve OEMs of Foundry hrancid HP Procurve Switches htranicd Hitachi Routers jerancid Juniper Networks E-series jrancid Juniper Networks mrancid MRTd mrvrancid MRV optical switches nrancid Netscreen firewalls nsrancid Netscaler nxrancid Cisco Nexus boxes prancid Procket Networks rivrancid Riverstone rrancid Redback srancid SMC switch (some Dell OEMs) trancid Netopia sDSL\/T1 routers tntrancid Lucent TNT xrancid Extreme switches xrrancid Cisco IOS-XR boxes zrancid Zebra routing software The command-line options are as follows: -V Prints package name and version strings. -d Display debugging information. -l Display somewhat less debugging information. -f rancid should interpret the next argument as a filename which contains the output it would normally collect from the device ( hostname) with clogin(1).","Process Name":"f10rancid","Link":"https:\/\/linux.die.net\/man\/1\/f10rancid"}},{"Process":{"Description":"This program generates a Python C\/API file (<modulename>module.c) that contains wrappers for given Fortran or C functions so that they can be called from Python. With the -c option the corresponding extension modules are built.","Process Name":"f2py","Link":"https:\/\/linux.die.net\/man\/1\/f2py"}},{"Process":{"Description":"rancid is a perl(1) script which uses the login scripts (see clogin(1)) to login to a device, execute commands to display the configuration, etc, then filters the output for formatting, security, and so on. rancid's product is a file with the name of it's last argument plus the suffix .new. For example, hostname.new. There are complementary scripts for other platforms and\/or manufacturers that are supported by rancid(1). Briefly, these are: agmrancid Cisco Anomaly Guard Module (AGM) arancid Alteon WebOS switches arrancid Arista Networks devices brancid Bay Networks (nortel) cat5rancid Cisco catalyst switches cssrancid Cisco content services switches erancid ADC-kentrox EZ-T3 mux f10rancid Force10 f5rancid F5 BigIPs fnrancid Fortinet Firewalls francid Foundry and HP procurve OEMs of Foundry hrancid HP Procurve Switches htranicd Hitachi Routers jerancid Juniper Networks E-series jrancid Juniper Networks mrancid MRTd mrvrancid MRV optical switches nrancid Netscreen firewalls nsrancid Netscaler nxrancid Cisco Nexus boxes prancid Procket Networks rivrancid Riverstone rrancid Redback srancid SMC switch (some Dell OEMs) trancid Netopia sDSL\/T1 routers tntrancid Lucent TNT xrancid Extreme switches xrrancid Cisco IOS-XR boxes zrancid Zebra routing software The command-line options are as follows: -V Prints package name and version strings. -d Display debugging information. -l Display somewhat less debugging information. -f rancid should interpret the next argument as a filename which contains the output it would normally collect from the device ( hostname) with clogin(1).","Process Name":"f5rancid","Link":"https:\/\/linux.die.net\/man\/1\/f5rancid"}},{"Process":{"Description":null,"Process Name":"factor","Link":"https:\/\/linux.die.net\/man\/1\/factor"}},{"Process":{"Description":"Draws what looks like a waving ribbon following a sinusoidal path.","Process Name":"fadeplot","Link":"https:\/\/linux.die.net\/man\/1\/fadeplot"}},{"Process":{"Description":null,"Process Name":"fail2ban-client","Link":"https:\/\/linux.die.net\/man\/1\/fail2ban-client"}},{"Process":{"Description":null,"Process Name":"fail2ban-regex","Link":"https:\/\/linux.die.net\/man\/1\/fail2ban-regex"}},{"Process":{"Description":"Fail2Ban v0.8.2 reads log file that contains password failure report and bans the corresponding IP addresses using firewall rules. Only use this command for debugging purpose. Start the server with fail2ban-client instead. The default behaviour is to start the server in background.","Process Name":"fail2ban-server","Link":"https:\/\/linux.die.net\/man\/1\/fail2ban-server"}},{"Process":{"Description":"fakechroot runs a command in an environment were is additional possibility to use chroot(8) command without root privileges. This is useful for allowing users to create own chrooted environment with possibility to install another packages without need for root privileges. fakechroot replaces more library functions (chroot(2), open(2), etc.) by ones that simulate the effect the real library functions would have had, had the user really been in chroot. These wrapper functions are in a shared library \/usr\/lib\/fakechroot\/libfakechroot.so which is loaded through the LD_PRELOAD mechanism of the dynamic loader. (See ld.so(8)) In fake chroot you can install Debian bootstrap with 'debootstrap --variant=fakechroot' command. In this environment you can use i.e. apt-get(8) command to install another packages from common user's account. In the current version, the fakechroot does not provide the fakeroot(1) functionality! You might to call fakechroot with fakeroot command, if you want to emulate root environment, i.e.: $ fakeroot fakechroot \/usr\/sbin\/chroot \/tmp\/debian \/bin\/sh\n# id\nuid=0(root) gid=0(root) groups=0(root)","Process Name":"fakechroot","Link":"https:\/\/linux.die.net\/man\/1\/fakechroot"}},{"Process":{"Description":null,"Process Name":"faked-sysv","Link":"https:\/\/linux.die.net\/man\/1\/faked-sysv"}},{"Process":{"Description":"If a fakeroot process wants to change the ownership of a file, then faked is the process that remembers that new owner. If later the same fakeroot process does a stat() for that filename, then the libfakeroot wrapped stat() call will first ask faked for the fake ownership etc of that file, and then report it.","Process Name":"faked-tcp","Link":"https:\/\/linux.die.net\/man\/1\/faked-tcp"}},{"Process":{"Description":"fakeroot runs a command in an environment wherein it appears to have root privileges for file manipulation. This is useful for allowing users to create archives (tar, ar, .deb etc.) with files in them with root permissions\/ownership. Without fakeroot one would need to have root privileges to create the constituent files of the archives with the correct permissions and ownership, and then pack them up, or one would have to construct the archives directly, without using the archiver. fakeroot works by replacing the file manipulation library functions (chmod(2), stat(2) etc.) by ones that simulate the effect the real library functions would have had, had the user really been root. These wrapper functions are in a shared library \/usr\/lib\/libfakeroot.so* which is loaded through the LD_PRELOAD mechanism of the dynamic loader. (See ld.so(8)) If you intend to build packages with fakeroot, please try building the fakeroot package first: the \"debian\/rules build\" stage has a few tests (testing mostly for bugs in old fakeroot versions). If those tests fail (for example because you have certain libc5 programs on your system), other packages you build with fakeroot will quite likely fail too, but possibly in much more subtle ways. Also, note that it's best not to do the building of the binaries themselves under fakeroot. Especially configure and friends don't like it when the system suddenly behaves differently from what they expect. (or, they randomly unset some environment variables, some of which fakeroot needs).","Process Name":"fakeroot-sysv","Link":"https:\/\/linux.die.net\/man\/1\/fakeroot-sysv"}},{"Process":{"Description":"fakeroot runs a command in an environment wherein it appears to have root privileges for file manipulation. This is useful for allowing users to create archives (tar, ar, .deb etc.) with files in them with root permissions\/ownership. Without fakeroot one would need to have root privileges to create the constituent files of the archives with the correct permissions and ownership, and then pack them up, or one would have to construct the archives directly, without using the archiver. fakeroot works by replacing the file manipulation library functions (chmod(2), stat(2) etc.) by ones that simulate the effect the real library functions would have had, had the user really been root. These wrapper functions are in a shared library \/usr\/lib\/libfakeroot.so* which is loaded through the LD_PRELOAD mechanism of the dynamic loader. (See ld.so(8)) If you intend to build packages with fakeroot, please try building the fakeroot package first: the \"debian\/rules build\" stage has a few tests (testing mostly for bugs in old fakeroot versions). If those tests fail (for example because you have certain libc5 programs on your system), other packages you build with fakeroot will quite likely fail too, but possibly in much more subtle ways. Also, note that it's best not to do the building of the binaries themselves under fakeroot. Especially configure and friends don't like it when the system suddenly behaves differently from what they expect. (or, they randomly unset some environment variables, some of which fakeroot needs).","Process Name":"fakeroot-tcp","Link":"https:\/\/linux.die.net\/man\/1\/fakeroot-tcp"}},{"Process":{"Description":"fallocate is used to preallocate blocks to a file. For filesystems which support the fallocate system call, this is done quickly by allocating blocks and marking them as uninitialized, requiring no IO to the data blocks. This is much faster than creating a file by filling it with zeros. As of the Linux Kernel v2.6.31, the fallocate system call is supported by the btrfs, ext4, ocfs2, and xfs filesystems. The exit code returned by fallocate is 0 on success and 1 on failure.","Process Name":"fallocate","Link":"https:\/\/linux.die.net\/man\/1\/fallocate"}},{"Process":{"Description":null,"Process Name":"false","Link":"https:\/\/linux.die.net\/man\/1\/false"}},{"Process":{"Description":"This program uses your configuration's \"installprivlib\" directory to look up the full paths to those pod pages. Any files in that directory whose names start in \"perlfaq\" will be printed to the standard output, one per line. This is normally used in backticks to produce a list of filenames for other commands.","Process Name":"faqpods","Link":"https:\/\/linux.die.net\/man\/1\/faqpods"}},{"Process":{"Description":"\"fastjar\" is an implementation of Sun's jar utility that comes with the JDK , written entirely in C, and runs in a fraction of the time while being feature compatible. If any file is a directory then it is processed recursively. The manifest file name and the archive file name needs to be specified in the same order the -m and -f flags are specified.","Process Name":"fastjar","Link":"https:\/\/linux.die.net\/man\/1\/fastjar"}},{"Process":{"Description":null,"Process Name":"fastrm","Link":"https:\/\/linux.die.net\/man\/1\/fastrm"}},{"Process":{"Description":"Undelete files from FAT filesystems. fatback v0.1.1 (c) 2000-2001 DoD Computer Forensics Lab By SrA Nicholas Harbour -o, --output= DIR specifies a directory to place output files -a, --auto auto undelete mode. non-interactively recovers all deleted files -l, --log= LOGFILE specifies a file to audit log to. --level= LOGLEVEL control how much audit logging information to display. 1 is lowest 10 is highest -p, --partition= PNUM go directly to PNUM partition -s, --single force into single partition mode -z, --sectsize= SIZE adjust the sector size. default is 512 -m, --mmap use mmap() file I\/O for improved performance -h, --help display this help screen","Process Name":"fatback","Link":"https:\/\/linux.die.net\/man\/1\/fatback"}},{"Process":{"Description":"fax provides a simple user interface to the efax(1) and efix(1) programs. It allows you to send text or Postscript files as faxes and receive, print or preview received faxes. The fax help command prints a summary of the possible commands. To send a fax, the original files need to be converted from ASCII or Postscript into a particular bit-map format (TIFF with Group 3 encoding). This can be done automatically by the fax send command or you can use the fax make command to do the conversion before sending the fax. The conversion will create one file per page. These files will have the name of the original file with the page number as an additional suffix. For example, running fax make doc.ps on the two-page postscript file doc.ps would generate the files doc.ps.001 and doc.ps.002. When sending a fax with the fax send command you may dial the number manually and use the -m option or you may give the phone number on the command line. The names of the files to be sent are given on the command line, usually by using wildcards. For example, to send a multi-page fax consisting of the files doc.ps.001, doc.ps.002, and so on, you could use the command fax send 555-1212 doc.ps.0* (if you had already run the fax make command) or simply fax send 555-1212 doc.ps. If the number is busy the script will wait and try again. Use the fax receive command to answer the phone and receive a fax. If a file name is specified the received fax will be stored in files with the given file name plus an extension equal to the page number. If no options are given, the received fax will be stored in files having a name given by the date and time and an extension equal to the page number. For example, a fax received beginning on July 4 at 3:05:20 pm will generate files 0704150520.001, 0704150520.002, and so on. The fax print, fax view, and fax rm commands are used to print, preview or remove received fax files. As with the send command the file names are usually given using wildcards. If efax has been installed for automatic fax reception you can use the fax queue command to check for files in the incoming spool directory. The fax script can also be configured to print received faxes or e-mail them as MIME attachments with type image\/tiff-f. For convenience the fax print, view and rm commands will first check for the named files in this spool directory. The fax status command shows the status of the automatic receive process once, or every t seconds. Privileged users can use the fax stop and fax start commands to stop and restart the fax reception daemon. The fax answer command is used for unattended reception of faxes. It is normally placed in the inittab(5) or ttytab(5) file and is run automatically by init(8). The -v option displays verbose messages. Other features of the fax script are documented within the script: - a directory that lets you specify recipients by name instead of number - the fax new command to create a simple cover page and start up a text editor - the fax makefont command converts a Postscript font to a bit-mapped font for use in headers or text","Process Name":"fax","Link":"https:\/\/linux.die.net\/man\/1\/fax"}},{"Process":{"Description":null,"Process Name":"fax2ps","Link":"https:\/\/linux.die.net\/man\/1\/fax2ps"}},{"Process":{"Description":"Fax2tiff creates a TIFF file containing CCITT Group 3 or Group 4 encoded data from one or more files containing ''raw'' Group 3 or Group 4 encoded data (typically obtained directly from a fax modem). By default, each row of data in the resultant TIFF file is 1-dimensionally encoded and padded or truncated to 1728 pixels, as needed. The resultant image is a set of low resolution (98 lines\/inch) or medium resolution (196 lines\/inch) pages, each of which is a single strip of data. The generated file conforms to the TIFF Class F ( FAX ) specification for storing facsimile data. This means, in particular, that each page of the data does not include the trailing return to control ( RTC ) code; as required for transmission by the CCITT Group 3 specifications. The old, ''classic'', format is created if the -c option is used. (The Class F format can also be requested with the -f option.) The default name of the output image is fax.tif; this can be changed with the -o option. Each input file is assumed to be a separate page of facsimile data from the same document. The order in which input files are specified on the command line is the order in which the resultant pages appear in the output file.","Process Name":"fax2tiff","Link":"https:\/\/linux.die.net\/man\/1\/fax2tiff"}},{"Process":{"Description":"faxalter changes the state of one or more Hyla FAX jobs that are queued for transmission. Numerous parameters can be altered; see the options below. An id is a number that identifies a job that has been submitted to Hyla FAX ; these numbers are printed by sendfax(1) when jobs are queued for transmission and can also be displayed with faxstat(1). Unprivileged users can alter the parameters of jobs that they own; clients with administrative privileges on a server can manipulate any job (see the -A option below). The user may be specified by the FAXUSER environment variable. By default, faxalter contacts the facsimile server on the host specified in the FAXSERVER environment variable; but consult the -h option for more information.","Process Name":"faxalter","Link":"https:\/\/linux.die.net\/man\/1\/faxalter"}},{"Process":{"Description":null,"Process Name":"faxcover","Link":"https:\/\/linux.die.net\/man\/1\/faxcover"}},{"Process":{"Description":"","Process Name":"faxformat","Link":"https:\/\/linux.die.net\/man\/1\/faxformat"}},{"Process":{"Description":"faxmail takes an electronic mail message on standard input and converts it to P OST S CRIPT ® in a form that is suitable for transmission as a facsimile. The converted document is either written to the standard output or, if direct delivery is enabled, it is submitted directly to a Hyla FAX server for transmission. faxmail is designed for use in constructing electronic mail to facsimile gateway services. For example, mail aliases may be created to automatically transmit electronic mail; e.g. sam: \"|\/usr\/bin\/faxmail -d sam@5551212\" or faxmail may be used as a ''mail transport agent'', extracting the necessary delivery information directly from the envelope of the mail message. If faxmail is invoked without enabling direct delivery then it just formats the mail message on the standard input and writes the result to the standard output. To enable direct delivery the -d option must be specified on the command line; see below for more information.","Process Name":"faxmail","Link":"https:\/\/linux.die.net\/man\/1\/faxmail"}},{"Process":{"Description":"Display the fax queue set up by faxspool(1). faxq looks for all the jobs queued by faxspool(1) to \/var\/spool\/fax\/outgoing\/*. For each job in the queue, faxq displays some status informations about it. If no options are specified, one line per queued fax job is printed, displaying the job number, sender, number of pages and target fax number.","Process Name":"faxq","Link":"https:\/\/linux.die.net\/man\/1\/faxq"}},{"Process":{"Description":"Remove job(s) from the fax queue set up by faxspool(1). faxrm removes queued fax jobs. Call with a list of job-IDs to remove specific jobs. Call with no job IDs to be asked interactively about all jobs you own (if run by root, all jobs). For job-id, use the strings returned by faxq(1) (e.g. F000015), without the ''\/JOB'' extention. If you are not the owner of the fax job (as per the 'user xyz' statement inside the JOB file), you are not allowed to remove the fax job. Only root is permitted to remove another user's faxes. If the job is locked (most likely because sendfax(8) is active sending it), faxrm doesn't attempt to remove it. Instead, it prints a warning message on stderr and goes on to the next job.","Process Name":"faxrm","Link":"https:\/\/linux.die.net\/man\/1\/faxrm"}},{"Process":{"Description":"Run the fax queue set up by faxspool(1), try to send all faxes, record result, remove job and send notify mails. faxrunq looks for all the jobs queued by faxspool(1) to \/var\/spool\/fax\/outgoing\/*. For each job in the queue, faxrunq tries to send it, using sendfax(8). If the send succeeds, the job is removed from the queue, and a \"success\" mail is sent to the originator of the spooled job. If the send fails, it's logged, and faxrunq proceeds to the next job. If the job fails five times \"fatally\", that is, not with a locked or engaged line, but with \"NO CARRIER\" (no fax machine, or line noise), the job is suspended, and the requestor gets a mail, telling him so. faxrunq can be run from the command line (but make sure the user doing this has write access to the modem device and to the fax queue, that is, usually this should be done by \"fax\" or \"root\"). In a production environment, it's more useful to start faxrunq from cron(8) in regular intervals, like \"run it every 5 minutes\". See the cron(8) and\/or crontab(1) man pages for this (which man page exists depends on your system).","Process Name":"faxrunq","Link":"https:\/\/linux.die.net\/man\/1\/faxrunq"}},{"Process":{"Description":"Queue the named files for later transmission with sendfax(8). The input files are converted to G3 fax files, spooled to \/var\/spool\/fax\/outgoing\/<dir>\/f*.g3, and queued for transmsssion to the fax address \"phone-number\". On top of each page, faxspool puts a header line, telling the other side the number of pages, your fax id, ..., whatever you like. The format of this line is configurable via the file \/etc\/mgetty+sendfax\/faxheader and per-user via the file $HOME\/.faxheader. (you can select another one with the \"-h\" option, for example, one for your business faxes and one for the private stuff). This file should contain a few lines of text, normally only one line, but more than one line is permitted. The text may use the tokens @T@ for the remote telephone number, @U@ for the sending user name, @N@ for his full name (fifth field of \/etc\/passwd, if not given with \"-F\"), @P@ for the page number and @M@ for the total number of pages. @D@ will be replaced by the string specified with the \"-D\" option (see below), @DATE@ will be substituted by the output of the 'date' command, and @ID@ stands for the sender's fax number (FAX_STATION_ID). Finally, @S@ will be substituted by the JOB ID (Fxxxxxx). If \"phone-number\" contains non-numeric characters, faxspool interprets it as an alias and tries to look it up in the files \/etc\/mgetty+sendfax\/faxaliases and $HOME\/.faxnrs. These files have a very simple format: one line per alias, alias name first, whitespace (tab or blank), phone number. Optionally, you can place a short description of the receiver after the phone number, this will be used as if it had been specified with \"-D\" (an explicit \"-D\" flag overrides this). Example: gert 0893244814 Gert Doering Access control is handled similar to the way \"crontab\" does it: if a file \/etc\/mgetty+sendfax\/fax.allow exists, only those users listed in that file (one name per line) may use the fax service. If it does not exist, but a file \/etc\/mgetty+sendfax\/fax.deny exists, all users but those listed in that file may use faxspool(1), and if neither file exists, only root may send faxes. (Note: if the user name in the fax.allow file is followed by a blank, the rest of that line is ignored. Some other fax spooling software uses this to store additional information about the user sending the request). Optionally, faxspool can generate user-customizable fax cover pages. It is quite easy to set up: if a file \/usr\/lib64\/mgetty+sendfax\/make.coverpg exists and is executable, it is run with all relevant source\/destination data on the command line, and its output is sent as the first page of the resulting fax. If $HOME\/.make.coverpg exists, this file is used instead. See coverpg(1) for details.","Process Name":"faxspool","Link":"https:\/\/linux.die.net\/man\/1\/faxspool"}},{"Process":{"Description":null,"Process Name":"faxstat","Link":"https:\/\/linux.die.net\/man\/1\/faxstat"}},{"Process":{"Description":"fbgs is a simple wrapper script which takes a PostScript or pdf file as input, renders the pages using ghostscript into a temporary directory and finally calls fbi to display them.","Process Name":"fbgs","Link":"https:\/\/linux.die.net\/man\/1\/fbgs"}},{"Process":{"Description":"fbi displays the specified file(s) on the linux console using the framebuffer device. PhotoCD, jpeg, ppm, gif, tiff, xwd, bmp and png are supported directly. For other formats fbi tries to use ImageMagick's convert.","Process Name":"fbi","Link":"https:\/\/linux.die.net\/man\/1\/fbi"}},{"Process":{"Description":null,"Process Name":"fbpanel","Link":"https:\/\/linux.die.net\/man\/1\/fbpanel"}},{"Process":{"Description":"fbrun is basically equivalent to the \"Run...\" dialog in other desktop environments. This means that it is an easy way to start a program that isn't contained in the menu (or needs a special set of parameters for this particular invocation). Another way fbrun can be useful is to be called from the menu with a preloaded command line that you can edit and then execute. An example might be sshing to a very long host name with lots of options of which one changes all the time. In this case, you could add an entry for fbrun to your menu that contains all the options and the host name. When you use said entry, you could edit the line as necessary and execute it.","Process Name":"fbrun","Link":"https:\/\/linux.die.net\/man\/1\/fbrun"}},{"Process":{"Description":null,"Process Name":"fbsetbg","Link":"https:\/\/linux.die.net\/man\/1\/fbsetbg"}},{"Process":{"Description":null,"Process Name":"fbsetroot","Link":"https:\/\/linux.die.net\/man\/1\/fbsetroot"}},{"Process":{"Description":null,"Process Name":"fbterm","Link":"https:\/\/linux.die.net\/man\/1\/fbterm"}},{"Process":{"Description":"fbtv is a program for watching TV with your linux box. It runs on top of a graphic framebuffer device (\/dev\/fb0). You'll need a new 2.1.x kernel to play with this. fbtv shares the config file ($HOME\/.xawtv) with the xawtv application. Check the xawtv(1) manpage for details about the config file format.","Process Name":"fbtv","Link":"https:\/\/linux.die.net\/man\/1\/fbtv"}},{"Process":{"Description":"","Process Name":"fc","Link":"https:\/\/linux.die.net\/man\/1\/fc"}},{"Process":{"Description":null,"Process Name":"fc-cache","Link":"https:\/\/linux.die.net\/man\/1\/fc-cache"}},{"Process":{"Description":"fc-cat reads the font information from cache files or related to font directories and emits it in ASCII form.","Process Name":"fc-cat","Link":"https:\/\/linux.die.net\/man\/1\/fc-cat"}},{"Process":{"Description":"fc-list lists fonts and styles available on the system for applications using fontconfig. If any elements are specified, only those are printed. Otherwise family and style are printed, unless verbose output is requested.","Process Name":"fc-list","Link":"https:\/\/linux.die.net\/man\/1\/fc-list"}},{"Process":{"Description":"fc-match matches pattern (empty pattern by default) using the normal fontconfig matching rules to find the best font available. If --sort is given, the sorted list of best matching fonts is displayed. The --all option works like --sort except that no pruning is done on the list of fonts. If any elements are specified, only those are printed. Otherwise short file name, family, and style are printed, unless verbose output is requested.","Process Name":"fc-match","Link":"https:\/\/linux.die.net\/man\/1\/fc-match"}},{"Process":{"Description":null,"Process Name":"fc-query","Link":"https:\/\/linux.die.net\/man\/1\/fc-query"}},{"Process":{"Description":"fc-scan scans file(s) recursively and prints out font pattern for each face found.","Process Name":"fc-scan","Link":"https:\/\/linux.die.net\/man\/1\/fc-scan"}},{"Process":{"Description":null,"Process Name":"fcaps","Link":"https:\/\/linux.die.net\/man\/1\/fcaps"}},{"Process":{"Description":"Compresses the specified files or standard input. Each file is replaced by a file with the extension .F, but only if the file got smaller. If no files are specified, the compression is applied to the standard input and is written to standard output regardless of the results. Compressed files can be restored to their original form by specifying the -d option, or by running melt or unfreeze (both linked to freeze), on the .F files or the standard input. If the output file exists, it will not be overwritten unless the -f flag is given. If -f is not specified and freeze is run in the foreground, the user is prompted as to whether the file should be overwritten. If the -g flag is given, a slightly less powerful (compression rate is 1.5% less), but somewhat faster heuristic is used. This flag can be used more than once (this mode is quite useful when freezing bitmaps) for additional speedup. If you want to improve compression rate at the cost of speed, use -x flag. It means \"maximum compression\" (the speed may degrade substantially when freezing bitmaps). If the -f flag is given, all files specified are replaced with .F files - even if the file didn't get smaller. When file names are given, the ownership (if run by root), modes, accessed and modified times are maintained between the file and its .F version. In this respect, freeze can be used for archival purposes, yet can still be used with make(1) after melting. The -c option causes the results of the freeze\/melt operation to be written to stdout; no files are changed. The fcat program is the same as specifying -c to melt (all files are unpacked and written to stdout). The -v (verbose) option causes the diagnostics (at the end of each file processing) to be printed to stderr, and the -vv option causes the progress indicator to be drawn to the same place. Type is a token preceded by a '+' or a '--', which defines the type of following files in the command string. An explicite definition of the file's type can give up to 2% of additional compression. The list of types is stored in file \/usr\/lib\/freeze.cnf. Types may be abbreviated while not ambigious. You can also determine values for the static Huffman table by using a list of 8 numbers separated by commas instead of type. Freeze uses the Lempel-Ziv algorithm on the first pass and the dynamic Huffman algorithm on the second one. The size of sliding window is 8K, and the maximum length of matched string is 256. The positions on the window are coded using a static Huffman table. A two byte magic number is prepended to the file to ensure that neither melting of random text nor refreezing of already frozen text are attempted. In addition, the characteristics of the static Huffman table being used during freeze is written to the file so that these characteristics may be adapted to concrete conditions. The amount of compression obtained depends on the size of the input file and the distribution of character substrings and their probabilities. Typically, text files, such as C programs, are reduced by 60-75%, executable files are reduced by 50%. Compression is generally much better than that achieved by LZW coding (as used in compress), or Huffman coding (pack), though takes more time to compute. If the -V (version) flag is given, the program's version number and compilation options are printed. The exit status is normally 0; if the last file gets bigger after freezing, the exit status is 2; if an error occurs, the exit status is 1.","Process Name":"fcat","Link":"https:\/\/linux.die.net\/man\/1\/fcat"}},{"Process":{"Description":"fcatch Runs the frysk application crash stack tracing utility fcatch is a small utility that uses the frysk engine to generate stack backtraces from an application crash, or upon receiving a variety of signals.","Process Name":"fcatch","Link":"https:\/\/linux.die.net\/man\/1\/fcatch"}},{"Process":{"Description":null,"Process Name":"fchksize","Link":"https:\/\/linux.die.net\/man\/1\/fchksize"}},{"Process":{"Description":"The chksum program builds checksums for files (build mode) and compares file checksums against a checksum list (check mode). Two steps are used to build a checksum: \u2022 Binary message digest A binary message digest is created using one of the following algorithms: MD5 , RIPEMD-160 , SHA-1 (default), SHA-224 , SHA-256 , SHA-384 or SHA-512 . \u2022 Text representation A text representation of the message data is created using one of the following encodings: ASCII-Hex, ASCII-85 (default) or reverse ASCII-85 .","Process Name":"fchksum","Link":"https:\/\/linux.die.net\/man\/1\/fchksum"}},{"Process":{"Description":"fcore Executes the frysk core dump utility. fcore is a utility that constructs a core-file from a running process, and writes it to disk. -a, -allmaps Write all readable segment maps. The default level is to attempt to emulate gcore segment writing strategy. By specifying this option, fcore will write all readable maps. -o output-file Specifies the name of the core file. Default is core. The core-file extension will still use the pid of the process being dumped. --console LOG=LEVEL,... Enable logging to the console and at the specified log level. The log level can be: [ OFF | SEVERE | WARNING | INFO | CONFIG | FINE | FINER | FINEST]. --log LOG=LEVEL,... Specify the file logging level. The log level can be: [ OFF | SEVERE | WARNING | INFO | CONFIG | FINE | FINER | FINEST].","Process Name":"fcore","Link":"https:\/\/linux.die.net\/man\/1\/fcore"}},{"Process":{"Description":"Most MPI users will probably not need to use the fctl command; see lamclean(1). This command is only installed if LAM\/MPI was configured with the --with-trillium switch. The fctl command controls important file daemon (filed) attributes and support features. With no options, fctl reports the current working directory that is prepended to all partial pathnames on rfopen(2). A file name on the command line becomes the new working directory. Another important role of fctl is cleaning the debris that can form in filed in certain application crashes or bugs. Any file descriptor can be swept clean by indicating its handle (number) with the -c option. If desired, all descriptors, save stdio (0, 1, 2), can be cleaned and filed effectively reset with the -S option.","Process Name":"fctl","Link":"https:\/\/linux.die.net\/man\/1\/fctl"}},{"Process":{"Description":null,"Process Name":"fd2pascal","Link":"https:\/\/linux.die.net\/man\/1\/fd2pascal"}},{"Process":{"Description":"","Process Name":"fd2ps","Link":"https:\/\/linux.die.net\/man\/1\/fd2ps"}},{"Process":{"Description":null,"Process Name":"fdebuginfo","Link":"https:\/\/linux.die.net\/man\/1\/fdebuginfo"}},{"Process":{"Description":"fdebugrpm Installs missing debuginfo packages as returned by utility, fdebuginfo fdebugrpm is a bash script that runs the utility fdebuginfo and allows the user to install the missing debuginfo packages. User needs to be in the sudoers list for succesful install. --console LOG=LEVEL,... Enable logging to the console and at the specified log level. The log level can be: [ OFF | SEVERE | WARNING | INFO | CONFIG | FINE | FINER | FINEST]. --log LOG=LEVEL,... Specify the file logging level. The log level can be: [ OFF | SEVERE | WARNING | INFO | CONFIG | FINE | FINER | FINEST].","Process Name":"fdebugrpm","Link":"https:\/\/linux.die.net\/man\/1\/fdebugrpm"}},{"Process":{"Description":null,"Process Name":"fdesign","Link":"https:\/\/linux.die.net\/man\/1\/fdesign"}},{"Process":{"Description":"Converts PDF formular data (FDF) into something (Con)TeX(t) can handle. These commands are usually called indirectly during ConTeXt file processing.","Process Name":"fdf2tex","Link":"https:\/\/linux.die.net\/man\/1\/fdf2tex"}},{"Process":{"Description":"dot draws directed graphs. It works well on DAGs and other graphs that can be drawn as hierarchies. It reads attributed graph files and writes drawings. By default, the output format dot is the input file with layout coordinates appended. neato draws undirected graphs using ''spring'' models (see Kamada and Kawai, Information Processing Letters 31:1, April 1989). Input files must be formatted in the dot attributed graph language. By default, the output of neato is the input graph with layout coordinates appended. twopi draws graphs using a radial layout (see G. Wills, Symposium on Graph Drawing GD'97, September, 1997). Basically, one node is chosen as the center and put at the origin. The remaining nodes are placed on a sequence of concentric circles centered about the origin, each a fixed radial distance from the previous circle. All nodes distance 1 from the center are placed on the first circle; all nodes distance 1 from a node on the first circle are placed on the second circle; and so forth. circo draws graphs using a circular layout (see Six and Tollis, GD '99 and ALENEX '99, and Kaufmann and Wiese, GD '02.) The tool identifies biconnected components and draws the nodes of the component on a circle. The block-cutpoint tree is then laid out using a recursive radial algorithm. Edge crossings within a circle are minimized by placing as many edges on the circle's perimeter as possible. In particular, if the component is outerplanar, the component will have a planar layout. If a node belongs to multiple non-trivial biconnected components, the layout puts the node in one of them. By default, this is the first non-trivial component found in the search from the root component. fdp draws undirected graphs using a ''spring'' model. It relies on a force-directed approach in the spirit of Fruchterman and Reingold (cf. Software-Practice & Experience 21(11), 1991, pp. 1129-1164). sfdp also draws undirected graphs using the ''spring'' model described above, but it uses a multi-scale approach to produce layouts of large graphs in a reasonably short time.","Process Name":"fdp","Link":"https:\/\/linux.die.net\/man\/1\/fdp"}},{"Process":{"Description":"Searches the given path for duplicate files. Such files are found by comparing file sizes and MD5 signatures, followed by a byte-by-byte comparison.","Process Name":"fdupes","Link":"https:\/\/linux.die.net\/man\/1\/fdupes"}},{"Process":{"Description":"This tool create review request for rpm packages for Fedora. It starts a scratch build on koji, upload your file onto fedorapeople, create the review request and add a comment that the scratch-build on koji was successful.","Process Name":"fedora-create-review","Link":"https:\/\/linux.die.net\/man\/1\/fedora-create-review"}},{"Process":{"Description":null,"Process Name":"fedora-review","Link":"https:\/\/linux.die.net\/man\/1\/fedora-review"}},{"Process":{"Description":"fedpkg is a script to interact with the RPM Packaging system. Global Options --config CONFIG, -c CONFIG Specify a config file to use --dist DIST Override the discovered distribution --user USER Override the discovered user name --path PATH Define the directory to work in (defaults to cwd) -v Run with verbose debug output -q Run quietly only displaying errors","Process Name":"fedpkg","Link":"https:\/\/linux.die.net\/man\/1\/fedpkg"}},{"Process":{"Description":null,"Process Name":"feh","Link":"https:\/\/linux.die.net\/man\/1\/feh"}},{"Process":{"Description":"feh-cam is a perl wrapper for feh which simplifies viewing webcams using keyed bookmarks. It helps manage viewing your favourite webcam sites with feh.","Process Name":"feh-cam","Link":"https:\/\/linux.die.net\/man\/1\/feh-cam"}},{"Process":{"Description":"ferm is a frontend for iptables. It reads the rules from a structured configuration file and calls iptables(8) to insert them into the running kernel. ferm's goal is to make firewall rules easy to write and easy to read. It tries to reduce the tedious task of writing down rules, thus enabling the firewall administrator to spend more time on developing good rules than the proper implementation of the rule. To achieve this, ferm uses a simple but powerful configuration language, which allows variables, functions, arrays, blocks. It also allows you to include other files, allowing you to create libraries of commonly used structures and functions. ferm, pronounced \"firm\", stands for \"For Easy Rule Making\".","Process Name":"ferm","Link":"https:\/\/linux.die.net\/man\/1\/ferm"}},{"Process":{"Description":"Festival is a general purpose text-to-speech system. As well as simply rendering text as speech it can be used in an interactive command mode for testing and developing various aspects of speech synthesis technology. Festival has two major modes, command and tts (text-to-speech). When in command mode input (from file or interactively) is interpreted by the command interpreter. When in tts mode input is rendered as speech. When in command mode filenames that start with a left paranthesis are treated as literal commands and evaluated.","Process Name":"festival","Link":"https:\/\/linux.die.net\/man\/1\/festival"}},{"Process":{"Description":null,"Process Name":"festival_client","Link":"https:\/\/linux.die.net\/man\/1\/festival_client"}},{"Process":{"Description":null,"Process Name":"fetchlog","Link":"https:\/\/linux.die.net\/man\/1\/fetchlog"}},{"Process":{"Description":"fetchmail is a mail-retrieval and forwarding utility; it fetches mail from remote mailservers and forwards it to your local (client) machine's delivery system. You can then handle the retrieved mail using normal mail user agents such as mutt(1), elm(1) or mail(1). The fetchmail utility can be run in a daemon mode to repeatedly poll one or more systems at a specified interval. The fetchmail program can gather mail from servers supporting any of the common mail-retrieval protocols: POP2 (legacy, to be removed from future release), POP3, IMAP2bis, IMAP4, and IMAP4rev1. It can also use the ESMTP ETRN extension and ODMR. (The RFCs describing all these protocols are listed at the end of this manual page.) While fetchmail is primarily intended to be used over on-demand TCP\/IP links (such as SLIP or PPP connections), it may also be useful as a message transfer agent for sites which refuse for security reasons to permit (sender-initiated) SMTP transactions with sendmail. SUPPORT, TROUBLESHOOTING For troubleshooting, tracing and debugging, you need to increase fetchmail's verbosity to actually see what happens. To do that, please run both of the two following commands, adding all of the options you'd normally use. env LC_ALL=C fetchmail -V -v --nodetach --nosyslog (This command line prints in English how fetchmail understands your configuration.) env LC_ALL=C fetchmail -vvv  --nodetach --nosyslog (This command line actually runs fetchmail with verbose English output.) Also see item #G3 in fetchmail's FAQ You can omit the LC_ALL=C part above if you want output in the local language (if supported). However if you are posting to mailing lists, please leave it in. The maintainers do not necessarily understand your language, please use English. CONCEPTS If fetchmail is used with a POP or an IMAP server (but not with ETRN or ODMR), it has two fundamental modes of operation for each user account from which it retrieves mail: singledrop- and multidrop-mode. In singledrop-mode, fetchmail assumes that all messages in the user's account (mailbox) are intended for a single recipient. The identity of the recipient will either default to the local user currently executing fetchmail, or will need to be explicitly specified in the configuration file. fetchmail uses singledrop-mode when the fetchmailrc configuration contains at most a single local user specification for a given server account. In multidrop-mode, fetchmail assumes that the mail server account actually contains mail intended for any number of different recipients. Therefore, fetchmail must attempt to deduce the proper \"envelope recipient\" from the mail headers of each message. In this mode of operation, fetchmail almost resembles a mail transfer agent (MTA). Note that neither the POP nor IMAP protocols were intended for use in this fashion, and hence envelope information is often not directly available. The ISP must stores the envelope information in some message header and. The ISP must also store one copy of the message per recipient. If either of the conditions is not fulfilled, this process is unreliable, because fetchmail must then resort to guessing the true envelope recipient(s) of a message. This usually fails for mailing list messages and Bcc:d mail, or mail for multiple recipients in your domain. fetchmail uses multidrop-mode when more than one local user and\/or a wildcard is specified for a particular server account in the configuration file. In ETRN and ODMR modes, these considerations do not apply, as these protocols are based on SMTP, which provides explicit envelope recipient information. These protocols always support multiple recipients. As each message is retrieved, fetchmail normally delivers it via SMTP to port 25 on the machine it is running on (localhost), just as though it were being passed in over a normal TCP\/IP link. fetchmail provides the SMTP server with an envelope recipient derived in the manner described previously. The mail will then be delivered according to your MTA's rules (the Mail Transfer Agent is usually sendmail(8), exim(8), or postfix(8)). Invoking your system's MDA (Mail Delivery Agent) is the duty of your MTA. All the delivery-control mechanisms (such as .forward files) normally available through your system MTA and local delivery agents will therefore be applied as usual. If your fetchmail configuration sets a local MDA (see the --mda option), it will be used directly instead of talking SMTP to port 25. If the program fetchmailconf is available, it will assist you in setting up and editing a fetchmailrc configuration. It runs under the X window system and requires that the language Python and the Tk toolkit (with Python bindings) be present on your system. If you are first setting up fetchmail for single-user mode, it is recommended that you use Novice mode. Expert mode provides complete control of fetchmail configuration, including the multidrop features. In either case, the 'Autoprobe' button will tell you the most capable protocol a given mailserver supports, and warn you of potential problems with that server.","Process Name":"fetchmail","Link":"https:\/\/linux.die.net\/man\/1\/fetchmail"}},{"Process":{"Description":"fexe Print a processe or corefile's full executable path to standard output. -v Use more verbose output, the process, executable, and \/proc\/PID\/exe contents are printed.","Process Name":"fexe","Link":"https:\/\/linux.die.net\/man\/1\/fexe"}},{"Process":{"Description":"This GUI application connects to robots and displays their current battery charge level.","Process Name":"ffbatterymon","Link":"https:\/\/linux.die.net\/man\/1\/ffbatterymon"}},{"Process":{"Description":"This tool operates on log files recorded using the bblogger plugin. It reads the binary log file and prints human readable output to the console. It can also enable or disable a currently ongoing logging process. The tool can watch log files as they are written (in a way similar to the unix tool tail(1)). It can also print an entry at a specific index, or the meta information about a log file. In replay mode it will print the information to the console in the same timing that it was recorded. Log files can be repaired, for example if the logging process ended prematurely (e.g. because of a segfault in a plugin you are debugging). It will update the meta information in the file as required for other tool operations. It can enable or disable logging in a currently running bblogger. Finally, the log files can be converted to other formats compatible with other tools. For replaying a log file to the blackboard use the fflogreplay plugin.","Process Name":"ffbblog","Link":"https:\/\/linux.die.net\/man\/1\/ffbblog"}},{"Process":{"Description":"This program connects to a Fawkes instance over the network and can query and modify configuration settings. To use this tool, a command string is formed that consists of a command and its arguments. A number of options is available to modify the behavior of the program.","Process Name":"ffconfig","Link":"https:\/\/linux.die.net\/man\/1\/ffconfig"}},{"Process":{"Description":"This GUI application allows for viewing and editing the configuration of a running Fawkes instance remotely.","Process Name":"ffconfiggui","Link":"https:\/\/linux.die.net\/man\/1\/ffconfiggui"}},{"Process":{"Description":"ffe is a program for extracting fields from flat file records and displaying them in different formats. ffe relies on the configuration file to control input file structure and the output format.","Process Name":"ffe","Link":"https:\/\/linux.die.net\/man\/1\/ffe"}},{"Process":{"Description":"The interface generator takes an XML interface definition file as input and generates C and Lua\/C code. If this code is compiled into a shared library it can be used to access a blackboard interface from C++ or Lua applications. Definition File Format The interface is defined by an interface document specifying one <interface> element. It comprises (optional) constants, (mandatory) data fields, and (optional) messages. Example: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE interface SYSTEM \"interface.dtd\">\n<interface name=\"NameThisInterface\" author=\"Author Name\" year=\"2010\">\n  <constants>\n  <!-- constant definitions -->\n  <\/constants>\n  <data>\n    <comment>Commentary on interface.<\/comment>\n    <!-- field definitions -->\n  <\/data>\n  <message name=\"MessageTypeName\">\n    <comment>Set the test int to the given value.<\/comment>\n    <!-- reference and field definitions -->\n  <\/message>\n  <!-- Any number of additional message types -->\n<\/interface> Constants The constants are optional. It comprises constant values of arbitrary types or enumerations. Example: <constant type=\"int32\" value=\"5\" name=\"CONSTANT_NAME\">Comment<\/constant> Constants are exported as static class members of the interface. The value of the field is a comment used for documentation purposes. Example: <enum name=\"TestEnum\">\n  <comment>Demonstrating enums<\/comment>\n  <item name=\"TEST_ENUM_1\">Item 1<\/item>\n  <item name=\"TEST_ENUM_2\">Item 2<\/item>\n<\/enum> Enumerations are symbolic names of type integer. The field text are comments about the overall enumeration and the enumeration items. Fields Data is stored in fields in the interfaces. Data can be one of the following types: * string * byte (equivalent to uint8) * char (equivalent to char) * int8 * uint8 * int16 * uint16 * int32 * uint32 * int64 * uint64 (not recommended, see below) * bool * float * double * custom enum types (as specified in the constants) Note that uint64 (and to some degree int64) can cause problems. Lua for example supports integers only up to 52 bits. Java does not support unsigned types, therefore it is limited to int64 (although not supported as of now, it may be in the future). If you think that you need 64 bit integers and need the full range, be aware of these problems and state this clearly in the documentation of the module in question. Number and boolean types can be used to form statically sized arrays. For this add an attribute length with the number of elements in the array. The same attribute must be given for strings to denote the maximum length (including null termination). Example: <field type=\"bool\" name=\"test_bool\">Test Bool<\/field>\n<field type=\"int32\" name=\"test_int\">Test integer<\/field>\n<field type=\"string\" length=\"30\" name=\"test_string\">A test string<\/field>\n<field type=\"int32\" length=\"30\" name=\"test_array\">Integer array<\/field> Messages Messages are defined as sub-documents. Any number of messages can be defined for an interface. Example: <message name=\"SetTestInt\">\n  <comment>Set the test int to the given value.<\/comment>\n  <ref>test_int<\/ref>\n<\/message>\n<message name=\"Calculate\">\n  <comment>Adds Summand and Addend.<\/comment>\n  <field type=\"int32\" name=\"summand\">Summand<\/field>\n  <field type=\"int32\" name=\"addend\">Addend<\/field>\n<\/message> The <ref> field can be used to reference fields of the interface. An appropriate field with the given name and the same type as in the interface is then added. Fields can otherwise be specified in the same way they are for the interface. References and fields can be mixed in a message.","Process Name":"ffifacegen","Link":"https:\/\/linux.die.net\/man\/1\/ffifacegen"}},{"Process":{"Description":null,"Process Name":"ffinfo","Link":"https:\/\/linux.die.net\/man\/1\/ffinfo"}},{"Process":{"Description":"This tool is used to provide a joystick connected to the local machine to a Fawkes instance via the blackboard, or to print debug output about joystick data in the blackboard. Generally, the joystick plugin should be preferred, because it operates faster within Fawkes. This is especially useful for joysticks communicating wirelessly with a USB dongle. For cabled joysticks however, this may not be feasible. In such situations, the joystick can be connected to a laptop or desktop, and the data is transmitted via a remote blackboard connection to the robot. In the second mode, the tool prints out changes to the joystick blackboard interface. This can be used to verify and debug joystick communication, and to ensure that data is written to the correct blackboard. Both modes are mutually exclusive. If -l is given on the command line, the second mode will be used, regardless of other parameters.","Process Name":"ffjoystick","Link":"https:\/\/linux.die.net\/man\/1\/ffjoystick"}},{"Process":{"Description":null,"Process Name":"fflaser_deadspots","Link":"https:\/\/linux.die.net\/man\/1\/fflaser_deadspots"}},{"Process":{"Description":"This program connects to a Fawkes instance over the network and shows all log messages created by that instance.","Process Name":"fflogview","Link":"https:\/\/linux.die.net\/man\/1\/fflogview"}},{"Process":{"Description":"As a general rule, options are applied to the next specified file. Therefore, order is important, and you can have the same option on the command line multiple times. Each occurrence is then applied to the next input or output file. * To set the video bitrate of the output file to 64kbit\/s: ffmpeg -i input.avi -b 64k output.avi * To force the frame rate of the output file to 24 fps: ffmpeg -i input.avi -r 24 output.avi * To force the frame rate of the input file (valid for raw formats only) to 1 fps and the frame rate of the output file to 24 fps: ffmpeg -r 1 -i input.m2v -r 24 output.avi The format option may be needed for raw input files. By default, FFmpeg tries to convert as losslessly as possible: It uses the same audio and video parameters for the outputs as the one specified for the inputs.","Process Name":"ffmpeg","Link":"https:\/\/linux.die.net\/man\/1\/ffmpeg"}},{"Process":{"Description":"As a general rule, options are applied to the next specified file. Therefore, order is important, and you can have the same option on the command line multiple times. Each occurrence is then applied to the next input or output file. * To set the video bitrate of the output file to 64kbit\/s: ffmpeg -i input.avi -b 64k output.avi * To force the frame rate of the input and output file to 24 fps: ffmpeg -r 24 -i input.avi output.avi * To force the frame rate of the output file to 24 fps: ffmpeg -i input.avi -r 24 output.avi * To force the frame rate of input file to 1 fps and the output file to 24 fps: ffmpeg -r 1 -i input.avi -r 24 output.avi The format option may be needed for raw input files. By default, FFmpeg tries to convert as losslessly as possible: It uses the same audio and video parameters for the outputs as the one specified for the inputs.","Process Name":"ffmpeg-kino","Link":"https:\/\/linux.die.net\/man\/1\/ffmpeg-kino"}},{"Process":{"Description":null,"Process Name":"ffnetloggui","Link":"https:\/\/linux.die.net\/man\/1\/ffnetloggui"}},{"Process":{"Description":"FFplay is a very simple and portable media player using the FFmpeg libraries and the SDL library. It is mostly used as a testbed for the various FFmpeg APIs.","Process Name":"ffplay","Link":"https:\/\/linux.die.net\/man\/1\/ffplay"}},{"Process":{"Description":null,"Process Name":"ffplugin","Link":"https:\/\/linux.die.net\/man\/1\/ffplugin"}},{"Process":{"Description":"This GUI application allows for viewing the current status in terms of loaded plugins. It also allows to order Fawkes to load or unload plugins, and shows short descriptions of the plugins. Use the connect button to connect to a Fawkes instance, click the checkbox next to a plugin name to load or unload it. If you would like a more condensed view set \/apps\/fawkes\/plugingui\/description_as_tooltip to true. This will remove the descriptive line from the main window and instead show it as a tooltip if you stop the mouse over the plugin name.","Process Name":"ffplugingui","Link":"https:\/\/linux.die.net\/man\/1\/ffplugingui"}},{"Process":{"Description":"FFprobe gathers information from multimedia streams and prints it in human- and machine-readable fashion. For example it can be used to check the format of the container used by a multimedia stream and the format and type of each media stream contained in it. If a filename is specified in input, ffprobe will try to open and probe the file content. If the file cannot be opened or recognized as a multimedia file, a positive exit code is returned. FFprobe may be employed both as a standalone application or in combination with a textual filter, which may perform more sophisticated processing, e.g. statistical processing or plotting. Options are used to list some of the formats supported by ffprobe or for specifying which information to display, and for setting how ffprobe will show it. FFprobe output is designed to be easily parsable by a textual filter, and consists of one or more sections of the form: [SECTION]\nkey1=val1\n...\nkeyN=valN\n[\/SECTION] Metadata tags stored in the container or in the streams are recognized and printed in the corresponding '' FORMAT '' or '' STREAM '' section, and are prefixed by the string '' TAG: ''.","Process Name":"ffprobe","Link":"https:\/\/linux.die.net\/man\/1\/ffprobe"}},{"Process":{"Description":"This tool is used in RoboCup to communicate instructions from a referee box (refbox) to robots. It receives information from the refbox, and processes and resends it in a way suitable for robots running Fawkes. It implements several different protocols for different leagues and years. The information is send to the robot either using the Fawkes world info protocol or using remote blackboard connections. The refboxcomm plugin should be preferred if at all possible. It can be used for multicast and broadcast protocols like msl2010 and spl.","Process Name":"ffrefboxrep","Link":"https:\/\/linux.die.net\/man\/1\/ffrefboxrep"}},{"Process":{"Description":null,"Process Name":"ffserver","Link":"https:\/\/linux.die.net\/man\/1\/ffserver"}},{"Process":{"Description":"fftw-wisdom is a utility to generate FFTW wisdom files, which contain saved information about how to optimally compute (Fourier) transforms of various sizes. FFTW is a free library to compute discrete Fourier transforms in one or more dimensions, for arbitrary sizes, and of both real and complex data, among other related operations. More information on FFTW can be found at the FFTW home page: http:\/\/www.fftw.org Programs using FFTW can be written to load wisdom from an arbitrary file, string, or other source. Moreover, it is likely that many FFTW-using programs will load the system wisdom file, which is stored in \/etc\/fftw\/wisdom by default. fftw-wisdom can be used to create or add to such wisdom files. In its most typical usage, the wisdom file can be created to pre-plan a canonical set of sizes (see below) via: fftw-wisdom -v -c -o wisdom (this will take many hours, which can be limited by the -t option) and the output wisdom file can then be copied (as root) to \/etc\/fftw\/ or whatever. The fftw-wisdom program normally writes the wisdom directly to standard output, but this can be changed via the -o option, as in the example above. If the system wisdom file \/etc\/fftw\/wisdom already exists, then fftw-wisdom reads this existing wisdom (unless the -n option is specified) and outputs both the old wisdom and any newly created wisdom. In this way, it can be used to add new transform sizes to the existing system wisdom (or other wisdom file, with the -w option).","Process Name":"fftw-wisdom","Link":"https:\/\/linux.die.net\/man\/1\/fftw-wisdom"}},{"Process":{"Description":"fftw-wisdom-to-conf is a utility to generate C configuration routines from FFTW wisdom files, where the latter contain saved information about how to optimally compute (Fourier) transforms of various sizes. A configuration routine is a C subroutine that you link into your program, replacing a routine of the same name in the FFTW library, that determines which parts of FFTW are callable by your program. The reason to do this is that, if you only need transforms of a limited set of sizes and types, and if you are statically linking your program, then using a configuration file generated from wisdom for those types can substantially reduce the size of your executable. (Otherwise, because of FFTW's dynamic nature, all of FFTW's transform code must be linked into any program using FFTW.) FFTW is a free library to compute discrete Fourier transforms in one or more dimensions, for arbitrary sizes, and of both real and complex data, among other related operations. More information on FFTW can be found at the FFTW home page: http:\/\/www.fftw.org fftw-wisdom-to-conf reads wisdom from standard input and writes the configuration to standard output. It can easily be combined with the fftw-wisdom tool, for example: fftw-wisdom -n cof1024 cob1024 -o wisdom fftw-wisdom-to-conf < wisdom > conf.c will create a configuration \"conf.c\" containing only those parts of FFTW needed for the optimized complex forwards and backwards out-of-place transforms of size 1024 (also saving the wisdom itself in \"wisdom\"). Alternatively, you can run your actual program, export wisdom for all plans that were created (ideally in FFTW_PATIENT or FFTW_EXHAUSTIVE mode), use this as input for fftw-wisdom-to-conf, and then re-link your program with the resulting configuration routine. Note that the configuration routine does not contain the wisdom, only the routines necessary to implement the wisdom, so your program should also import the wisdom in order to benefit from the pre-optimized plans.","Process Name":"fftw-wisdom-to-conf","Link":"https:\/\/linux.die.net\/man\/1\/fftw-wisdom-to-conf"}},{"Process":{"Description":"fftwf-wisdom is a utility to generate FFTW wisdom files, which contain saved information about how to optimally compute (Fourier) transforms of various sizes. FFTW is a free library to compute discrete Fourier transforms in one or more dimensions, for arbitrary sizes, and of both real and complex data, among other related operations. More information on FFTW can be found at the FFTW home page: http:\/\/www.fftw.org Programs using FFTW can be written to load wisdom from an arbitrary file, string, or other source. Moreover, it is likely that many FFTW-using programs will load the system wisdom file, which is stored in \/etc\/fftw\/wisdomf by default. fftwf-wisdom can be used to create or add to such wisdom files. In its most typical usage, the wisdom file can be created to pre-plan a canonical set of sizes (see below) via: fftwf-wisdom -v -c -o wisdomf (this will take many hours, which can be limited by the -t option) and the output wisdomf file can then be copied (as root) to \/etc\/fftw\/ or whatever. The fftwf-wisdom program normally writes the wisdom directly to standard output, but this can be changed via the -o option, as in the example above. If the system wisdom file \/etc\/fftw\/wisdomf already exists, then fftwf-wisdom reads this existing wisdom (unless the -n option is specified) and outputs both the old wisdom and any newly created wisdom. In this way, it can be used to add new transform sizes to the existing system wisdom (or other wisdom file, with the -w option).","Process Name":"fftwf-wisdom","Link":"https:\/\/linux.die.net\/man\/1\/fftwf-wisdom"}},{"Process":{"Description":null,"Process Name":"fftwl-wisdom","Link":"https:\/\/linux.die.net\/man\/1\/fftwl-wisdom"}},{"Process":{"Description":"This GUI application listens on the network to world info protocol multicast communication and draws the received information on the screen. A soccer scenario is assumed.","Process Name":"ffwiviewer","Link":"https:\/\/linux.die.net\/man\/1\/ffwiviewer"}},{"Process":{"Description":"","Process Name":"fg","Link":"https:\/\/linux.die.net\/man\/1\/fg"}},{"Process":{"Description":"If the active Virtual Terminal is \/dev\/ttyN, then prints N on standard output. If the console is a serial console, then \"serial\" is printed instead. --next-available Will show the next unallocated virtual terminal. Normally 6 virtual terminals are allocated, with number 7 used for X; this will return \"8\" in this case.","Process Name":"fgconsole","Link":"https:\/\/linux.die.net\/man\/1\/fgconsole"}},{"Process":{"Description":"grep searches the named input FILEs (or standard input if no files are named, or if a single hyphen-minus (-) is given as file name) for lines containing a match to the given PATTERN. By default, grep prints the matching lines. In addition, two variant programs egrep and fgrep are available. egrep is the same as grep -E. fgrep is the same as grep -F. Direct invocation as either egrep or fgrep is deprecated, but is provided to allow historical applications that rely on them to run unmodified.","Process Name":"fgrep","Link":"https:\/\/linux.die.net\/man\/1\/fgrep"}},{"Process":{"Description":"fhpd is a command line oriented debugger based on the Frysk framework that follows the HPD Version 1 standard.","Process Name":"fhpd","Link":"https:\/\/linux.die.net\/man\/1\/fhpd"}},{"Process":{"Description":null,"Process Name":"fiascotopnm","Link":"https:\/\/linux.die.net\/man\/1\/fiascotopnm"}},{"Process":{"Description":"Fig2dev translates fig code in the named fig-file into the specified graphics language and puts them in out-file. The default fig-file and out-file are standard input and standard output, respectively Xfig (Facility for Interactive Generation of figures) is a screen-oriented tool which runs under the X Window System, and allows the user to draw and manipulate objects interactively. This version of fig2dev is compatible with xfig versions 1.3, 1.4, 2.0, 2.1, 3.0, 3.1 and 3.2. Xfig version 3.2.3 and later saves and allows the user to edit comments for each Fig object. These comments are output with several of the output languages, such as PostScript, CGM, EMF, LaTeX, MetaFont, PicTeX, (as % comments), tk (as # comments), and pic (as .\\\" comments).","Process Name":"fig2dev","Link":"https:\/\/linux.die.net\/man\/1\/fig2dev"}},{"Process":{"Description":"fig2ps converts figures produced by XFig into postscript or PDF, processing the text with LaTeX. It takes advantage of the pstex and pstext_t export formats of fig2dev. fig2ps converts successively all the files given as arguments. It's behaviour is governed by quite a few options. In all the options, the = sign is optional, so that fig2ps --bbox=dvips file.fig and fig2ps --bbox dvips file.fig are equivalent.","Process Name":"fig2eps","Link":"https:\/\/linux.die.net\/man\/1\/fig2eps"}},{"Process":{"Description":"fig2ps converts figures produced by XFig into postscript or PDF, processing the text with LaTeX. It takes advantage of the pstex and pstext_t export formats of fig2dev. fig2ps converts successively all the files given as arguments. It's behaviour is governed by quite a few options. In all the options, the = sign is optional, so that fig2ps --bbox=dvips file.fig and fig2ps --bbox dvips file.fig are equivalent.","Process Name":"fig2pdf","Link":"https:\/\/linux.die.net\/man\/1\/fig2pdf"}},{"Process":{"Description":null,"Process Name":"fig2ps","Link":"https:\/\/linux.die.net\/man\/1\/fig2ps"}},{"Process":{"Description":null,"Process Name":"fig2ps2tex","Link":"https:\/\/linux.die.net\/man\/1\/fig2ps2tex"}},{"Process":{"Description":"The fig2vect program converts Fig images (created by i.e. XFig, jFig or WinFig) to other vector formats (*.mp, *.eps, *.pdf, *.tex, *.svg). Most output formats are intended for use with LaTeX.","Process Name":"fig2vect","Link":"https:\/\/linux.die.net\/man\/1\/fig2vect"}},{"Process":{"Description":"This manual page documents version 5.04 of the file command. file tests each argument in an attempt to classify it. There are three sets of tests, performed in this order: filesystem tests, magic tests, and language tests. The first test that succeeds causes the file type to be printed. The type printed will usually contain one of the words text (the file contains only printing characters and a few common control characters and is probably safe to read on an ASCII terminal), executable (the file contains the result of compiling a program in a form understandable to some UNIX kernel or another), or data meaning anything else (data is usually 'binary' or non-printable). Exceptions are well-known file formats (core files, tar archives) that are known to contain binary data. When modifying magic files or the program itself, make sure to preserve these keywords. Users depend on knowing that all the readable files in a directory have the word 'text' printed. Don't do as Berkeley did and change 'shell commands text' to 'shell script'. The filesystem tests are based on examining the return from a stat(2) system call. The program checks to see if the file is empty, or if it's some sort of special file. Any known file types appropriate to the system you are running on (sockets, symbolic links, or named pipes (FIFOs) on those systems that implement them) are intuited if they are defined in the system header file The magic tests are used to check for files with data in particular fixed formats. The canonical example of this is a binary executable (compiled program) a.out file, whose format is defined in #include <a.out.h> and possibly #include <exec.h> in the standard include directory. These files have a 'magic number' stored in a particular place near the beginning of the file that tells the UNIX operating system that the file is a binary executable, and which of several types thereof. The concept of a 'magic' has been applied by extension to data files. Any file with some invariant identifier at a small fixed offset into the file can usually be described in this way. The information identifying these files is read from the compiled magic file \/usr\/share\/misc\/magic.mgc, or the files in the directory \/usr\/share\/misc\/magic if the compiled file does not exist. In addition, if $HOME\/.magic.mgc or $HOME\/.magic exists, it will be used in preference to the system magic files. If \/etc\/magic exists, it will be used together with other magic files. If a file does not match any of the entries in the magic file, it is examined to see if it seems to be a text file. ASCII, ISO-8859-x, non-ISO 8-bit extended-ASCII character sets (such as those used on Macintosh and IBM PC systems), UTF-8-encoded Unicode, UTF-16-encoded Unicode, and EBCDIC character sets can be distinguished by the different ranges and sequences of bytes that constitute printable text in each set. If a file passes any of these tests, its character set is reported. ASCII, ISO-8859-x, UTF-8, and extended-ASCII files are identified as 'text' because they will be mostly readable on nearly any terminal; UTF-16 and EBCDIC are only 'character data' because, while they contain text, it is text that will require translation before it can be read. In addition, file will attempt to determine other characteristics of text-type files. If the lines of a file are terminated by CR, CRLF, or NEL, instead of the Unix-standard LF, this will be reported. Files that contain embedded escape sequences or overstriking will also be identified. Once file has determined the character set used in a text-type file, it will attempt to determine in what language the file is written. The language tests look for particular strings (cf. #include <names.h> ) that can appear anywhere in the first few blocks of a file. For example, the keyword .br indicates that the file is most likely a troff(1) input file, just as the keyword struct indicates a C program. These tests are less reliable than the previous two groups, so they are performed last. The language test routines also test for some miscellany (such as tar(1) archives). Any file that cannot be identified as having been written in any of the character sets listed above is simply said to be 'data'.","Process Name":"file","Link":"https:\/\/linux.die.net\/man\/1\/file"}},{"Process":{"Description":null,"Process Name":"file2pacdep.plug","Link":"https:\/\/linux.die.net\/man\/1\/file2pacdep.plug"}},{"Process":{"Description":"file65 prints file information for files in the o65 object format.","Process Name":"file65","Link":"https:\/\/linux.die.net\/man\/1\/file65"}},{"Process":{"Description":"Filebench is a file system and storage benchmark that allows to generate a large variety of workloads. Unlike typical benchmarks it is very flexible and allows to minutely specify (any) applications' behaviour using extensive Workload Model Language (WML). Filebench uses loadable workload personalities to allow easy emulation of complex applications (e.g., mail, web, file, and database servers). Filebench is quick to set up and easy to use compared to deploying real applications. It is also a handy tool for micro-benchmarking. Filebench includes many features to facilitate file system benchmarking: [bu] Multiple workload types support via loadable personalities [bu] Ships with more than 40 pre-defined personalities, including the one that describe mail, web, file, and database servers behaviour [bu] Easy to add new personalities using reach Workload Model Language (WML) [bu] Multi-process and multi-thread workload support [bu] Configurable directory hierarchies with depth, width, and file sizes set to given statistical distributions [bu] Support of asynchronous I\/O and process synchronization primitives [bu] Integrated statistics for throughput, latency, and CPU cycle counts per system call","Process Name":"filebench","Link":"https:\/\/linux.die.net\/man\/1\/filebench"}},{"Process":{"Description":"filedaemon operates on input files matching a glob(3) pattern. Each matching input file is passed to a child program (specified in the PROGRAM argument) which is expected to read input data from standard input and write the results to standard output. filedaemon's job is simply to handle the mechanics of directory polling, file globbing, and routing of input and output files on behalf of the child program. Use of two dashes (--) after all filedaemon command-line switches allows PROGRAM_ARGS to be interpreted by the PROGRAM rather than filedaemon itself. While they are not strictly required if you do not need to pass arguments to PROGRAM , they should be used for consistency.","Process Name":"filedaemon","Link":"https:\/\/linux.die.net\/man\/1\/filedaemon"}},{"Process":{"Description":"Takes a stream of files and multplexes into separate files under the specified directory.","Process Name":"files2dirs","Link":"https:\/\/linux.die.net\/man\/1\/files2dirs"}},{"Process":{"Description":"Takes a stream of files and multplexes into separate files under the specified directory.","Process Name":"files2dirs.pl","Link":"https:\/\/linux.die.net\/man\/1\/files2dirs.pl"}},{"Process":{"Description":null,"Process Name":"filezilla","Link":"https:\/\/linux.die.net\/man\/1\/filezilla"}},{"Process":{"Description":"The CUPS filter interface provides a standard method for adding support for new document types to CUPS. Each filter is capable of converting from one or more input formats to another format that can either be printed directly or piped into another filter to get it to a printable format. Filters must be capable of reading from a filename on the command-line or from the standard input, copying the standard input to a temporary file as required by the file format. All output must be sent to the standard output. The command name (argv[0]) is set to the name of the destination printer.","Process Name":"filter","Link":"https:\/\/linux.die.net\/man\/1\/filter"}},{"Process":{"Description":"filter1d is a general time domain filter for multiple column time series data. The user specifies the number of columns of input and which column is the time. (See -N option below). The fastest operation occurs when the input time series are equally spaced and have no gaps or outliers and the special options are not needed. filter1d has options -L, -Q, and -S for unevenly sampled data with gaps. infile Multi-column ASCII (or binary, see -b) file holding data values to be filtered. -F Sets the filter type. Choose among convolution and non-convolution filters. Append the filter code followed by the full filter width in same units as time column. Available convolution filters are: ( b) Boxcar: All weights are equal. ( c) Cosine Arch: Weights follow a cosine arch curve. ( g) Gaussian: Weights are given by the Gaussian function. ( f) Custom: Instead of width give name of a one-column file with your own weight coefficients. Non-convolution filters are: ( m) Median: Returns median value. ( p) Maximum likelihood probability (a mode estimator): Return modal value. If more than one mode is found we return their average value. Append - or + to the filter width if you rather want to return the smallest or largest of the modal values. ( l) Lower: Return the minimum of all values. ( L) Lower: Return minimum of all positive values only. ( u) Upper: Return maximum of all values. ( U) Upper: Return maximum or all negative values only. Upper case type B, C, G, M, P, F will use robust filter versions: i.e., replace outliers (2.5 L1 scale off median) with median during filtering. In the case of L|U it is possible that no data passes the initial sign test; in that case the filter will return 0.0.","Process Name":"filter1d","Link":"https:\/\/linux.die.net\/man\/1\/filter1d"}},{"Process":{"Description":null,"Process Name":"filterdiff","Link":"https:\/\/linux.die.net\/man\/1\/filterdiff"}},{"Process":{"Description":"The program reformats mysqldump output. For each table a line break is inserted before the data of the second table entry and all following entries.","Process Name":"filtmsql","Link":"https:\/\/linux.die.net\/man\/1\/filtmsql"}},{"Process":{"Description":"finch is a console-based modular messaging client based on libpurple which is capable of connecting to AIM, MSN, Yahoo!, XMPP, ICQ, IRC, SILC, Novell GroupWise, Lotus Sametime, Zephyr, Gadu-Gadu, and QQ all at once. It has many common features found in other clients, as well as many unique features. Finch is not endorsed by or affiliated with America Online, ICQ, Microsoft, or Yahoo.","Process Name":"finch","Link":"https:\/\/linux.die.net\/man\/1\/finch"}},{"Process":{"Description":"This manual page documents the GNU version of find. GNU find searches the directory tree rooted at each given file name by evaluating the given expression from left to right, according to the rules of precedence (see section OPERATORS), until the outcome is known (the left hand side is false for and operations, true for or), at which point find moves on to the next file name. If you are using find in an environment where security is important (for example if you are using it to search directories that are writable by other users), you should read the \"Security Considerations\" chapter of the findutils documentation, which is called Finding Files and comes with findutils. That document also includes a lot more detail and discussion than this manual page, so you may find it a more useful source of information.","Process Name":"find","Link":"https:\/\/linux.die.net\/man\/1\/find"}},{"Process":{"Description":null,"Process Name":"find-compounds.pl","Link":"https:\/\/linux.die.net\/man\/1\/find-compounds.pl"}},{"Process":{"Description":"find2perl is a little translator to convert find command lines to equivalent Perl code. The resulting code is typically faster than running find itself. \"paths\" are a set of paths where find2perl will start its searches and \"predicates\" are taken from the following list. \"! PREDICATE\" Negate the sense of the following predicate. The \"!\" must be passed as a distinct argument, so it may need to be surrounded by whitespace and\/or quoted from interpretation by the shell using a backslash (just as with using find(1)). \"( PREDICATES )\" Group the given PREDICATES . The parentheses must be passed as distinct arguments, so they may need to be surrounded by whitespace and\/or quoted from interpretation by the shell using a backslash (just as with using find(1)). \"PREDICATE1 PREDICATE2\" True if _both_ PREDICATE1 and PREDICATE2 are true; PREDICATE2 is not evaluated if PREDICATE1 is false. \"PREDICATE1 -o PREDICATE2\" True if either one of PREDICATE1 or PREDICATE2 is true; PREDICATE2 is not evaluated if PREDICATE1 is true. \"-follow\" Follow (dereference) symlinks. The checking of file attributes depends on the position of the \"-follow\" option. If it precedes the file check option, an \"stat\" is done which means the file check applies to the file the symbolic link is pointing to. If \"-follow\" option follows the file check option, this now applies to the symbolic link itself, i.e. an \"lstat\" is done. \"-depth\" Change directory traversal algorithm from breadth-first to depth-first. \"-prune\" Do not descend into the directory currently matched. \"-xdev\" Do not traverse mount points (prunes search at mount-point directories). \"-name GLOB\" File name matches specified GLOB wildcard pattern. GLOB may need to be quoted to avoid interpretation by the shell (just as with using find(1)). \"-iname GLOB\" Like \"-name\", but the match is case insensitive. \"-path GLOB\" Path name matches specified GLOB wildcard pattern. \"-ipath GLOB\" Like \"-path\", but the match is case insensitive. \"-perm PERM\" Low-order 9 bits of permission match octal value PERM . \"-perm -PERM\" The bits specified in PERM are all set in file's permissions. \"-type X\" The file's type matches perl's \"-X\" operator. \"-fstype TYPE\" Filesystem of current path is of type TYPE (only NFS\/non-NFS distinction is implemented). \"-user USER\" True if USER is owner of file. \"-group GROUP\" True if file's group is GROUP . \"-nouser\" True if file's owner is not in password database. \"-nogroup\" True if file's group is not in group database. \"-inum INUM\" True file's inode number is INUM . \"-links N\" True if (hard) link count of file matches N (see below). \"-size N\" True if file's size matches N (see below) N is normally counted in 512-byte blocks, but a suffix of \"c\" specifies that size should be counted in characters (bytes) and a suffix of \"k\" specifies that size should be counted in 1024-byte blocks. \"-atime N\" True if last-access time of file matches N (measured in days) (see below). \"-ctime N\" True if last-changed time of file's inode matches N (measured in days, see below). \"-mtime N\" True if last-modified time of file matches N (measured in days, see below). \"-newer FILE\" True if last-modified time of file matches N. \"-print\" Print out path of file (always true). If none of \"-exec\", \"-ls\", \"-print0\", or \"-ok\" is specified, then \"-print\" will be added implicitly. \"-print0\" Like -print, but terminates with \\0 instead of \\n. \"-exec OPTIONS ;\" exec() the arguments in OPTIONS in a subprocess; any occurrence of {} in OPTIONS will first be substituted with the path of the current file. Note that the command \"rm\" has been special-cased to use perl's unlink() function instead (as an optimization). The \";\" must be passed as a distinct argument, so it may need to be surrounded by whitespace and\/or quoted from interpretation by the shell using a backslash (just as with using find(1)). \"-ok OPTIONS ;\" Like -exec, but first prompts user; if user's response does not begin with a y, skip the exec. The \";\" must be passed as a distinct argument, so it may need to be surrounded by whitespace and\/or quoted from interpretation by the shell using a backslash (just as with using find(1)). \"-eval EXPR\" Has the perl script eval() the EXPR . \"-ls\" Simulates \"-exec ls -dils {} ;\" \"-tar FILE\" Adds current output to tar-format FILE . \"-cpio FILE\" Adds current output to old-style cpio-format FILE . \"-ncpio FILE\" Adds current output to \"new\"-style cpio-format FILE . Predicates which take a numeric argument N can come in three forms: * N is prefixed with a +: match values greater than N\n* N is prefixed with a -: match values less than N\n* N is not prefixed with either + or -: match only values equal to N","Process Name":"find2perl","Link":"https:\/\/linux.die.net\/man\/1\/find2perl"}},{"Process":{"Description":"findcon allows the user to search for files with a specified context. Results can be filtered by object class as described below.","Process Name":"findcon","Link":"https:\/\/linux.die.net\/man\/1\/findcon"}},{"Process":{"Description":"findg reads an input FORTRAN file, fortran-file, from the standard input and prints out those lines (along with line numbers) which contain calls to old plotting entries from the pre-GKS NCAR plot package. In conversions to GKS, the user should examine these calls for possible changes. Many entries will require no change. For details on needed changes, see the \"Conversion Guide\" section of the Version 2.00 User's Guide. If the single character $ is read in column 1 of any input line, then a complete list of the entry points being searched for is printed, and execution is terminated.","Process Name":"findg","Link":"https:\/\/linux.die.net\/man\/1\/findg"}},{"Process":{"Description":"findproxyfile returns full path to a GSI Proxy file, either in the proxy cache maintained by the GridSite G-HTTPS and delegation portType functions, or in other standard places. If a User DN is given findproxyfile uses the value of the --proxycache argument, the GRST_PROXY_PATH or the compile time default to detemine the location of the proxy cache directory. The directory is searched for a proxy having the given User DN and Delegation ID. (If no Delegation ID is specificed, then the default value is used.) If findproxyfile does not find a proxy or if a User DN is not given, but --outsidecache was given, then the environment variable X509_USER_PROXY and the standard location \/tmp\/x509up_uUID are searched as well.","Process Name":"findproxyfile","Link":"https:\/\/linux.die.net\/man\/1\/findproxyfile"}},{"Process":{"Description":"\"findrule\" mostly borrows the interface from GNU find(1) to provide a command-line interface onto the File::Find::Rule heirarchy of modules. The syntax for expressions is the rule name, preceded by a dash, followed by an optional argument. If the argument is an opening parenthesis it is taken as a list of arguments, terminated by a closing parenthesis. Some examples: find -file -name ( foo bar ) files named \"foo\" or \"bar\", below the current directory. find -file -name foo -bar files named \"foo\", that have pubs (for this is what our ficticious \"bar\" clause specifies), below the current directory. find -file -name ( -bar ) files named \"-bar\", below the current directory. In this case if we'd have omitted the parenthesis it would have parsed as a call to name with no arguments, followed by a call to -bar. Supported switches I'm very slack. Please consult the File::Find::Rule manpage for now, and prepend - to the commands that you want. Extra bonus switches findrule automatically loads all of your installed File::Find::Rule::* extension modules, so check the documentation to see what those would be.","Process Name":"findrule","Link":"https:\/\/linux.die.net\/man\/1\/findrule"}},{"Process":{"Description":"This perl script is part of the samba(7) suite. findsmb is a perl script that prints out several pieces of information about machines on a subnet that respond to SMB name query requests. It uses nmblookup(1) and smbclient(1) to obtain this information.","Process Name":"findsmb","Link":"https:\/\/linux.die.net\/man\/1\/findsmb"}},{"Process":{"Description":null,"Process Name":"finger","Link":"https:\/\/linux.die.net\/man\/1\/finger"}},{"Process":{"Description":null,"Process Name":"fio","Link":"https:\/\/linux.die.net\/man\/1\/fio"}},{"Process":{"Description":null,"Process Name":"fio_generate_plots","Link":"https:\/\/linux.die.net\/man\/1\/fio_generate_plots"}},{"Process":{"Description":null,"Process Name":"firefox","Link":"https:\/\/linux.die.net\/man\/1\/firefox"}},{"Process":{"Description":"firehol is an iptables firewall generator producing stateful iptables packet filtering firewalls, on Linux hosts and routers with any number of network interfaces, any number of routes, any number of services served, any number of complexity between variations of the services (including positive and negative expressions). firehol is a language to express firewalling rules, not just a script that produces some kind of a firewall. The goals of firehol are: \u2022 Being as easy as possible Independently of the security skills he\/she has, firehol allows to create and understand complex firewalls in just a few seconds. The configuration files are very easy to type and read. \u2022 Being as secure as possible. By allowing explicitly only the wanted traffic to flow firehol secures your system. firehol produces stateful rules for any service or protocol, in both directions of the firewall. \u2022 Being as open as possible. Althoug firehol is pre-configured for a large number of services, you can configure any service you like and firehol will turn it into a client, a server, or a router. \u2022 Being as flexible as possible. firehol can be used by end users and guru administrators requiring extremely complex firewalls. firehol configuration files are BASH scripts; you can write in them anything BASH accepts, including variables, pipes, loops, conditions, calls to external programs, run other BASH scripts with firehol directives in them, etc. \u2022 Being as simple as possible. firehol is easy to install on any modern Linux system; only one file is required, no compilations involved.","Process Name":"firehol","Link":"https:\/\/linux.die.net\/man\/1\/firehol"}},{"Process":{"Description":"This GUI application allows for inspecting a running FireVision systems by viewing images and visualizing color maps. Its second purpose is to calibrate the vision system, e.g. by creating mirror calibration files and creating color maps.","Process Name":"firestation","Link":"https:\/\/linux.die.net\/man\/1\/firestation"}},{"Process":{"Description":null,"Process Name":"firstaidkit","Link":"https:\/\/linux.die.net\/man\/1\/firstaidkit"}},{"Process":{"Description":"This man page tries to describe the inner workings of the plugin system for FirstAidKit. It also contains useful information to develop a plugin for firstaidkit.","Process Name":"firstaidkit-plugin","Link":"https:\/\/linux.die.net\/man\/1\/firstaidkit-plugin"}},{"Process":{"Description":"","Process Name":"fish","Link":"https:\/\/linux.die.net\/man\/1\/fish"}},{"Process":{"Description":"","Process Name":"fish_config","Link":"https:\/\/linux.die.net\/man\/1\/fish_config"}},{"Process":{"Description":"","Process Name":"fish_indent","Link":"https:\/\/linux.die.net\/man\/1\/fish_indent"}},{"Process":{"Description":"","Process Name":"fish_pager","Link":"https:\/\/linux.die.net\/man\/1\/fish_pager"}},{"Process":{"Description":null,"Process Name":"fish_prompt","Link":"https:\/\/linux.die.net\/man\/1\/fish_prompt"}},{"Process":{"Description":"","Process Name":"fish_update_completions","Link":"https:\/\/linux.die.net\/man\/1\/fish_update_completions"}},{"Process":{"Description":"","Process Name":"fishd","Link":"https:\/\/linux.die.net\/man\/1\/fishd"}},{"Process":{"Description":"The fishpoke program is used to trigger a poll immediately instead of waiting for the configured polling interval. This man page refers to the client for the Fishpolld server.","Process Name":"fishpoke","Link":"https:\/\/linux.die.net\/man\/1\/fishpoke"}},{"Process":{"Description":"Fishpoll is meant to deal with a common situation. You have a data source (a version control repository, a database, etc). You have something built from that data source (a web site, say). When the data source changes, you want them to rebuild against the latest commits.","Process Name":"fishpolld","Link":"https:\/\/linux.die.net\/man\/1\/fishpolld"}},{"Process":{"Description":"fisql is very similar to the \"isql\" utility programs distributed by Sybase and Microsoft. Like them, fisql uses the command \"go\" on a line by itself as a separator between batches.","Process Name":"fisql","Link":"https:\/\/linux.die.net\/man\/1\/fisql"}},{"Process":{"Description":"fitcircle reads lon,lat [or lat,lon] values from the first two columns on standard input [or xyfile]. These are converted to Cartesian three-vectors on the unit sphere. Then two locations are found: the mean of the input positions, and the pole to the great circle which best fits the input positions. The user may choose one or both of two possible solutions to this problem. The first is called -L1 and the second is called -L2. When the data are closely grouped along a great circle both solutions are similar. If the data have large dispersion, the pole to the great circle will be less well determined than the mean. Compare both solutions as a qualitative check. The -L1 solution is so called because it approximates the minimization of the sum of absolute values of cosines of angular distances. This solution finds the mean position as the Fisher average of the data, and the pole position as the Fisher average of the cross-products between the mean and the data. Averaging cross-products gives weight to points in proportion to their distance from the mean, analogous to the \"leverage\" of distant points in linear regression in the plane. The -L2 solution is so called because it approximates the minimization of the sum of squares of cosines of angular distances. It creates a 3 by 3 matrix of sums of squares of components of the data vectors. The eigenvectors of this matrix give the mean and pole locations. This method may be more subject to roundoff errors when there are thousands of data. The pole is given by the eigenvector corresponding to the smallest eigenvalue; it is the least-well represented factor in the data and is not easily estimated by either method. -L Specify the desired norm as 1 or 2, or use -L or -L3 to see both solutions.","Process Name":"fitcircle","Link":"https:\/\/linux.die.net\/man\/1\/fitcircle"}},{"Process":{"Description":null,"Process Name":"fitstopnm","Link":"https:\/\/linux.die.net\/man\/1\/fitstopnm"}},{"Process":{"Description":"This manpage documents briefly the fityk and cfityk programs for nonlinear fitting of analytical functions (especially peak-shaped) to data (usually experimental data). In other words, for nonlinear peak separation and analysis. It was developed for analyzing diffraction patterns, but can be also used in other fields, since concepts and operations specific for crystallography are separated from the rest of the program. Fityk offers various nonlinear fitting methods, subtracting background, calibrating data, easy placement of peaks and changing peak parameters, automation of common tasks with scripts, and much more. The main advantage of the program is a flexibility - parameters of peaks can be arbitrarily binded with each other, eg. width of peak can be an independent variable, can be the same as width of other peak or can be given by complicated - common for all peaks - formula. The program comes in two versions: fityk , the GUI version and cfityk , the command line version. A user manual for fityk and cfityk can be found in \/usr\/share\/doc\/fityk\/fitykhelp.html.","Process Name":"fityk","Link":"https:\/\/linux.die.net\/man\/1\/fityk"}},{"Process":{"Description":null,"Process Name":"fix132x43","Link":"https:\/\/linux.die.net\/man\/1\/fix132x43"}},{"Process":{"Description":"fixcvsdiff modifies diff files created from the cvs diff command, where files have been added or removed. CVS tends to create diff files that patch(1) mis-interprets. The diff file must retain the lines beginning with \"Index:\" in order for the correction to work.","Process Name":"fixcvsdiff","Link":"https:\/\/linux.die.net\/man\/1\/fixcvsdiff"}},{"Process":{"Description":"Fixdlsrps is a perl filter which \"fixes\" PostScript generated from the DviLaser\/PS driver so that it works correctly with Angus Duggan's psutils package.","Process Name":"fixdlsrps","Link":"https:\/\/linux.die.net\/man\/1\/fixdlsrps"}},{"Process":{"Description":null,"Process Name":"fixfmps","Link":"https:\/\/linux.die.net\/man\/1\/fixfmps"}},{"Process":{"Description":"fixincludes tries to reduce the number of #includes in C++ source files. Much of it's processing is specific to KDE sources and so it might not work so well with sources for non-KDE applications. The following problems are identified by fixincludes: \u2022 Including headers that are no longer supported but which exist for compatibility with older Qt\/KDE versions; \u2022 Including the same file multiple times. There is also an experimental mode which tries removing each #include one at a time (with a few exceptions) to see whether the source still compiles. Note that this experimental mode will modify the original sources. By default the sources will not be modified; the identified problems will simply be written to standard output. The list of C++ sources to examine should be given on the command-line. If no files are given, all C++ sources in or beneath the current directory will be examined (with the exception of directories whose Makefile.am contains -UQT_NO_COMPAT or -UKDE_NO_COMPAT) This utility is part of the KDE Software Development Kit.","Process Name":"fixincludes","Link":"https:\/\/linux.die.net\/man\/1\/fixincludes"}},{"Process":{"Description":"fixkrf checks a specified keyrec file to ensure that the referenced encryption key files exist where listed. If a key is not where the keyrec specifies it should be, then fixkrf will search the given directories for those keys and adjust the keyrec to match reality. If a key of a particular filename is found in multiple places, a warning will be printed and the keyrec file will not be changed for that key.","Process Name":"fixkrf","Link":"https:\/\/linux.die.net\/man\/1\/fixkrf"}},{"Process":{"Description":"Fixmacps is a perl filter which \"fixes\" PostScript generated from a Macintosh PC that it works correctly with Angus Duggan's psutils package.","Process Name":"fixmacps","Link":"https:\/\/linux.die.net\/man\/1\/fixmacps"}},{"Process":{"Description":null,"Process Name":"fixproc","Link":"https:\/\/linux.die.net\/man\/1\/fixproc"}},{"Process":{"Description":"Try to fix common PostScript problems that break postprocessing.","Process Name":"fixps","Link":"https:\/\/linux.die.net\/man\/1\/fixps"}},{"Process":{"Description":"Fixpsditps is a perl filter which \"fixes\" PostScript from Transcript's psdit program so that it works correctly with Angus Duggan's psutils package.","Process Name":"fixpsditps","Link":"https:\/\/linux.die.net\/man\/1\/fixpsditps"}},{"Process":{"Description":"Fixpspps is a perl filter which \"fixes\" PostScript from PSPrint so that it works correctly with Angus Duggan's psutils package.","Process Name":"fixpspps","Link":"https:\/\/linux.die.net\/man\/1\/fixpspps"}},{"Process":{"Description":null,"Process Name":"fixscribeps","Link":"https:\/\/linux.die.net\/man\/1\/fixscribeps"}},{"Process":{"Description":"Fixtpps is a perl filter which \"fixes\" PostScript generated from the Troff Tpscript driver so that it works correctly with Angus Duggan's psutils package.","Process Name":"fixtpps","Link":"https:\/\/linux.die.net\/man\/1\/fixtpps"}},{"Process":{"Description":"Fixwfwps is a perl filter which \"fixes\" PostScript from Word for Windows so that it works correctly with Angus Duggan's psutils package.","Process Name":"fixwfwps","Link":"https:\/\/linux.die.net\/man\/1\/fixwfwps"}},{"Process":{"Description":"Fixwpps is a perl filter which \"fixes\" PostScript from WordPerfect 5.0 and 5.1 so that it works correctly with Angus Duggan's psutils package.","Process Name":"fixwpps","Link":"https:\/\/linux.die.net\/man\/1\/fixwpps"}},{"Process":{"Description":null,"Process Name":"fixwwps","Link":"https:\/\/linux.die.net\/man\/1\/fixwwps"}},{"Process":{"Description":"Fiz is used to analyze damaged zoo archives and locate directory entries and file data in them. The current version of fiz is 2.0 and it is meant to be used in conjunction with zoo version 2.0. Fiz makes no assumptions about archive structure. Instead, it simply searches the entire subject archive for tag values that mark the locations of directory entries and file data. In a zoo archive, a directory entry contains information about a stored file such as its name, whether compressed or not, and its timestamp. The file data are the actual data for the archived file, and may be either the original data, or the result of compressing the file. For each directory entry found, fiz prints where in the archive it is located, the directory path and filename(s) found in it, whether the directory entry appears to be corrupted (indicated by [*CRC Error*]), and the value of the pointer to the file data that is found in the directory entry. For each block of file data found in the archive, fiz prints where in the archive the block begins. In the case of an undamaged archive, the pointer to file data found in a directory entry will correspond to where fiz actually locates the data. Here is some sample output from fiz: ****************\n    2526: DIR  [changes] ==>   95\n    2587: DATA\n****************\n    3909: DIR  [copyrite] ==> 1478\n    3970: DATA\n    4769: DATA\n**************** In such output, DIR indicates where fiz found a directory entry in the archive, and DATA indicates where fiz found file data in the archive. Filenames located by fiz are enclosed in square brackets, and the notation \"==> 95\" indicates that the directory entry found by fiz at position 2526 has a file data pointer to position 95. In actuality, fiz found file data at positions 2587, 3970, and 4769. Since fiz found only two directory entries, and each directory entry corresponds to one file, one of the file data positions is an artifact. Once the locations of directory entries and file data are found, the @ modifier to zoo's archive list and extract commands can be used and the archive contents selectively listed or extracted, skipping the damaged portion. This is further described in the documentation for zoo(1). In the above case, commands to try giving to zoo might be x@2526,2587 (extract beginning at position 2526, and get file data from position 2587), x@3090,3970 (extract at 3090, get data from 3970) and x@3909,4769 (extract at 3909, get data from 4769). Once a correctly-matched directory entry\/file data pair is found, zoo will in most cases synchronize with and correctly extract all files subsequently found in the archive. Trial and error should allow all undamaged files to be extracted. Also note that self-extracting archives created using sez (the Self-Extracting Zoo utility for MS-DOS), which are normally executed on an MS-DOS system for extraction, can be extracted on non-MSDOS systems in a similar way.","Process Name":"fiz","Link":"https:\/\/linux.die.net\/man\/1\/fiz"}},{"Process":{"Description":"flac is a command-line tool for encoding, decoding, testing and analyzing FLAC streams.","Process Name":"flac","Link":"https:\/\/linux.die.net\/man\/1\/flac"}},{"Process":{"Description":null,"Process Name":"flame","Link":"https:\/\/linux.die.net\/man\/1\/flame"}},{"Process":{"Description":"The flatbat command applies flat-field correction to a set of source frames. It means, that it divides the source frames by the flat frame pixel-by-pixel and the result is multiplied by median value of the flat frame. The resulting image is written to output file. The source frames and also flat frame must be in FITS format and of same dimensions. The output file is in FITS format too.","Process Name":"flatbat","Link":"https:\/\/linux.die.net\/man\/1\/flatbat"}},{"Process":{"Description":"flatbeh synthetize a VHDL behavioral data-flow description from a structural description. It flattens the structural description (it can be a hierarchy of macro block) until the cells which have a behavioral description. Then it raise all the equations and create a behavioral description of the root file.","Process Name":"flatbeh","Link":"https:\/\/linux.die.net\/man\/1\/flatbeh"}},{"Process":{"Description":"flatlo FLATen LOgical figure flatlo logical_figure instance output_name flattens \"instance\" in \"logical_figure\" flatlo -r logical_figure output_name flattens logical_figure to the catalog flatlo -t logical_figure output_name flattens logical_figure to the transistors","Process Name":"flatlo","Link":"https:\/\/linux.die.net\/man\/1\/flatlo"}},{"Process":{"Description":null,"Process Name":"flatph","Link":"https:\/\/linux.die.net\/man\/1\/flatph"}},{"Process":{"Description":"flea is a shell script which helps you to submit a bug report against the mutt(1) mail user agent. If you invoke flea, you'll first be prompted for a short description of the problem you experience. This will be used as the bug report's subject line, so it should be concise, but informative. You are then asked to assign an initial severity level to the problem you observe; flea will give you a description which severity level is appropriate or not. Then, you are asked for the location of a core dump (normally named core) which may have been left over by a crash of your mutt(1). You can just type lqnorq here, or you can enter the path leading to a core dump. flea will try to use either sdb(1), dbx(1), or gdb(1) to extract some information from this core dump which may be helpful to developers in order to determine the reason for the crash. Finally, you are asked whether or not you want to include personal and system mutt(1) configuration files with the bug report. If at all possible, we urge you to answer these questions with lqyesrq, since a reference configuration makes it incredibly easier to track down a problem. If you are using Debian GNU\/Linux, flea will now check whether or not mutt has been installed as a Debian package on your system, and suggest to file the bug against the mutt(1) and Debian bug tracking systems. This option was added since the mutt(1) project uses another instantiation of the Debian bug tracking system, so submitting bugs against both systems in one pass is simple. You are then dropped into your favorite editor as determined by the EDITOR and VISUAL environment variables. Please give us details about the problem in the empty space below the line reading lqPlease type your report below this linerq. We are most interested in precise information on what symptoms you observe and what steps may be used to reproduce the bug. Chances are that problems which can easily be reproduced will be fixed quickly. So please take some time when filling out this part of the template. The remainder of the template contains various kinds of information gathered from your system, including output of the uname(1) command, output from mutt(1) itself, and your system's mutt(1) configuration files. You may wish to browse through this part of the bug report form in order to avoid leaking confidential information to the public. If you leave the editor, flea will give you the option to review, re-edit, submit, or abandon your bug report. If you decide to submit it, a mail message containing your report will be sent to <submit@bugs.guug.de>. You'll receive a copy of this message. While your bug report is being processed by the bug tracking system, you will receive various e-mail messages from the bug tracking system informing you about what's going on: Once your bug report has been entered into the bug tracking system, it will be assigned a unique serial number about which you are informed via e-mail. If you wish to submit additional information about the bug, you can just send it to the address serial@bugs.guug.de. Later, you will most likely receive questions from the developers about the problem you observed, and you will eventually be informed that your bug report has been closed. This means that the bug has been fixed at least in the cvs(1) repository. If the answers you receive don't satisfy you, don't hesitate to contact the developers directly under mutt-dev@mutt.org. You can also browse your bug report and all additional information and replies connected to it using the bug tracking system's Web interface under the following URL: http:\/\/bugs.guug.de\/ ENVIRONMENT flea will use the following environment variables: EMAIL Your electronic mail address. Will be used to set the bug report's From header, and to send you a copy of the report. LOGNAME Your login name. If the EMAIL environment variable isn't set, this will be used instead to send you a copy of the report. Setting the sender will be left to sendmail(1) on your system. REPLYTO If set, the bug report will contain a Reply-To header with the e-mail address contained in this environment variable. ORGANIZATION If set, the bug report will contain an Organization header with the contents of this environment variable. PAGER If set, this environment variable will be expected to contain the path to your favorite pager for viewing the bug report. If unset, more(1) will be used. VISUAL If set, this environment variable will be expected to contain the path to your favorite visual editor. EDITOR If set, this environment variable will be expected to contain the path to your favorite editor. This variable is examined if and only if the VISUAL environment variable is unset. If EDITOR is unset, vi(1) will be used to edit the bug report. FILES core If present, this file may contain a post-mortem memory dump of mutt. It will be inspected using the debugger installed on your system. SEE ALSO dbx(1), gdb(1), lynx(1), mutt(1), muttrc(5), sdb(1), sendmail(1), uname(1), vi(1) The mutt bug tracking system: http:\/\/bugs.guug.de\/ AUTHOR flea and this manual page were written by Thomas Roessler <roessler@does-not-exist.org>. Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"flea","Link":"https:\/\/linux.die.net\/man\/1\/flea"}},{"Process":{"Description":null,"Process Name":"flex","Link":"https:\/\/linux.die.net\/man\/1\/flex"}},{"Process":{"Description":null,"Process Name":"flex++","Link":"https:\/\/linux.die.net\/man\/1\/flex++"}},{"Process":{"Description":"flickcurl is a program that can call some of the Flickr APIs, plus help with authentication by turning mobile authentication method frobs into full auth_tokens with -a NNN-NNN-NNN and writing the ~\/.flickcurl.conf file. Use flickcurl -h to get a list of the supported commands and their arguments.","Process Name":"flickcurl","Link":"https:\/\/linux.die.net\/man\/1\/flickcurl"}},{"Process":{"Description":null,"Process Name":"flickcurl-config","Link":"https:\/\/linux.die.net\/man\/1\/flickcurl-config"}},{"Process":{"Description":"Batch image uploader for the Flickr.com service. flickr_upload may also be useful for generating authentication tokens against other API keys\/secrets (i.e. for embedding in scripts).","Process Name":"flickr_upload","Link":"https:\/\/linux.die.net\/man\/1\/flickr_upload"}},{"Process":{"Description":null,"Process Name":"flickrdf","Link":"https:\/\/linux.die.net\/man\/1\/flickrdf"}},{"Process":{"Description":"flipdiff exchanges the order of two patch files that apply one after the other. The patches must be \"clean\": the context lines must match and there should be no mis-matched offsets. The swapped patches are sent to standard output, with a marker line (\"=== 8< === cut here === 8< ===\") between them, unless the --in-place option is passed. In that case, the output is written back to the original input files.","Process Name":"flipdiff","Link":"https:\/\/linux.die.net\/man\/1\/flipdiff"}},{"Process":{"Description":"Flipflop draws a grid of 3D colored tiles that change positions with each other.","Process Name":"flipflop","Link":"https:\/\/linux.die.net\/man\/1\/flipflop"}},{"Process":{"Description":null,"Process Name":"flipscreen3d","Link":"https:\/\/linux.die.net\/man\/1\/flipscreen3d"}},{"Process":{"Description":"This utility manages flock(2) locks from within shell scripts or the command line. The first and second forms wraps the lock around the executing a command, in a manner similar to su(1) or newgrp(1). It locks a specified file or directory, which is created (assuming appropriate permissions), if it does not already exist. The third form is convenient inside shell scripts, and is usually used the following manner: ( flock -s 200 # ... commands executed under lock ... ) 200>\/var\/lock\/mylockfile The mode used to open the file doesn't matter to flock; using > or >> allows the lockfile to be created if it does not already exist, however, write permission is required; using < requires that the file already exists but only read permission is required. By default, if the lock cannot be immediately acquired, flock waits until the lock is available.","Process Name":"flock","Link":"https:\/\/linux.die.net\/man\/1\/flock"}},{"Process":{"Description":"clogin is an expect(1) script to automate the process of logging into a Cisco router, catalyst switch, Extreme switch, Juniper ERX\/E-series, Procket Networks, or Redback router. There are complementary scripts for Alteon, Avocent (Cyclades), Bay Networks (nortel), ADC-kentrox EZ-T3 mux, Foundry, HP Procurve Switches and Cisco AGMs, Hitachi Routers, Juniper Networks, MRV optical switch, Netscreen firewalls, Netscaler, Riverstone, Netopia, and Lucent TNT, named alogin, avologin, blogin, elogin, flogin, fnlogin, hlogin, htlogin, jlogin, mrvlogin, nlogin, nslogin, rivlogin, tlogin, and tntlogin, respectively. clogin reads the .cloginrc file for its configuration, then connects and logs into each of the routers specified on the command line in the order listed. Command-line options exist to override some of the directives found in the .cloginrc configuration file. The command-line options are as follows: -S Save the configuration on exit, if the device prompts at logout time. This only has affect when used with -s. -V Prints package name and version strings. -c Command to be run on each router list on the command-line. Multiple commands maybe listed by separating them with semi-colons (;). The argument should be quoted to avoid shell expansion. -d Enable expect debugging. -E Specifies a variable to pass through to scripts (-s). For example, the command-line option -Efoo=bar will produce a global variable by the name Efoo with the initial value \"bar\". -e Specify a password to be supplied when gaining enable privileges on the router(s). Also see the password directive of the .cloginrc file. -f Specifies an alternate configuration file. The default is $HOME\/.cloginrc. -p Specifies a password associated with the user specified by the -u option, user directive of the .cloginrc file, or the Unix username of the user. -s The filename of an expect(1) script which will be sourced after the login is successful and is expected to return control to clogin, with the connection to the router intact, when it is done. Note that clogin disables log_user of expect(1)when -s is used. Example script(s) can be found in share\/rancid\/*.exp. -t Alters the timeout interval; the period that clogin waits for an individual command to return a prompt or the login process to produce a prompt or failure. The argument is in seconds. -u Specifies the username used when prompted. The command-line option overrides any user directive found in .cloginrc. The default is the current Unix username. -v Specifies a vty password, that which is prompted for upon connection to the router. This overrides the vty password of the .cloginrc file's password directive. -w Specifies the username used if prompted when gaining enable privileges. The command-line option overrides any user or enauser directives found in .cloginrc. The default is the current Unix username. -x Similar to the -c option; -x specifies a file with commands to run on each of the routers. The commands must not expect additional input, such as 'copy rcp startup-config' does. For example: show version\nshow logging -y Specifies the encryption algorithm for use with the ssh(1) -c option. The default encryption type is often not supported. See the ssh(1) man page for details. The default is 3des.","Process Name":"flogin","Link":"https:\/\/linux.die.net\/man\/1\/flogin"}},{"Process":{"Description":null,"Process Name":"flow","Link":"https:\/\/linux.die.net\/man\/1\/flow"}},{"Process":{"Description":"The flow-capture utility will receive and store NetFlow exports to disk. The flow files are rotated rotationstimes per day and expiration of old flow files can be configured by number of files or total space utilization. Files are stored in workdir and can optionally be stored in additional levels of directories. Active files created by flow-capture begin with 'tmp'. Files that are complete begin with 'ft'. When the remoteip is configured only flows from that exporter will be processed, this is the most secure and recommended configuration. When the localip is configured flow-capture will only process flows sent to the localip IP address. If remoteip is 0 (not configured) flows from any source IP address are accepted. Multiple non aggregated PDU versions may be accepted at once to support Cisco's Catalyst 6500 NetFlow implementation which exports from both the supervisor and MSFC with the same IP address and same port but different export versions. In this case the exports will be stored in the format specified by pdu_version or whichever export type is received first. NetFlow exports are UDP and do not employ congestion control or a retransmission mechanism. If the server flow-capture is configured on is too busy, or the network is congested or lossy NetFlow exports will be lost. An estimate of lost flows is recorded in the flow files, and logged via syslog. Most servers will provide a count of dropped packets due to full socket buffers via the netstat utility. For example netstat -s | grep full will provide a count of UDP packets dropped due to full socket buffers. If this is a persistent occurrence either flow-capture will need a larger server or the compression level should be decreased with -z. A SIGHUP signal will cause flow-capture to close the current file and create a new one. A SIGQUIT or SIGTERM signal will cause flow-capture to close the current file and exit.","Process Name":"flow-capture","Link":"https:\/\/linux.die.net\/man\/1\/flow-capture"}},{"Process":{"Description":"The flow-cat utility processes files and\/or directories of files in the flow-tools format. The resulting concatenated data set is written to the standard output or file specified by -o. If file is a single dash ('-') or absent, flow-cat will read from the standard input.","Process Name":"flow-cat","Link":"https:\/\/linux.die.net\/man\/1\/flow-cat"}},{"Process":{"Description":"The flow-dscan utility is used to detect suspicious activity such as port scanning, host scanning, and flows with unusually high octets or packets. A source and destination suppress list is supported to help prevent false alarms due to hosts such as nameservers or popular web servers that exchange traffic with a large number of hosts. Alarms are logged to syslog or stderr. The internal state of flow-dscan can be saved and loaded to allow for interrupted operation. flow-dscan will work best if configured to only watch only inbound or outbound traffic by using the input or output interface filter option. The host scanner works by counting the length of the destination IP hash chain. If it goes above 64, then the src is considered to be scanning. The port scanner works by keeping a bitmap of the destination port number < 1024 per destination IP. If it goes above 64, the src is considered to be port scanning the destination. When a src has been flagged as scanning it will not be reported again until the record is aged out and enough flows trigger it again. A SIGHUP signal will instruct flow-dscan to reload the suppress list. A SIGUSR1 signal will instruct flow-dscan to dump its internal state.","Process Name":"flow-dscan","Link":"https:\/\/linux.die.net\/man\/1\/flow-dscan"}},{"Process":{"Description":"The flow-expire utility will remove the oldest flow files in a directory based on either a count of files or space utilization. The directory is recursively searched for flow files. Files that do not have a flow-tools signature will be ignored. The internal timestamp is used so backups or copies of the flow files that do not retain the original timestamp will not impact the operation of flow-expire. flow-expire is typically used to manage storage in a distributed environment where flows are collected on a different server than they are archived.","Process Name":"flow-expire","Link":"https:\/\/linux.die.net\/man\/1\/flow-expire"}},{"Process":{"Description":"The flow-export utility will convert flow-tools flow files to ASCII CSV, cflowd, pcap, wire, mySQL, and PGSQL format.","Process Name":"flow-export","Link":"https:\/\/linux.die.net\/man\/1\/flow-export"}},{"Process":{"Description":"The flow-fanout utility will replicate flows arriving on localip\/remoteip\/port to destination(s) specified by localip\/remoteip\/port. Flows processed by multiple exporters will be mixed into a single output stream. This functionality appeared to support Cisco Catalyst exports and may have other uses. A SIGQUIT or SIGTERM signal will cause flow-fanout to exit.","Process Name":"flow-fanout","Link":"https:\/\/linux.die.net\/man\/1\/flow-fanout"}},{"Process":{"Description":"The flow-filter utility will filter flows based on user selectable criteria. The IP address filters are defined in flow.acl or by the filename specified by -f. Other filters such as input interface and ports are defined on the command line. These filters accept range and negation operators, ie -i1-15 for input interfaces 1 through 15 or -i1,15 for input interfaces 1 and 15, or !1,15 for not input interfaces 1 and 15. The syntax is kludgy and needs reworked but works for most applications.","Process Name":"flow-filter","Link":"https:\/\/linux.die.net\/man\/1\/flow-filter"}},{"Process":{"Description":null,"Process Name":"flow-gen","Link":"https:\/\/linux.die.net\/man\/1\/flow-gen"}},{"Process":{"Description":"The flow-header utility will display the flow meta information flow-tools uses internally.","Process Name":"flow-header","Link":"https:\/\/linux.die.net\/man\/1\/flow-header"}},{"Process":{"Description":"The flow-import utility will convert data from cflowd and ASCII CSV files into flow-tools format.","Process Name":"flow-import","Link":"https:\/\/linux.die.net\/man\/1\/flow-import"}},{"Process":{"Description":"The flow-log2rrd utility processes the STAT lines generated by flow-capture and flow-fanout and converts them into RRD files. RRD's are stored as rrd_path\/capture|fanout.hostname.srcip.dstip.port.rrd. The Datastores are flows,pkts,lost representing flows, packets, and lost flows respectively. Flow-fanout generates an additional DS nobufs indicating the number of times a write() failed with ENOBUFS.","Process Name":"flow-log2rrd","Link":"https:\/\/linux.die.net\/man\/1\/flow-log2rrd"}},{"Process":{"Description":"The flow-mask utility is used to modify the source and destination mask length's in flow records.","Process Name":"flow-mask","Link":"https:\/\/linux.die.net\/man\/1\/flow-mask"}},{"Process":{"Description":"The flow-merge utility processes files and\/or directories of files in the flow-tools format. The resulting merged data set is written to the standard output or file specified by -o. If file is a single dash ('-') or absent, flow-merge will read from the standard input. Unlike flow-cat, flow-merge interleaves flow records preserving the relative chronological order.","Process Name":"flow-merge","Link":"https:\/\/linux.die.net\/man\/1\/flow-merge"}},{"Process":{"Description":"The flow-nfilter utility will filter flows based on user selectable criteria. Filters are defined in a configuration file and are composed of primitives and a definition. Definitions contain match lines grouped to form logical AND and OR operations on the flow using the selected primitives. A definition may contain the invert command which will invert the result of the evaluation. Words in the configuration file of the form @VAR or @{VAR:default} will be expanded at run-time by setting variable names with the -v option. Filter primitives begin with the filter-primitive keyword followed by a symbolic name. Each primitive has a type defined below. A list of permit and or deny keywords followed by an argument are later evaulated to determine if the flow is permitted or denied. The default action for a primitive is to deny which may be changed with the default keyword. Symbolic substitutions are done where appropriate. The match keyword in a definition selects the criteria to match a primitive. A match type may allow more than one type of primitive, for example the src-ip-addr match type will accept any of {ip-address, ip-address-mask, ip-address-prefix} primitive types. Primitive type          Type       Description\/Example\n-------------------------------------------------------------------\nas                      Bucket     Autonomous System Number.\n                                   600,159,3112\n\nip-address-prefix-len   Numeric    Integer from 0 to 32.\n                                   16-31\n\nip-protocol             Bucket     Integer from 0 to 255.\n                                   6,17,1\n\nip-tos                  Bucket     Integer from 0 to 255 with mask.\n                                   0xA0\/0xE0\n\nip-tcp-flags            Bucket     Integer from 0 to 255 with mask.\n                                   0x2\/0x2\n\nifindex                 Bucket     Integer from 0 to 65535\n                                   0,5,10\n\nengine                  Bucket     Integer from 0 to 255.\n                                   0\n\nip-port                 Bucket     Integer from 0 to 65535.\n                                   80,8080,23,22\n\nip-address              Hash       List of IP Addresses.\n                                   10.0.0.1\n\nip-address-mask         List       List of IP address\/mask pairs.\n                                   10.1.0.0 255.255.0.0\n\nip-address-prefix       Trie       List of IP address\/mask pairs.\n                                   10.1\/16\n\ntag                     Hash       List of tags.\n                                   0xFF00\n\ntag-mask                List       List of tags.\n                                   0xF000\/0xFF00\n\ncounter                 List       List of Integers with qualifier.\n                                   lt 32\n\ntime                    List       List of relative time specifiers.\n                                   gt 5:00\n\ntime-date               List       List of absolute time specifiers.\n                                   gt December 12, 2002 5:13:21\n\ndouble                  List       List of doubles with qualifier.\n                                   lt 32.0\n\nrate                    Element    Rate is calculated as 1\/rate.\n                                   permit 100\n\nMatch type              Description             Primitives accepted\n-------------------------------------------------------------------\nsource-as               Source AS               as\n\ndestination-as          Destination AS          as\n\nip-source-address       Source IP Address       ip-address,\n                                                ip-address-mask,\n                                                ip-address-prefix\n\nip-destination-address  Destination IP Address  ip-address,\n                                                ip-address-mask,\n                                                ip-address-prefix\n\nip-exporter-address     Exporter IP Address     ip-address,\n                                                ip-address-mask,\n                                                ip-address-prefix\n\nip-nexthop-address      NextHop IP Address      ip-address,\n                                                ip-address-mask,\n                                                ip-address-prefix\n\nip-shortcut-address     Shortcut IP Address     ip-address,\n                                                ip-address-mask,\n                                                ip-address-prefix\n\nip-protocol             IP Protocol             ip-protocol\n\nip-source-address-prefix-len\n                        Source IP address       ip-address-prefix-len\n                        prefix length\n\nip-destination-address-prefix-len\n                        Destination IP address  ip-address-prefix-len\n                        prefix length\n\nip-tos                  IP Type Of Service      ip-tos\n\nip-marked-tos           IP Type Of Service      ip-tos\n\nip-tcp-flags            IP\/TCP Flags            ip-tcp-flags\n\nip-source-port          Source IP Port          ip-port\n                        eg TCP\/UDP\n\nip-destination-port     Destination IP Port     ip-port\n                        eg TCP\/UDP\n\ninput-interface         Source ifIndex          ifindex\n                        eg Input Interface\n\noutput-interface        Destination ifIndex     ifindex\n                        eg Output Interface\n\nstart-time              Start Time of flow      time, time-date\n\nend-time                End Time of Flow        time, time-date\n\nflows                   Number of flows         counter\n\noctets                  Number of octets        counter\n\npackets                 Number of packets       counter\n\nduration                Duration of flow in ms  counter\n\nengine-id               Engine ID               engine\n\nengine-type             Engine Type             engine\n\nsource-tag              Source Tag              tag, tag-mask\n\ndestination-tag         Destination Tag         tag, tag-mask\n\npps                     Packets Per Second      double\n\nbps                     Bits Per Second         double\n\nrandom-sample           Random Sample           rate","Process Name":"flow-nfilter","Link":"https:\/\/linux.die.net\/man\/1\/flow-nfilter"}},{"Process":{"Description":"The flow-print utility will display flow data in ASCII using pre-defined formats selectable with -f. Some of the less descriptive column headers are defined below\n\nHeader     Description\n-------------------------------\nSif           Source Interface ifIndex.\nDiF           Destination Interface ifIndex.\nPr            Protocol.\nP             Protocol.\nSrcP          Source Port.\nDstP          Destination Port.\nPkts          Packets.\nOctets        Octets (Bytes).\nActive        Time in ms the flow was active.\nB\/Pk          Bytes per Packet.\nTs            Type of Service.\nFl            Flags, for TCP the cumulative or of the TCP control bits.\nsrcAS         Source AS.\ndstAS         Destination AS.\nStart         Start time of the flow.\nEnd           End time of the flow.\nrouter_sc     V7 IP address of router producing shorcuts.\npeer_nexthop  V6 IP address of peer next hop IP address.\nencap i\/o     Version 6 only.  Encapsulation size in\/out\nduration      Time in ms the flow was active.\ninput         Input Interface ifIndex.\noutput        Output Interface ifIndex.\nflows         Number of aggregated flows.\nmTos          V8.x ToS of pkts that exceeded the contract.\nxpackets      V8.x Packets that exceed the contract.","Process Name":"flow-print","Link":"https:\/\/linux.die.net\/man\/1\/flow-print"}},{"Process":{"Description":null,"Process Name":"flow-receive","Link":"https:\/\/linux.die.net\/man\/1\/flow-receive"}},{"Process":{"Description":"The flow-report utility will generate reports from flow data. The reports are easy to parse ASCII text that can be used by a front end to produce readable reports, graphs, and charts. Reports are definied in a configuration file by the 'stat-report' keyword followed by a report name. Each report has a type defined below and other commands. Reports are grouped into a definition with the 'stat-definition' keyword followed by a definition name. Each definition can invoke a filter and optionally apply tags. Words in the configuration file of the form @VAR or @{VAR:default} will be expanded at run-time by setting variable names with the -v option. Generated reports consist of comment lines and report lines. Comment lines begin with a # and include details such as the options used, report name, records in the report, and the report line format. Some of the more verbose comments can be controlled with the +header and +xheader options. By default this information is not displayed. A column title beginning with the string rec precedes the report lines. Report lines consist of key fields, such as an IP address and calculated totals for that key such as the number of flows. The summary-detail report is a little bit different from other reports in that it has multiple title lines and no key fields. The column titles are described below.  +time_real   Difference between the real time of the first and last\n              flow.\n +aflowtime   Total time of the flows \/ Total number of flows.\n +aps         Total Octets \/ Total Packets (Average Packet Size)\n +afs         Total Octets \/ Total Flows \/ (Average Flow Size)\n +apf         Total Packets \/ Total Flows (Average Packets \/ Flow)\n +fps         Total Flows \/ (Last End Time of Flow -\n                First Start Time of Flow) (Average Flows \/ Second)\n +fps_real    (Average Flows \/ Second in realtime)\n +psizeN      Average Packet Size buckets.\n +fpsizeN     Packets \/ Flow buckets.\n +fosizeN     Octets \/ Flow buckets.\n +ftimeN      Time \/ Flow buckets.\n\n ignores     Flows with a packet count of 0.\n\n SSS-count   Count of of an item, example source-ip-address-count\n\n SSS*        key fields, example source-ip-address\n  index       Report line index.\n  first       Time of first flow in unix_secs format.\n  last        Time of last flow in unix_secs format.\n\n flows       Summation of flows\/key.\n  octets      Summation of octets\/key.\n  packets     Summation of packets\/key.\n  duration    End time of Flow - Start time of Flow.\n  avg-bps     Average Bits\/Second.\n  min-bps     Minimum Bits\/Second.\n  max-bps     Maximum Bits\/Second.\n  avg-pps     Average Packets\/Second.\n  min-pps     Minimum Packets\/Second.\n  max-pps     Maximum Packets\/Second.\n  frecs       Records used in average calculations.\n\nNote fields with a + are only available in the summary-detail report. The PPS and BPS calculations will not always be correct due to flows which only have one packet, or some other condition where the start time is equal to the end time. In this case these flows are not used in the PPS and BPS calculations. To facilitate aggregating multiple reports and retaining the PPS and BPS fields, the number of flows counted is available in the frecs field. stat-report command          Description\/Example\n------------------------------------------------------------------------\ntype                         Define the report type.\n                             type destination-tag\n\nfilter                       Apply this filter definition.\n                             filter permit-only-tcp\n\nscale                        Scale report by n.\n                             scale 100\n\ntag-mask                     Apply source and destination mask to tag.\n                             tag-mask 0xFF00 0xFF00\n\nip-source-address-format     Format of source IP address.\n                             address    -  address, ie 128.146.1.7\n                             prefix-len -  address\/len ie 128.146.1.7\/24\n                             prefix-mask-  prefix\/len 128.146.1\/24\n\nip-destination-address-format\n                             Format of destination IP address.\n                             address    -  address, ie 128.146.1.7\n                             prefix-len -  address\/len ie 128.146.1.7\/24\n                             prefix-mask-  prefix\/len 128.146.1\/24\n\noutput                       Start an output configuration.  Multiple\n                             output configurations can be configured\n                             per report.\n\noutput option                Description\/Example\n-------------------------------------------------------------------------\n\npath                         Pathname of output.  If the path begins\n                             with a | the output is a pipe.  The\n                             pathname is formatted through strftime().\n                             Directories not in the path are\n                             automatically created.\n                             path \/tmp\/%Y\/%m\/%d\/foo.out\n\ntime                         What time to use when formatting the\n                             pathname with strftime.\n                             now         - current time\n                             start       - first flow\n                             end         - last flow\n                             mid         - average of first and last.\n\ntally                        Emit a % total line every n records.\n                             tally 10\n\nformat                       Output format.  Currently only ascii.\n                             format ascii\n\nsort                         Sort on a field.  + ascending, - descending.\n                             sort +flows    - sort on the flows field\n\n                            Sortable fields are flows,octets,packets,\n                             duration,avg-pps,min-pps,max-pps,avg-bps,\n                             min-bps,max-bps\n\nrecords                      Truncate report at n records.\n                             records 10\n\nfields                       Enable\/Disable fields with +\/-.  Fields:\n                             index,first,last,flows,octets,packets,\n                             duration,pps,bps,other,key,key1,key2,\n                             key3,key4,count.\n                             fields +key,+flows,+octets,+packets,\n\n                            For reports with one key, the key\n                             field is referenced with key, else\n                             key1,key2,key3,etc\n\n                            Note that the count field is only available\n                             in select reports, those which end in\n                             -count.\n\noptions                      Enable\/Disable options with +\/-\n                             +header        - include header.\n                             +xheader       - include extra header.\n                             +totals        - include a totals line.\n                             +percent-total - report in % total form.\n                             +names         - use symbolic names.\n                             options +header,+xheader\n\nstat-definition option       Description\/Example\n-------------------------------------------------------------------------\nfilter                       Apply this filter definition.\n                             filter default\n\ntag                          Apply this tag definition.\n                             tag default\n\nmask                         Apply this mask definition.\n                             mask default\n\nreport                       Invoke this report.  Multiple reports can\n                             be set.\n                             report foo\n\ntime-series                  How often to produce a report in seconds.\n                             time-series 60\n\nglobal options               Description\/Example\n-------------------------------------------------------------------------\ninclude-tag                  Specify path to include tag definitions.\n                             include-tag \/flows\/tags\/test1\n\ninclude-filter               Specify path to include filter definitions.\n                             include-filter \/flows\/filters\/test1\n\ninclude-mask                 Specify path to include mask definitions.\n                             include-filter \/flows\/masks\/test1\n\nReport type                  Summarization Key Elements.\n------------------------------------------------------------------------\nsummary-detail               Totals plus quick breakdown.\n\nsummary-counters             Totals only.\n\npacket-size                  Average packet size distribution.\n\noctets                       Octets per flow distribution.\n\npackets                      Packets per flow distribution.\n\nip-source-port               IP Source Port.\n\nip-destination-port          IP Destination Port.\n\nip-source\/destination-port   IP Source\/Destination Port pair.\n\nbps                          Bits\/Second distribution.\n\npps                          Packets\/Second distribution.\n\nip-destination-address-type\n                             IP class with ASM\/SSM Multicast breakout.\n\nip-protocol                  IP Protocol.\n\nip-tos                       IP Type of Service.\n\nip-next-hop-address          IP Next Hop Address.\n\nip-source-address            IP Source Address.\n\nip-destination-address       IP Destination Address.\n\nip-source\/destination-address\n                             IP Source\/Destination Address pair.\n\nip-exporter-address          IP Exporter Address.\n\ninput-interface              Input Interface.\n\noutput-interface             Output Interface.\n\ninput\/output-interface       Input\/Output Interface pair.\n\nsource-as                    Source AS.\n\ndestination-as               Destination AS.\n\nsource\/destination-as        Source\/Destination AS.\n\nip-source-address\/source-as  IP Source Addrss and Source AS.\n\nip-destination-address\/source-as\n                             IP Destination Address and Source AS.\n\nip-source-address\/destination-as\n                             IP Source Address and Destination AS.\n\nip-destination-address\/destination-as\n                             IP Destination Address and Destination AS.\n\nip-source\/destination-address\/source-as\n                             IP Source\/Destination Address and Source AS.\n\nip-source\/destination-address\/destination-as\n                             IP Source\/Destination Address and\n                             Destination AS.\n\nip-source\/destination-address\/source\/destination-as\n                             IP Source\/Destination Address and\n                             Source\/Destination AS.\n\nip-source-address\/input-interface\n                             IP Source Address and Input Interface.\n\nip-destination-address\/input-interface\n                             IP Destination Address and Input Interface.\n\nip-source-address\/output-interface\n                             IP Source Address and Output Interface.\n\nip-destination-address\/output-interface\n                             IP Destination Address and Output Interface.\n\nip-source\/destination-address\/input-interface\n                             IP Source\/Destination Address and\n                             Input Interface.\n\nip-source\/destination-address\/output-interface\n                             IP Source\/Destination Address and\n                             Output Interface.\n\nip-source\/destination-address\/input\/output-interface\n                             IP Source\/Destination Address and\n                             Input\/Output Interface.\n\ninput-interface\/source-as    Input Interface and Source AS.\n\ninput-interface\/destination-as\n                             Input Interface and Destination AS.\n\noutput-interface\/source-as\n                             Output Interface and Source AS.\n\noutput-interface\/destination-as\n                             Output Interface and Destination AS.\n\ninput-interface\/source\/destination-as\n                             Input Interface and Source\/Destination AS.\n\noutput-interface\/source\/destination-as\n                             Output Interface and Source\/Destination AS.\n\ninput\/output-interface\/source\/destination-as\n                             Input\/Output Interface and\n                             Source\/Destination AS.\n\nengine-id                    Engine ID.\n\nengine-type                  Engine Type.\n\nsource-tag                   Source Tag.\n\ndestination-tag              Destination Tag.\n\nsource\/destination-tag       Source\/Destination Tag.\n\nip-source-address\/ip-source-port\n                             IP Source Address and IP Source Port.\n\nip-source-address\/ip-destination-port\n                             IP Source Address and IP Destination Port.\n\nip-destination-address\/ip-source-port\n                             IP Destination Address and IP Source Port.\n\nip-destination-address\/ip-destination-port\n                             IP Destination Address and\n                             IP Destination Port.\n\nip-source-address\/ip-source\/destination-port\n                             IP Source Address and\n                             IP Source\/Destination Port.\n\nip-destination-address\/ip-source\/destination-port\n                             IP Destination Address and\n                             IP Source\/Destination Port.\n\nip-source\/destination-address\/ip-source-port\n                             IP Source\/Destination Address and\n                             IP Source Port.\n\nip-source\/destination-address\/ip-destination-port\n                             IP Source\/Destination Address and\n                             IP Destination Port.\n\nip-source\/destination-address\/ip-source\/destination-port\n                             IP Source\/Destination Address and\n                             IP Source\/Destination Port.\n\nip-source-address\/input\/output-interface\n                             IP Source Address and\n                             Input\/Output Interface.\n\nip-destination-address\/input\/output-interface\n                             IP Destination Address and\n                             Input\/Output Interface.\n\nip-source-address\/source\/destination-as\n                             IP Source Address and\n                             Source\/Destination AS.\n\nip-destination-address\/source\/destination-as\n                             IP Destination Address and\n                             Source\/Destination AS.\n\nip-address                   IP Address (both source and destination).\n\nip-port                      IP Port (both source and destination).\n\nip-source-address-destination-count\n                             Count of destination IP addresses associated\n                             with a source IP address.\n\nip-destination-address-source-count\n                             Count of source IP addresses associated\n                             with a destination IP address.\n\nlinear-interpolated-flows-octets-packets\n                             Linear interpolated distribution of flows,\n                             octets and packets.  The distribution is\n                             done across the start and end time of the\n                             flow.\n\nfirst                        First packet of flow distribution.\n\nlast                         Last packet of flow distribution.\n\nduration                     Duration of flow distribution.\n\nip-source-address\/source-tag\n                             IP Source Address and\n                             Source tag.\n\nip-source-address\/destination-tag\n                             IP Source Address and\n                             Destination tag.\n\nip-destination-address\/source-tag\n                             IP Destination Address and\n                             Source tag.\n\nip-destination-address\/destination-tag\n                             IP Destination Address and\n                             Destination tag.\n\nip-source\/destination-address\/source\/destination-tag\n                             IP Source\/Destination Address and\n                             Source\/Destination tag.\n\nip-source\/destination-address\/ip-protocol\/ip-tos\n                             IP Source\/Destination Address, IP Protocol,\n                             and ToS.\n\nip-source\/destination-address\/ip-protocol\/ip-tos\/ip-source\/destination-port\n                             IP Source\/Destination Addess, IP Protocol,\n                             IP Tos, IP Source\/Destination Port.","Process Name":"flow-report","Link":"https:\/\/linux.die.net\/man\/1\/flow-report"}},{"Process":{"Description":"The flow-rpt2rrd utility processes the CSV output of flow-report into RRDtool format. The aggregates for a key are each stored as a DS in RRD filename {rrd_path,\"\/\",key,rrd_postfix,\".rrd\"}. By default a DS is created for flows, octets, and packets. The key must be specified, for example an ip-port report could use smtp,nntp,ssh,telnet as the keys which would create a separate RRD for each key.","Process Name":"flow-rpt2rrd","Link":"https:\/\/linux.die.net\/man\/1\/flow-rpt2rrd"}},{"Process":{"Description":"The flow-rptfmt utility processes the CSV output of flow-report into formatted ASCII or HTML. Sorting, maximum display lines, field filter, header display, and name substitution are supported during post processing. Additionally an alarm can be set for use in CGI scripts to limit the CPU time of formatting.","Process Name":"flow-rptfmt","Link":"https:\/\/linux.die.net\/man\/1\/flow-rptfmt"}},{"Process":{"Description":null,"Process Name":"flow-send","Link":"https:\/\/linux.die.net\/man\/1\/flow-send"}},{"Process":{"Description":"The flow-split utility will split a flow file into smaller files based on the the number of flows or the ammount of time that has passed.","Process Name":"flow-split","Link":"https:\/\/linux.die.net\/man\/1\/flow-split"}},{"Process":{"Description":null,"Process Name":"flow-stat","Link":"https:\/\/linux.die.net\/man\/1\/flow-stat"}},{"Process":{"Description":null,"Process Name":"flow-tag","Link":"https:\/\/linux.die.net\/man\/1\/flow-tag"}},{"Process":{"Description":"Flow-tools is library and a collection of programs used to collect, send, process, and generate reports from NetFlow data. The tools can be used together on a single server or distributed to multiple servers for large deployments. The flow-toools library provides an API for development of custom applications for NetFlow export versions 1,5,6 and the 14 currently defined version 8 subversions. A Perl and Python interface have been contributed and are included in the distribution. Flow data is collected and stored by default in host byte order, yet the files are portable across big and little endian architectures. Commands that utilize the network use a localip\/remoteip\/port designation for communication. \"localip\" is the IP address the host will use as a source for sending or bind to when receiving NetFlow PDU's (ie the destination address of the exporter. Configuring the \"localip\" to 0 will force the kernel to decide what IP address to use for sending and listen on all IP addresses for receiving. \"remoteip\" is the destination IP address used for sending or the expected address of the source when receiving. If the \"remoteip\" is 0 then the application will accept flows from any source address. The \"port\" is the UDP port number used for sending or receiving. When using multicast addresses the localip\/remoteip\/port is used to represent the source, group, and port respectively. Flows are exported from a router in a number of different configurable versions. A flow is a collection of key fields and additional data. The flow key is {srcaddr, dstaddr, input, output, srcport, dstport, prot, ToS}. Flow-tools supports one export version per file. Export versions 1, 5, 6, and 7 all maintain {nexthop, dPkts, dOctets, First, Last, flags}, ie the next-hop IP address, number of packets, number of octets (bytes), start time, end time, and flags such as the TCP header bits. Version 5 adds the additional fields {src_as, dst_as, src_mask, dst_mask}, ie source AS, destination AS, source network mask, and destination network mask. Version 7 which is specific to the Catalyst switches adds in addition to the version 5 fields {router_sc}, which is the Router IP address which populates the flow cache shortcut in the Supervisor. Version 6 which is not officially supported by Cisco adds in addition to the version 5 fields {in_encaps, out_encaps, peer_nexthop}, ie the input and output interface encapsulation size, and the IP address of the next hop within the peer. Version 1 exports do not contain a sequence number and therefore should be avoided, although it is safe to store the data as version 1 if the additional fields are not used. Version 8 IOS NetFlow is a second level flow cache that reduces the data exported from the router. There are currently 11 formats, all of which provide {dFlows, dOctets, dPkts, First, Last} for the key fields. 8.1 -  Source and Destination AS, Input and Output interface\n8.2 -  Protocol and Port\n8.3 -  Source Prefix and Input interface\n8.4 -  Destination Prefix and Output interface\n8.5 -  Source\/Destination Prefix and Input\/Output interface\n8.9 -  8.1 + ToS\n8.10 - 8.2 + ToS\n8.11 - 8.3 + ToS\n8.12 - 8.5 + ToS\n8.13 - 8.2 + ToS\n8.14 - 8.3 + ports + ToS Version 8 CatIOS NetFlow appears to be a less fine grained first level flow cache. 8.6 - Destination IP, ToS, Marked ToS,\n8.7 - Source\/Destination IP, Input\/Output interface, ToS, Marked ToS,\n8.8 - Source\/Destination IP, Source\/Destination Port,\n      Input\/Output interface, ToS, Marked ToS, The following programs are included in the flow-tools distribution. flow-capture - Collect, compress, store, and manage disk space for exported flows from a router. flow-cat - Concatenate flow files. Typically flow files will contain a small window of 5 or 15 minutes of exports. Flow-cat can be used to append files for generating reports that span longer time periods. flow-fanout - Replicate NetFlow datagrams to unicast or multicast destinations. Flow-fanout is used to facilitate multiple collectors attached to a single router. flow-report - Generate reports for NetFlow data sets. Reports include source\/destination IP pairs, source\/destination AS, and top talkers. Over 50 reports are currently supported. flow-tag - Tag flows based on IP address or AS #. Flow-tag is used to group flows by customer network. The tags can later be used with flow-fanout or flow-report to generate customer based traffic reports. flow-filter - Filter flows based on any of the export fields. Flow-filter is used in-line with other programs to generate reports based on flows matching filter expressions. flow-import - Import data from ASCII or cflowd format. flow-export - Export data to ASCII or cflowd format. flow-send - Send data over the network using the NetFlow protocol. flow-receive - Receive exports using the NetFlow protocol without storing to disk like flow-capture. flow-gen - Generate test data. flow-dscan - Simple tool for detecting some types of network scanning and Denial of Service attacks. flow-merge - Merge flow files in chronoligical order. flow-xlate - Perform translations on some flow fields. flow-expire - Expire flows using the same policy of flow-capture. flow-header - Display meta information in flow file. flow-split - Split flow files into smaller files based on size, time, or tags.","Process Name":"flow-tools","Link":"https:\/\/linux.die.net\/man\/1\/flow-tools"}},{"Process":{"Description":null,"Process Name":"flow-tools-examples","Link":"https:\/\/linux.die.net\/man\/1\/flow-tools-examples"}},{"Process":{"Description":"The flow-xlate utility is used to apply translations to flows. Translations are defined in a configuration file and are composed of actions and a definition to invoke action(s). The definitions are in the form of terms, each term can have a filter and multiple actions. Words in the configuration file of the form @VAR or @{VAR:default} will be expanded at run-time by setting variable names with the -v option. Translation actions begin with the xlate-action keyword followed by a symbolic name. Each action has a type defined below. Translation definitions begin with the xlate-definition keyword followed by a symbolic name. Each definition is composed of terms which are evaluated in the order of the configuration file. A term may invoke a filter to conditionally invoke an action. Action type\/sub-commands                Description\/Example\n------------------------------------------------------------------------\nip-source-address-to-network            Zero host bits based on mask.\nip-destination-address-to-network       Zero host bits based on mask.\n\n (no sub-commands)\n\nip-source-address-to-class-network      Zero source host bits to\n                                        match class.\nip-destination-address-to-class-network Zero dst host bits to\n                                        match class.\n\n (no sub-commands)\n\nip-source-address-anonymize             Anonymize source address.\nip-destination-address-anonymize        Anonymize destination address.\nip-address-anonymize                    Anonymize src\/dst address.\n\n   algorithm                           Algorithm.  cryptopan-aes128 is\n                                        currently supported.\n                                         algorithm cryptopan-aes128\n\n   key                                 Key.  Key is 128 bits in hex.\n                                         key 0123456789ABCDEFG\n\n   key-file                            File to load key from.  Key is\n                                        128 bits in hex.\n                                         key-file \/mfstmp\/secret-key\n\n   key-file-refresh                    How often to check the key file.\n                                        Interval is in minutes, the\n                                        optional second argument is\n                                        hour:min:sec to specify the\n                                        first refresh.  This example\n                                        will load a new key every day\n                                        at 12:00:00.\n                                         14400 12:00:00\n\nip-address-privacy-mask                 Apply a mask to the source and\n                                        destination address to remove\n                                        bits.\n\nip-port-privacy-mask                    Apply a mask to the source and\n                                        destination port to remove\n                                        bits.\n\ntag-mask                                Apply mask to the source and\n                                        destination tag.\n\n   mask                                Source and Destination mask\n                                        to apply.\n                                         mask 0xFFFF 0xFFFF\n\nscale                                   Scale packets and bytes.\n\n scale                                 Scale to apply.\n                                         scale 100\n\nreplace-source-as0                      Replace source AS 0\nreplace-destination-as0                 Replace destination AS 0\n\n as                                    AS replacement value.\n                                         as 3112","Process Name":"flow-xlate","Link":"https:\/\/linux.die.net\/man\/1\/flow-xlate"}},{"Process":{"Description":"flowdumper is a grep(1)-like utility for selecting and processing flows from cflowd or flow-tools raw flow files. The selection criteria are specified by using the \"-e\" option described below. flowdumper's primary features are the ability to: \u2022 Print the content of raw flow files in one of two built-in formats or a format of the users own. The built-in \"long\" format is much like that produced by the flowdump command supplied with cflowd. The \"short\", single-line format is suitable for subsequent post-processing by line-oriented filters like sed(1). \u2022 Act as a filter, reading raw flow input from either file(s) or standard input, and producing filtered raw flow output on standard output. This is similar to how grep(1) is often used on text files. \u2022 Select flows according to practically any criteria that can be expressed in perl syntax. The \"flow variables\" and other symbols available for use in the \"-e\" expression are those made available by the Cflow module when used like this: use Cflow qw(:flowvars :tcpflags :icmptypes :icmpcodes); See the Cflow perl documentation for full details on these values (i.e. \"perldoc Cflow\".) Most perl syntax is allowed in the expressions specified with the \"-e\", \"-I\", and \"-E\" options. See the perl man pages for full details on operators (\"man perlop\") and functions (\"man perlfunc\") available for use in those expressions. If run with no arguments, filters standard input to standard output. The options and their arguments, roughly in order of usefulness, are: \"-h\" shows the usage information mnemonic: 'h'elp \"-a\" print all flows implied if \"-e\" is not specified mnemonic: 'a'll \"-e\" expr evaluate this expression once per flow mnemonic: 'e'xpression \"-c\" print number of flows matched in input mnemonic: 'c'ount \"-s\" print flows in short (one-line) format, ignored with \"-n\" mnemonic: 's'hort \"-r\" print flows in the raw\/binary flow file format ignored with \"-n\" mnemonic: 'r'aw \"-R\" \"repacks\" and print flows in the raw\/binary flow file format requires \"-e\", ignored with \"-n\", useful with \"-p\" mnemonic: 'R'epack raw \"-n\" don't print matching flows mnemonic: like \"perl \"-n\"\" or \"sed \"-n\"\" \"-o\" output_file send output to the specified file. A single printf(3) string conversion specifier can be used within the output_file value (such as \"\/tmp\/%s.txt\") to make the output file name a function of the input file basename. mneomic: 'o'utput file \"-S\" print flows in the \"old\" short (one-line) format ignored with \"-n\" mnemonic: 'S'hort \"-v\" be verbose with messages mnemonic: 'v'erbose \"-V\" be very verbose with messages (implies \" \"-v\"\") mnemonic: 'V'ery verbose \"-I\" expr eval expression initially, before flow processing practically useless without \"-e\" mnemonic: 'I'nitial expression \"-E\" expr eval expression after flow processing is complete practically useless without \"-e\" mnemonic: 'E' ND expression \"-B\" file Load the specified BGP dump file using Net::ParseRouteTable. In your optional expression, you can now refer to these variables: $dst_as_path_arrayref\n$dst_origin_as\n$dst_peer_as\n$src_as_path_arrayref\n$src_origin_as\n$src_peer_as which will cause a lookup. Their values are undefined if the lookup fails. mnemonic: 'B' GP dump file \"-p\" prefix_mappings_file read file containing IPv4 prefix mappings in this format (one per line): 10.42.69.0\/24 -> 10.69.42.0\/24\n... When specifying this option, you can, and should at some point, call the ENCODE subroutine in your expressions to have it encode the IP address flowvars such as $Cflow::exporter, $Cflow::srcaddr, $Cflow::dstaddr, and $Cflow::nexthop. mnemonic: 'p'refixes","Process Name":"flowdumper","Link":"https:\/\/linux.die.net\/man\/1\/flowdumper"}},{"Process":{"Description":"fltk-config is a utility script that can be used to get information about the current version of FLTK that is installed on the system, what compiler and linker options to use when building FLTK-based applications, and to build simple FLTK applications. The following options are supported: --api-version Displays the current FLTK API version number, e.g. \"1.1\". --cc --cxx Displays the C\/C++ compiler that was used to compile FLTK. --cflags --cxxflags Displays the C\/C++ compiler options to use when compiling source files that use FLTK. --compile program.cxx Compiles the source file program.cxx into program. This option implies \"--post program\". -g Enables debugging information when compiling with the --compile option. --ldflags Displays the linker options to use when linking a FLTK application. --ldstaticflags Displays the linker options to use when linking a FLTK application to the static FLTK libraries. --libs Displays the full path to the FLTK library files, to be used for dependency checking. --use-gl Enables OpenGL support. --use-glut Enables GLUT support. --use-images Enables image file support. --version Displays the current FLTK version number, e.g. \"1.1.0\".","Process Name":"fltk-config","Link":"https:\/\/linux.die.net\/man\/1\/fltk-config"}},{"Process":{"Description":"fltrace Tracks runtime library calls from dynamically linked executables using the Frysk framework. fltrace uses the Frysk framework to track runtime library calls from dynamically linked executables.","Process Name":"fltrace","Link":"https:\/\/linux.die.net\/man\/1\/fltrace"}},{"Process":{"Description":"fluid is an interactive GUI designer for FLTK. When run with no arguments or with a filename, fluid will display the GUI hierarchy and any windows defined in the file. Functions, classes, windows, and GUI components can be manipulated as needed. When used with the -c option, fluid will create the necessary C++ header and code files in the current directory. You can override the default extensions, filenames, and directories using the -o and -h options.","Process Name":"fluid","Link":"https:\/\/linux.die.net\/man\/1\/fluid"}},{"Process":{"Description":"Models the physics of bouncing balls, or of particles in a gas or fluid, depending on the settings. If \"Shake Box\" is selected, then every now and then, the box will be rotated, changing which direction is down (in order to keep the settled balls in motion.)","Process Name":"fluidballs","Link":"https:\/\/linux.die.net\/man\/1\/fluidballs"}},{"Process":{"Description":"This is a port of the OSX screensaver flurry.","Process Name":"flurry","Link":"https:\/\/linux.die.net\/man\/1\/flurry"}},{"Process":{"Description":"fluxbox(1) provides configurable window decorations, a root menu to launch applications and a toolbar that shows the current workspace name, a set of application names and the current time. There is also a workspace menu to add or remove workspaces. The 'slit' can be used to dock small applications; e.g. most of the bbtools can use the slit. fluxbox(1) can iconify windows to the toolbar, in addition to adding the window to the Icons submenu of the workspace menu. One click and they reappear. A double-click on the titlebar of the window will shade it; i.e. the window will disappear, and only the titlebar will remain visible. fluxbox(1) uses its own graphics class to render its images on the fly. By using style files, you can determine in great detail how your desktop looks. fluxbox styles are compatible with those of Blackbox 0.65 or earlier versions, so users migrating can still use their current favourite themes. fluxbox(1) supports the majority of the Extended Window Manager Hints (EWMH) specification, as well as numerous other Window Hinting standards. This allows all compliant window managers to provide a common interface to standard features used by applications and desktop utilities.","Process Name":"fluxbox","Link":"https:\/\/linux.die.net\/man\/1\/fluxbox"}},{"Process":{"Description":"fluxbox-remote(1) is designed to allow scripts to execute most key commands from fluxbox(1). fluxbox-remote(1) will only work with fluxbox(1): its communications with fluxbox(1) are not standardized in any way. It is recommended that a standards-based tool such as wmctrl(1) be used whenever possible, in order for scripts to work with other window managers.","Process Name":"fluxbox-remote","Link":"https:\/\/linux.die.net\/man\/1\/fluxbox-remote"}},{"Process":{"Description":"What is a Style? Styles, sometimes referred to as Themes, are a graphical overlay for the fluxbox(1) window manager. If you wanted to get to know fluxbox, the styles would be the look of the look and feel. Styles are simple ASCII text files that tell fluxbox(1) how to generate the appearance of different components of the window manager. The default installation of fluxbox(1) is shipped with many classic examples that show a great deal of what one could do. To use one of the standard styles navigate to the System Styles menu under your main fluxbox(1) menu. fluxbox(1) uses its own graphics class to render its images on the fly. By using styles you can determine, at a great level of configurability, what your desktop will look like. Since fluxbox(1) was derived from blackbox many often wonder if old themes will work on the latest releases of fluxbox(1). Well they basically do, but you will have to tune them since the fluxbox(1) code has changed quite a bit since the initial grab.","Process Name":"fluxstyle","Link":"https:\/\/linux.die.net\/man\/1\/fluxstyle"}},{"Process":{"Description":"Draws a squadron of shiny 3D space-age jet-powered flying toasters, and associated toast, flying across your screen.","Process Name":"flyingtoasters","Link":"https:\/\/linux.die.net\/man\/1\/flyingtoasters"}},{"Process":{"Description":null,"Process Name":"fm","Link":"https:\/\/linux.die.net\/man\/1\/fm"}},{"Process":{"Description":"fm-submit is a tool to submit project release announcements to freshmeat.net via XML-RPC, from the command line. Release information is accepted from binary packages (RPMs) named in the command line, or from an email-like data block on standard input, or from command-line flags. If more than one data source is used, command-line values override values read from standard input, which in turn override values read from binary packages.","Process Name":"fm-submit","Link":"https:\/\/linux.die.net\/man\/1\/fm-submit"}},{"Process":{"Description":"Made to run on FSM descriptions, fmi supports the same subset of VHDL as syf (for further informations about this subset see syf(1) and fsm(5)). fmi uses a Reduced Ordered Binary Decision Diagrams representation and identifies equivalent states. After this step, it drives a new FSM where all equivalent states are replaced by a single state. As a restriction fmi doesn't handle don't cares (as this much more difficult).","Process Name":"fmi","Link":"https:\/\/linux.die.net\/man\/1\/fmi"}},{"Process":{"Description":null,"Process Name":"fmscan","Link":"https:\/\/linux.die.net\/man\/1\/fmscan"}},{"Process":{"Description":"Reformat each paragraph in the FILE(s), writing to standard output. The option -WIDTH is an abbreviated form of --width=DIGITS. Mandatory arguments to long options are mandatory for short options too. -c, --crown-margin preserve indentation of first two lines -p, --prefix= STRING reformat only lines beginning with STRING, reattaching the prefix to reformatted lines -s, --split-only split long lines, but do not refill -t, --tagged-paragraph indentation of first line different from second -u, --uniform-spacing one space between words, two after sentences -w, --width= WIDTH maximum line width (default of 75 columns) --help display this help and exit --version output version information and exit With no FILE, or when FILE is -, read standard input.","Process Name":"fmt","Link":"https:\/\/linux.die.net\/man\/1\/fmt"}},{"Process":{"Description":null,"Process Name":"fmtutil","Link":"https:\/\/linux.die.net\/man\/1\/fmtutil"}},{"Process":{"Description":"fmtutil is used to create or recreate format and hyphenation files or show information about format files. COMMAND is one of: --all recreate all format files --byfmt formatname (re)create the format for format formatname --byhyphen hyphenfile (re)create formats that depend on the hyphenation file hyphenfile --help print a summary of commands and options --missing create any missing format files --showhyphen formatname print the name of the hyphenation file for the format formatname","Process Name":"fmtutil-sys","Link":"https:\/\/linux.die.net\/man\/1\/fmtutil-sys"}},{"Process":{"Description":"clogin is an expect(1) script to automate the process of logging into a Cisco router, catalyst switch, Extreme switch, Juniper ERX\/E-series, Procket Networks, or Redback router. There are complementary scripts for Alteon, Avocent (Cyclades), Bay Networks (nortel), ADC-kentrox EZ-T3 mux, Foundry, HP Procurve Switches and Cisco AGMs, Hitachi Routers, Juniper Networks, MRV optical switch, Netscreen firewalls, Netscaler, Riverstone, Netopia, and Lucent TNT, named alogin, avologin, blogin, elogin, flogin, fnlogin, hlogin, htlogin, jlogin, mrvlogin, nlogin, nslogin, rivlogin, tlogin, and tntlogin, respectively. clogin reads the .cloginrc file for its configuration, then connects and logs into each of the routers specified on the command line in the order listed. Command-line options exist to override some of the directives found in the .cloginrc configuration file. The command-line options are as follows: -S Save the configuration on exit, if the device prompts at logout time. This only has affect when used with -s. -V Prints package name and version strings. -c Command to be run on each router list on the command-line. Multiple commands maybe listed by separating them with semi-colons (;). The argument should be quoted to avoid shell expansion. -d Enable expect debugging. -E Specifies a variable to pass through to scripts (-s). For example, the command-line option -Efoo=bar will produce a global variable by the name Efoo with the initial value \"bar\". -e Specify a password to be supplied when gaining enable privileges on the router(s). Also see the password directive of the .cloginrc file. -f Specifies an alternate configuration file. The default is $HOME\/.cloginrc. -p Specifies a password associated with the user specified by the -u option, user directive of the .cloginrc file, or the Unix username of the user. -s The filename of an expect(1) script which will be sourced after the login is successful and is expected to return control to clogin, with the connection to the router intact, when it is done. Note that clogin disables log_user of expect(1)when -s is used. Example script(s) can be found in share\/rancid\/*.exp. -t Alters the timeout interval; the period that clogin waits for an individual command to return a prompt or the login process to produce a prompt or failure. The argument is in seconds. -u Specifies the username used when prompted. The command-line option overrides any user directive found in .cloginrc. The default is the current Unix username. -v Specifies a vty password, that which is prompted for upon connection to the router. This overrides the vty password of the .cloginrc file's password directive. -w Specifies the username used if prompted when gaining enable privileges. The command-line option overrides any user or enauser directives found in .cloginrc. The default is the current Unix username. -x Similar to the -c option; -x specifies a file with commands to run on each of the routers. The commands must not expect additional input, such as 'copy rcp startup-config' does. For example: show version\nshow logging -y Specifies the encryption algorithm for use with the ssh(1) -c option. The default encryption type is often not supported. See the ssh(1) man page for details. The default is 3des.","Process Name":"fnlogin","Link":"https:\/\/linux.die.net\/man\/1\/fnlogin"}},{"Process":{"Description":null,"Process Name":"fnrancid","Link":"https:\/\/linux.die.net\/man\/1\/fnrancid"}},{"Process":{"Description":null,"Process Name":"fold","Link":"https:\/\/linux.die.net\/man\/1\/fold"}},{"Process":{"Description":"This script invokes gs(1) with the following options: -q -dNODISPLAY -dWRITESYSTEMDICT followed by the arguments from the command line. This will write out a PostScript Type 0 or Type 1 font as C code that can be linked with the interpreter.","Process Name":"font2c","Link":"https:\/\/linux.die.net\/man\/1\/font2c"}},{"Process":{"Description":"font2pfa converts a PostScript font. The program takes, as command line arguments, the name of a PostScript font file, encoded either in binary (.pfb) or ascii (.pfa) format, optionally followed by the name of the output file. If no filenames are supplied, the program reads from standard input and writes to standard output. The output will be ASCII encoded (.pfa format), unless the -binary or -pfb option is used, or the program is installed under a name that ends in \"pfb\". font2pfa depends on the capabilities of the \"PostScript::Font\" module. If your version supports True Type fonts, font2pfa will happily produce ASCII or binary encoded Type42 versions of True Type fonts.","Process Name":"font2pfa","Link":"https:\/\/linux.die.net\/man\/1\/font2pfa"}},{"Process":{"Description":"font2pfa converts a PostScript font. The program takes, as command line arguments, the name of a PostScript font file, encoded either in binary (.pfb) or ascii (.pfa) format, optionally followed by the name of the output file. If no filenames are supplied, the program reads from standard input and writes to standard output. The output will be ASCII encoded (.pfa format), unless the -binary or -pfb option is used, or the program is installed under a name that ends in \"pfb\". font2pfa depends on the capabilities of the \"PostScript::Font\" module. If your version supports True Type fonts, font2pfa will happily produce ASCII or binary encoded Type42 versions of True Type fonts.","Process Name":"font2pfb","Link":"https:\/\/linux.die.net\/man\/1\/font2pfb"}},{"Process":{"Description":"Takes a font file (.ttf, .afm, .pfa, .pfb and all other types supported by FreeType) and converts it into a SWF file. The SWF will contain the Font in SWF format (that is, a DefineFont2 Tag) as well as a textfield containing all the characters the font has. This means the resulting SWF will be viewable.","Process Name":"font2swf","Link":"https:\/\/linux.die.net\/man\/1\/font2swf"}},{"Process":{"Description":null,"Process Name":"fontc","Link":"https:\/\/linux.die.net\/man\/1\/fontc"}},{"Process":{"Description":"fontexport exports font files from the teTeX directory tree. fontexport can cope with fontnames conforming the 8.3-namescheme, too, just give it the -d-flag (d as \"dos\"). So you can export your teTeX fonts for someone else to use them with his or her emTeX.","Process Name":"fontexport","Link":"https:\/\/linux.die.net\/man\/1\/fontexport"}},{"Process":{"Description":null,"Process Name":"fontforge","Link":"https:\/\/linux.die.net\/man\/1\/fontforge"}},{"Process":{"Description":"The program fontimage loads a font, which may be in any format fontforge(1) can read, and then produces an image showing representative glyphs of the font.","Process Name":"fontimage","Link":"https:\/\/linux.die.net\/man\/1\/fontimage"}},{"Process":{"Description":"fontimport imports TFM and PK font files into appropriate places in the teTeX directory tree and revises teTeX's filename database accordingly. Each path may be a font file or a directory, either absolute or relative to the current directory. All subdirectories of a given directory are searched for font files. Don't specify directory names that are symbolic links, however, since (at least some versions of) find(1) won't dereference them. A font's destination is determined by variables in the file texmf.cnf. By default, fontimport will issue a warning to stderr instead of overwriting an existing font file. The -f flag forces overwriting. fontimport can cope with font names conforming the 8.3-namescheme, too, so you can import fonts from either a TDS-compliant or MSDOS installation. emTeX's font libraries are not supported. If you want to import fonts from a font library, you need to unpack it with emTeX's fontlib utility first. The full pathname of each font successfully imported is echoed to stdout. When fontimport can not parse a font's name, it will import the font using a path element of \"\/unknown\/\" and issue a warning on stderr. fontimport tries to guess the mode of PK files. With the -m flag you may specify a fallback mode to be used if the mode can't be guessed. Options: -m MF-MODE default Metafont mode for PK files if autodetection fails -d delete font files after copying (i.e., move them) -f force overwrite existing fonts -s do not use other destinations than the system's main texmf tree -t test mode: just echo each font's destination (if any); don't move or copy","Process Name":"fontimport","Link":"https:\/\/linux.die.net\/man\/1\/fontimport"}},{"Process":{"Description":"fontinst runs tex(1) with the --progname flag set to pretend to be the program fontinst (affecting Karl Berry paths). Typing CWfontinst fontinst.sty will start tex(1) and give the user an interactive prompt (CW*) where fontinst commands can be typed (such as CW\\latinfamily{ptm}{}\\bye).","Process Name":"fontinst","Link":"https:\/\/linux.die.net\/man\/1\/fontinst"}},{"Process":{"Description":"The program fontlint loads a font (or more than one), which may be in any format fontforge(1) can read, and checks the font for certain common errors * Intersecting contours * Incorrectly oriented contours * Extrema not marked by points And for PostScript fonts: * Too many points in a glyph * Too many hints in a glyph * Bad glyph name","Process Name":"fontlint","Link":"https:\/\/linux.die.net\/man\/1\/fontlint"}},{"Process":{"Description":null,"Process Name":"fontsampler","Link":"https:\/\/linux.die.net\/man\/1\/fontsampler"}},{"Process":{"Description":"Wrap a bitmap font or a set of bitmap fonts in a sfnt (TrueType or OpenType) wrapper.","Process Name":"fonttosfnt","Link":"https:\/\/linux.die.net\/man\/1\/fonttosfnt"}},{"Process":{"Description":"The Foomatic acceleration engine written in C (by Till), it computes printer\/driver combo XML files and the printer overview XML file.","Process Name":"foomatic-combo-xml","Link":"https:\/\/linux.die.net\/man\/1\/foomatic-combo-xml"}},{"Process":{"Description":"foomatic-compiledb generates all PPD files or Foomatic combo XML files for all possible printer\/driver combos, or only for a selected range of drivers. It generally should not be necessary except for people who want to generate a set of Foomatic data files for distribution. For configuring single printers foomatic-ppdfile(1) and especially foomatic-configure(1) will automagically compute just what they need. Options -t type Output file type, either ppd or xml. -f Force: Write into the destination directory even if it already exists. -j n n = number of work processes to run driver ... only generate data files for these driver(s)","Process Name":"foomatic-compiledb","Link":"https:\/\/linux.die.net\/man\/1\/foomatic-compiledb"}},{"Process":{"Description":null,"Process Name":"foomatic-configure","Link":"https:\/\/linux.die.net\/man\/1\/foomatic-configure"}},{"Process":{"Description":"","Process Name":"foomatic-gswrapper","Link":"https:\/\/linux.die.net\/man\/1\/foomatic-gswrapper"}},{"Process":{"Description":"","Process Name":"foomatic-perl-data","Link":"https:\/\/linux.die.net\/man\/1\/foomatic-perl-data"}},{"Process":{"Description":null,"Process Name":"foomatic-ppd-options","Link":"https:\/\/linux.die.net\/man\/1\/foomatic-ppd-options"}},{"Process":{"Description":"The first form of the foomatic-ppdfile to compute the spooler-independent Foomatic PPD file for any valid printer\/driver combo available in the Foomatic database, both for printing with foomatic-rip(1) and for applications\/clients being able to access the printer's options. The PPD file is returned on standard ouput. If the driver is not specified, the default driver is used. The second form of the foomatic-ppdfile program will search the printer database and return either all printer entries or those whose name and\/or model information match a regular expression. The last form prints a help message and exits. Options -d drivername The (optional) driver name to use. If the driver name is not supplied, the default or first driver in the printers compatible driver list is used. -p printer id The printer id. -w Return a PPD file that conforms to the Microsoft Windows format limitations. -A Return all printer entries in the database. -P pattern Return all printer entries in the database which match the regular expression.","Process Name":"foomatic-ppdfile","Link":"https:\/\/linux.die.net\/man\/1\/foomatic-ppdfile"}},{"Process":{"Description":"foomatic-printjob provides a spooler-independent interface to creating and managing printer jobs and print queues. Note that the first argument changes the mode of the command, as it combines the functionality of several System V-style printing commands. Options -s spooler Explicit spooler type. -P queuename Any commands specified should apply to this queue instead of the default. -o option=value Set option to value -o option Set the switch option -# n Print n copies file1 file2 ... Files to be printed, when no file is given, standard input will be printed -Q Query the jobs in a queue. If a list of users is specified, only those users' jobs will be queried. -Q -a Query the jobs in all queues. If a list of users is specified, only those users' jobs will be queried. -R [ - | jobid1 jobid2 ... ] Remove a job from a queue. Using - will remove all jobs. -C command [ arguments ] Execute control commands for queue\/job manipulation. The commands are the ones of the BSD \"lpc\" utility. Use the control command \"help\" to get a list of supported commands. Note: the amount of commands varies with the spooler, but the same commands given under different spoolers do the same thing. -i Interactive mode: You will be asked if foomatic-printjob is in doubt about something. Otherwise foomatic-printjob uses auto-detection or quits with an error. -S Save the chosen spooler as the default spooler -h Show this message or show a list of available options if a queue is specified","Process Name":"foomatic-printjob","Link":"https:\/\/linux.die.net\/man\/1\/foomatic-printjob"}},{"Process":{"Description":"foomatic-rip is a universal print filter which works with every known free software printer spooler. It has the following features: It translates PostScript and PDF (and also other file formats) from standard input to the printer's native language (usually put to standard output). The translation is done with an external renderer, usually Ghostscript (gs(1)). If no translation is needed (PostScript printer) the renderer's command line reduces to cat(1). The way how this translation is done is described in a PPD file. Printer capabilities, how to handle user options, and how to build the renderer command line is always described by PPD files, these PPD files usually come from Foomatic or can be the ones supplied by the manufacturers of PostScript printers. The PPD files are the same for all spoolers. foomatic-rip works with every known spooler (CUPS, LPRng, LPD, GNUlpr, PPR, PDQ, CPS, and without spooler). It auto-detects the spooler from which it was called by the command line options and environment variables which were supplied by the spooler. Non-PostScript\/PDF files are translated to PostScript before they are passed to the renderer. This is usually done by a2ps(1), enscript(1), or mpage(1). foomatic-rip auto-detects which program is installed, but manual configuration is also possible. foomatic-rip does not only apply option settings supplied by the user through the command line of the printing command, but also searches the entire job for embedded option settings (only PostScript jobs). Here not only settings affectimg the whole job are taken into account, but also settings in the page headers, which are only valid for the page where they were found, so applications which produce PostScript code with page-specific printer option settings are fully supported.","Process Name":"foomatic-rip","Link":"https:\/\/linux.die.net\/man\/1\/foomatic-rip"}},{"Process":{"Description":null,"Process Name":"for","Link":"https:\/\/linux.die.net\/man\/1\/for"}},{"Process":{"Description":"Recover files from a disk image based on headers and footers specified by the user. -h Show a help screen and exit. -V Show copyright information and exit. -d Turn on indirect block detection, this works well for Unix file systems. -T Time stamp the output directory so you don't have to delete the output dir when running multiple times. -v Enables verbose mode. This causes more information regarding the current state of the program to be displayed on the screen, and is highly recommended. -q Enables quick mode. In quick mode, only the start of each sector is searched for matching headers. That is, the header is searched only up to the length of the longest header. The rest of the sector, usually about 500 bytes, is ignored. This mode makes foremost run considerably faster, but it may cause you to miss files that are embedded in other files. For example, using quick mode you will not be able to find JPEG images embedded in Microsoft Word documents. Quick mode should not be used when examining NTFS file systems. Because NTFS will store small files inside the Master File Table, these files will be missed during quick mode. -Q Enables Quiet mode. Most error messages will be suppressed. -w Enables write audit only mode. No files will be extracted. -a Enables write all headers, perform no error detection in terms of corrupted files. -b number Allows you to specify the block size used in foremost. This is relevant for file naming and quick searches. The default is 512. ie. foremost -b 1024 image.dd -k number Allows you to specify the chunk size used in foremost. This can improve speed if you have enough RAM to fit the image in. It reduces the checking that occurs between chunks of the buffer. For example if you had > 500MB of RAM. ie. foremost -k 500 image.dd -i file The file is used as the input file. If no input file is specified or the input file cannot be read then stdin is used. -o directory Recovered files are written to the directory directory. -c file Sets the configuration file to use. If none is specified, the file \"foremost.conf\" from the current directory is used, if that doesn't exist then \"\/etc\/foremost.conf\" is used. The format for the configuration file is described in the default configuration file included with this program. See the CONFIGURATION FILE section below for more information. -s number Skips number blocks in the input file before beginning the search for headers. ie. foremost -s 512 -t jpeg -i \/dev\/hda1","Process Name":"foremost","Link":"https:\/\/linux.die.net\/man\/1\/foremost"}},{"Process":{"Description":"The forest program draws a fractal forest.","Process Name":"forest","Link":"https:\/\/linux.die.net\/man\/1\/forest"}},{"Process":{"Description":null,"Process Name":"formail","Link":"https:\/\/linux.die.net\/man\/1\/formail"}},{"Process":{"Description":"The fort77 utility is the interface to the FORTRAN compilation system; it shall accept the full FORTRAN-77 language defined by the ANSI X3.9-1978 standard. The system conceptually consists of a compiler and link editor. The files referenced by operands are compiled and linked to produce an executable file. It is unspecified whether the linking occurs entirely within the operation of fort77; some implementations may produce objects that are not fully resolved until the file is executed. If the -c option is present, for all pathname operands of the form file .f, the files: $(basename pathname.f).o shall be created or overwritten as the result of successful compilation. If the -c option is not specified, it is unspecified whether such .o files are created or deleted for the file .f operands. If there are no options that prevent link editing (such as -c) and all operands compile and link without error, the resulting executable file shall be written into the file named by the -o option (if present) or to the file a.out. The executable file shall be created as specified in the System Interfaces volume of IEEE Std 1003.1-2001, except that the file permissions shall be set to: S_IRWXO | S_IRWXG | S_IRWXU and that the bits specified by the umask of the process shall be cleared.","Process Name":"fort77","Link":"https:\/\/linux.die.net\/man\/1\/fort77"}},{"Process":{"Description":null,"Process Name":"forward","Link":"https:\/\/linux.die.net\/man\/1\/forward"}},{"Process":{"Description":"This binary is the integrated development environment of the Free Pascal Compiler (FPC) which is an advanced Turbo Pascal and Delphi (7.0) compatible multitarget Pascal compiler. The compiler engine is not based on GCC, but is completely standalone. The compiler uses ld(1) and can use as(1) (see parameter -Aas), but also has its own binary object writer. The current main targets are Go32V2 (Dos DJGPP extender), Freebsd, Linux, MacOS, MacOSX, MorphOS, Netware, OS\/2 and Win32. The other targets (M68K compilers for Atari and Amiga) are either based on older versions of the compiler or are still in development. This manpage is meant for quick-reference only. FPC comes with a great (2000+ pages) manual, which is updated constantly, while this man page can be out of date.","Process Name":"fp","Link":"https:\/\/linux.die.net\/man\/1\/fp"}},{"Process":{"Description":"fparser Executes the frysk standalone source code parser utility. fparser is a utility that will parse the source files listed in the header of an executable. It is used to test the internal parser for the frysk source window. -o [file to write DOM to] This allows a DOM to be output to an xml file that can be viewed\/analyzed with an editor. -help Print out a list of options for fparser.","Process Name":"fparser","Link":"https:\/\/linux.die.net\/man\/1\/fparser"}},{"Process":{"Description":"It is often useful to be able to easily paste text to the Fedora Pastebin at http:\/\/fpaste.org and this simple utility will do that and return the resulting URL so that people may examine the output. This can hopefully help folks who are for some reason stuck without X, working remotely, or any other reason they may be unable to paste something into the pastebin using a web browser.","Process Name":"fpaste","Link":"https:\/\/linux.die.net\/man\/1\/fpaste"}},{"Process":{"Description":"This binary is the main binary of the Free Pascal Compiler (FPC) which is a Turbo Pascal and Delphi (7.0) compatible standalone (non GCC frontend) multitarget Pascal compiler. The compiler uses ld(1) and can use as(1) (see parameter -Aas), but also has its own binary object writer. The current main targets are Go32V2 (Dos DJGPP extender), Freebsd, Linux, MacOS, MacOSX, MorphOS, Netware, OS\/2 and Win32. The other targets (M68K compilers for Atari and Amiga) are either based on older versions of the compiler or are still in development. This manpage is meant for quick-reference only. FPC comes with a great (2000+ pages) manual, which is updated constantly, while this man page can be out of date.","Process Name":"fpc","Link":"https:\/\/linux.die.net\/man\/1\/fpc"}},{"Process":{"Description":"fpcmake reads a Makefile.fpc and converts it to a Makefile suitable for reading by GNU make to compile your projects. It is similar in functionality to GNU autoconf or Imake for making X projects.","Process Name":"fpcmake","Link":"https:\/\/linux.die.net\/man\/1\/fpcmake"}},{"Process":{"Description":"makes a configuration file for the compiler. fpcmkcfg TO BE FILLED.","Process Name":"fpcmkcfg","Link":"https:\/\/linux.die.net\/man\/1\/fpcmkcfg"}},{"Process":{"Description":"Compiles a .res file into .o file. fpcres TO BE FILLED.","Process Name":"fpcres","Link":"https:\/\/linux.die.net\/man\/1\/fpcres"}},{"Process":{"Description":null,"Process Name":"fpcsubst","Link":"https:\/\/linux.die.net\/man\/1\/fpcsubst"}},{"Process":{"Description":"fpdns is a program that remotely determines DNS server versions. It does this by sending a series of borderline DNS queries which are compared against a table of responses and server versions. False positives or incorrect versions may be reported when trying to identify a set of servers residing behind a load-balancing apparatus where the servers are of different implementations, when a specific implementation behaves like a forwarder, behind a firewall without statefull inspection or without Application Intelligence.","Process Name":"fpdns","Link":"https:\/\/linux.die.net\/man\/1\/fpdns"}},{"Process":{"Description":"fpdoc scans a Free Pascal unit source file and generates documentation for it. The documentation can be in various formats (currently HTML and LaTeX) It can read various description files which contain the description for the various symbols found in the unit file.","Process Name":"fpdoc","Link":"https:\/\/linux.die.net\/man\/1\/fpdoc"}},{"Process":{"Description":"fppkg TO BE FILLED.","Process Name":"fppkg","Link":"https:\/\/linux.die.net\/man\/1\/fppkg"}},{"Process":{"Description":"fprcp reads a .rc file and preprocesses it, writing the result to standard output as it goes along. It replaces defined constants with their values, so windres can process the resulting file and create a resource.","Process Name":"fprcp","Link":"https:\/\/linux.die.net\/man\/1\/fprcp"}},{"Process":{"Description":"This manual page documents briefly the fprintd command-line utilities. The fprintd daemon is accessed through D-Bus by those command-line utilities.","Process Name":"fprintd","Link":"https:\/\/linux.die.net\/man\/1\/fprintd"}},{"Process":{"Description":null,"Process Name":"francid","Link":"https:\/\/linux.die.net\/man\/1\/francid"}},{"Process":{"Description":"free displays the total amount of free and used physical and swap memory in the system, as well as the buffers used by the kernel. The shared memory column should be ignored; it is obsolete. Options The -b switch displays the amount of memory in bytes; the -k switch (set by default) displays it in kilobytes; the -m switch displays it in megabytes. The -t switch displays a line containing the totals. The -o switch disables the display of a \"buffer adjusted\" line. If the -o option is not specified, free subtracts buffer memory from the used memory and adds it to the free memory reported. The -s switch activates continuous polling delay seconds apart. You may actually specify any floating point number for delay, usleep(3) is used for microsecond resolution delay times. The -l switch shows detailed low and high memory statistics. The -V switch displays version information.","Process Name":"free","Link":"https:\/\/linux.die.net\/man\/1\/free"}},{"Process":{"Description":"freebcp is a utility program distributed with FreeTDS. freebcp replicates (in part at least) the functionality of the \"bcp\" utility programs distributed by Sybase and Microsoft. freebcp makes use of the db-lib bcp API provided by FreeTDS. This API is also available to application developers. The manual pages or online help for Sybase or SQL Server can be referenced for more detailed information on \"bcp\" functionality.","Process Name":"freebcp","Link":"https:\/\/linux.die.net\/man\/1\/freebcp"}},{"Process":{"Description":"Search all given file system trees for identical files and link them to the most frequently referenced inode or if equally referenced to the inode of the first file tree. If the devices differ a symbolic link is used instead of a hard link. Symbolic links will not replace files, when at least one of the directory trees is not starting with a '\/'. -a equivalent to -gup. It is provided to allow simple compatibility to freedups by William Stearns, where -a has the opposite meaning of -n for freedup. -b Set the base directory, which is the current directory by default. Usually of interest when using freedup in scripts together with partial directory path names. -c count file space savings per file. A message denotes the number of saved files. -d requires the modification time stamps to be equal. Most frequently of interest for source files in combination with make. When used a second time (toggled to off) arbitrary time stamps are allowed. -D <sec> requires the modification time stamps to be within the given range of seconds. Use -d to switch it off. -e <env> reads a setup environment from the config file $HOME\/.freedup.cfg and presets the variables to the values for that environment. If there is no such environment registered, the variables are stored before execution. The directories are NOT stored. If you want to add them, d it manually as directory=<tree> and pay attention not to provide unneeded trailing spaces. -f requires the path-stripped file names to be equal. No real use for this except for paranoia and testing reasons. -g requires groups of the files to be equal for linking. (see -p) -h shows this help. All other option are ignored then. -H Normally, when two or more files point to the same disk area they are not treated as duplicates and therefore not reported; this option will change this behaviour. Use it as in fdupes -rH. -i activate interactive mode. You are prompted to decide on each file, whether to delete or to link it. The list of files contains a number and a letter to reference each file. The number may be used to delete a file. Entering the corresponding letter invokes it to be linked to the file referenced by the first given letter. The special characters '#', '@', '<', '>', '-', and '+' suggest linking all files to the first given, the first with the most links, the oldest, the newest, the smallest, or the biggest respectively. The smallest and biggest reflect different sizes when using extra styles, otherwise they default to oldest and newest. In this case hardlinks on the same device and symbolic links for different devices are suggested. In order to ease your decisions the file names are also preceeded by colon separated device number, inode, permissions, and the link counter in square brackets. The interactive mode may be a replacement for fdupes -rd. The options -in make freedup to behave like fdupes -r. -k <key> sets the linking order, i.e. which file will replace the others. Depending on your options the may have influenc e on timestamps, permissions, ownership, and in case of more than two identical files als which files are treated as linked and which are not. Allowed keys are the special characters from interactive mode. -l Establish only those links that are hardlinks, do nothing if symlinks are required, e.g. caused by different file systems. Makes freedup do nothing in combination with -w. -m <bytes> deprecated: only link files larger than the amount of bytes given. Better performance will be achieved using -o \"-size +<bytes>c\". -n do not execute the link command. -o <opts> pass an option string to the initially called find command. Be aware that the find command only applies directory arguments, not to standard input. Since there are usually spaces enclosed be aware to set quotes right. Only the last given -o option will be used. This disables the internal tree-scanning and replaces it with a pipe. The option string is appended to find . -type f . Use -o -true to enable the pipe instead of the internal routine. -p requires file permissions to be equal. Security issues may be assumed. The author does not believe so, since the examined file is already present with the later permissions and linking only takes place with the same content (hash cheating will fail). On the other hand, this might be a way to propagate improper settings. -P <mask> set the file permission mask that will be used during comparison of permissions. The mask needs to be given as and octal number. Default is to use 7777. When setting the mask -p is forced to on automatically. -q suppress any informational output (also that to stderr) except in case of severe errors. -s generate symbolic links although some given paths are given without a leading slash. This might lead to broken links since it works well from the current directory but unlikely in all hierarchies. This only affects symbolic links, since the inode numbers of hard links are unique within each file system. -t <type> selects the hash method that will be used. Valid choices are sha512, sha384, sha256, sha224, sha1, md5, sum. If the type is incomplete the first matching type will be chosen automatically. If this option is not used and an internal hash method is compiled into the program, this will be used. Warnings and error messages are only available when running with -v. -T when replacing a file by a link, the modification time of that directory changes. If you want to keep the time stamp of the directory like it was before linking its file, then use this option. The access time is restored to the pre-linking value as well, but this might not last long when freedups continues to investigate the file tree. -u requires users of the files to be equal for linking. (see -p) -V show version and copyright. -v runs in verbose mode, i.e. it displays shell commands to perform linking. In difference to the embedded commands, the displayed commands to not do establish the link before deleting the former file. -w Establish all links as weak symbolic links only. This alters the behaviour fundamentally. Please watch out when using this option in combination with -s. In consequence files may be hard to retrieve from different working directories. -x <style> selects the comparison style that will be used. Valid choices are auto, mp3, mp4, mpc, ogg or jpg. If the type is incomplete the first matching type will be chosen automatically. If this option is used only the internal hash method can be used, no external one. Please be aware, that this disables full comparison of the files and can result in loss of information. -# <num> This option manages the way that hash sum evaluation influences the comparison algorithm. (zero) means not to use the hash sum at all. (one) enables \"classic\" hash sum usage, i.e. to evaluate or retrieve hash sum before comparing two files. this mode is required for external hash functions. (two) enables the new \"on the fly\" evaluation, which reduces the read overhead. [default since version 1.3-1] other values are increased or lowered to match the above ones There is no way to disable that all files are compared byte-by-byte (only the extra mode limits this). -0 Do not link files that do have no contents, i.e. file size is zero. This avoids large link clusters. <dir> any directory to scan for duplicate files recursively. The recursion is activated per default. Use -o to restrict the search to the initial file system (-xdev) or to a maximum tree depth (-maxdepth). Many Options (-cdfnpsuv) are implemented as toggle switches. All given options are processed before starting the program. The final state of each option applies. <dir> trees given later are linked to the files found in earlier ones. Since a sorting algorithm is applied, there is no use in adding one directory tree several times, except certain additional options for find are provided. With no <dir> tree given, a list of files (NOT dirs!) will be read from standard input. This is useful in conjunction with locate(1L) and find(1L). An example would be find \/usr\/src -xdev -iname '*.h' -print | freedup -c","Process Name":"freedup","Link":"https:\/\/linux.die.net\/man\/1\/freedup"}},{"Process":{"Description":"FreeHDL is a parser\/compiler\/simulator suite for the hardware description language VHDL. VHDL'93 as well as VHDL'87 files are supported. FreeHDL-CONFIG can be used to obtain the installation (path) configuration of the FreeHDL package.","Process Name":"freehdl-config","Link":"https:\/\/linux.die.net\/man\/1\/freehdl-config"}},{"Process":{"Description":"FreeHDL-GENNODES is an extensible framework for representing an abstract syntax tree. The program reads the IN-FILE and generates output for CHUNK in OUT-FILE, according to CMD. When CMD is 'header', FreeHDL-GENNODES writes a header file to OUT-FILE that should be included by the users of the CHUNK. When it is 'impl', it writes the implementation of CHUNK.","Process Name":"freehdl-gennodes","Link":"https:\/\/linux.die.net\/man\/1\/freehdl-gennodes"}},{"Process":{"Description":"FreeHDL is a parser\/compiler\/simulator suite for the hardware description language VHDL. VHDL'93 as well as VHDL'87 files are supported. FreeHDL-V2CC translates the original VHDL source FILEs into C++.","Process Name":"freehdl-v2cc","Link":"https:\/\/linux.die.net\/man\/1\/freehdl-v2cc"}},{"Process":{"Description":"Freehoo is a free console based messenger for Yahoo IM Service with GNU Bash like tab completion \/ editing and GNU Emacs like extensibility. -s, --status= STATUS Default login status number -u, --user= YAHOOID User login account name -?, --help Give this help list --usage Give a short usage message -V, --version Print program version Mandatory or optional arguments to long options are also mandatory or optional for any corresponding short options.","Process Name":"freehoo","Link":"https:\/\/linux.die.net\/man\/1\/freehoo"}},{"Process":{"Description":null,"Process Name":"freewrl","Link":"https:\/\/linux.die.net\/man\/1\/freewrl"}},{"Process":{"Description":"Compresses the specified files or standard input. Each file is replaced by a file with the extension .F, but only if the file got smaller. If no files are specified, the compression is applied to the standard input and is written to standard output regardless of the results. Compressed files can be restored to their original form by specifying the -d option, or by running melt or unfreeze (both linked to freeze), on the .F files or the standard input. If the output file exists, it will not be overwritten unless the -f flag is given. If -f is not specified and freeze is run in the foreground, the user is prompted as to whether the file should be overwritten. If the -g flag is given, a slightly less powerful (compression rate is 1.5% less), but somewhat faster heuristic is used. This flag can be used more than once (this mode is quite useful when freezing bitmaps) for additional speedup. If you want to improve compression rate at the cost of speed, use -x flag. It means \"maximum compression\" (the speed may degrade substantially when freezing bitmaps). If the -f flag is given, all files specified are replaced with .F files - even if the file didn't get smaller. When file names are given, the ownership (if run by root), modes, accessed and modified times are maintained between the file and its .F version. In this respect, freeze can be used for archival purposes, yet can still be used with make(1) after melting. The -c option causes the results of the freeze\/melt operation to be written to stdout; no files are changed. The fcat program is the same as specifying -c to melt (all files are unpacked and written to stdout). The -v (verbose) option causes the diagnostics (at the end of each file processing) to be printed to stderr, and the -vv option causes the progress indicator to be drawn to the same place. Type is a token preceded by a '+' or a '--', which defines the type of following files in the command string. An explicite definition of the file's type can give up to 2% of additional compression. The list of types is stored in file \/usr\/lib\/freeze.cnf. Types may be abbreviated while not ambigious. You can also determine values for the static Huffman table by using a list of 8 numbers separated by commas instead of type. Freeze uses the Lempel-Ziv algorithm on the first pass and the dynamic Huffman algorithm on the second one. The size of sliding window is 8K, and the maximum length of matched string is 256. The positions on the window are coded using a static Huffman table. A two byte magic number is prepended to the file to ensure that neither melting of random text nor refreezing of already frozen text are attempted. In addition, the characteristics of the static Huffman table being used during freeze is written to the file so that these characteristics may be adapted to concrete conditions. The amount of compression obtained depends on the size of the input file and the distribution of character substrings and their probabilities. Typically, text files, such as C programs, are reduced by 60-75%, executable files are reduced by 50%. Compression is generally much better than that achieved by LZW coding (as used in compress), or Huffman coding (pack), though takes more time to compute. If the -V (version) flag is given, the program's version number and compilation options are printed. The exit status is normally 0; if the last file gets bigger after freezing, the exit status is 2; if an error occurs, the exit status is 1.","Process Name":"freeze","Link":"https:\/\/linux.die.net\/man\/1\/freeze"}},{"Process":{"Description":"freshclam is a virus database update tool for ClamAV.","Process Name":"freshclam","Link":"https:\/\/linux.die.net\/man\/1\/freshclam"}},{"Process":{"Description":"addr2line translates addresses into file names and line numbers. Given an address in an executable or an offset in a section of a relocatable object, it uses the debugging information to figure out which file name and line number are associated with it. The executable or relocatable object to use is specified with the -e option. The default is the file a.out. The section in the relocatable object to use is specified with the -j option. addr2line has two modes of operation. In the first, hexadecimal addresses are specified on the command line, and addr2line displays the file name and line number for each address. In the second, addr2line reads hexadecimal addresses from standard input, and prints the file name and line number for each address on standard output. In this mode, addr2line may be used in a pipe to convert dynamically chosen addresses. The format of the output is FILENAME:LINENO . The file name and line number for each input address is printed on separate lines. If the -f option is used, then each FILENAME:LINENO line is preceded by FUNCTIONNAME which is the name of the function containing the address. If the -i option is used and the code at the given address is present there because of inlining by the compiler then the { FUNCTIONNAME } FILENAME:LINENO information for the inlining function will be displayed afterwards. This continues recursively until there is no more inlining to report. If the -a option is used then the output is prefixed by the input address. If the -p option is used then the output for each input address is displayed on one, possibly quite long, line. If -p is not used then the output is broken up into multiple lines, based on the paragraphs above. If the file name or function name can not be determined, addr2line will print two question marks in their place. If the line number can not be determined, addr2line will print 0.","Process Name":"frv-linux-gnu-addr2line","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-addr2line"}},{"Process":{"Description":null,"Process Name":"frv-linux-gnu-ar","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-ar"}},{"Process":{"Description":null,"Process Name":"frv-linux-gnu-as","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-as"}},{"Process":{"Description":"The C ++ and Java languages provide function overloading, which means that you can write many functions with the same name, providing that each function takes parameters of different types. In order to be able to distinguish these similarly named functions C ++ and Java encode them into a low-level assembler name which uniquely identifies each different version. This process is known as mangling. The c++filt [1] program does the inverse mapping: it decodes (demangles) low-level names into user-level names so that they can be read. Every alphanumeric word (consisting of letters, digits, underscores, dollars, or periods) seen in the input is a potential mangled name. If the name decodes into a C ++ name, the C ++ name replaces the low-level name in the output, otherwise the original word is output. In this way you can pass an entire assembler source file, containing mangled names, through c++filt and see the same source file containing demangled names. You can also use c++filt to decipher individual symbols by passing them on the command line: c++filt <symbol> If no symbol arguments are given, c++filt reads symbol names from the standard input instead. All the results are printed on the standard output. The difference between reading names from the command line versus reading names from the standard input is that command line arguments are expected to be just mangled names and no checking is performed to separate them from surrounding text. Thus for example: c++filt -n _Z1fv will work and demangle the name to \"f()\" whereas: c++filt -n _Z1fv, will not work. (Note the extra comma at the end of the mangled name which makes it invalid). This command however will work: echo _Z1fv, | c++filt -n and will display \"f(),\", i.e., the demangled name followed by a trailing comma. This behaviour is because when the names are read from the standard input it is expected that they might be part of an assembler source file where there might be extra, extraneous characters trailing after a mangled name. For example: .type   _Z1fv, @function","Process Name":"frv-linux-gnu-c++filt","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-c++filt"}},{"Process":{"Description":"The C preprocessor, often known as cpp, is a macro processor that is used automatically by the C compiler to transform your program before compilation. It is called a macro processor because it allows you to define macros, which are brief abbreviations for longer constructs. The C preprocessor is intended to be used only with C, C ++ , and Objective-C source code. In the past, it has been abused as a general text processor. It will choke on input which does not obey C's lexical rules. For example, apostrophes will be interpreted as the beginning of character constants, and cause errors. Also, you cannot rely on it preserving characteristics of the input which are not significant to C-family languages. If a Makefile is preprocessed, all the hard tabs will be removed, and the Makefile will not work. Having said that, you can often get away with using cpp on things which are not C. Other Algol-ish programming languages are often safe (Pascal, Ada, etc.) So is assembly, with caution. -traditional-cpp mode preserves more white space, and is otherwise more permissive. Many of the problems can be avoided by writing C or C ++ style comments instead of native language comments, and keeping macros simple. Wherever possible, you should use a preprocessor geared to the language you are writing in. Modern versions of the GNU assembler have macro facilities. Most high level programming languages have their own conditional compilation and inclusion mechanism. If all else fails, try a true general text processor, such as GNU M4. C preprocessors vary in some details. This manual discusses the GNU C preprocessor, which provides a small superset of the features of ISO Standard C. In its default mode, the GNU C preprocessor does not do a few things required by the standard. These are features which are rarely, if ever, used, and may cause surprising changes to the meaning of a program which does not expect them. To get strict ISO Standard C, you should use the -std=c90, -std=c99 or -std=c11 options, depending on which version of the standard you want. To get all the mandatory diagnostics, you must also use -pedantic. This manual describes the behavior of the ISO preprocessor. To minimize gratuitous differences, where the ISO preprocessor's behavior does not conflict with traditional semantics, the traditional preprocessor should behave the same way. The various differences that do exist are detailed in the section Traditional Mode. For clarity, unless noted otherwise, references to CPP in this manual refer to GNU CPP .","Process Name":"frv-linux-gnu-cpp","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-cpp"}},{"Process":{"Description":"dlltool reads its inputs, which can come from the -d and -b options as well as object files specified on the command line. It then processes these inputs and if the -e option has been specified it creates a exports file. If the -l option has been specified it creates a library file and if the -z option has been specified it creates a def file. Any or all of the -e, -l and -z options can be present in one invocation of dlltool. When creating a DLL , along with the source for the DLL , it is necessary to have three other files. dlltool can help with the creation of these files. The first file is a .def file which specifies which functions are exported from the DLL , which functions the DLL imports, and so on. This is a text file and can be created by hand, or dlltool can be used to create it using the -z option. In this case dlltool will scan the object files specified on its command line looking for those functions which have been specially marked as being exported and put entries for them in the .def file it creates. In order to mark a function as being exported from a DLL , it needs to have an -export:<name_of_function> entry in the .drectve section of the object file. This can be done in C by using the asm() operator: asm (\".section .drectve\");\nasm (\".ascii \\\"-export:my_func\\\"\");\n\nint my_func (void) { ... } The second file needed for DLL creation is an exports file. This file is linked with the object files that make up the body of the DLL and it handles the interface between the DLL and the outside world. This is a binary file and it can be created by giving the -e option to dlltool when it is creating or reading in a .def file. The third file needed for DLL creation is the library file that programs will link with in order to access the functions in the DLL (an 'import library'). This file can be created by giving the -l option to dlltool when it is creating or reading in a .def file. If the -y option is specified, dlltool generates a delay-import library that can be used instead of the normal import library to allow a program to link to the dll only as soon as an imported function is called for the first time. The resulting executable will need to be linked to the static delayimp library containing __delayLoadHelper2(), which in turn will import LoadLibraryA and GetProcAddress from kernel32. dlltool builds the library file by hand, but it builds the exports file by creating temporary files containing assembler statements and then assembling these. The -S command line option can be used to specify the path to the assembler that dlltool will use, and the -f option can be used to pass specific flags to that assembler. The -n can be used to prevent dlltool from deleting these temporary assembler files when it is done, and if -n is specified twice then this will prevent dlltool from deleting the temporary object files it used to build the library. Here is an example of creating a DLL from a source file dll.c and also creating a program (from an object file called program.o) that uses that DLL: gcc -c dll.c\ndlltool -e exports.o -l dll.lib dll.o\ngcc dll.o exports.o -o dll.dll\ngcc program.o dll.lib -o program dlltool may also be used to query an existing import library to determine the name of the DLL to which it is associated. See the description of the -I or --identify option.","Process Name":"frv-linux-gnu-dlltool","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-dlltool"}},{"Process":{"Description":"elfedit updates the ELF header of ELF files which have the matching ELF machine and file types. The options control how and which fields in the ELF header should be updated. elffile... are the ELF files to be updated. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files.","Process Name":"frv-linux-gnu-elfedit","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-elfedit"}},{"Process":{"Description":"When you invoke GCC , it normally does preprocessing, compilation, assembly and linking. The \"overall options\" allow you to stop this process at an intermediate stage. For example, the -c option says not to run the linker. Then the output consists of object files output by the assembler. Other options are passed on to one stage of processing. Some options control the preprocessor and others the compiler itself. Yet other options control the assembler and linker; most of these are not documented here, since you rarely need to use any of them. Most of the command-line options that you can use with GCC are useful for C programs; when an option is only useful with another language (usually C ++ ), the explanation says so explicitly. If the description for a particular option does not mention a source language, you can use that option with all supported languages. The gcc program accepts options and file names as operands. Many options have multi-letter names; therefore multiple single-letter options may not be grouped: -dv is very different from -d -v. You can mix options and other arguments. For the most part, the order you use doesn't matter. Order does matter when you use several options of the same kind; for example, if you specify -L more than once, the directories are searched in the order specified. Also, the placement of the -l option is significant. Many options have long names starting with -f or with -W---for example, -fmove-loop-invariants, -Wformat and so on. Most of these have both positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"frv-linux-gnu-gcc","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-gcc"}},{"Process":{"Description":"gcov is a test coverage program. Use it in concert with GCC to analyze your programs to help create more efficient, faster running code and to discover untested parts of your program. You can use gcov as a profiling tool to help discover where your optimization efforts will best affect your code. You can also use gcov along with the other profiling tool, gprof, to assess which parts of your code use the greatest amount of computing time. Profiling tools help you analyze your code's performance. Using a profiler such as gcov or gprof, you can find out some basic performance statistics, such as: \u2022 how often each line of code executes \u2022 what lines of code are actually executed \u2022 how much computing time each section of code uses Once you know these things about how your code works when compiled, you can look at each module to see which modules should be optimized. gcov helps you determine where to work on optimization. Software developers also use coverage testing in concert with testsuites, to make sure software is actually good enough for a release. Testsuites can verify that a program works as expected; a coverage program tests to see how much of the program is exercised by the testsuite. Developers can then determine what kinds of test cases need to be added to the testsuites to create both better testing and a better final product. You should compile your code without optimization if you plan to use gcov because the optimization, by combining some lines of code into one function, may not give you as much information as you need to look for 'hot spots' where the code is using a great deal of computer time. Likewise, because gcov accumulates statistics by line (at the lowest resolution), it works best with a programming style that places only one statement on each line. If you use complicated macros that expand to loops or to other control structures, the statistics are less helpful---they only report on the line where the macro call appears. If your complex macros behave like functions, you can replace them with inline functions to solve this problem. gcov creates a logfile called sourcefile.gcov which indicates how many times each line of a source file sourcefile.c has executed. You can use these logfiles along with gprof to aid in fine-tuning the performance of your programs. gprof gives timing information you can use along with the information you get from gcov. gcov works only on code compiled with GCC . It is not compatible with any other profiling or test coverage mechanism.","Process Name":"frv-linux-gnu-gcov","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-gcov"}},{"Process":{"Description":null,"Process Name":"frv-linux-gnu-gprof","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-gprof"}},{"Process":{"Description":null,"Process Name":"frv-linux-gnu-ld","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-ld"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"frv-linux-gnu-ld.bfd","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-ld.bfd"}},{"Process":{"Description":"nlmconv converts the relocatable i386 object file infile into the NetWare Loadable Module outfile, optionally reading headerfile for NLM header information. For instructions on writing the NLM command file language used in header files, see the linkers section, NLMLINK in particular, of the NLM Development and Tools Overview, which is part of the NLM Software Developer's Kit (\" NLM SDK \"), available from Novell, Inc. nlmconv uses the GNU Binary File Descriptor library to read infile; nlmconv can perform a link step. In other words, you can list more than one object file for input if you list them in the definitions file (rather than simply specifying one input file on the command line). In this case, nlmconv calls the linker for you.","Process Name":"frv-linux-gnu-nlmconv","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-nlmconv"}},{"Process":{"Description":null,"Process Name":"frv-linux-gnu-nm","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-nm"}},{"Process":{"Description":"The GNU objcopy utility copies the contents of an object file to another. objcopy uses the GNU BFD Library to read and write the object files. It can write the destination object file in a format different from that of the source object file. The exact behavior of objcopy is controlled by command-line options. Note that objcopy should be able to copy a fully linked file between any two formats. However, copying a relocatable object file between any two formats may not work as expected. objcopy creates temporary files to do its translations and deletes them afterward. objcopy uses BFD to do all its translation work; it has access to all the formats described in BFD and thus is able to recognize most formats without being told explicitly. objcopy can be used to generate S-records by using an output target of srec (e.g., use -O srec). objcopy can be used to generate a raw binary file by using an output target of binary (e.g., use -O binary). When objcopy generates a raw binary file, it will essentially produce a memory dump of the contents of the input object file. All symbols and relocation information will be discarded. The memory dump will start at the load address of the lowest section copied into the output file. When generating an S-record or a raw binary file, it may be helpful to use -S to remove sections containing debugging information. In some cases -R will be useful to remove sections which contain information that is not needed by the binary file. Note---objcopy is not able to change the endianness of its input files. If the input format has an endianness (some formats do not), objcopy can only copy the inputs into file formats that have the same endianness or which have no endianness (e.g., srec). (However, see the --reverse-bytes option.)","Process Name":"frv-linux-gnu-objcopy","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-objcopy"}},{"Process":{"Description":"objdump displays information about one or more object files. The options control what particular information to display. This information is mostly useful to programmers who are working on the compilation tools, as opposed to programmers who just want their program to compile and work. objfile... are the object files to be examined. When you specify archives, objdump shows information on each of the member object files.","Process Name":"frv-linux-gnu-objdump","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-objdump"}},{"Process":{"Description":null,"Process Name":"frv-linux-gnu-ranlib","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-ranlib"}},{"Process":{"Description":null,"Process Name":"frv-linux-gnu-readelf","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-readelf"}},{"Process":{"Description":"The GNU size utility lists the section sizes---and the total size---for each of the object or archive files objfile in its argument list. By default, one line of output is generated for each object file or each module in an archive. objfile... are the object files to be examined. If none are specified, the file \"a.out\" will be used.","Process Name":"frv-linux-gnu-size","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-size"}},{"Process":{"Description":null,"Process Name":"frv-linux-gnu-strings","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-strings"}},{"Process":{"Description":"GNU strip discards all symbols from object files objfile. The list of object files may include archives. At least one object file must be given. strip modifies the files named in its argument, rather than writing modified copies under different names.","Process Name":"frv-linux-gnu-strip","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-strip"}},{"Process":{"Description":"windmc reads message definitions from an input file (.mc) and translate them into a set of output files. The output files may be of four kinds: \"h\" A C header file containing the message definitions. \"rc\" A resource file compilable by the windres tool. \"bin\" One or more binary files containing the resource data for a specific message language. \"dbg\" A C include file that maps message id's to their symbolic name. The exact description of these different formats is available in documentation from Microsoft. When windmc converts from the \"mc\" format to the \"bin\" format, \"rc\", \"h\", and optional \"dbg\" it is acting like the Windows Message Compiler.","Process Name":"frv-linux-gnu-windmc","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-windmc"}},{"Process":{"Description":null,"Process Name":"frv-linux-gnu-windres","Link":"https:\/\/linux.die.net\/man\/1\/frv-linux-gnu-windres"}},{"Process":{"Description":null,"Process Name":"frysk","Link":"https:\/\/linux.die.net\/man\/1\/frysk"}},{"Process":{"Description":"fc-cache scans the font directories on the system and builds font information cache files for applications using fontconfig for their font handling. If directory arguments are not given, fc-cache uses each directory in the current font configuration. Each directory is scanned for font files readable by FreeType. A cache is created which contains properties of each font and the associated filename. This cache is used to speed up application startup when using the fontconfig library. Note that fc-cache must be executed once per architecture to generate font information customized for that architecture. On a subsequent run, fc-cache will augment the cache information files with the information for the new architecture.","Process Name":"frysk-fc-cache","Link":"https:\/\/linux.die.net\/man\/1\/frysk-fc-cache"}},{"Process":{"Description":"fc-cat reads the font information from cache files or related to font directories and emits it in ASCII form.","Process Name":"frysk-fc-cat","Link":"https:\/\/linux.die.net\/man\/1\/frysk-fc-cat"}},{"Process":{"Description":null,"Process Name":"frysk-fc-list","Link":"https:\/\/linux.die.net\/man\/1\/frysk-fc-list"}},{"Process":{"Description":"fc-match matches font-pattern (empty pattern by default) using the normal fontconfig matching rules to find the best font available. If --sort is given, the sorted list of best matching fonts is displayed. With --verbose, the whole font pattern for each match is printed, otherwise only the file, family and style are printed..","Process Name":"frysk-fc-match","Link":"https:\/\/linux.die.net\/man\/1\/frysk-fc-match"}},{"Process":{"Description":null,"Process Name":"frysk-gdk-pixbuf-csource","Link":"https:\/\/linux.die.net\/man\/1\/frysk-gdk-pixbuf-csource"}},{"Process":{"Description":"gdk-pixbuf-query-loaders collects information about loadable modules for gdk-pixbuf and writes it to stdout. If called without arguments, it looks for modules in the gdk-pixbuf loader directory. If called with arguments, it looks for the specified modules. The arguments may be absolute or relative paths.","Process Name":"frysk-gdk-pixbuf-query-loaders","Link":"https:\/\/linux.die.net\/man\/1\/frysk-gdk-pixbuf-query-loaders"}},{"Process":{"Description":"glib-genmarshal is a small utility that generates C code marshallers for callback functions of the GClosure mechanism in the GObject sublibrary of GLib. The marshaller functions have a standard signature, they get passed in the invoking closure, an array of value structures holding the callback function parameters and a value structure for the return value of the callback. The marshaller is then responsible to call the respective C code function of the closure with all the parameters on the stack and to collect its return value.","Process Name":"frysk-glib-genmarshal","Link":"https:\/\/linux.die.net\/man\/1\/frysk-glib-genmarshal"}},{"Process":{"Description":null,"Process Name":"frysk-glib-gettextize","Link":"https:\/\/linux.die.net\/man\/1\/frysk-glib-gettextize"}},{"Process":{"Description":"glib-mkenums is a small perl-script utility that parses C code to extract enum definitions and produces enum descriptions based on text templates specified by the user. Most frequently this script is used to produce C code that contains enum values as strings so programs can provide value name strings for introspection.","Process Name":"frysk-glib-mkenums","Link":"https:\/\/linux.die.net\/man\/1\/frysk-glib-mkenums"}},{"Process":{"Description":null,"Process Name":"frysk-gobject-query","Link":"https:\/\/linux.die.net\/man\/1\/frysk-gobject-query"}},{"Process":{"Description":"gtk-query-immodules-2.0 collects information about loadable input method modules for GTK+ and writes it to stdout. If called without arguments, it looks for modules in the GTK+ input method module path. If called with arguments, it looks for the specified modules. The arguments may be absolute or relative paths.","Process Name":"frysk-gtk-query-immodules-2.0","Link":"https:\/\/linux.die.net\/man\/1\/frysk-gtk-query-immodules-2.0"}},{"Process":{"Description":"gtk-update-icon-cache creates mmap()able cache files for icon themes. It expects to be given the path to a icon theme directory containing an index.theme, e.g. \/usr\/share\/icons\/hicolor, and writes a icon-theme.cache containing cached information about the icons in the directory tree below the given directory. GTK+ can use the cache files created by gtk-update-icon-cache to avoid a lot of system call and disk seek overhead when the application starts. Since the format of the cache files allows them to be mmap()ed shared between multiple applications, the overall memory consumption is reduced as well.","Process Name":"frysk-gtk-update-icon-cache","Link":"https:\/\/linux.die.net\/man\/1\/frysk-gtk-update-icon-cache"}},{"Process":{"Description":null,"Process Name":"frysk-pango-querymodules","Link":"https:\/\/linux.die.net\/man\/1\/frysk-pango-querymodules"}},{"Process":{"Description":"fsck-larch reads an on-disk, committed B-tree created by the larch Python library, and verifies that it is internally consistent. It reports any problems it finds, but does not currently fix them.","Process Name":"fsck-larch","Link":"https:\/\/linux.die.net\/man\/1\/fsck-larch"}},{"Process":{"Description":null,"Process Name":"fsck.cpm","Link":"https:\/\/linux.die.net\/man\/1\/fsck.cpm"}},{"Process":{"Description":"fsdiff reads a command file (the default name is command.K) to get a list of transcripts. If the command file is empty, the transcript list is considered to be the null transcript. Included command files are read depth first. The first transcript listed has the lowest precedence, the next higher, and the last has the highest. If any special files are listed, the special.T transcript will have the absolute highest precedence. fsdiff walks the filesystem starting at path and compares the filesystem to the transcripts. Trailing '\/'s on path are clipped. If a transcript is positive, fsdiff checks all attributes of each file system object ( i.e. file, directory, link, etc ). If the -c option is given, checksums are also compared. If a transcript is negative, fsdiff checks only some of the attributes of the file system objects ( see TRANSCRIPTS section below ). There is only one special transcript, special.T, and it contains references to files that are host specific, eg. \/etc\/hostname.hme0. Any discrepancies are printed on the standard output or, with the -o option, to a file. The default is to print the differences as edits to the transcript to make it match the filesystem. If the edit direction chosen is -T or -A, the differences are printed as edits to the file system to make it match the transcript. A \"+\" at the beginning of a line indicates a file must be downloaded. A \"-\" indicates the given object ( file, directory, link etc ) must be removed.","Process Name":"fsdiff","Link":"https:\/\/linux.die.net\/man\/1\/fsdiff"}},{"Process":{"Description":null,"Process Name":"fsed.cpm","Link":"https:\/\/linux.die.net\/man\/1\/fsed.cpm"}},{"Process":{"Description":"fsgrab seeks into a file (normally a block device file), and copies blocks from the appropriate point in that file to standard output. The device defaults to \/dev\/hda1 if none is named specifically.","Process Name":"fsgrab","Link":"https:\/\/linux.die.net\/man\/1\/fsgrab"}},{"Process":{"Description":null,"Process Name":"fslint","Link":"https:\/\/linux.die.net\/man\/1\/fslint"}},{"Process":{"Description":"fslint is a toolset to find various problems with filesystems, including duplicate files and problematic filenames etc. To access the individual command line tools one can change to, or add to $PATH the \/usr\/share\/fslint\/fslint directory on a standard install. Each of the commands in that directory have a --help option which further details their parameters.","Process Name":"fslint-gui","Link":"https:\/\/linux.die.net\/man\/1\/fslint-gui"}},{"Process":{"Description":"Fslsfonts lists the fonts that match the given pattern. The wildcard character \"*\" may be used to match any sequence of characters (including none), and \"?\" to match any single character. If no pattern is given, \"*\" is assumed. The \"*\" and \"?\" characters must be quoted to prevent them from being expanded by the shell.","Process Name":"fslsfonts","Link":"https:\/\/linux.die.net\/man\/1\/fslsfonts"}},{"Process":{"Description":null,"Process Name":"fsm","Link":"https:\/\/linux.die.net\/man\/1\/fsm"}},{"Process":{"Description":"Fsnmp is a filter to be used by the LPRng print system to transfer print data from the print server to the printer using a TCP connection. Before the data transfer is started the filter uses SNMP to check whether the printer is ready and to retrieve the page count. After sending data the filter waits again until the printer is ready and retrieves the page count.","Process Name":"fsnmp","Link":"https:\/\/linux.die.net\/man\/1\/fsnmp"}},{"Process":{"Description":null,"Process Name":"fsp","Link":"https:\/\/linux.die.net\/man\/1\/fsp"}},{"Process":{"Description":"Converts FST files to VCD files on stdout.","Process Name":"fst2vcd","Link":"https:\/\/linux.die.net\/man\/1\/fst2vcd"}},{"Process":{"Description":null,"Process Name":"fstack","Link":"https:\/\/linux.die.net\/man\/1\/fstack"}},{"Process":{"Description":"Most MPI users will probably not need to use the fstate command. This command is only installed if LAM\/MPI was configured with the --with-trillium switch. The fstate command reports on open file descriptors maintained by the remote file daemon. With no options, information is printed on all open descriptors under the following headings: FD the LAM descriptor handle, different than the client handle or actual UNIX handle FLAGS the open flags: R for read, W for write, A for active, L for locked active FLOW the total number of bytes transferred in both directions CLIENT the node ID and process ID of the last client NAME the file name The file deamon is a UNIX process, and UNIX processes are limited in the number of UNIX file descriptors (fd) that they can open at any one time. Yet the number of LAM processes is essentially unlimited. LAM descriptors are associated with each client's open user descriptors, and are used by LAM to identify the file. They are temporarily assigned UNIX file descriptors. When this happens, the status of the filed descriptor is active. Locked filed descriptors remain active continuously.","Process Name":"fstate","Link":"https:\/\/linux.die.net\/man\/1\/fstate"}},{"Process":{"Description":"fstep Instruction steps a given program using the Frysk framework. fstep uses the Frysk framework to instruction step a given program.","Process Name":"fstep","Link":"https:\/\/linux.die.net\/man\/1\/fstep"}},{"Process":{"Description":null,"Process Name":"fstobdf","Link":"https:\/\/linux.die.net\/man\/1\/fstobdf"}},{"Process":{"Description":"This program is part of Netpbm(1). fstopgm reads a Usenix FaceSaver(tm) file as input and produces a PGM image as output. FaceSaver(tm) files sometimes have rectangular pixels. While fstopgm won't re-scale them into square pixels for you, it will give you the precise pamscale command that will do the job. Because of this, reading a FaceSaver(tm) image is a two-step process. First you do: fstopgm > \/dev\/null This will tell you whether you need to use pamscale. Then use one of the following pipelines: fstopgm | pgmnorm\nfstopgm | pamscale -whatever | pgmnorm To go to PBM, you want something more like one of these: fstopgm | pamenlarge 3 | pgmnorm | pamditherbw\nfstopgm | pamenlarge 3 | pamscale <whatever> | pgmnorm | pamditherbw You want to enlarge when going to a bitmap because otherwise you lose information; but enlarging by more than 3 does not look good. FaceSaver is a registered trademark of Metron Computerware Ltd. of Oakland, CA.","Process Name":"fstopgm","Link":"https:\/\/linux.die.net\/man\/1\/fstopgm"}},{"Process":{"Description":null,"Process Name":"ftnchek","Link":"https:\/\/linux.die.net\/man\/1\/ftnchek"}},{"Process":{"Description":"The ftop program displays progress information for the open files and file systems in a Linux system. As processes read and write files, ftop displays data rates and time estimates. Its feature-rich interface is similar to top, and includes extensive run-time configuration options. While this manual page contains the full documentation for ftop, the built-in online help is the best source for the most up to date documentation. To access the online help, run ftop with the -h command line option, or simply press 'h' while ftop is running.","Process Name":"ftop","Link":"https:\/\/linux.die.net\/man\/1\/ftop"}},{"Process":{"Description":null,"Process Name":"ftp","Link":"https:\/\/linux.die.net\/man\/1\/ftp"}},{"Process":{"Description":"ftp.proxy is a proxy server for a subset of the file tranfer protocol described in RFC 959. It forwards traffic between a client and a server without looking too much if both hosts do real FTP. The FTP server can be either given on the command line or supplied by the client. ftp.proxy can be started from a TCP superserver like inetd(1) or tcpproxy(1). but can also bind to a TCP\/IP port on it's own and run in standalone (or daemon) mode. Protocol Support ftp.proxy supports the following FTP commands: ABOR, ACCT, APPE, CDUP, CWD, DELE, FEAT, LIST, MDTM, MKD, MODE, NLIST, NOOP, PASS, PASV, PORT, PWD, QUIT, RETR, REST, RNFR, RNTO, RMD, SITE, SIZE, SMNT, STAT, STOR, SYST, TYPE, USER, XCUP, XCWD, XMKD, XPWD, XRMD Transfer of structured data is not supported. Command Parameters By default ftp.proxy does not accept blanks in command parameters. This is to protect your UNIX server against users who work on computers where these things are usual. To allow blanks the option -b must be given on the command line. Notice that blanks at the beginning or end of the parameter are still not supported. The 'SITE' is in neither case affected by this limitation, ftp.proxy accepts always blanks in 'SITE' parameters. The option -y enables ftp.proxy to accept data connections from different remote interfaces. Try to avoid using this option, because it can cause security problems (see HISTORY for details). Server Selection If client-side server selection it turned on with the -e option the user must select the FTP server he wants to use with the '@' notation. Instead of specifying the real ftp server on the command line the user has to connect to the gateway machine where ftp.proxy is running and to enter the username in the form remote-user@ remote-ftp.server The password that is send to the proxy server is the password required for logging into remote-ftp-server with the account remote-user. In situations where the FTP client doesn't support usernames containing an '@' the percent sign '%' might be used for that. Access Control If an access control program is given with the -a option on the command line the connection data is passed to the acp before the server is contacted. The acp should return 0 as exit code to grant access and another value to deny. The access controller receives the following variables: PROXY_INTERFACE, PROXY_PORT interface and port where the client is connected to the proxy. PROXY_CLIENT, PROXY_CLIENTNAME IP number an name of the connected client. PROXY_SERVER, PROXY_SERVERPORT, PROXY_SERVERNAME IP number, port and name of the FTP server the client wants to contact. PROXY_SERVERLOGIN the supplied username for the FTP server. PROXY_USERNAME, PROXY_PASSWD supplied username and password for usage of the proxy server. The values for PROXY_USERNAME and PROXY_PASSWD are taken from the supplied remote username and password if they contain a colon ':'. In this case the local authentication data is taken from the left side of the colon and the remaining right side is passed on to the server. Furthermore the acp's stdout is connected to the FTP client and it's stderr is read by ftp.proxy which writes the acp's stderr output to syslog. Notice also that a non-zero acp exit code signals ftp.proxy that something's wrong and that ftp.proxy should terminate. Connection Translation Beginning with version 1.1.6 ftp.proxy supports connection translation programs (ctp's). A ctp can completly overwrite the user's server selection and login. If configured the ctp is called before the acp. It receives the same environment variables like the acp and returns server and login information that should ftp.proxy for the server connection on it's stdout. The format of the ctp output lines is variable [ <whitespace>] = [ <whitespace>] value where variable is one of SERVERNAME, SERVERLOGIN, SERVERPASSWD, SERVERPORT and value the corresponding value. Alternativly to these four variables you can use the shorter forms SERVER, LOGIN, PASSWD, PORT as variable names. Furthermore the case of the variable names doesn't matter and any whitespace around value is ignored. The ctp can deny the proxy request by exiting with an non-zero exit code, In which case ftp.proxy drops the connection immediately. Alternativly the ctp can also print a line starting with -ERR, which is written to syslog before the connection is closed. Command Control If a command control program (ccp) is given with the -c option this program is called for the FTP commands APPE, CDUP, CWD, DELE, LIST, MDTM, MKD, NLST, RETR, RNFR, RNTO, RMD, SIZE, STAT, STOR, STOU, XCUP, XCWD, XMKD, XRMD The ccp returns an exit code of 0 to grant and any other to deny access (the exit code to the 'QUIT' command is ignored). For the ccp the same variables as for acp's are set with the addition of PROXY_COMMAND, PROXY_PARAMETER FTP command and parameter (if set). PROXY_SESSION a unique identifier for the proxy session. PROXY_CCPCOLL, the client's number of collisions with the ccp's permission rules (number of 'permission denied' responses). The ccp's stdout and stderr are connected to ftp.proxy. A one line message written to stdout by the ccp goes to syslog, while a message one stderr is sent to the client. If this message does not contain a status ftp.proxy substitutes a '553' code. If the message is empty the client gets a simle '553 permission denied'. Notice that the stderr message is only used if the ccp returns an exit code other the zero. On normal program termination ('QUIT' command or timeout) the ccp is called with the command '+EXIT' to do some final clean up. It is not reliable that the ccp receives the '+EXIT' event. There are lots of possiblities that the proxy terminates without generating it, e.g. client timeout, server error or signal reciption by the proxy. Monitor Mode The -m option puts ftp.proxy into the monitor mode. ftp.proxy will then try to keep track of the client's current directory on the server side. With this information the file parameter for the commands APPE, CDUP, CWD, DELE, LIST, MDTM, MKD NLST, RETR, RNFR, RNTO, RMD, SIZE, STOR, XCUP, XCWD, XMKD, XRMD is converted into an absolute path. This value is then used in syslog messages and given to a ccp in the PROXY_FTPPATH variable. Furthermore the variable PROXY_FTPHOME contains the user's initial directory which is assumed to be his home directory. The 'LIST' and 'NLIST' command may have a parameter or not. If it is absent ftp.proxy sets the parameter to '*' but this affects only the PROXY_FTPPATH variable, not the command that is sent to the server. For the 'CDUP' command PROXY_FTPPATH contains the full path of the target directory. Monitoring may not work with all server systems since the output of the 'PWD' command which is used by ftp.proxy to get the current directory in not completely defined. If the directory can not be clearly determined ftp.proxy will terminate.","Process Name":"ftp.proxy","Link":"https:\/\/linux.die.net\/man\/1\/ftp.proxy"}},{"Process":{"Description":"ftpasswd is a Perl script designed to create and manage AuthUserFiles and AuthGroupFiles of the correct format for proftpd. Full documentation for ftpasswd can be found at: http:\/\/www.proftpd.org\/docs\/utils\/ftpasswd.html","Process Name":"ftpasswd","Link":"https:\/\/linux.die.net\/man\/1\/ftpasswd"}},{"Process":{"Description":"The ftpcount command shows the current number of connections per server and virtualhost\/anonymous configuration defined in the proftpd.conf file. Connections spawned by inetd are counted separately from those created by a master proftpd standalone server.","Process Name":"ftpcount","Link":"https:\/\/linux.die.net\/man\/1\/ftpcount"}},{"Process":{"Description":null,"Process Name":"ftpmail","Link":"https:\/\/linux.die.net\/man\/1\/ftpmail"}},{"Process":{"Description":"ftpquota is a Perl script designed to create and manage limits and tally files for the mod_quotatab + mod_quotatab_file module combination for proftpd. Full documentation for ftpquota can be found at: http:\/\/www.proftpd.org\/docs\/utils\/ftpquota.html","Process Name":"ftpquota","Link":"https:\/\/linux.die.net\/man\/1\/ftpquota"}},{"Process":{"Description":"The ftptop command displays the current status of FTP sessions in a continuously updating top like format.","Process Name":"ftptop","Link":"https:\/\/linux.die.net\/man\/1\/ftptop"}},{"Process":{"Description":null,"Process Name":"ftpwho","Link":"https:\/\/linux.die.net\/man\/1\/ftpwho"}},{"Process":{"Description":"ftrace Runs the frysk systemcall tracing utility. ftrace is a small utility that uses the frysk engine to trace systemcalls in a similar manner to strace.","Process Name":"ftrace","Link":"https:\/\/linux.die.net\/man\/1\/ftrace"}},{"Process":{"Description":null,"Process Name":"func","Link":"https:\/\/linux.die.net\/man\/1\/func"}},{"Process":{"Description":"Func's delegation feature allows an overlord to execute commands through proxies (minions which also run as overlords) to reduce XMLRPC overhead and the amount of time required to execute commands over large Func networks. To accomplish this task, Func needs to know where each proxy and minion sits in the Func network, and for the sake of expediency, this data is stored within a map file on the overlord. func-build-map, when run on an overlord, recursively probes through one's Func network, discovering its topology. When complete, it stores a mapfile, encoded in YAML , in \/var\/lib\/func\/map. If you utilize delegation frequently, we recommend running this tool as a cron job to ensure that your mapfile remains up to date with changes in your Func network topology.","Process Name":"func-build-map","Link":"https:\/\/linux.die.net\/man\/1\/func-build-map"}},{"Process":{"Description":null,"Process Name":"func-inventory","Link":"https:\/\/linux.die.net\/man\/1\/func-inventory"}},{"Process":{"Description":"Func includes a powerful Python API for addressing the overlord via software. When writing an app in a language other than Python, func-transmit can be used to operate the overlord and recieve structured results from it. This tool, func-transmit, is intended to be called using pipes, sending formatted data into stdin and recieving return codes via stdout. It also can be invoked using the shell.","Process Name":"func-transmit","Link":"https:\/\/linux.die.net\/man\/1\/func-transmit"}},{"Process":{"Description":"funcalc is a calculator program that allows arbitrary expressions to be constructed, compiled, and executed on columns in a Funtools table ( FITS binary table or raw event file). It works by integrating user-supplied expression(s) into a template C program, then compiling and executing the program. funcalc expressions are C statements, although some important simplifications (such as automatic declaration of variables) are supported. funcalc expressions can be specified in three ways: on the command line using the -e [expression] switch, in a file using the -f [file] switch, or from stdin (if neither -e nor -f is specified). Of course a file containing funcalc expressions can be read from stdin. Each invocation of funcalc requires an input Funtools table file to be specified as the first command line argument. The output Funtools table file is the second optional argument. It is needed only if an output FITS file is being created (i.e., in cases where the funcalc expression only prints values, no output file is needed). If input and output file are both specified, a third optional argument can specify the list of columns to activate (using FunColumnActivate()). Note that funcalc determines whether or not to generate code for writing an output file based on the presence or absence of an output file argument. A funcalc expression executes on each row of a table and consists of one or more C statements that operate on the columns of that row (possibly using temporary variables). Within an expression, reference is made to a column of the current row using the C struct syntax cur-[colname]>, e.g. cur->x, cur->pha, etc. Local scalar variables can be defined using C declarations at very the beginning of the expression, or else they can be defined automatically by funcalc (to be of type double). Thus, for example, a swap of columns x and y in a table can be performed using either of the following equivalent funcalc expressions: double temp;\ntemp = cur->x;\ncur->x = cur->y;\ncur->y = temp; or: temp = cur->x;\ncur->x = cur->y;\ncur->y = temp; When this expression is executed using a command such as: funcalc -f swap.expr itest.ev otest.ev the resulting file will have values of the x and y columns swapped. By default, the data type of the variable for a column is the same as the data type of the column as stored in the file. This can be changed by appending \":[dtype]\" to the first reference to that column. In the example above, to force x and y to be output as doubles, specify the type 'D' explicitly: temp = cur->x:D;\ncur->x = cur->y:D;\ncur->y = temp; Data type specifiers follow standard FITS table syntax for defining columns using TFORM: \u2022 A: ASCII characters \u2022 B: unsigned 8-bit char \u2022 I: signed 16-bit int \u2022 U: unsigned 16-bit int (not standard FITS ) \u2022 J: signed 32-bit int \u2022 V: unsigned 32-bit int (not standard FITS ) \u2022 E: 32-bit float \u2022 D: 64-bit float \u2022 X: bits (treated as an array of chars) Note that only the first reference to a column should contain the explicit data type specifier. Of course, it is important to handle the data type of the columns correctly. One of the most frequent cause of error in funcalc programming is the implicit use of the wrong data type for a column in expression. For example, the calculation: dx = (cur->x - cur->y)\/(cur->x + cur->y); usually needs to be performed using floating point arithmetic. In cases where the x and y columns are integers, this can be done by reading the columns as doubles using an explicit type specification: dx = (cur->x:D - cur->y:D)\/(cur->x + cur->y); Alternatively, it can be done using C type-casting in the expression: dx = ((double)cur->x - (double)cur->y)\/((double)cur->x + (double)cur->y); In addition to accessing columns in the current row, reference also can be made to the previous row using prev-[colname]>, and to the next row using next-[colname]>. Note that if prev-[colname]> is specified in the funcalc expression, the very first row is not processed. If next-[colname]> is specified in the funcalc expression, the very last row is not processed. In this way, prev and next are guaranteed always to point to valid rows. For example, to print out the values of the current x column and the previous y column, use the C fprintf function in a funcalc expression: fprintf(stdout, \"%d %d\\n\", cur->x, prev->y); New columns can be specified using the same cur-[colname]> syntax by appending the column type (and optional tlmin\/tlmax\/binsiz specifiers), separated by colons. For example, cur->avg:D will define a new column of type double. Type specifiers are the same those used above to specify new data types for existing columns. For example, to create and output a new column that is the average value of the x and y columns, a new \"avg\" column can be defined: cur->avg:D = (cur->x + cur->y)\/2.0 Note that the final ';' is not required for single-line expressions. As with FITS TFORM data type specification, the column data type specifier can be preceded by a numeric count to define an array, e.g., \"10I\" means a vector of 10 short ints, \"2E\" means two single precision floats, etc. A new column only needs to be defined once in a funcalc expression, after which it can be used without re-specifying the type. This includes reference to elements of a column array: cur->avg[0]:2D = (cur->x + cur->y)\/2.0;\ncur->avg[1] = (cur->x - cur->y)\/2.0; The 'X' (bits) data type is treated as a char array of dimension (numeric_count\/8), i.e., 16X is processed as a 2-byte char array. Each 8-bit array element is accessed separately: cur->stat[0]:16X  = 1;\ncur->stat[1]      = 2; Here, a 16-bit column is created with the MSB is set to 1 and the LSB set to 2. By default, all processed rows are written to the specified output file. If you want to skip writing certain rows, simply execute the C \"continue\" statement at the end of the funcalc expression, since the writing of the row is performed immediately after the expression is executed. For example, to skip writing rows whose average is the same as the current x value: cur->avg[0]:2D = (cur->x + cur->y)\/2.0;\ncur->avg[1] = (cur->x - cur->y)\/2.0;\nif( cur->avg[0] == cur->x )\n  continue; If no output file argument is specified on the funcalc command line, no output file is opened and no rows are written. This is useful in expressions that simply print output results instead of generating a new file: fpv = (cur->av3:D-cur->av1:D)\/(cur->av1+cur->av2:D+cur->av3);\nfbv =  cur->av2\/(cur->av1+cur->av2+cur->av3);\nfpu = ((double)cur->au3-cur->au1)\/((double)cur->au1+cur->au2+cur->au3);\nfbu =  cur->au2\/(double)(cur->au1+cur->au2+cur->au3);\nfprintf(stdout, \"%f\\t%f\\t%f\\t%f\\n\", fpv, fbv, fpu, fbu); In the above example, we use both explicit type specification (for \"av\" columns) and type casting (for \"au\" columns) to ensure that all operations are performed in double precision. When an output file is specified, the selected input table is processed and output rows are copied to the output file. Note that the output file can be specified as \"stdout\" in order to write the output rows to the standard output. If the output file argument is passed, an optional third argument also can be passed to specify which columns to process. In a FITS binary table, it sometimes is desirable to copy all of the other FITS extensions to the output file as well. This can be done by appending a '+' sign to the name of the extension in the input file name. See funtable for a related example. funcalc works by integrating the user-specified expression into a template C program called tabcalc.c. The completed program then is compiled and executed. Variable declarations that begin the funcalc expression are placed in the local declaration section of the template main program. All other lines are placed in the template main program's inner processing loop. Other details of program generation are handled automatically. For example, column specifiers are analyzed to build a C struct for processing rows, which is passed to FunColumnSelect() and used in FunTableRowGet(). If an unknown variable is used in the expression, resulting in a compilation error, the program build is retried after defining the unknown variable to be of type double. Normally, funcalc expression code is added to funcalc row processing loop. It is possible to add code to other parts of the program by placing this code inside special directives of the form: [directive name]\n  ... code goes here ...\nend The directives are: \u2022 global add code and declarations in global space, before the main routine. \u2022 local add declarations (and code) just after the local declarations in main \u2022 before add code just before entering the main row processing loop \u2022 after add code just after exiting the main row processing loop Thus, the following funcalc expression will declare global variables and make subroutine calls just before and just after the main processing loop: global\n  double v1, v2;\n  double init(void);\n  double finish(double v);\nend\nbefore\n  v1  = init();\nend\n... process rows, with calculations using v1 ...\nafter\n  v2 = finish(v1);\n  if( v2 < 0.0 ){\n    fprintf(stderr, \"processing failed %g -> %g\\n\", v1, v2);\n    exit(1);\n  }\nend Routines such as init() and finish() above are passed to the generated program for linking using the -l [link directives ...] switch. The string specified by this switch will be added to the link line used to build the program (before the funtools library). For example, assuming that init() and finish() are in the library libmysubs.a in the \/opt\/special\/lib directory, use: funcalc  -l \"-L\/opt\/special\/lib -lmysubs\" ... User arguments can be passed to a compiled funcalc program using a string argument to the \"-a\" switch. The string should contain all of the user arguments. For example, to pass the integers 1 and 2, use: funcalc -a \"1 2\" ... The arguments are stored in an internal array and are accessed as strings via the ARGV (n) macro. For example, consider the following expression: local\n  int pmin, pmax;\nend\n\nbefore\n  pmin=atoi(ARGV(0));\n  pmax=atoi(ARGV(1));\nend\n\nif( (cur->pha >= pmin) && (cur->pha <= pmax) )\n  fprintf(stderr, \"%d %d %d\\n\", cur->x, cur->y, cur->pha); This expression will print out x, y, and pha values for all rows in which the pha value is between the two user-input values: funcalc -a '1 12' -f foo snr.ev'[cir 512 512 .1]'\n512 512 6\n512 512 8\n512 512 5\n512 512 5\n512 512 8\n\nfuncalc -a '5 6' -f foo snr.ev'[cir 512 512 .1]'\n512 512 6\n512 512 5\n512 512 5 Note that it is the user's responsibility to ensure that the correct number of arguments are passed. The ARGV (n) macro returns a NULL if a requested argument is outside the limits of the actual number of args, usually resulting in a SEGV if processed blindly. To check the argument count, use the ARGC macro: local\n  long int seed=1;\n  double limit=0.8;\nend\n\nbefore\n  if( ARGC >= 1 ) seed = atol(ARGV(0));\n  if( ARGC >= 2 ) limit = atof(ARGV(1));\n  srand48(seed);\nend\n\nif ( drand48() > limit ) continue; The macro WRITE_ROW expands to the FunTableRowPut() call that writes the current row. It can be used to write the row more than once. In addition, the macro NROW expands to the row number currently being processed. Use of these two macros is shown in the following example: if( cur->pha:I == cur->pi:I ) continue;\na = cur->pha;\ncur->pha = cur->pi;\ncur->pi = a;\ncur->AVG:E  = (cur->pha+cur->pi)\/2.0;\ncur->NR:I = NROW;\nif( NROW < 10 ) WRITE_ROW; If the -p [prog] switch is specified, the expression is not executed. Rather, the generated executable is saved with the specified program name for later use. If the -n switch is specified, the expression is not executed. Rather, the generated code is written to stdout. This is especially useful if you want to generate a skeleton file and add your own code, or if you need to check compilation errors. Note that the comment at the start of the output gives the compiler command needed to build the program on that platform. (The command can change from platform to platform because of the use of different libraries, compiler switches, etc.) As mentioned previously, funcalc will declare a scalar variable automatically (as a double) if that variable has been used but not declared. This facility is implemented using a sed script named funcalc.sed, which processes the compiler output to sense an undeclared variable error. This script has been seeded with the appropriate error information for gcc, and for cc on Solaris, DecAlpha, and SGI platforms. If you find that automatic declaration of scalars is not working on your platform, check this sed script; it might be necessary to add to or edit some of the error messages it senses. In order to keep the lexical analysis of funcalc expressions (reasonably) simple, we chose to accept some limitations on how accurately C comments, spaces, and new-lines are placed in the generated program. In particular, comments associated with local variables declared at the beginning of an expression (i.e., not in a local...end block) will usually end up in the inner loop, not with the local declarations: \/* this comment will end up in the wrong place (i.e, inner loop) *\/\ndouble a; \/* also in wrong place *\/\n\/* this will be in the the right place (inner loop) *\/\nif( cur->x:D == cur->y:D ) continue; \/* also in right place *\/\na = cur->x;\ncur->x = cur->y;\ncur->y = a;\ncur->avg:E  = (cur->x+cur->y)\/2.0; Similarly, spaces and new-lines sometimes are omitted or added in a seemingly arbitrary manner. Of course, none of these stylistic blemishes affect the correctness of the generated code. Because funcalc must analyze the user expression using the data file(s) passed on the command line, the input file(s) must be opened and read twice: once during program generation and once during execution. As a result, it is not possible to use stdin for the input file: funcalc cannot be used as a filter. We will consider removing this restriction at a later time. Along with C comments, funcalc expressions can have one-line internal comments that are not passed on to the generated C program. These internal comment start with the # character and continue up to the new-line: double a; # this is not passed to the generated C file\n# nor is this\na = cur->x;\ncur->x = cur->y;\ncur->y = a;\n\/* this comment is passed to the C file *\/\ncur->avg:E  = (cur->x+cur->y)\/2.0; As previously mentioned, input columns normally are identified by their being used within the inner event loop. There are rare cases where you might want to read a column and process it outside the main loop. For example, qsort might use a column in its sort comparison routine that is not processed inside the inner loop (and therefore not implicitly specified as a column to be read). To ensure that such a column is read by the event loop, use the explicit keyword. The arguments to this keyword specify columns that should be read into the input record structure even though they are not mentioned in the inner loop. For example: explicit pi pha will ensure that the pi and pha columns are read for each row, even if they are not processed in the inner event loop. The explicit statement can be placed anywhere. Finally, note that funcalc currently works on expressions involving FITS binary tables and raw event files. We will consider adding support for image expressions at a later point, if there is demand for such support from the community.","Process Name":"funcalc","Link":"https:\/\/linux.die.net\/man\/1\/funcalc"}},{"Process":{"Description":null,"Process Name":"funcd","Link":"https:\/\/linux.die.net\/man\/1\/funcd"}},{"Process":{"Description":null,"Process Name":"funced","Link":"https:\/\/linux.die.net\/man\/1\/funced"}},{"Process":{"Description":"funcen iteratively calculates the centroid position within one or more regions of a Funtools table ( FITS binary table or raw event file). Starting with an input table, an initial region specification, and an iteration count, the program calculates the average x and y position within the region and then uses this new position as the region center for the next iteration. Iteration terminates when the maximum number of iterations is reached or when the input tolerance distance is met for that region. A count of events in the final region is then output, along with the pixel position value (and, where available, WCS position). The first argument to the program specifies the Funtools table file to process. Since the file must be read repeatedly, a value of \"stdin\" is not permitted when the number of iterations is non-zero. Use Funtools Bracket Notation to specify FITS extensions and filters. The second required argument is the initial region descriptor. Multiple regions are permitted. However, compound regions (accelerators, variable argument regions and regions connected via boolean algebra) are not permitted. Points and polygons also are illegal. These restrictions might be lifted in a future version, if warranted. The -n (iteration number) switch specifies the maximum number of iterations to perform. The default is 0, which means that the program will simply count and display the number of events in the initial region(s). Note that when iterations is 0, the data can be input via stdin. The -t (tolerance) switch specifies a floating point tolerance value. If the distance between the current centroid position value and the last position values is less than this value, iteration terminates. The default value is 1 pixel. The -v (verbosity) switch specifies the verbosity level of the output. The default is 0, which results in a single line of output for each input region consisting of the following values: counts x y [ra dec coordsys] The last 3 WCS values are output if WCS information is available in the data file header. Thus, for example: [sh] funcen -n 0 snr.ev \"cir 505 508 5\"\n915 505.00 508.00 345.284038 58.870920 j2000\n\n[sh] funcen -n 3 snr.ev \"cir 505 508 5\"\n1120 504.43 509.65 345.286480 58.874587 j2000 The first example simply counts the number of events in the initial region. The second example iterates the centroid calculation three times to determine a final \"best\" position. Higher levels of verbosity obviously imply more verbose output. At level 1, the output essentially contains the same information as level 0, but with keyword formatting: [sh] funcen -v 1 -n 3 snr.ev \"cir 505 508 5\"\nevent_file:     snr.ev\ninitial_region: cir 505 508 5\ntolerance:      1.0000\niterations:     1\n\nevents:         1120\nx,y(physical):  504.43 509.65\nra,dec(j2000):  345.286480 58.874587\nfinal_region1:  cir 504.43 509.65 5 Level 2 outputs results from intermediate calculations as well. Ordinarily, region filtering is performed using analytic (event) filtering, i.e. that same style of filtering as is performed by fundisp and funtable. Use the -i switch to specify image filtering, i.e. the same style filtering as is performed by funcnts. Thus, you can perform a quick calculation of counts in regions, using either the analytic or image filtering method, by specifying the -n 0 and optional -i switches. These two method often give different results because of how boundary events are processed: [sh] funcen  snr.ev \"cir 505 508 5\"\n915 505.00 508.00 345.284038 58.870920 j2000\n\n[sh] funcen -i snr.ev \"cir 505 508 5\"\n798 505.00 508.00 345.284038 58.870920 j2000 See Region Boundaries for more information about how boundaries are calculated using these two methods.","Process Name":"funcen","Link":"https:\/\/linux.die.net\/man\/1\/funcen"}},{"Process":{"Description":"funcnts counts photons in the specified source regions and reports the results for each region. Regions are specified using the Spatial Region Filtering mechanism. Photons are also counted in the specified bkgd regions applied to the same data file or a different data file. (Alternatively, a constant background value in counts\/pixel**2 can be specified.) The bkgd regions are either paired one-to-one with source regions or pooled and normalized by area, and then subtracted from the source counts in each region. Displayed results include the bkgd-subtracted counts in each region, as well as the error on the counts, the area in each region, and the surface brightness (cnts\/area**2) calculated for each region. The first argument to the program specifies the FITS input image, array, or raw event file to process. If \"stdin\" is specified, data are read from the standard input. Use Funtools Bracket Notation to specify FITS extensions, image sections, and filters. The optional second argument is the source region descriptor. If no region is specified, the entire field is used. The background arguments can take one of two forms, depending on whether a separate background file is specified. If the source file is to be used for background as well, the third argument can be either the background region, or a constant value denoting background cnts\/pixel. Alternatively, the third argument can be a background data file, in which case the fourth argument is the background region. If no third argument is specified, a constant value of 0 is used (i.e., no background). In summary, the following command arguments are valid: [sh] funcnts sfile                        # counts in source file\n[sh] funcnts sfile sregion                # counts in source region\n[sh] funcnts sfile sregion bregion        # bkgd reg. is from source file\n[sh] funcnts sfile sregion bvalue         # bkgd reg. is constant\n[sh] funcnts sfile sregion bfile bregion  # bkgd reg. is from separate file NB: unlike other Funtools programs, source and background regions are specified as separate arguments on the command line, rather than being placed inside brackets as part of the source and background filenames. This is because regions in funcnts are not simply used as data filters, but also are used to calculate areas, exposure, etc. If you put the source region inside the brackets (i.e. use it simply as a filter) rather than specifying it as argument two, the program still will only count photons that pass the region filter. However, the area calculation will be performed on the whole field, since field() is the default source region. This rarely is the desired behavior. On the other hand, with FITS binary tables, it often is useful to put a column filter in the filename brackets, so that only events matching the column filter are counted inside the region. For example, to extract the counts within a radius of 22 pixels from the center of the FITS binary table snr.ev and subtract the background determined from the same image within an annulus of radii 50-100 pixels: [sh] funcnts snr.ev \"circle(502,512,22)\" \"annulus(502,512,50,100)\"\n# source\n#   data file:        snr.ev\n#   degrees\/pix:      0.00222222\n# background\n#   data file:        snr.ev\n# column units\n#   area:             arcsec**2\n#   surf_bri:         cnts\/arcsec**2\n#   surf_err:         cnts\/arcsec**2\n\n# background-subtracted results\n reg   net_counts     error   background    berror      area  surf_bri  surf_err\n---- ------------ --------- ------------ --------- --------- --------- ---------\n   1     3826.403    66.465      555.597     5.972  96831.98     0.040     0.001\n\n# the following source and background components were used:\nsource region(s)\n----------------\ncircle(502,512,22)\n\n reg       counts    pixels\n---- ------------ ---------\n   1     4382.000      1513\n\nbackground region(s)\n--------------------\nannulus(502,512,50,100)\n\n reg       counts    pixels\n---- ------------ ---------\nall      8656.000     23572 The area units for the output columns labeled \"area\", \"surf_bri\" (surface brightness) and \"surf_err\" will be given either in arc-seconds (if appropriate WCS information is in the data file header(s)) or in pixels. If the data file has WCS info, but you do not want arc-second units, use the -p switch to force output in pixels. Also, regions having zero area are not normally included in the primary (background-subtracted) table, but are included in the secondary source and bkgd tables. If you want these regions to be included in the primary table, use the -z switch. Note that a simple sed command will extract the background-subtracted results for further analysis: [sh] cat funcnts.sed\n1,\/---- .*\/d\n\/^$\/,$d\n\n[sh] sed -f funcnts.sed funcnts.out\n1     3826.403    66.465      555.597     5.972  96831.98     0.040     0.001 If separate source and background files are specified, funcnts will attempt to normalize the the background area so that the background pixel size is the same as the source pixel size. This normalization can only take place if the appropriate WCS information is contained in both files (e.g. degrees\/pixel values in CDELT ). If either file does not contain the requisite size information, the normalization is not performed. In this case, it is the user's responsibility to ensure that the pixel sizes are the same for the two files. Normally, if more than one background region is specified, funcnts will combine them all into a single region and use this background region to produce the background-subtracted results for each source region. The -m (match multiple backgrounds) switch tells funcnts to make a one to one correspondence between background and source regions, instead of using a single combined background region. For example, the default case is to combine 2 background regions into a single region and then apply that region to each of the source regions: [sh] funcnts snr.ev \"annulus(502,512,0,22,n=2)\" \"annulus(502,512,50,100,n=2)\"\n# source\n#   data file:        snr.ev\n#   degrees\/pix:      0.00222222\n# background\n#   data file:        snr.ev\n# column units\n#   area:             arcsec**2\n#   surf_bri:         cnts\/arcsec**2\n#   surf_err:         cnts\/arcsec**2\n\n# background-subtracted results\n reg   net_counts     error   background    berror      area  surf_bri  surf_err\n---- ------------ --------- ------------ --------- --------- --------- ---------\n   1     3101.029    56.922      136.971     1.472  23872.00     0.130     0.002\n   2      725.375    34.121      418.625     4.500  72959.99     0.010     0.000\n\n# the following source and background components were used:\nsource region(s)\n----------------\nannulus(502,512,0,22,n=2)\n\n reg       counts    pixels\n---- ------------ ---------\n   1     3238.000       373\n   2     1144.000      1140\n\nbackground region(s)\n--------------------\nannulus(502,512,50,100,n=2)\n\n reg       counts    pixels\n---- ------------ ---------\nall      8656.000     23572 Note that the basic region filter rule \"each photon is counted once and no photon is counted more than once\" still applies when using The -m to match background regions. That is, if two background regions overlap, the overlapping pixels will be counted in only one of them. In a worst-case scenario, if two background regions are the same region, the first will get all the counts and area and the second will get none. Using the -m switch causes funcnts to use each of the two background regions independently with each of the two source regions: [sh] funcnts -m snr.ev \"annulus(502,512,0,22,n=2)\" \"ann(502,512,50,100,n=2)\"\n# source\n#   data file:        snr.ev\n#   degrees\/pix:      0.00222222\n# background\n#   data file:        snr.ev\n# column units\n#   area:             arcsec**2\n#   surf_bri:         cnts\/arcsec**2\n#   surf_err:         cnts\/arcsec**2\n\n# background-subtracted results\n reg   net_counts     error   background    berror      area  surf_bri  surf_err\n---- ------------ --------- ------------ --------- --------- --------- ---------\n   1     3087.015    56.954      150.985     2.395  23872.00     0.129     0.002\n   2      755.959    34.295      388.041     5.672  72959.99     0.010     0.000\n\n# the following source and background components were used:\nsource region(s)\n----------------\nannulus(502,512,0,22,n=2)\n\n reg       counts    pixels\n---- ------------ ---------\n   1     3238.000       373\n   2     1144.000      1140\n\nbackground region(s)\n--------------------\nann(502,512,50,100,n=2)\n\n reg       counts    pixels\n---- ------------ ---------\n   1     3975.000      9820\n   2     4681.000     13752 Note that most floating point quantities are displayed using \"f\" format. You can change this to \"g\" format using the -g switch. This can be useful when the counts in each pixel is very small or very large. If you want maximum precision and don't care about the columns lining up nicely, use -G, which outputs all floating values as %.14g. When counting photons using the annulus and panda (pie and annuli) shapes, it often is useful to have access to the radii (and panda angles) for each separate region. The -r switch will add radii and angle columns to the output table: [sh] funcnts -r snr.ev \"annulus(502,512,0,22,n=2)\" \"ann(502,512,50,100,n=2)\"\n# source\n#   data file:        snr.ev\n#   degrees\/pix:      0.00222222\n# background\n#   data file:        snr.ev\n# column units\n#   area:             arcsec**2\n#   surf_bri:         cnts\/arcsec**2\n#   surf_err:         cnts\/arcsec**2\n#   radii:            arcsecs\n#   angles:           degrees\n\n# background-subtracted results\n reg   net_counts     error   background    berror      area  surf_bri  surf_err   radius1   radius2    angle1    angle2\n---- ------------ --------- ------------ --------- --------- --------- --------- --------- --------- --------- ---------\n   1     3101.029    56.922      136.971     1.472  23872.00     0.130     0.002      0.00     88.00        NA        NA\n   2      725.375    34.121      418.625     4.500  72959.99     0.010     0.000     88.00    176.00        NA        NA\n\n# the following source and background components were used:\nsource region(s)\n----------------\nannulus(502,512,0,22,n=2)\n\n reg       counts    pixels\n---- ------------ ---------\n   1     3238.000       373\n   2     1144.000      1140\n\nbackground region(s)\n--------------------\nann(502,512,50,100,n=2)\n\n reg       counts    pixels\n---- ------------ ---------\nall      8656.000     23572 Radii are given in units of pixels or arc-seconds (depending on the presence of WCS info), while the angle values (when present) are in degrees. These columns can be used to plot radial profiles. For example, the script funcnts.plot in the funtools distribution) will plot a radial profile using gnuplot (version 3.7 or above). A simplified version of this script is shown below: #!\/bin\/sh\n\nif [ x\"$1\" = xgnuplot ]; then\n  if [ x'which gnuplot 2>\/dev\/null' = x ]; then\n    echo \"ERROR: gnuplot not available\"\n    exit 1\n  fi\n  awk '\n  BEGIN{HEADER=1; DATA=0; FILES=\"\"; XLABEL=\"unknown\"; YLABEL=\"unknown\"}\n  HEADER==1{\n    if( $1 == \"#\" && $2 == \"data\" && $3 == \"file:\" ){\n      if( FILES != \"\" ) FILES = FILES \",\"\n      FILES = FILES $4\n    }\n    else if( $1 == \"#\" && $2 == \"radii:\" ){\n      XLABEL = $3\n    }\n    else if( $1 == \"#\" && $2 == \"surf_bri:\" ){\n      YLABEL = $3\n    }\n    else if( $1 == \"----\" ){\n      printf \"set nokey; set title \\\"funcnts(%s)\\\"\\n\", FILES\n      printf \"set xlabel \\\" radius(%s)\\\"\\n\", XLABEL\n      printf \"set ylabel \\\"surf_bri(%s)\\\"\\n\", YLABEL\n      print  \"plot \\\"-\\\" using 3:4:6:7:8 with boxerrorbars\"\n      HEADER = 0\n      DATA = 1\n      next\n    }\n  }\n  DATA==1{\n    if( NF == 12 ){\n      print $9, $10, ($9+$10)\/2, $7, $8, $7-$8, $7+$8, $10-$9\n    }\n    else{\n      exit\n    }\n  }\n  ' │ gnuplot -persist - 1>\/dev\/null 2>&1\n\nelif [ x\"$1\" = xds9 ]; then\n  awk '\n  BEGIN{HEADER=1; DATA=0; XLABEL=\"unknown\"; YLABEL=\"unknown\"}\n  HEADER==1{\n    if( $1 == \"#\" && $2 == \"data\" && $3 == \"file:\" ){\n      if( FILES != \"\" ) FILES = FILES \",\"\n      FILES = FILES $4\n    }\n    else if( $1 == \"#\" && $2 == \"radii:\" ){\n      XLABEL = $3\n    }\n    else if( $1 == \"#\" && $2 == \"surf_bri:\" ){\n      YLABEL = $3\n    }\n    else if( $1 == \"----\" ){\n      printf \"funcnts(%s) radius(%s) surf_bri(%s) 3\\n\", FILES, XLABEL, YLABEL\n      HEADER = 0\n      DATA = 1\n      next\n    }\n  }\n  DATA==1{\n    if( NF == 12 ){\n      print $9, $7, $8\n    }\n    else{\n      exit\n    }\n  }\n  '\nelse\n  echo \"funcnts -r ... │ funcnts.plot [ds9│gnuplot]\"\n  exit 1\nfi Thus, to run funcnts and plot the results using gnuplot (version 3.7 or above), use: funcnts -r snr.ev \"annulus(502,512,0,50,n=5)\" ...  │ funcnts.plot gnuplot The -s (sum) switch causes funcnts to produce an additional table of summed (integrated) background subtracted values, along with the default table of individual values: [sh] funcnts -s snr.ev \"annulus(502,512,0,50,n=5)\" \"annulus(502,512,50,100)\"\n# source\n#   data file:        snr.ev\n#   degrees\/pix:      0.00222222\n# background\n#   data file:        snr.ev\n# column units\n#   area:             arcsec**2\n#   surf_bri:         cnts\/arcsec**2\n#   surf_err:         cnts\/arcsec**2\n\n# summed background-subtracted results\nupto   net_counts     error   background    berror      area  surf_bri  surf_err\n---- ------------ --------- ------------ --------- --------- --------- ---------\n   1     2880.999    54.722      112.001     1.204  19520.00     0.148     0.003\n   2     3776.817    65.254      457.183     4.914  79679.98     0.047     0.001\n   3     4025.492    71.972     1031.508    11.087 179775.96     0.022     0.000\n   4     4185.149    80.109     1840.851    19.786 320831.94     0.013     0.000\n   5     4415.540    90.790     2873.460    30.885 500799.90     0.009     0.000\n\n# background-subtracted results\n reg       counts     error   background    berror      area  surf_bri  surf_err\n---- ------------ --------- ------------ --------- --------- --------- ---------\n   1     2880.999    54.722      112.001     1.204  19520.00     0.148     0.003\n   2      895.818    35.423      345.182     3.710  60159.99     0.015     0.001\n   3      248.675    29.345      574.325     6.173 100095.98     0.002     0.000\n   4      159.657    32.321      809.343     8.699 141055.97     0.001     0.000\n   5      230.390    37.231     1032.610    11.099 179967.96     0.001     0.000\n\n# the following source and background components were used:\nsource region(s)\n----------------\nannulus(502,512,0,50,n=5)\n\n reg       counts    pixels      sumcnts    sumpix\n---- ------------ --------- ------------ ---------\n   1     2993.000       305     2993.000       305\n   2     1241.000       940     4234.000      1245\n   3      823.000      1564     5057.000      2809\n   4      969.000      2204     6026.000      5013\n   5     1263.000      2812     7289.000      7825\n\nbackground region(s)\n--------------------\nannulus(502,512,50,100)\n\n reg       counts    pixels\n---- ------------ ---------\nall      8656.000     23572 The -t and -e switches can be used to apply timing and exposure corrections, respectively, to the data. Please note that these corrections are meant to be used qualitatively, since application of more accurate correction factors is a complex and mission-dependent effort. The algorithm for applying these simple corrections is as follows: C =  Raw Counts in Source Region\nAc=  Area of Source Region\nTc=  Exposure time for Source Data\nEc=  Average exposure in Source Region, from exposure map\n\nB=   Raw Counts in Background Region\nAb=  Area of Background Region\nTb=  (Exposure) time for Background Data\nEb=  Average exposure in Background Region, from exposure map Then, Net Counts in Source region is Net=  C - B * (Ac*Tc*Ec)\/(Ab*Tb*Eb) with the standard propagation of errors for the Error on Net. The net rate would then be Net Rate = Net\/(Ac*Tc*Ec) The average exposure in each region is calculated by summing up the pixel values in the exposure map for the given region and then dividing by the number of pixels in that region. Exposure maps often are generated at a block factor > 1 (e.g., block 4 means that each exposure pixel contains 4x4 pixels at full resolution) and funcnts will deal with the blocking automatically. Using the -e switch, you can supply both source and background exposure files (separated by \";\"), if you have separate source and background data files. If you do not supply a background exposure file to go with a separate background data file, funcnts assumes that exposure already has been applied to the background data file. In addition, it assumes that the error on the pixels in the background data file is zero. NB: The -e switch assumes that the exposure map overlays the image file exactly, except for the block factor. Each pixel in the image is scaled by the block factor to access the corresponding pixel in the exposure map. If your exposure map does not line up exactly with the image, do not use the -e exposure correction. In this case, it still is possible to perform exposure correction if both the image and the exposure map have valid WCS information: use the -w switch so that the transformation from image pixel to exposure pixel uses the WCS information. That is, each pixel in the image region will be transformed first from image coordinates to sky coordinates, then from sky coordinates to exposure coordinates. Please note that using -w can increase the time required to process the exposure correction considerably. A time correction can be applied to both source and background data using the -t switch. The value for the correction can either be a numeric constant or the name of a header parameter in the source (or background) file: [sh] funcnts -t 23.4 ...            # number for source\n[sh] funcnts -t \"LIVETIME;23.4\" ... # param for source, numeric for bkgd When a time correction is specified, it is applied to the net counts as well (see algorithm above), so that the units of surface brightness become cnts\/area**2\/sec. The -i (interval) switch is used to run funcnts on multiple column-based intervals with only a single pass through the data. It is equivalent to running funcnts several times with a different column filter added to the source and background data each time. For each interval, the full funcnts output is generated, with a linefeed character (^L) inserted between each run. In addition, the output for each interval will contain the interval specification in its header. Intervals are very useful for generating X-ray hardness ratios efficiently. Of course, they are only supported when the input data are contained in a table. Two formats are supported for interval specification. The most general format is semi-colon-delimited list of filters to be used as intervals: funcnts -i \"pha=1:5;pha=6:10;pha=11:15\" snr.ev \"circle(502,512,22)\" ... Conceptually, this will be equivalent to running funcnts three times: funcnts snr.ev'[pha=1:5]' \"circle(502,512,22)\"\nfuncnts snr.ev'[pha=6:10]' \"circle(502,512,22)\"\nfuncnts snr.ev'[pha=11:15]' \"circle(502,512,22)\" However, using the -i switch will require only one pass through the data. Note that complex filters can be used to specify intervals: funcnts -i \"pha=1:5&&pi=4;pha=6:10&&pi=5;pha=11:15&&pi=6\" snr.ev ... The program simply runs the data through each filter in turn and generates three funcnts outputs, separated by the line-feed character. In fact, although the intent is to support intervals for hardness ratios, the specified filters do not have to be intervals at all. Nor does one \"interval\" filter have to be related to another. For example: funcnts -i \"pha=1:5;pi=6:10;energy=11:15\" snr.ev \"circle(502,512,22)\" ... is equivalent to running funcnts three times with unrelated filter specifications. A second interval format is supported for the simple case in which a single column is used to specify multiple homogeneous intervals for that column. In this format, a column name is specified first, followed by intervals: funcnts -i \"pha;1:5;6:10;11:15\" snr.ev \"circle(502,512,22)\" ... This is equivalent to the first example, but requires less typing. The funcnts program will simply prepend \"pha=\" before each of the specified intervals. (Note that this format does not contain the \"=\" character in the column argument.) Ordinarily, when funcnts is run on a FITS binary table (or a raw event table), one integral count is accumulated for each row (event) contained within a given region. The -v \"scol[;bcol]\" (value column) switch will accumulate counts using the value from the specified column for the given event. If only a single column is specified, it is used for both the source and background regions. Two separate columns, separated by a semi-colon, can be specified for source and background. The special token '$none' can be used to specify that a value column is to be used for one but not the other. For example, 'pha;$none' will use the pha column for the source but use integral counts for the background, while '$none;pha' will do the converse. If the value column is of type logical, then the value used will be 1 for T and 0 for F. Value columns are used, for example, to integrate probabilities instead of integral counts. If the -T (rdb table) switch is used, the output will conform to starbase\/rdb data base format: tabs will be inserted between columns rather than spaces and line-feed will be inserted between tables. Finally, note that funcnts is an image program, even though it can be run directly on FITS binary tables. This means that image filtering is applied to the rows in order to ensure that the same results are obtained regardless of whether a table or the equivalent binned image is used. Because of this, however, the number of counts found using funcnts can differ from the number of events found using row-filter programs such as fundisp or funtable For more information about these difference, see the discussion of Region Boundaries.","Process Name":"funcnts","Link":"https:\/\/linux.die.net\/man\/1\/funcnts"}},{"Process":{"Description":"Funcone performs a cone search on the RA and Dec columns of a FITS binary table. The distance from the center RA , Dec position to the RA , Dec in each row in the table is calculated. Rows whose distance is less than the specified radius are output. The first argument to the program specifies the FITS file, raw event file, or raw array file. If \"stdin\" is specified, data are read from the standard input. Use Funtools Bracket Notation to specify FITS extensions, and filters. The second argument is the output FITS file. If \"stdout\" is specified, the FITS binary table is written to the standard output. The third and fourth required arguments are the RA and Dec center position. By default, RA is specified in hours while Dec is specified in degrees. You can change the units of either of these by appending the character \"d\" (degrees), \"h\" (hours) or \"r\" (radians). Sexagesimal notation is supported, with colons or spaces separating hms and dms. (When using spaces, please ensure that the entire string is quoted.) The fifth required argument is the radius of the cone search. By default, the radius value is given in degrees. The units can be changed by appending the character \"d\" (degrees), \"r\" (radians), \"'\" (arc minutes) or '\"' (arc seconds). By default, all columns of the input file are copied to the output file. Selected columns can be output using an optional sixth argument in the form: \"column1 column1 ... columnN\" A seventh argument allows you to output selected columns from the list file when -j switch is used. Note that the RA and Dec columns used in the cone calculation must not be de-selected. Also by default, the RA and Dec column names are named \" RA \" and \"Dec\", and are given in units of hours and degrees respectively. You can change both the name and the units using the -r [ RA ] and\/or -d [Dec] switches. Once again, one of \"h\", \"d\", or \"r\" is appended to the column name to specify units but in this case, there must be a colon \":\" between the name and the unit specification. If the -l [listfile] switch is used, then one or more of the center RA , center Dec, and radius can be taken from a list file (which can be a FITS table or an ASCII column text file). In this case, the third (center RA ), fourth (center Dec), and fifth (radius) command line arguments can either be a column name in the list file (if that parameter varies) or else a numeric value (if that parameter is static). When a column name is specified for the RA , Dec, or radius, you can append a colon followed by \"h\", \"d\", or \"r\" to specify units (also ' and \" for radius). The cone search algorithm is run once for each row in the list, taking RA , Dec, and radius values from the specified columns or from static numeric values specified on the command line. When using a list, all valid rows from each iteration are written to a single output file. Use the -x switch to help delineate which line of the list file was used to produce the given output row(s). This switch causes the values for the center RA , Dec, radius, and row number to be appended to the output file, in columns called RA_CEN , DEC_CEN , RAD_CEN and CONE_KEY , respectively. Alternatively, the -j (join) switch will append all columns from the list row to the output row (essentially a join of the list row and input row), along with the CONE_KEY row number. These two switches are mutually exclusive. The -X and -J switches write out the same data as their lower case counterparts for each row satisfying a cone search. In addition, these switches also write out rows from the event file that do not satisfy any cone search. In such cases, that CONE_KEY column will be given a value of -1 and the center and list position information will be set to zero for the given row. Thus, all rows of the input event file are guaranteed to be output, with rows satisfying at least one cone search having additional search information. The -L switch acts similarly to the -l switch in that it takes centers from a list file. However, it also implicitly sets the -j switch, so that output rows are the join of the input event row and the center position row. In addition, this switch also writes out all center position rows for which no event satisfies the cone search criteria of that row. The CONE_KEY column will be given a value of -2 for center rows that were not close to any data row and the event columns will be zeroed out for such rows. In this way, all centers rows are guaranteed to be output at least once. If any of \"all row\" switches (-X, -J, or -L) are specified, then a new column named JSTAT is added to the output table. The positive values in this column indicate the center position row number (starting from 1) in the list file that this data row successful matched in a cone search. A value of -1 means that the data row did not match any center position. A value of -2 means that the center position was not matched by any data row. Given a center position and radius, the cone search algorithm calculates limit parameters for a box enclosing the specified cone, and only tests rows whose positions values lie within those limits. For small files, the overhead associated with this cone limit filtering can cause the program to run more slowly than if all events were tested. You can turn off cone limit filtering using the -n switch to see if this speeds up the processing (especially useful when processing a large list of positions). For example, the default cone search uses columns \" RA \" and \"Dec\" in hours and degrees (respectively) and RA position in hours, Dec and radius in degrees: funone in.fits out.fits 23.45 34.56 0.01 To specify the RA position in degrees: funcone in.fits out.fits 23.45d 34.56 0.01 To get RA and Dec from a list but use a static value for radius (and also write identifying info for each row in the list): funcone -x -l list.txt in.fits out.fits MYRA MYDec 0.01 User specified columns in degrees, RA position in hours (sexagesimal notation), Dec position in degrees (sexagesimal notation) and radius in arc minutes: funcone -r myRa:d -d myDec in.fits out.fits 12:30:15.5 30:12 15'","Process Name":"funcone","Link":"https:\/\/linux.die.net\/man\/1\/funcone"}},{"Process":{"Description":"","Process Name":"funcsave","Link":"https:\/\/linux.die.net\/man\/1\/funcsave"}},{"Process":{"Description":"","Process Name":"function","Link":"https:\/\/linux.die.net\/man\/1\/function"}},{"Process":{"Description":null,"Process Name":"functions","Link":"https:\/\/linux.die.net\/man\/1\/functions"}},{"Process":{"Description":"fundisp displays the data in the specified FITS Extension and\/or Image Section of a FITS file, or in a Section of a non-FITS array or raw event file. The first argument to the program specifies the FITS input image, array, or raw event file to display. If \"stdin\" is specified, data are read from the standard input. Use Funtools Bracket Notation to specify FITS extensions, image sections, and filters. If the data being displayed are columns (either in a FITS binary table or a raw event file), the individual rows are listed. Filters can be added using bracket notation. Thus: [sh] fundisp \"test.ev[time-(int)time>.15]\"\n       X       Y     PHA        PI             TIME         DX         DY\n ------- ------- ------- --------- ---------------- ---------- ----------\n      10       8      10         8          17.1600       8.50      10.50\n       9       9       9         9          17.1600       9.50       9.50\n      10       9      10         9          18.1600       9.50      10.50\n      10       9      10         9          18.1700       9.50      10.50\n       8      10       8        10          17.1600      10.50       8.50\n       9      10       9        10          18.1600      10.50       9.50\n       9      10       9        10          18.1700      10.50       9.50\n      10      10      10        10          19.1600      10.50      10.50\n      10      10      10        10          19.1700      10.50      10.50\n      10      10      10        10          19.1800      10.50      10.50 [ NB: The FITS binary table test file test.ev, as well as the FITS image test.fits, are contained in the funtools funtest directory.] When a table is being displayed using fundisp, a second optional argument can be used to specify the columns to display. For example: [sh] fundisp \"test.ev[time-(int)time>=.99]\" \"x y time\"\n        X        Y                  TIME\n -------- -------- ---------------------\n        5       -6           40.99000000\n        4       -5           59.99000000\n       -1        0          154.99000000\n       -2        1          168.99000000\n       -3        2          183.99000000\n       -4        3          199.99000000\n       -5        4          216.99000000\n       -6        5          234.99000000\n       -7        6          253.99000000 The special column $REGION can be specified to display the region id of each row: [sh $] fundisp \"test.ev[time-(int)time>=.99&&annulus(0 0 0 10 n=3)]\" 'x y time $REGION'\n        X        Y                  TIME     REGION\n -------- -------- --------------------- ----------\n        5       -6           40.99000000          3\n        4       -5           59.99000000          2\n       -1        0          154.99000000          1\n       -2        1          168.99000000          1\n       -3        2          183.99000000          2\n       -4        3          199.99000000          2\n       -5        4          216.99000000          2\n       -6        5          234.99000000          3\n       -7        6          253.99000000          3 Here only rows with the proper fractional time and whose position also is within one of the three annuli are displayed. Columns can be excluded from display using a minus sign before the column: [sh $] fundisp \"test.ev[time-(int)time>=.99]\" \"-time\"\n        X        Y      PHA         PI          DX          DY\n -------- -------- -------- ---------- ----------- -----------\n        5       -6        5         -6        5.50       -6.50\n        4       -5        4         -5        4.50       -5.50\n       -1        0       -1          0       -1.50        0.50\n       -2        1       -2          1       -2.50        1.50\n       -3        2       -3          2       -3.50        2.50\n       -4        3       -4          3       -4.50        3.50\n       -5        4       -5          4       -5.50        4.50\n       -6        5       -6          5       -6.50        5.50\n       -7        6       -7          6       -7.50        6.50 All columns except the time column are displayed. The special column $N can be specified to display the ordinal value of each row. Thus, continuing the previous example: fundisp \"test.ev[time-(int)time>=.99]\" '-time $n'\n       X        Y      PHA         PI          DX          DY          N\n ------- -------- -------- ---------- ----------- ----------- ----------\n       5       -6        5         -6        5.50       -6.50        337\n       4       -5        4         -5        4.50       -5.50        356\n      -1        0       -1          0       -1.50        0.50        451\n      -2        1       -2          1       -2.50        1.50        465\n      -3        2       -3          2       -3.50        2.50        480\n      -4        3       -4          3       -4.50        3.50        496\n      -5        4       -5          4       -5.50        4.50        513\n      -6        5       -6          5       -6.50        5.50        531\n      -7        6       -7          6       -7.50        6.50        550 Note that the column specification is enclosed in single quotes to protect '$n' from begin expanded by the shell. In general, the rules for activating and de-activating columns are: \u2022 If only exclude columns are specified, then all columns but the exclude columns will be activated. \u2022 If only include columns are specified, then only the specified columns are activated. \u2022 If a mixture of include and exclude columns are specified, then all but the exclude columns will be active; this last case is ambiguous and the rule is arbitrary. In addition to specifying columns names explicitly, the special symbols + and - can be used to activate and de-activate all columns. This is useful if you want to activate the $REGION column along with all other columns. According to the rules, the syntax \"$REGION\" only activates the region column and de-activates the rest. Use \"+ $REGION\" to activate all columns as well as the region column. If the data being displayed are image data (either in a FITS primary image, a FITS image extension, or an array file), an mxn pixel display is produced, where m and n are the dimensions of the image. By default, pixel values are displayed using the same data type as in the file. However, for integer data where the BSCALE and BZERO header parameters are present, the data is displayed as floats. In either case, the display data type can be overridden using an optional second argument of the form: bitpix=n where n is 8,16,32,-32,-64, for unsigned char, short, int, float and double, respectively. Of course, running fundisp on anything but the smallest image usually results in a display whose size makes it unreadable. Therefore, one can uses bracket notation (see below) to apply section and\/or blocking to the image before generating a display. For example: [sh] fundisp \"test.fits[2:6,2:7]\" bitpix=-32\n                   2          3          4          5          6\n          ---------- ---------- ---------- ---------- ----------\n       2:       3.00       4.00       5.00       6.00       7.00\n       3:       4.00       5.00       6.00       7.00       8.00\n       4:       5.00       6.00       7.00       8.00       9.00\n       5:       6.00       7.00       8.00       9.00      10.00\n       6:       7.00       8.00       9.00      10.00      11.00\n       7:       8.00       9.00      10.00      11.00      12.00 Note that is is possible to display a FITS binary table as an image simply by passing the table through funimage first: [sh] .\/funimage test.ev stdout │ fundisp \"stdin[2:6,2:7]\" bitpix=8\n                2       3       4       5       6\n          ------- ------- ------- ------- -------\n       2:       3       4       5       6       7\n       3:       4       5       6       7       8\n       4:       5       6       7       8       9\n       5:       6       7       8       9      10\n       6:       7       8       9      10      11\n       7:       8       9      10      11      12 If the -l (list) switch is used, then an image is displayed as a list containing the columns: X, Y, VAL . For example: fundisp -l \"test1.fits[2:6,2:7]\" bitpix=-32\n          X          Y         VAL\n ---------- ---------- -----------\n          2          2        6.00\n          3          2        1.00\n          4          2        1.00\n          5          2        1.00\n          6          2        1.00\n          2          3        1.00\n          3          3        5.00\n          4          3        1.00\n          5          3        1.00\n          6          3        1.00\n          2          4        1.00\n          3          4        1.00\n          4          4        4.00\n          5          4        1.00\n          6          4        1.00\n          2          5        1.00\n          3          5        1.00\n          4          5        1.00\n          5          5        3.00\n          6          5        1.00\n          2          6        1.00\n          3          6        1.00\n          4          6        1.00\n          5          6        1.00\n          6          6        2.00\n          2          7        1.00\n          3          7        1.00\n          4          7        1.00\n          5          7        1.00\n          6          7        1.00 If the -n (nohead) switch is used, then no header is output for tables. This is useful, for example, when fundisp output is being directed into gnuplot. The fundisp program uses a default set of display formats: datatype      TFORM   format\n--------      -----   --------\ndouble        D       \"%21.8f\"\nfloat         E       \"%11.2f\"\nint           J       \"%10d\"\nshort         I       \"%8d\"\nbyte          B       \"%6d\"\nstring        A       \"%12.12s\"\nbits          X       \"%8x\"\nlogical       L       \"%1x\" Thus, the default display of 1 double and 2 shorts gives: [sh] fundisp snr.ev \"time x y\"\n\n                  TIME        X        Y\n --------------------- -------- --------\n     79494546.56818075      546      201\n     79488769.94469175      548      201\n     ... You can change the display format for individual columns or for all columns of a given data types by means of the -f switch. The format string that accompanies -f is a space-delimited list of keyword=format values. The keyword values can either be column names (in which case the associated format pertains only to that column) or FITS table TFORM specifiers (in which case the format pertains to all columns having that data type). For example, you can change the double and short formats for all columns like this: [sh] fundisp -f \"D=%22.11f I=%3d\" snr.ev \"time x y\"\n\n                  TIME   X   Y\n---------------------- --- ---\n  79494546.56818075478 546 201\n  79488769.94469174743 548 201\n  ... Alternatively, you can change the format of the time and x columns like this: [sh] fundisp -f \"time=%22.11f x=%3d\" snr.ev \"time x y\"\n\n                  TIME   X        Y\n---------------------- --- --------\n  79494546.56818075478 546      201\n  79488769.94469174743 548      201\n  ... Note that there is a potential conflict if a column has the same name as one of the TFORM specifiers. In the examples above, the the \"X\" column in the table has the same name as the X (bit) datatype. To resolve this conflict, the format string is processed such that TFORM datatype specifiers are checked for first, using a case-sensitive comparison. If the specified format value is not an upper case TFORM value, then a case-insensitive check is made on the column name. This means that, in the examples above, \"X=%3d\" will refer to the X (bit) datatype, while \"x=%3d\" will refer to the X column: [sh] fundisp -f \"X=%3d\" snr.ev \"x y\"\n\n       X        Y\n-------- --------\n     546      201\n     548      201\n     ...\n\n[sh] fundisp -f \"x=%3d\" snr.ev \"x y\"\n\n  X        Y\n--- --------\n546      201\n548      201\n... As a rule, therefore, it is best always to specify the column name in lower case and TFORM data types in upper case. The -f [format] will change the format for a single execution of fundisp. You also can use the FUN_FORMAT envronment variable to change the format for all invocations of fundisp. The format of this environment variable's value is identical to that used with the -f switch. This global value can be overridden in individual cases by use of the -f [format] switch. Caveats: Please also note that it is the user's responsibility to match the format specifier to the column data type correctly. Also note that, in order to maintain visual alignment between names and columns, the column name will be truncated (on the left) if the format width is less than the length of the name. However, truncation is not performed if the output is in RDB format (using the -T switch). [An older-style format string is supported but deprecated. It consists of space-delimited C format statements for all data types, specified in the following order: double float int short byte string bit. This order of the list is based on the assumption that people generally will want to change the float formats. If \"-\" is entered instead of a format statement for a given data type, the default format is used. Also, the format string can be terminated without specifying all formats, and defaults will be used for the rest of the list. Note that you must supply a minimum field width, i.e., \"%6d\" and \"%-6d\" are legal, \"%d\" is not legal. By using -f [format], you can change the double and short formats like this: [sh] fundisp -f \"22.11f - - 3d\" snr.ev \"time x y\"\n\n                   TIME   X   Y\n ---------------------- --- ---\n   79494546.56818075478 546 201\n   79488769.94469174743 548 201\n   ... NB: This format is deprecated and will be removed in a future release.] The -F[c] switch can be used to specify a (single-character) column separator (where the default is a space). Note that column formatting will almost certainly also add spaces to pad individual columns to the required width. These can be removed with a program such as sed, at the cost of generating unaligned columns. For example: fundisp -F',' snr.ev'[cir 512 512 .1]' X, Y, PHA , PI , TIME , DX , DY --------,--------,--------,--------,---------------------,--------,-------- 512, 512, 6, 7, 79493997.45854475, 578, 574 512, 512, 8, 9, 79494575.58943175, 579, 573 512, 512, 5, 6, 79493631.03866175, 578, 575 512, 512, 5, 5, 79493290.86521725, 578, 575 512, 512, 8, 9, 79493432.00990875, 579, 573 fundisp -F',' snr.ev'[cir 512 512 .1]' │ sed 's\/ *, *\/,\/g' X,Y,PHA,PI,TIME,DX,DY --------,--------,--------,--------,---------------------,--------,-------- 512,512,6,7,79493997.45854475,578,574 512,512,8,9,79494575.58943175,579,573 512,512,5,6,79493631.03866175,578,575 512,512,5,5,79493290.86521725,578,575 512,512,8,9,79493432.00990875,579,573 fundisp -f \"x=%3d y=%3d pi=%1d pha=%1d time=%20.11f dx=%3d dy=%3d\" -F',' snr.ev'[cir 512 512 .1]' │ sed 's\/ *, *\/,\/g' X,Y,A,I,TIME,DX,DY ---,---,-,-,--------------------,---,--- 512,512,6,7,79493997.45854474604,578,574 512,512,8,9,79494575.58943174779,579,573 512,512,5,6,79493631.03866174817,578,575 512,512,5,5,79493290.86521725357,578,575 512,512,8,9,79493432.00990875065,579,573 If the -T (rdb table) switch is used, the output will conform to starbase\/rdb data base format: tabs will be inserted between columns rather than spaces. This format is not available when displaying image pixels (except in conjunction with the -l switch). Finally, note that fundisp can be used to create column filters from the auxiliary tables in a FITS file. For example, the following shell code will generate a good-time interval ( GTI ) filter for X-ray data files that contain a standard GTI extension: #!\/bin\/sh\nsed '1,\/---- .*\/d\n\/^$\/,$d' │ awk 'tot>0{printf \"││\"};{printf \"time=\"$1\":\"$2; tot++}' If this script is placed in a file called \"mkgti\", it can be used in a command such as: fundisp foo.fits\"[GTI]\" │ mkgti > gti.filter The resulting filter file can then be used in various funtools programs: funcnts foo.fits\"[@gti.filter]\" ... to process only the events in the good-time intervals.","Process Name":"fundisp","Link":"https:\/\/linux.die.net\/man\/1\/fundisp"}},{"Process":{"Description":"funhead displays the FITS header parameters in the specified FITS Extension. The first argument to the program specifies the Funtools input file to display. If \"stdin\" is specified, data are read from the standard input. Funtools Bracket Notation is used to specify particular FITS extension to process. Normally, the full 80 characters of each header card is output, followed by a new-line. If the -a switch is specified, the header from each FITS extensions in the file is displayed. Note, however, that the -a switch does not work with FITS files input via stdin. We hope to remove this restriction in a future release. If the -s switch is specified, only 79 characters are output before the new-line. This helps the display on 80 character terminals. If the -t switch is specified, the data type of the parameter is output as a one character prefix, followed by 77 characters of the param. The parameter data types are defined as: FUN_PAR_UNKNOWN ('u'), FUN_PAR_COMMENT ('c'), FUN_PAR_LOGICAL ('l'), FUN_PAR_INTEGER ('i'), FUN_PAR_STRING ('s'), FUN_PAR_REAL ('r'), FUN_PAR_COMPLEX ('x'). If the -L (rdb table) switch is used, the output will conform to starbase\/rdb data base list format. For example to display the EVENTS extension (binary table): [sh] funhead \"foo.fits[EVENTS]\"\nXTENSION= 'BINTABLE'            \/  FITS 3D BINARY TABLE\nBITPIX  =                    8  \/  Binary data\nNAXIS   =                    2  \/  Table is a matrix\nNAXIS1  =                   20  \/  Width of table in bytes\nNAXIS2  =                30760  \/  Number of entries in table\nPCOUNT  =                    0  \/  Random parameter count\nGCOUNT  =                    1  \/  Group count\nTFIELDS =                    7  \/  Number of fields in each row\nEXTNAME = 'EVENTS  '            \/  Table name\nEXTVER  =                    1  \/  Version number of table\nTFORM1  = '1I      '            \/  Data type for field\nTTYPE1  = 'X       '            \/  Label for field\nTUNIT1  = '        '            \/  Physical units for field\nTFORM2  = '1I      '            \/  Data type for field\n  etc. ...\nEND To display the third header: [sh] funhead \"foo.fits[3]\"\nXTENSION= 'BINTABLE'            \/  FITS 3D BINARY TABLE\nBITPIX  =                    8  \/  Binary data\nNAXIS   =                    2  \/  Table is a matrix\nNAXIS1  =                   32  \/  Width of table in bytes\nNAXIS2  =                   40  \/  Number of entries in table\nPCOUNT  =                    0  \/  Random parameter count\nGCOUNT  =                    1  \/  Group count\nTFIELDS =                    7  \/  Number of fields in each row\nEXTNAME = 'TGR     '            \/  Table name\nEXTVER  =                    1  \/  Version number of table\nTFORM1  = '1D      '            \/  Data type for field\n  etc. ...\nEND To display the primary header (i.e., extension 0): sh> funhead \"coma.fits[0]\"\nSIMPLE  =                    T \/STANDARD FITS FORMAT\nBITPIX  =                   16 \/2-BYTE TWOS-COMPL INTEGER\nNAXIS   =                    2 \/NUMBER OF AXES\nNAXIS1  =                  800 \/\nNAXIS2  =                  800 \/\nDATATYPE= 'INTEGER*2'          \/SHORT INTEGER\nEND The funhead program also can edit (i.e. add, delete, or modify) or display individual headers parameters. Edit mode is signalled by the presence of two additional command-line arguments: output file and edit command file, in that order. Edit mode acts as a filter: the output file will contain the entire input FITS file, including other extensions. The edit command file can be \"stdin\", in which case edit command are read from the standard input. The edit command file contains parameter comments (having '#' in the first column) and delete and assignment(modify or add) operations. A delete operation is specified by preceding the parameter name with a minus sign \"-\". A display operation (very useful in interactive sessions, i.e., where the edit commands are taken from stdin) is specified by preceding the parameter name with a question mark \"?\". In either case, a parameter value need not be specified. An assignment operation is specified in the same two ways that a parameter is specified in a text header (but without the comment character that precedes header params), i.e.: \u2022 FITS-style comments have an equal sign \"=\" between the keyword and value and an optional slash \"\/\" to signify a comment. The strict FITS rules on column positions are not enforced. \u2022 Free-form comments can have an optional colon separator between the keyword and value. In the absence of quote, all tokens after the keyword are part of the value, i.e. no comment is allowed. For example, the following interactive session checks for the existence of parameters, adds new parameters, modifies them, and modifies and deletes existing parameters: sh$ .\/funhead snr.ev foo.fits -\n# look for FOO1\n? FOO1\nWARNING: FOO1 not found\n# add new foo1\nFOO1 = 100\n# add foo2\nFOO2 = 200\n# reset foo1 to a different value\nFOO1 -1\n# delete foo2\n-FOO2\n# change existing value\nEXTVER 2\n? XS-SORT\nXS-SORT = 'EOF     '            \/  type of event sort\n# delete existing value\n-XS-SORT\n# exit\n^D See Column-based Text Files for more information about header parameter format.","Process Name":"funhead","Link":"https:\/\/linux.die.net\/man\/1\/funhead"}},{"Process":{"Description":"funhist creates a one-dimensional histogram from the specified columns of a FITS Extension binary table of a FITS file (or from a non-FITS raw event file), or from a FITS image or array, and writes that histogram as an ASCII table. Alternatively, the program can perform a 1D projection of one of the image axes. The first argument to the program is required, and specifies the Funtools file: FITS table or image, raw event file, or array. If \"stdin\" is specified, data are read from the standard input. Use Funtools Bracket Notation to specify FITS extensions, and filters. For a table, the second argument also is required. It specifies the column to use in generating the histogram. If the data file is of type image (or array), the column is optional: if \"x\" (or \"X\"), \"y\" (or \"Y\") is specified, then a projection is performed over the x (dim1) or y (dim2) axes, respectively. (That is, this projection will give the same results as a histogram performed on a table containing the equivalent x,y event rows.) If no column name is specified or \"xy\" (or \" XY \") is specified for the image, then a histogram is performed on the values contained in the image pixels. The argument that follows is optional and specifies the number of bins to use in creating the histogram and, if desired, the range of bin values. For image and table histograms, the range should specify the min and max data values. For image histograms on the x and y axes, the range should specify the min and max image bin values. If this argument is omitted, the number of output bins for a table is calculated either from the TLMIN\/TLMAX headers values (if these exist in the table FITS header for the specified column) or by going through the data to calculate the min and max value. For an image, the number of output bins is calculated either from the DATAMIN\/DATAMAX header values, or by going through the data to calculate min and max value. (Note that this latter calculation might fail if the image cannot be fit in memory.) If the data are floating point (table or image) and the number of bins is not specified, an arbitrary default of 128 is used. For binary table processing, the -w (bin width) switch can be used to specify the width of each bin rather than the number of bins. Thus: funhist test.ev pha 1:100:5 means that 5 bins of width 20 are used in the histogram, while: funhist -w test.ev pha 1:100:5 means that 20 bins of width 5 are used in the histogram. The data are divvied up into the specified number of bins and the resulting 1D histogram (or projection) is output in ASCII table format. For a table, the output displays the low_edge (inclusive) and hi_edge (exclusive) values for the data. For example, a 15-row table containing a \"pha\" column whose values range from -7.5 to 7.5 can be processed thus: [sh] funhist test.ev pha\n# data file:        \/home\/eric\/data\/test.ev\n# column:           pha\n# min,max,bins:     -7.5 7.5 15\n\n   bin     value               lo_edge               hi_edge\n------ --------- --------------------- ---------------------\n     1        22           -7.50000000           -6.50000000\n     2        21           -6.50000000           -5.50000000\n     3        20           -5.50000000           -4.50000000\n     4        19           -4.50000000           -3.50000000\n     5        18           -3.50000000           -2.50000000\n     6        17           -2.50000000           -1.50000000\n     7        16           -1.50000000           -0.50000000\n     8        30           -0.50000000            0.50000000\n     9        16            0.50000000            1.50000000\n    10        17            1.50000000            2.50000000\n    11        18            2.50000000            3.50000000\n    12        19            3.50000000            4.50000000\n    13        20            4.50000000            5.50000000\n    14        21            5.50000000            6.50000000\n    15        22            6.50000000            7.50000000\n\n[sh] funhist test.ev pha 1:6\n# data file:          \/home\/eric\/data\/test.ev\n# column:             pha\n# min,max,bins:       0.5 6.5 6\n\n   bin     value               lo_edge               hi_edge\n------ --------- --------------------- ---------------------\n     1        16            0.50000000            1.50000000\n     2        17            1.50000000            2.50000000\n     3        18            2.50000000            3.50000000\n     4        19            3.50000000            4.50000000\n     5        20            4.50000000            5.50000000\n     6        21            5.50000000            6.50000000\n\n[sh] funhist test.ev pha 1:6:3\n# data file:          \/home\/eric\/data\/test.ev\n# column:             pha\n# min,max,bins:       0.5 6.5 3\n\n   bin     value               lo_edge               hi_edge\n------ --------- --------------------- ---------------------\n     1        33            0.50000000            2.50000000\n     2        37            2.50000000            4.50000000\n     3        41            4.50000000            6.50000000 For a table histogram, the -n(normalize) switch can be used to normalize the bin value by the width of the bin (i.e., hi_edge-lo_edge): [sh] funhist -n test.ev pha 1:6:3\n# data file:          test.ev\n# column:             pha\n# min,max,bins:       0.5 6.5 3\n# width normalization (val\/(hi_edge-lo_edge)) is applied\n\n   bin                 value               lo_edge               hi_edge\n------ --------------------- --------------------- ---------------------\n     1           16.50000000            0.50000000            2.50000000\n     2            6.16666667            2.50000000            4.50000000\n     3            4.10000000            4.50000000            6.50000000 This could used, for example, to produce a light curve with values having units of counts\/second instead of counts. For an image histogram, the output displays the low and high image values (both inclusive) used to generate the histogram. For example, in the following example, 184 pixels had a value of 1, 31 had a value of 2, while only 2 had a value of 3,4,5,6, or 7: [sh] funhist test.fits\n# data file:           \/home\/eric\/data\/test.fits\n# min,max,bins:        1 7 7\n\n   bin                 value                lo_val                hi_val\n------ --------------------- --------------------- ---------------------\n     1          184.00000000            1.00000000            1.00000000\n     2           31.00000000            2.00000000            2.00000000\n     3            2.00000000            3.00000000            3.00000000\n     4            2.00000000            4.00000000            4.00000000\n     5            2.00000000            5.00000000            5.00000000\n     6            2.00000000            6.00000000            6.00000000\n     7            2.00000000            7.00000000            7.00000000 For the axis projection of an image, the output displays the low and high image bins (both inclusive) used to generate the projection. For example, in the following example, 21 counts had their X bin value of 2, etc.: [sh] funhist test.fits x 2:7\n# data file:            \/home\/eric\/data\/test.fits\n# column:               X\n# min,max,bins: 2 7 6\n\n   bin                 value                lo_bin                hi_bin\n------ --------------------- --------------------- ---------------------\n     1           21.00000000            2.00000000            2.00000000\n     2           20.00000000            3.00000000            3.00000000\n     3           19.00000000            4.00000000            4.00000000\n     4           18.00000000            5.00000000            5.00000000\n     5           17.00000000            6.00000000            6.00000000\n     6           16.00000000            7.00000000            7.00000000\n\n[sh] funhist test.fits x 2:7:2\n# data file:            \/home\/eric\/data\/test.fits\n# column:               X\n# min,max,bins: 2 7 2\n\n   bin                 value                lo_bin                hi_bin\n------ --------------------- --------------------- ---------------------\n     1           60.00000000            2.00000000            4.00000000\n     2           51.00000000            5.00000000            7.00000000 You can use gnuplot or other plotting programs to graph the results, using a script such as: #!\/bin\/sh\nsed -e '1,\/---- .*\/d\n\/^$\/,$d' │ \\\nawk '\\\nBEGIN{print \"set nokey; set title \\\"funhist\\\"; set xlabel \\\"bin\\\"; set ylabel \\\"counts\\\"; plot \\\"-\\\" with boxes\"}   \\\n{print $3, $2, $4-$3}'        │ \\\ngnuplot -persist - 1>\/dev\/null 2>&1 Similar plot commands are supplied in the script funhist.plot: funhist test.ev pha ...  │ funhist.plot gnuplot","Process Name":"funhist","Link":"https:\/\/linux.die.net\/man\/1\/funhist"}},{"Process":{"Description":"funimage creates a primary FITS image from the specified FITS Extension and\/or Image Section of a FITS file, or from an Image Section of a non-FITS array, or from a raw event file. The first argument to the program specifies the FITS input image, array, or raw event file to process. If \"stdin\" is specified, data are read from the standard input. Use Funtools Bracket Notation to specify FITS extensions, image sections, and filters. The second argument is the output FITS file. If \"stdout\" is specified, the FITS image is written to the standard output. By default, the output pixel values are of the same data type as those of the input file (or type \"int\" when binning a table), but this can be overridden using an optional third argument of the form: bitpix=n where n is 8,16,32,-32,-64, for unsigned char, short, int, float and double, respectively. If the input data are of type image, the appropriate section is extracted and blocked (based on how the Image Section is specified), and the result is written to the FITS primary image. When an integer image containing the BSCALE and BZERO keywords is converted to float, the pixel values are scaled and the scaling keywords are deleted from the output header. When converting integer scaled data to integer (possibly of a different size), the pixels are not scaled and the scaling keywords are retained. If the input data is a binary table or raw event file, these are binned into an image, from which a section is extracted and blocked, and written to a primary FITS image. In this case, it is necessary to specify the two columns that will be used in the 2D binning. This can be done on the command line using the bincols=(x,y) keyword: funcnts \"foo.ev[EVENTS,bincols=(detx,dety)]\" The full form of the bincols= specifier is: bincols=([xname[:tlmin[:tlmax:[binsiz]]]],[yname[:tlmin[:tlmax[:binsiz]]]]) where the tlmin, tlmax, and binsiz specifiers determine the image binning dimensions: dim = (tlmax - tlmin)\/binsiz     (floating point data)\ndim = (tlmax - tlmin)\/binsiz + 1 (integer data) Using this syntax, it is possible to bin any two columns of a binary table at any bin size. Note that the tlmin, tlmax, and binsiz specifiers can be omitted if TLMIN , TLMAX , and TDBIN header parameters (respectively) are present in the FITS binary table header for the column in question. Note also that if only one parameter is specified, it is assumed to be tlmax, and tlmin defaults to 1. If two parameters are specified, they are assumed to be tlmin and tlmax. See Binning FITS Binary Tables and Non-FITS Event Files for more information about binning parameters. By default, a new 2D FITS image file is created and the image is written to the primary HDU . If the -a (append) switch is specified, the image is appended to an existing FITS file as an IMAGE extension. (If the output file does not exist, the switch is effectively ignored and the image is written to the primary HDU .) This can be useful in a shell programming environment when processing multiple FITS images that you want to combine into a single final FITS file. funimage also can take input from a table containing columns of x, y, and value (e.g., the output from fundisp -l which displays each image x and y and the number of counts at that position.) When the -l (list) switch is used, the input file is taken to be a FITS or ASCII table containing (at least) three columns that specify the x and y image coordinates and the value of that image pixel. In this case, funimage requires four extra arguments: xcolumn:xdims, ycolumn:ydims, vcolumn and bitpix=n. The x and y col:dim information takes the form: name:dim               # values range from 1 to dim\nname:min:max           # values range from min to max\nname:min:max:binsiz    # dimensions scaled by binsize In particular, the min value should be used whenever the minimum coordinate value is something other than one. For example: funimage -l foo.lst foo.fits xcol:0:512 ycol:0:512 value bitpix=-32 The list feature also can be used to read unnamed columns from standard input: simply replace the column name with a null string. Note that the dimension information is still required: funimage -l stdin foo.fits \"\":0:512 \"\":0:512 \"\" bitpix=-32\n240 250 1\n255 256 2\n...\n^D The list feature provides a simple way to generate a blank image. If you pass a Column-based Text File to funimage in which the text header contains the required image information, then funimage will correctly make a blank image. For example, consider the following text file (called foo.txt): x:I:1:10  y:I:1:10\n------    ------\n0         0 This text file defines two columns, x and y, each of data type 32-bit int and image dimension 10. The command: funimage foo.txt foo.fits bitpix=8 will create an empty FITS image called foo.fits containing a 10x10 image of unsigned char: fundisp foo.fits\n         1      2      3      4      5      6      7      8      9     10\n    ------ ------ ------ ------ ------ ------ ------ ------ ------ ------\n10:      0      0      0      0      0      0      0      0      0      0\n 9:      0      0      0      0      0      0      0      0      0      0\n 8:      0      0      0      0      0      0      0      0      0      0\n 7:      0      0      0      0      0      0      0      0      0      0\n 6:      0      0      0      0      0      0      0      0      0      0\n 5:      0      0      0      0      0      0      0      0      0      0\n 4:      0      0      0      0      0      0      0      0      0      0\n 3:      0      0      0      0      0      0      0      0      0      0\n 2:      0      0      0      0      0      0      0      0      0      0\n 1:      1      0      0      0      0      0      0      0      0      0 Note that the text file must contain at least one row of data. However, in the present example, event position 0,0 is outside the limits of the image and will be ignored. (You can, of course, use real x,y values to seed the image with data.) Furthermore, you can use the TEXT filter specification to obviate the need for an input text file altogether. The following command will create the same 10x10 char image without an actual input file:   funimage stdin'[TEXT(x:I:10,y:I:10)]' foo.fits bitpix=8 < \/dev\/null\nor\n  funimage \/dev\/null'[TEXT(x:I:10,y:I:10)]' foo.fits bitpix=8 You also can use either of these methods to generate a region mask simply by appending a region inside the filter brackets and specfying mask=all along with the bitpix. For example, the following command will generate a 10x10 char mask using 3 regions: funimage stdin'[TEXT(x:I:10,y:I:10),cir(5,5,4),point(10,1),-cir(5,5,2)]' \\\nfoo.fits bitpix=8,mask=all < \/dev\/null The resulting mask looks like this: fundisp foo.fits\n         1      2      3      4      5      6      7      8      9     10\n    ------ ------ ------ ------ ------ ------ ------ ------ ------ ------\n10:      0      0      0      0      0      0      0      0      0      0\n 9:      0      0      0      0      0      0      0      0      0      0\n 8:      0      0      1      1      1      1      1      0      0      0\n 7:      0      1      1      1      1      1      1      1      0      0\n 6:      0      1      1      0      0      0      1      1      0      0\n 5:      0      1      1      0      0      0      1      1      0      0\n 4:      0      1      1      0      0      0      1      1      0      0\n 3:      0      1      1      1      1      1      1      1      0      0\n 2:      0      0      1      1      1      1      1      0      0      0\n 1:      0      0      0      0      0      0      0      0      0      2 You can use funimage to create 1D image projections along the x or y axis using the -p [x│y] switch. This capability works for both images and tables. For example consider a FITS table named ev.fits containing the following rows:        X        Y\n-------- --------\n       1        1\n       1        2\n       1        3\n       1        4\n       1        5\n       2        2\n       2        3\n       2        4\n       2        5\n       3        3\n       3        4\n       3        5\n       4        4\n       4        5\n       5        5 A corresponding 5x5 image, called dim2.fits, would therefore contain:             1          2          3          4          5\n   ---------- ---------- ---------- ---------- ----------\n5:          1          1          1          1          1\n4:          1          1          1          1          0\n3:          1          1          1          0          0\n2:          1          1          0          0          0\n1:          1          0          0          0          0 A projection along the y axis can be performed on either the table or the image: funimage -p y ev.fits stdout │ fundisp stdin\n            1          2          3          4          5\n   ---------- ---------- ---------- ---------- ----------\n1:          1          2          3          4          5\n\nfunimage -p y dim2.fits stdout │ fundisp stdin\n            1          2          3          4          5\n   ---------- ---------- ---------- ---------- ----------\n1:          1          2          3          4          5 Furthermore, you can create a 1D image projection along any column of a table by using the bincols=[column] filter specification and specifying a single column. For example, the following command projects the same 1D image along the y axis of a table as use of the -p y switch: funimage ev.fits'[bincols=y]' stdout │ fundisp stdin\n            1          2          3          4          5\n   ---------- ---------- ---------- ---------- ----------\n1:          1          2          3          4          5 Examples: Create a FITS image from a FITS binary table: [sh] funimage test.ev test.fits Display the FITS image generated from a blocked section of FITS binary table: [sh]  funimage \"test.ev[2:8,3:7,2]\" stdout │ fundisp stdin\n                  1         2         3\n          --------- --------- ---------\n       1:        20        28        36\n       2:        28        36        44","Process Name":"funimage","Link":"https:\/\/linux.die.net\/man\/1\/funimage"}},{"Process":{"Description":"The funindex script creates an index for the specified column (key) by running funtable -s (sort) and then saving the column value and the record number for each sorted row. This index will be used automatically by funtools filtering of that column, provided the index file's modification date is later than that of the data file. The first required argument is the name of the FITS binary table to index. Please note that text files cannot be indexed at this time. The second required argument is the column (key) name to index. While multiple keys can be specified in principle, the funtools index processing assume a single key and will not recognize files containing multiple keys. By default, the output index file name is [root]_[key].idx, where [root] is the root of the input file. Funtools looks for this specific file name when deciding whether to use an index for faster filtering. Therefore, the optional third argument (output file name) should not be used for funtools processing. For example, to create an index on column Y for a given FITS file, use: funindex foo.fits Y This will generate an index named foo_y.idx, which will be used by funtools for filters involving the Y column.","Process Name":"funindex","Link":"https:\/\/linux.die.net\/man\/1\/funindex"}},{"Process":{"Description":"funionfs concatenates directories mentioned in options (and upper_dir ) to mountpoint. Also, although some (but not all) of the directories are read-only, you still can write in resulting mounted directory. For example, if you have files a\/a and b\/b and mount mountpoint r with \"-o dirs=a=ro:b=ro:w=rw\", then you will see files r\/a and r\/b. You will be able to write to them, and original files will remain intact (changes will go to w ). Note that it is impossible to have file with name ending by del_string value, if you create r\/a_DELETED~ in mentioned example, it will dissapear together with a.","Process Name":"funionfs","Link":"https:\/\/linux.die.net\/man\/1\/funionfs"}},{"Process":{"Description":null,"Process Name":"funjoin","Link":"https:\/\/linux.die.net\/man\/1\/funjoin"}},{"Process":{"Description":"funmerge merges FITS data from one or more FITS Binary Table files or raw event files. The first argument to the program specifies the first input FITS table or raw event file. If \"stdin\" is specified, data are read from the standard input. Use Funtools Bracket Notation to specify FITS extensions and row filters. Subsequent arguments specify additional event files and tables to merge. ( NB: Stdin cannot not be used for any of these additional input file arguments.) The last argument is the output FITS file. The columns in each input table must be identical. If an input file begins with the '@' character, it is processed as an include file, i.e., as a text file containing event file names (as well as blank lines and\/or comment lines starting with the '#' sign). If standard input is specified as an include file ('@stdin'), then file names are read from the standard input until EOF (^D). Event files and include files can be mixed on a command line. Rows from each table are written sequentially to the output file. If the switch -f [colname] is specified on the command line, an additional column is added to each row containing the number of the file from which that row was taken (starting from one). In this case, the corresponding file names are stored in the header parameters having the prefix FUNFIL , i.e., FUNFIL01 , FUNFIL02 , etc. Using the -w switch (or -x switch as described below), funmerge also can adjust the position column values using the WCS information in each file. (By position columns, we mean the columns that the table is binned on, i.e., those columns defined by the bincols= switch, or (X,Y) by default.) To perform WCS alignment, the WCS of the first file is taken as the base WCS . Each position in subsequent files is adjusted by first converting it to the sky coordinate in its own WCS coordinate system, then by converting this sky position to the sky position of the base WCS , and finally converting back to a pixel position in the base system. Note that in order to perform WCS alignment, the appropriate WCS and TLMIN\/TLMAX keywords must already exist in each FITS file. When performing WCS alignment, you can save the original positions in the output file by using the -x (for \"xtra\") switch instead of the -w switch (i.e., using this switch also implies using -w) The old positions are saved in columns having the same name as the original positional columns, with the added prefix \" OLD_ \". Examples: Merge two tables, and preserve the originating file number for each row in the column called \" FILE \" (along with the corresponding file name in the header): [sh] funmerge -f \"FILE\" test.ev test2.ev merge.ev Merge two tables with WCS alignment, saving the old position values in 2 additional columns: [sh] funmerge -x test.ev test2.ev merge.ev This program only works on raw event files and binary tables. We have not yet implemented image and array merging.","Process Name":"funmerge","Link":"https:\/\/linux.die.net\/man\/1\/funmerge"}},{"Process":{"Description":null,"Process Name":"funsky","Link":"https:\/\/linux.die.net\/man\/1\/funsky"}},{"Process":{"Description":"funtable selects rows from the specified FITS Extension (binary table only) of a FITS file, or from a non-FITS raw event file, and writes those rows to a FITS binary table file. It also will create a FITS binary table from an image or a raw array file. The first argument to the program specifies the FITS file, raw event file, or raw array file. If \"stdin\" is specified, data are read from the standard input. Use Funtools Bracket Notation to specify FITS extensions, and filters. The second argument is the output FITS file. If \"stdout\" is specified, the FITS binary table is written to the standard output. By default, all columns of the input file are copied to the output file. Selected columns can be output using an optional third argument in the form: \"column1 column1 ... columnN\" The funtable program generally is used to select rows from a FITS binary table using Table Filters and\/or Spatial Region Filters. For example, you can copy only selected rows (and output only selected columns) by executing in a command such as: [sh] funtable \"test.ev[pha==1&&pi==10]\" stdout \"x y pi pha\" │ fundisp stdin\n       X       Y     PHA        PI\n ------- ------- ------- ---------\n       1      10       1        10\n       1      10       1        10\n       1      10       1        10\n       1      10       1        10\n       1      10       1        10\n       1      10       1        10\n       1      10       1        10\n       1      10       1        10\n       1      10       1        10\n       1      10       1        10 The special column $REGION can be specified to write the region id of each row: [sh $] funtable \"test.ev[time-(int)time>=.99&&annulus(0 0 0 10 n=3)]\" stdout 'x y time $REGION' │ fundisp stdin\n        X        Y                  TIME     REGION\n -------- -------- --------------------- ----------\n        5       -6           40.99000000          3\n        4       -5           59.99000000          2\n       -1        0          154.99000000          1\n       -2        1          168.99000000          1\n       -3        2          183.99000000          2\n       -4        3          199.99000000          2\n       -5        4          216.99000000          2\n       -6        5          234.99000000          3\n       -7        6          253.99000000          3 Here only rows with the proper fractional time and whose position also is within one of the three annuli are written. Columns can be excluded from display using a minus sign before the column: [sh $] funtable \"test.ev[time-(int)time>=.99]\" stdout \"-time\" │ fundisp stdin\n        X        Y      PHA         PI          DX          DY\n -------- -------- -------- ---------- ----------- -----------\n        5       -6        5         -6        5.50       -6.50\n        4       -5        4         -5        4.50       -5.50\n       -1        0       -1          0       -1.50        0.50\n       -2        1       -2          1       -2.50        1.50\n       -3        2       -3          2       -3.50        2.50\n       -4        3       -4          3       -4.50        3.50\n       -5        4       -5          4       -5.50        4.50\n       -6        5       -6          5       -6.50        5.50\n       -7        6       -7          6       -7.50        6.50 All columns except the time column are written. In general, the rules for activating and de-activating columns are: \u2022 If only exclude columns are specified, then all columns but the exclude columns will be activated. \u2022 If only include columns are specified, then only the specified columns are activated. \u2022 If a mixture of include and exclude columns are specified, then all but the exclude columns will be active; this last case is ambiguous and the rule is arbitrary. In addition to specifying columns names explicitly, the special symbols + and - can be used to activate and de-activate all columns. This is useful if you want to activate the $REGION column along with all other columns. According to the rules, the syntax \"$REGION\" only activates the region column and de-activates the rest. Use \"+ $REGION\" to activate all columns as well as the region column. Ordinarily, only the selected table is copied to the output file. In a FITS binary table, it sometimes is desirable to copy all of the other FITS extensions to the output file as well. This can be done by appending a '+' sign to the name of the extension in the input file name. For example, the first command below copies only the EVENT table, while the second command copies other extensions as well: [sh] funtable \"\/proj\/rd\/data\/snr.ev[EVENTS]\" events.ev\n[sh] funtable \"\/proj\/rd\/data\/snr.ev[EVENTS+]\" eventsandmore.ev If the input file is an image or a raw array file, then funtable will generate a FITS binary table from the pixel values in the image. Note that it is not possible to specify the columns to output (using command-line argument 3). Instead, there are two ways to create such a binary table from an image. By default, a 3-column table is generated, where the columns are \"X\", \"Y\", and \" VALUE \". For each pixel in the image, a single row (event) is generated with the \"X\" and \"Y\" columns assigned the dim1 and dim2 values of the image pixel, respectively and the \" VALUE \" column assigned the value of the pixel. With sort of table, running funhist on the \" VALUE \" column will give the same results as running funhist on the original image. If the -i (\"individual\" rows) switch is specified, then only the \"X\" and \"Y\" columns are generated. In this case, each positive pixel value in the image generates n rows (events), where n is equal to the integerized value of that pixel (plus 0.5, for floating point data). In effect, -i approximately recreates the rows of a table that would have been binned into the input image. (Of course, this is only approximately correct, since the resulting x,y positions are integerized.) If the -s [col1 col2 ... coln] (\"sort\") switch is specified, the output rows of a binary table will be sorted using the specified columns as sort keys. The sort keys must be scalar columns and also must be part of the output file (i.e. you cannot sort on a column but not include it in the output). This facility uses the _sort program (included with funtools), which must be accessible via your path. For binary tables, the -m (\"multiple files\") switch will generate a separate file for each region in the filter specification i.e. each file contains only the rows from that region. Rows which pass the filter but are not in any region also are put in a separate file. The separate output file names generated by the -m switch are produced automatically from the root output file to contain the region id of the associated region. (Note that region ids start at 1, so that the file name associated with id 0 contains rows that pass the filter but are not in any given region.) Output file names are generated as follows: \u2022 A $n specification can be used anywhere in the root file name (suitably quoted to protect it from the shell) and will be expanded to be the id number of the associated region. For example: funtable -m input.fits'[cir(512,512,1);cir(520,520,1)...]' 'foo.goo_$n.fits' will generate files named foo.goo_0.fits (for rows not in any region but still passing the filter), foo.goo_1.fits (rows in region id #1, the first region), foo.goo_2.fits (rows in region id #2), etc. Note that single quotes in the output root are required to protect the '$' from the shell. \u2022 If $n is not specified, then the region id will be placed before the first dot (.) in the filename. Thus: funtable -m input.fits'[cir(512,512,1);cir(520,520,1)...]' foo.evt.fits will generate files named foo0.evt.fits (for rows not in any region but still passing the filter), foo1.evt.fits (rows in region id #1), foo2.evt.fits (rows in region id #2), etc. \u2022 If no dot is specified in the root output file name, then the region id will be appended to the filename. Thus: funtable -m input.fits'[cir(512,512,1);cir(520,520,1)...]' 'foo_evt' will generate files named foo_evt0 (for rows not in any region but still passing the filter), foo_evt1 (rows in region id #1), foo_evt2 (rows in region id #2), etc. The multiple file mechanism provide a simple way to generate individual source data files with a single pass through the data. By default, a new FITS file is created and the binary table is written to the first extension. If the -a (append) switch is specified, the table is appended to an existing FITS file as a BINTABLE extension. Note that the output FITS file must already exist. If the -z (\"zero\" pixel values) switch is specified and -i is not specified, then pixels having a zero value will be output with their \" VALUE \" column set to zero. Obviously, this switch does not make sense when individual events are output.","Process Name":"funtable","Link":"https:\/\/linux.die.net\/man\/1\/funtable"}},{"Process":{"Description":"[ NB: This program has been deprecated in favor of the ASCII text processing support in funtools. You can now perform fundisp on funtools ASCII output files (specifying the table using bracket notation) to extract tables and columns.] The funtbl script extracts a specified table (without the header and comments) from a funtools ASCII output file and writes the result to the standard output. The first non-switch argument is the ASCII input file name (i.e. the saved output from funcnts, fundisp, funhist, etc.). If no filename is specified, stdin is read. The -n switch specifies which table (starting from 1) to extract. The default is to extract the first table. The -c switch is a space-delimited list of column numbers to output, e.g. -c \"1 3 5\" will extract the first three odd-numbered columns. The default is to extract all columns. The -s switch specifies the separator string to put between columns. The default is a single space. The -h switch specifies that column names should be added in a header line before the data is output. Without the switch, no header is prepended. The -p program switch allows you to specify an awk-like program to run instead of the default (which is host-specific and is determined at build time). The -T switch will output the data in rdb format (i.e., with a 2-row header of column names and dashes, and with data columns separated by tabs). The -help switch will print out a message describing program usage. For example, consider the output from the following funcnts command: [sh] funcnts -sr snr.ev \"ann 512 512 0 9 n=3\"\n# source\n#   data file:        \/proj\/rd\/data\/snr.ev\n#   arcsec\/pixel:     8\n# background\n#   constant value:   0.000000\n# column units\n#   area:             arcsec**2\n#   surf_bri:         cnts\/arcsec**2\n#   surf_err:         cnts\/arcsec**2\n\n# summed background-subtracted results\nupto   net_counts     error   background    berror      area  surf_bri  surf_err\n---- ------------ --------- ------------ --------- --------- --------- ---------\n   1      147.000    12.124        0.000     0.000   1600.00     0.092     0.008\n   2      625.000    25.000        0.000     0.000   6976.00     0.090     0.004\n   3     1442.000    37.974        0.000     0.000  15936.00     0.090     0.002\n\n# background-subtracted results\n reg   net_counts     error   background    berror      area  surf_bri  surf_err\n---- ------------ --------- ------------ --------- --------- --------- ---------\n   1      147.000    12.124        0.000     0.000   1600.00     0.092     0.008\n   2      478.000    21.863        0.000     0.000   5376.00     0.089     0.004\n   3      817.000    28.583        0.000     0.000   8960.00     0.091     0.003\n\n# the following source and background components were used:\nsource_region(s)\n----------------\nann 512 512 0 9 n=3\n\n reg       counts    pixels      sumcnts    sumpix\n---- ------------ --------- ------------ ---------\n   1      147.000        25      147.000        25\n   2      478.000        84      625.000       109\n   3      817.000       140     1442.000       249 There are four tables in this output. To extract the last one, you can execute: [sh] funcnts -s snr.ev \"ann 512 512 0 9 n=3\" │ funtbl -n 4\n1 147.000 25 147.000 25\n2 478.000 84 625.000 109\n3 817.000 140 1442.000 249 Note that the output has been re-formatted so that only a single space separates each column, with no extraneous header or comment information. To extract only columns 1,2, and 4 from the last example (but with a header prepended and tabs between columns), you can execute: [sh] funcnts -s snr.ev \"ann 512 512 0 9 n=3\" │ funtbl -c \"1 2 4\" -h -n 4 -s \"\\t\"\n#reg    counts  sumcnts\n1       147.000 147.000\n2       478.000 625.000\n3       817.000 1442.000 Of course, if the output has previously been saved in a file named foo.out, the same result can be obtained by executing: [sh] funtbl -c \"1 2 4\" -h -n 4 -s \"\\t\" foo.out\n#reg    counts  sumcnts\n1       147.000 147.000\n2       478.000 625.000\n3       817.000 1442.000","Process Name":"funtbl","Link":"https:\/\/linux.die.net\/man\/1\/funtbl"}},{"Process":{"Description":null,"Process Name":"funzip","Link":"https:\/\/linux.die.net\/man\/1\/funzip"}},{"Process":{"Description":"fuse-zip is a fuse filesystem, that enables any program to work with a ZIP archive as though it is a plain directory. Unlike KIO or Gnome VFS, it can be used in any application without modifications. Unlike other FUSE filesystems, only fuse-zip provides write support to ZIP archives. Also, fuse-zip is faster that all known implementations on large archives with many files.","Process Name":"fuse-zip","Link":"https:\/\/linux.die.net\/man\/1\/fuse-zip"}},{"Process":{"Description":null,"Process Name":"fuser","Link":"https:\/\/linux.die.net\/man\/1\/fuser"}},{"Process":{"Description":"fusesmb is a Network Neighborhood (Samba shares) filesystem. It works like smbfs, but instead of accessing one share at a time, all computers and workgroups are accessible at once from a single filesystem mount.","Process Name":"fusesmb","Link":"https:\/\/linux.die.net\/man\/1\/fusesmb"}},{"Process":{"Description":"fusioninventory-agent is an agent for OCS Inventory server and FusionInventory for GLPI servers. It creates local inventory of the machines (hardware and software) and send it to the server. It can also write it in a local XML file. This agent is the successor of the former linux_agent which was released with OCS 1.01 and prior. It also replaces the Solaris\/AIX\/BSD unofficial agents. Supported systems: Windows (since Windows 2000) GNU\/Linux MacOSX Solaris FreeBSD NetBSD OpenBSD AIX HP-UX GNU\/kFreeBSD","Process Name":"fusioninventory-agent","Link":"https:\/\/linux.die.net\/man\/1\/fusioninventory-agent"}},{"Process":{"Description":"","Process Name":"fusioninventory-agent-config","Link":"https:\/\/linux.die.net\/man\/1\/fusioninventory-agent-config"}},{"Process":{"Description":null,"Process Name":"fusioninventory-esx","Link":"https:\/\/linux.die.net\/man\/1\/fusioninventory-esx"}},{"Process":{"Description":"This tool can be used to test your server, do benchmark or push inventory from off-line machine.","Process Name":"fusioninventory-injector","Link":"https:\/\/linux.die.net\/man\/1\/fusioninventory-injector"}},{"Process":{"Description":null,"Process Name":"fuzzyflakes","Link":"https:\/\/linux.die.net\/man\/1\/fuzzyflakes"}},{"Process":{"Description":"Open the first available Bumblebee 2 camera on the IEEE1394 bus and read the Triclops context and write it to file for later offline usage. This is useful when post-processing images offline at a later time.","Process Name":"fvbb2gettric","Link":"https:\/\/linux.die.net\/man\/1\/fvbb2gettric"}},{"Process":{"Description":"Open the first available Bumblebee 2 camera on the IEEE1394 bus and show information about the internal settings and values.","Process Name":"fvbb2info","Link":"https:\/\/linux.die.net\/man\/1\/fvbb2info"}},{"Process":{"Description":"Open the first available Bumblebee 2 camera on the IEEE1394 bus and read the rectification information, verify it with previously downloaded data or print information.","Process Name":"fvbb2rectlut","Link":"https:\/\/linux.die.net\/man\/1\/fvbb2rectlut"}},{"Process":{"Description":"This program applies some useful postprocessing steps to colormaps to make them more reliable. It basically computes a weighted sum to fill holes in the colormap. Input and output files are FireVision colormap files.","Process Name":"fvcmpp","Link":"https:\/\/linux.die.net\/man\/1\/fvcmpp"}},{"Process":{"Description":null,"Process Name":"fvconverter","Link":"https:\/\/linux.die.net\/man\/1\/fvconverter"}},{"Process":{"Description":null,"Process Name":"fvfuseviewer","Link":"https:\/\/linux.die.net\/man\/1\/fvfuseviewer"}},{"Process":{"Description":null,"Process Name":"fvlistfwcams","Link":"https:\/\/linux.die.net\/man\/1\/fvlistfwcams"}},{"Process":{"Description":"Explore FireVision instances on the network and interact with those instances. In the first form the network is explored using mDNS-SD service discovery to find FireVision Fountain instances. These are then queried for existing images and lookup tables. In the second form the available images and lookup tables are queried from a specified host. The third form finally allows for retrieving images and retrieving or pushing colormaps reading from or writing to file.","Process Name":"fvnet","Link":"https:\/\/linux.die.net\/man\/1\/fvnet"}},{"Process":{"Description":"List, save, or cleanup shared memory segments created by FireVision. FireVision stores images retrieved from cameras and lookup tables in shared memory segments. Olugins can decide to create additional segments. All shared memory segments have a unique ID. A list of currently existing segments can be retrieved. For images a snapshot can be written into a FireVision raw file. Segments might require cleanup if the creating process has died without closing the segments. You can also choose to cleanup segments of specified types. Segments for which are currently used are detected and not removed.","Process Name":"fvshmem","Link":"https:\/\/linux.die.net\/man\/1\/fvshmem"}},{"Process":{"Description":null,"Process Name":"fvshowyuv","Link":"https:\/\/linux.die.net\/man\/1\/fvshowyuv"}},{"Process":{"Description":"This program reads in all FireVision raw files of the specified directory. It assumes Bumblebee stereo camera images in these files and converts them to YUV422 planar color images. These are then written side by side into a JPEG file.","Process Name":"fvstereodecoder","Link":"https:\/\/linux.die.net\/man\/1\/fvstereodecoder"}},{"Process":{"Description":"Retrieve images via FireVision and display them in a X11 window. In the first form the image is read from a shared memory image segment with the given shmem_id on the local host. The second form retrieves the image via the network and FireVision Fountain connecting to the given host (and optionally port) and reading the image for image_id. The third format allows for reading an image from the given file. The last form is the most flexible as it allows you to pass in an arbitrary camera argument string to open any camera supported by FireVision. All forms share common options described below. Additionally there are keys that can be pressed in the window to influence the behavior. By default the viewer will start with continuous mode disabled, that means that the user needs to press the space button to retrieve the next image.","Process Name":"fvviewer","Link":"https:\/\/linux.die.net\/man\/1\/fvviewer"}},{"Process":{"Description":"fzputtygen is part of FileZilla. It is used to convert private keys from OpenSSH or ssh.com format into a format understood by fzsftp. It usually gets called by FileZilla and is not intended to be used directly. fzputtygen is based on the puttygen component of PuTTY.","Process Name":"fzputtygen","Link":"https:\/\/linux.die.net\/man\/1\/fzputtygen"}},{"Process":{"Description":"fzsftp is part of FileZilla. It is used to handle SFTP (SSH File Transfer Protocol) connections. It usually gets called by FileZilla and is not intended to be used directly. fzsftp is based on the psftp component of PuTTY.","Process Name":"fzsftp","Link":"https:\/\/linux.die.net\/man\/1\/fzsftp"}}]