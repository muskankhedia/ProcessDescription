[{"Process":{"Description":"h2pas attempts to convert a C header file to a pascal unit. it can handle most C constructs that one finds in a C header file, and attempts to translate them to their pascal counterparts. see the CONSTRUCTS section for a full description of what the translator can handle.","Process Name":"h2pas","Link":"https:\/\/linux.die.net\/man\/1\/h2pas"}},{"Process":{"Description":"h2paspp reads one or more C header files and preprocesses them, writing the result to files with the same name as the originals as it goes along. It does not accept all preprocesser tokens of C, but takes care of the following preprocessor directives: #define symbol Defines the new symbol symbol. Note that macros are not supported. #if symbol The text following this directive is included if symbol is defined. #ifdef symbol The text following this directive is included if symbol is defined. #ifndef symbol The text following this directive is included if symbol is not defined. #include filename Include directives are removed, unless the -I option was given, in which case the include file is included and written to the output file. #undef symbol The symbol symbol is undefined.","Process Name":"h2paspp","Link":"https:\/\/linux.die.net\/man\/1\/h2paspp"}},{"Process":{"Description":"h2ph converts any C header files specified to the corresponding Perl header file format. It is most easily run while in \/usr\/include: cd \/usr\/include; h2ph * sys\/* or cd \/usr\/include; h2ph * sys\/* arpa\/* netinet\/* or cd \/usr\/include; h2ph -r -l . The output files are placed in the hierarchy rooted at Perl's architecture dependent library directory. You can specify a different hierarchy with a -d switch. If run with no arguments, filters standard input to standard output.","Process Name":"h2ph","Link":"https:\/\/linux.die.net\/man\/1\/h2ph"}},{"Process":{"Description":null,"Process Name":"h2root","Link":"https:\/\/linux.die.net\/man\/1\/h2root"}},{"Process":{"Description":null,"Process Name":"h2xs","Link":"https:\/\/linux.die.net\/man\/1\/h2xs"}},{"Process":{"Description":null,"Process Name":"h8300-linux-gnu-addr2line","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-addr2line"}},{"Process":{"Description":null,"Process Name":"h8300-linux-gnu-ar","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-ar"}},{"Process":{"Description":"GNU as is really a family of assemblers. If you use (or have used) the GNU assembler on one architecture, you should find a fairly similar environment when you use it on another architecture. Each version has much in common with the others, including object file formats, most assembler directives (often called pseudo-ops) and assembler syntax. as is primarily intended to assemble the output of the GNU C compiler \"gcc\" for use by the linker \"ld\". Nevertheless, we've tried to make as assemble correctly everything that other assemblers for the same machine would assemble. Any exceptions are documented explicitly. This doesn't mean as always uses the same syntax as another assembler for the same architecture; for example, we know of several incompatible versions of 680x0 assembly language syntax. Each time you run as it assembles exactly one source program. The source program is made up of one or more files. (The standard input is also a file.) You give as a command line that has zero or more input file names. The input files are read (from left file name to right). A command line argument (in any position) that has no special meaning is taken to be an input file name. If you give as no file names it attempts to read one input file from the as standard input, which is normally your terminal. You may have to type ctl-D to tell as there is no more program to assemble. Use -- if you need to explicitly name the standard input file in your command line. If the source is empty, as produces a small, empty object file. as may write warnings and error messages to the standard error file (usually your terminal). This should not happen when a compiler runs as automatically. Warnings report an assumption made so that as could keep assembling a flawed program; errors report a grave problem that stops the assembly. If you are invoking as via the GNU C compiler, you can use the -Wa option to pass arguments through to the assembler. The assembler arguments must be separated from each other (and the -Wa) by commas. For example: gcc -c -g -O -Wa,-alh,-L file.c This passes two options to the assembler: -alh (emit a listing to standard output with high-level and assembly source) and -L (retain local symbols in the symbol table). Usually you do not need to use this -Wa mechanism, since many compiler command-line options are automatically passed to the assembler by the compiler. (You can call the GNU compiler driver with the -v option to see precisely what options it passes to each compilation pass, including the assembler.)","Process Name":"h8300-linux-gnu-as","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-as"}},{"Process":{"Description":"The C ++ and Java languages provide function overloading, which means that you can write many functions with the same name, providing that each function takes parameters of different types. In order to be able to distinguish these similarly named functions C ++ and Java encode them into a low-level assembler name which uniquely identifies each different version. This process is known as mangling. The c++filt [1] program does the inverse mapping: it decodes (demangles) low-level names into user-level names so that they can be read. Every alphanumeric word (consisting of letters, digits, underscores, dollars, or periods) seen in the input is a potential mangled name. If the name decodes into a C ++ name, the C ++ name replaces the low-level name in the output, otherwise the original word is output. In this way you can pass an entire assembler source file, containing mangled names, through c++filt and see the same source file containing demangled names. You can also use c++filt to decipher individual symbols by passing them on the command line: c++filt <symbol> If no symbol arguments are given, c++filt reads symbol names from the standard input instead. All the results are printed on the standard output. The difference between reading names from the command line versus reading names from the standard input is that command line arguments are expected to be just mangled names and no checking is performed to separate them from surrounding text. Thus for example: c++filt -n _Z1fv will work and demangle the name to \"f()\" whereas: c++filt -n _Z1fv, will not work. (Note the extra comma at the end of the mangled name which makes it invalid). This command however will work: echo _Z1fv, | c++filt -n and will display \"f(),\", i.e., the demangled name followed by a trailing comma. This behaviour is because when the names are read from the standard input it is expected that they might be part of an assembler source file where there might be extra, extraneous characters trailing after a mangled name. For example: .type   _Z1fv, @function","Process Name":"h8300-linux-gnu-c++filt","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-c++filt"}},{"Process":{"Description":null,"Process Name":"h8300-linux-gnu-cpp","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-cpp"}},{"Process":{"Description":"dlltool reads its inputs, which can come from the -d and -b options as well as object files specified on the command line. It then processes these inputs and if the -e option has been specified it creates a exports file. If the -l option has been specified it creates a library file and if the -z option has been specified it creates a def file. Any or all of the -e, -l and -z options can be present in one invocation of dlltool. When creating a DLL , along with the source for the DLL , it is necessary to have three other files. dlltool can help with the creation of these files. The first file is a .def file which specifies which functions are exported from the DLL , which functions the DLL imports, and so on. This is a text file and can be created by hand, or dlltool can be used to create it using the -z option. In this case dlltool will scan the object files specified on its command line looking for those functions which have been specially marked as being exported and put entries for them in the .def file it creates. In order to mark a function as being exported from a DLL , it needs to have an -export:<name_of_function> entry in the .drectve section of the object file. This can be done in C by using the asm() operator: asm (\".section .drectve\");\nasm (\".ascii \\\"-export:my_func\\\"\");\n\nint my_func (void) { ... } The second file needed for DLL creation is an exports file. This file is linked with the object files that make up the body of the DLL and it handles the interface between the DLL and the outside world. This is a binary file and it can be created by giving the -e option to dlltool when it is creating or reading in a .def file. The third file needed for DLL creation is the library file that programs will link with in order to access the functions in the DLL (an 'import library'). This file can be created by giving the -l option to dlltool when it is creating or reading in a .def file. If the -y option is specified, dlltool generates a delay-import library that can be used instead of the normal import library to allow a program to link to the dll only as soon as an imported function is called for the first time. The resulting executable will need to be linked to the static delayimp library containing __delayLoadHelper2(), which in turn will import LoadLibraryA and GetProcAddress from kernel32. dlltool builds the library file by hand, but it builds the exports file by creating temporary files containing assembler statements and then assembling these. The -S command line option can be used to specify the path to the assembler that dlltool will use, and the -f option can be used to pass specific flags to that assembler. The -n can be used to prevent dlltool from deleting these temporary assembler files when it is done, and if -n is specified twice then this will prevent dlltool from deleting the temporary object files it used to build the library. Here is an example of creating a DLL from a source file dll.c and also creating a program (from an object file called program.o) that uses that DLL: gcc -c dll.c\ndlltool -e exports.o -l dll.lib dll.o\ngcc dll.o exports.o -o dll.dll\ngcc program.o dll.lib -o program dlltool may also be used to query an existing import library to determine the name of the DLL to which it is associated. See the description of the -I or --identify option.","Process Name":"h8300-linux-gnu-dlltool","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-dlltool"}},{"Process":{"Description":"elfedit updates the ELF header of ELF files which have the matching ELF machine and file types. The options control how and which fields in the ELF header should be updated. elffile... are the ELF files to be updated. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files.","Process Name":"h8300-linux-gnu-elfedit","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-elfedit"}},{"Process":{"Description":"When you invoke GCC , it normally does preprocessing, compilation, assembly and linking. The \"overall options\" allow you to stop this process at an intermediate stage. For example, the -c option says not to run the linker. Then the output consists of object files output by the assembler. Other options are passed on to one stage of processing. Some options control the preprocessor and others the compiler itself. Yet other options control the assembler and linker; most of these are not documented here, since you rarely need to use any of them. Most of the command-line options that you can use with GCC are useful for C programs; when an option is only useful with another language (usually C ++ ), the explanation says so explicitly. If the description for a particular option does not mention a source language, you can use that option with all supported languages. The gcc program accepts options and file names as operands. Many options have multi-letter names; therefore multiple single-letter options may not be grouped: -dv is very different from -d -v. You can mix options and other arguments. For the most part, the order you use doesn't matter. Order does matter when you use several options of the same kind; for example, if you specify -L more than once, the directories are searched in the order specified. Also, the placement of the -l option is significant. Many options have long names starting with -f or with -W---for example, -fmove-loop-invariants, -Wformat and so on. Most of these have both positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"h8300-linux-gnu-gcc","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-gcc"}},{"Process":{"Description":"gcov is a test coverage program. Use it in concert with GCC to analyze your programs to help create more efficient, faster running code and to discover untested parts of your program. You can use gcov as a profiling tool to help discover where your optimization efforts will best affect your code. You can also use gcov along with the other profiling tool, gprof, to assess which parts of your code use the greatest amount of computing time. Profiling tools help you analyze your code's performance. Using a profiler such as gcov or gprof, you can find out some basic performance statistics, such as: \u2022 how often each line of code executes \u2022 what lines of code are actually executed \u2022 how much computing time each section of code uses Once you know these things about how your code works when compiled, you can look at each module to see which modules should be optimized. gcov helps you determine where to work on optimization. Software developers also use coverage testing in concert with testsuites, to make sure software is actually good enough for a release. Testsuites can verify that a program works as expected; a coverage program tests to see how much of the program is exercised by the testsuite. Developers can then determine what kinds of test cases need to be added to the testsuites to create both better testing and a better final product. You should compile your code without optimization if you plan to use gcov because the optimization, by combining some lines of code into one function, may not give you as much information as you need to look for 'hot spots' where the code is using a great deal of computer time. Likewise, because gcov accumulates statistics by line (at the lowest resolution), it works best with a programming style that places only one statement on each line. If you use complicated macros that expand to loops or to other control structures, the statistics are less helpful---they only report on the line where the macro call appears. If your complex macros behave like functions, you can replace them with inline functions to solve this problem. gcov creates a logfile called sourcefile.gcov which indicates how many times each line of a source file sourcefile.c has executed. You can use these logfiles along with gprof to aid in fine-tuning the performance of your programs. gprof gives timing information you can use along with the information you get from gcov. gcov works only on code compiled with GCC . It is not compatible with any other profiling or test coverage mechanism.","Process Name":"h8300-linux-gnu-gcov","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-gcov"}},{"Process":{"Description":"\"gprof\" produces an execution profile of C, Pascal, or Fortran77 programs. The effect of called routines is incorporated in the profile of each caller. The profile data is taken from the call graph profile file (gmon.out default) which is created by programs that are compiled with the -pg option of \"cc\", \"pc\", and \"f77\". The -pg option also links in versions of the library routines that are compiled for profiling. \"Gprof\" reads the given object file (the default is \"a.out\") and establishes the relation between its symbol table and the call graph profile from gmon.out. If more than one profile file is specified, the \"gprof\" output shows the sum of the profile information in the given profile files. \"Gprof\" calculates the amount of time spent in each routine. Next, these times are propagated along the edges of the call graph. Cycles are discovered, and calls into a cycle are made to share the time of the cycle. Several forms of output are available from the analysis. The flat profile shows how much time your program spent in each function, and how many times that function was called. If you simply want to know which functions burn most of the cycles, it is stated concisely here. The call graph shows, for each function, which functions called it, which other functions it called, and how many times. There is also an estimate of how much time was spent in the subroutines of each function. This can suggest places where you might try to eliminate function calls that use a lot of time. The annotated source listing is a copy of the program's source code, labeled with the number of times each line of the program was executed.","Process Name":"h8300-linux-gnu-gprof","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-gprof"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"h8300-linux-gnu-ld","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-ld"}},{"Process":{"Description":"nlmconv converts the relocatable i386 object file infile into the NetWare Loadable Module outfile, optionally reading headerfile for NLM header information. For instructions on writing the NLM command file language used in header files, see the linkers section, NLMLINK in particular, of the NLM Development and Tools Overview, which is part of the NLM Software Developer's Kit (\" NLM SDK \"), available from Novell, Inc. nlmconv uses the GNU Binary File Descriptor library to read infile; nlmconv can perform a link step. In other words, you can list more than one object file for input if you list them in the definitions file (rather than simply specifying one input file on the command line). In this case, nlmconv calls the linker for you.","Process Name":"h8300-linux-gnu-nlmconv","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-nlmconv"}},{"Process":{"Description":"GNU nm lists the symbols from object files objfile.... If no object files are listed as arguments, nm assumes the file a.out. For each symbol, nm shows: \u2022 The symbol value, in the radix selected by options (see below), or hexadecimal by default. \u2022 The symbol type. At least the following types are used; others are, as well, depending on the object file format. If lowercase, the symbol is usually local; if uppercase, the symbol is global (external). There are however a few lowercase symbols that are shown for special global symbols (\"u\", \"v\" and \"w\"). \"A\" The symbol's value is absolute, and will not be changed by further linking. \"B\" \"b\" The symbol is in the uninitialized data section (known as BSS ). \"C\" The symbol is common. Common symbols are uninitialized data. When linking, multiple common symbols may appear with the same name. If the symbol is defined anywhere, the common symbols are treated as undefined references. \"D\" \"d\" The symbol is in the initialized data section. \"G\" \"g\" The symbol is in an initialized data section for small objects. Some object file formats permit more efficient access to small data objects, such as a global int variable as opposed to a large global array. \"i\" For PE format files this indicates that the symbol is in a section specific to the implementation of DLLs. For ELF format files this indicates that the symbol is an indirect function. This is a GNU extension to the standard set of ELF symbol types. It indicates a symbol which if referenced by a relocation does not evaluate to its address, but instead must be invoked at runtime. The runtime execution will then return the value to be used in the relocation. \"N\" The symbol is a debugging symbol. \"p\" The symbols is in a stack unwind section. \"R\" \"r\" The symbol is in a read only data section. \"S\" \"s\" The symbol is in an uninitialized data section for small objects. \"T\" \"t\" The symbol is in the text (code) section. \"U\" The symbol is undefined. \"u\" The symbol is a unique global symbol. This is a GNU extension to the standard set of ELF symbol bindings. For such a symbol the dynamic linker will make sure that in the entire process there is just one symbol with this name and type in use. \"V\" \"v\" The symbol is a weak object. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the weak symbol becomes zero with no error. On some systems, uppercase indicates that a default value has been specified. \"W\" \"w\" The symbol is a weak symbol that has not been specifically tagged as a weak object symbol. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the symbol is determined in a system-specific manner without error. On some systems, uppercase indicates that a default value has been specified. \"-\" The symbol is a stabs symbol in an a.out object file. In this case, the next values printed are the stabs other field, the stabs desc field, and the stab type. Stabs symbols are used to hold debugging information. \"?\" The symbol type is unknown, or object file format specific. \u2022 The symbol name.","Process Name":"h8300-linux-gnu-nm","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-nm"}},{"Process":{"Description":"The GNU objcopy utility copies the contents of an object file to another. objcopy uses the GNU BFD Library to read and write the object files. It can write the destination object file in a format different from that of the source object file. The exact behavior of objcopy is controlled by command-line options. Note that objcopy should be able to copy a fully linked file between any two formats. However, copying a relocatable object file between any two formats may not work as expected. objcopy creates temporary files to do its translations and deletes them afterward. objcopy uses BFD to do all its translation work; it has access to all the formats described in BFD and thus is able to recognize most formats without being told explicitly. objcopy can be used to generate S-records by using an output target of srec (e.g., use -O srec). objcopy can be used to generate a raw binary file by using an output target of binary (e.g., use -O binary). When objcopy generates a raw binary file, it will essentially produce a memory dump of the contents of the input object file. All symbols and relocation information will be discarded. The memory dump will start at the load address of the lowest section copied into the output file. When generating an S-record or a raw binary file, it may be helpful to use -S to remove sections containing debugging information. In some cases -R will be useful to remove sections which contain information that is not needed by the binary file. Note---objcopy is not able to change the endianness of its input files. If the input format has an endianness (some formats do not), objcopy can only copy the inputs into file formats that have the same endianness or which have no endianness (e.g., srec). (However, see the --reverse-bytes option.)","Process Name":"h8300-linux-gnu-objcopy","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-objcopy"}},{"Process":{"Description":"objdump displays information about one or more object files. The options control what particular information to display. This information is mostly useful to programmers who are working on the compilation tools, as opposed to programmers who just want their program to compile and work. objfile... are the object files to be examined. When you specify archives, objdump shows information on each of the member object files.","Process Name":"h8300-linux-gnu-objdump","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-objdump"}},{"Process":{"Description":"ranlib generates an index to the contents of an archive and stores it in the archive. The index lists each symbol defined by a member of an archive that is a relocatable object file. You may use nm -s or nm --print-armap to list this index. An archive with such an index speeds up linking to the library and allows routines in the library to call each other without regard to their placement in the archive. The GNU ranlib program is another form of GNU ar; running ranlib is completely equivalent to executing ar -s.","Process Name":"h8300-linux-gnu-ranlib","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-ranlib"}},{"Process":{"Description":"readelf displays information about one or more ELF format object files. The options control what particular information to display. elffile... are the object files to be examined. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files. This program performs a similar function to objdump but it goes into more detail and it exists independently of the BFD library, so if there is a bug in BFD then readelf will not be affected.","Process Name":"h8300-linux-gnu-readelf","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-readelf"}},{"Process":{"Description":"The GNU size utility lists the section sizes---and the total size---for each of the object or archive files objfile in its argument list. By default, one line of output is generated for each object file or each module in an archive. objfile... are the object files to be examined. If none are specified, the file \"a.out\" will be used.","Process Name":"h8300-linux-gnu-size","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-size"}},{"Process":{"Description":null,"Process Name":"h8300-linux-gnu-strings","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-strings"}},{"Process":{"Description":"GNU strip discards all symbols from object files objfile. The list of object files may include archives. At least one object file must be given. strip modifies the files named in its argument, rather than writing modified copies under different names.","Process Name":"h8300-linux-gnu-strip","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-strip"}},{"Process":{"Description":"windmc reads message definitions from an input file (.mc) and translate them into a set of output files. The output files may be of four kinds: \"h\" A C header file containing the message definitions. \"rc\" A resource file compilable by the windres tool. \"bin\" One or more binary files containing the resource data for a specific message language. \"dbg\" A C include file that maps message id's to their symbolic name. The exact description of these different formats is available in documentation from Microsoft. When windmc converts from the \"mc\" format to the \"bin\" format, \"rc\", \"h\", and optional \"dbg\" it is acting like the Windows Message Compiler.","Process Name":"h8300-linux-gnu-windmc","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-windmc"}},{"Process":{"Description":"windres reads resources from an input file and copies them into an output file. Either file may be in one of three formats: \"rc\" A text format read by the Resource Compiler. \"res\" A binary format generated by the Resource Compiler. \"coff\" A COFF object or executable. The exact description of these different formats is available in documentation from Microsoft. When windres converts from the \"rc\" format to the \"res\" format, it is acting like the Windows Resource Compiler. When windres converts from the \"res\" format to the \"coff\" format, it is acting like the Windows \"CVTRES\" program. When windres generates an \"rc\" file, the output is similar but not identical to the format expected for the input. When an input \"rc\" file refers to an external filename, an output \"rc\" file will instead include the file contents. If the input or output format is not specified, windres will guess based on the file name, or, for the input file, the file contents. A file with an extension of .rc will be treated as an \"rc\" file, a file with an extension of .res will be treated as a \"res\" file, and a file with an extension of .o or .exe will be treated as a \"coff\" file. If no output file is specified, windres will print the resources in \"rc\" format to standard output. The normal use is for you to write an \"rc\" file, use windres to convert it to a COFF object file, and then link the COFF file into your application. This will make the resources described in the \"rc\" file available to Windows.","Process Name":"h8300-linux-gnu-windres","Link":"https:\/\/linux.die.net\/man\/1\/h8300-linux-gnu-windres"}},{"Process":{"Description":null,"Process Name":"h_add","Link":"https:\/\/linux.die.net\/man\/1\/h_add"}},{"Process":{"Description":"Fields can be deleted from Sphere file headers using the utility h_delete. In its first form, the specified file(s) is copied to a new directory the -D dir option is present; otherwise the file(s) is to be modified in-place. In its second form, the specified file is written to a new file if the -o outfile option is present; otherwise the file is to be modified in-place. The -D and -o options are not compatible. At least one file must be present on the command line, as well as at least one field. If a file is to be modified in-place and the changes do not alter the size of its header, the new header is written over the old one and the speech samples do not need to be copied.","Process Name":"h_delete","Link":"https:\/\/linux.die.net\/man\/1\/h_delete"}},{"Process":{"Description":null,"Process Name":"h_edit","Link":"https:\/\/linux.die.net\/man\/1\/h_edit"}},{"Process":{"Description":"The command h_read reads the headers of the '.wav' files specified on the command line and, by default, prints the names and values of all header fields one-per-line. Various options alter the behavior of the program.","Process Name":"h_read","Link":"https:\/\/linux.die.net\/man\/1\/h_read"}},{"Process":{"Description":null,"Process Name":"h_strip","Link":"https:\/\/linux.die.net\/man\/1\/h_strip"}},{"Process":{"Description":null,"Process Name":"hackedbox","Link":"https:\/\/linux.die.net\/man\/1\/hackedbox"}},{"Process":{"Description":null,"Process Name":"hadd","Link":"https:\/\/linux.die.net\/man\/1\/hadd"}},{"Process":{"Description":"hal-disable-polling can be used to to disable and enable media detection on drives with removable storage. For more information about both the big picture and specific HAL properties, refer to the HAL spec which can be found in \/usr\/share\/doc\/hal-0.5.14\/spec\/hal-spec.html depending on the distribution.","Process Name":"hal-disable-polling","Link":"https:\/\/linux.die.net\/man\/1\/hal-disable-polling"}},{"Process":{"Description":"hal-get-capability finds device object in the HAL device database by looking at device capabilities. For more information about both the big picture and specific HAL properties, refer to the HAL spec which can be found in \/usr\/share\/doc\/hal-0.5.14\/spec\/hal-spec.html depending on the distribution.","Process Name":"hal-find-by-capability","Link":"https:\/\/linux.die.net\/man\/1\/hal-find-by-capability"}},{"Process":{"Description":null,"Process Name":"hal-find-by-property","Link":"https:\/\/linux.die.net\/man\/1\/hal-find-by-property"}},{"Process":{"Description":"hal-get-property retrieves a property from a device object in the HAL device database. For more information about both the big picture and specific HAL properties, refer to the HAL spec which can be found in \/usr\/share\/doc\/hal-0.5.14\/spec\/hal-spec.html depending on the distribution.","Process Name":"hal-get-property","Link":"https:\/\/linux.die.net\/man\/1\/hal-get-property"}},{"Process":{"Description":"hal-is-caller-locked-out determines if a specific caller is locked out of a specific D-Bus interface on a specific device. For more information about both the big picture and specific HAL properties, refer to the HAL spec which can be found in \/usr\/share\/doc\/hal-0.5.14\/spec\/hal-spec.html depending on the distribution.","Process Name":"hal-is-caller-locked-out","Link":"https:\/\/linux.die.net\/man\/1\/hal-is-caller-locked-out"}},{"Process":{"Description":"hal-lock can be used to acquire a lock on a given interface either on a given device or globally. For more information about both the big picture and the semantics of HAL locks, refer to the HAL spec which can be found in \/usr\/share\/doc\/hal-0.5.14\/spec\/hal-spec.html depending on the distribution.","Process Name":"hal-lock","Link":"https:\/\/linux.die.net\/man\/1\/hal-lock"}},{"Process":{"Description":"hal-set-property sets a property on a device object in the HAL device database. For more information about both the big picture and specific HAL properties, refer to the HAL spec which can be found in \/usr\/share\/doc\/hal-0.5.14\/spec\/hal-spec.html depending on the distribution.","Process Name":"hal-set-property","Link":"https:\/\/linux.die.net\/man\/1\/hal-set-property"}},{"Process":{"Description":"Draws the gravity force in each point on the screen seen through a halftone dot pattern. The gravity force is calculated from a set of moving mass points. View it from a distance for best effect.","Process Name":"halftone","Link":"https:\/\/linux.die.net\/man\/1\/halftone"}},{"Process":{"Description":"halibut reads the given set of input files, assembles them into a document, and outputs that document in one or more formats. The available command-line options can configure what formats Halibut should output in, and can also configure other things about the way Halibut works.","Process Name":"halibut","Link":"https:\/\/linux.die.net\/man\/1\/halibut"}},{"Process":{"Description":"The halo program draws cool patterns based on circles.","Process Name":"halo","Link":"https:\/\/linux.die.net\/man\/1\/halo"}},{"Process":{"Description":"Executes command with args while holding an exclusive lock on the file lockfile. If the file is locked by another process, halockrun will wait until the lock becomes available and executes the command then. halockrun is often used to avoid cron job overruns. halockrun uses operating system locks (POSIX -- fcntl(2)) that makes it very resistant to stale locks. -a Async mode. Starts command in the background. halockrun itself exits immediately. The exitcode of the executed command is lost. -c Create mode. Creates the lockfile if it doesn't exist. The default behavior is to exit with exitcode if the specified lockfile doesn't exist. -e exitcode Changes the exitcode returned by halockrun on fail to exitcode The default value is 99. -E nexitcode Changes the \"lock busy\" exitcode returned by halockrun The default value for this option is exitcode in non-blocking mode and 1 in testing-, and noop-mode. -n Non-blocking mode. Exits immediately with nexitcode if the lockfile is locked by another process. -f Fork mode. The normal behavior of halockrun is to get the lock and call execvp(2) to execute the command specified. If fork mode, halockrun will do a fork(2) and run the command in its own process. In this case the parent--halockrun itself--holds the lock on lockfile. The split in two processes opens the risk that the parent halockrun--that holds the lock--dies and leaves command unprotected. To minimize this risk, halockrun ignores the following signals: TERM, HUP, INT, PIPE, QUIT, USR1, and USR2. This option is useful to make sure that the command does not see any difference when running under protection of halockrun. In other words, the fork mode makes sure that the open file handle to lockfile is not visible to command. Fork mode has also influence on the -t switch, because that will not return the PID of command but of the parent halockrun process. This option is new to version V0.99.08, prior this version halockrun has always fork(2)ed. The default was changed because reliability is more important than transparency--in that case. If you are in doubt about this option: don't use it. -t Test mode. Checks if lockfile is currently busy, and prints the PID of the process that holds the lock--if any. See also -N for that purpose. The exitcode is 0 if the lock is free or nexitcode if it is busy. Any other error--such as \"lockfile not found\"--will result in exitcode. Regardless of the lock status, halockrun will not run any command. When the -t flags is specified the flags -a, -n and -f are meaningless and will be ignored if they are specified. -N No-op mode. The -N flag is semantically equivalent to: halockrun lockfile \/bin\/true It supplements the -t switch because it blocks. It can be combined with the -n option to change the blocking behavior. However, it doesn't print the PID of the the lock holder if it is busy. The -N switch is more portable and used to synchronize with a running process. The -t switch is used to obtain the PID of the running process. When the -N flag is specified, the -a and -f flags are meaningless and will be ignored if they are specified. -v[v] Verbose mode and more verbose mode--almost debug mode.","Process Name":"halockrun","Link":"https:\/\/linux.die.net\/man\/1\/halockrun"}},{"Process":{"Description":null,"Process Name":"hammerhead","Link":"https:\/\/linux.die.net\/man\/1\/hammerhead"}},{"Process":{"Description":"HAProxy is a TCP\/HTTP reverse proxy which is particularly suited for high availability environments. Indeed, it can: - route HTTP requests depending on statically assigned cookies ; - spread the load among several servers while assuring server persistence through the use of HTTP cookies ; - switch to backup servers in the event a main one fails ; - accept connections to special ports dedicated to service monitoring ; - stop accepting connections without breaking existing ones ; - add\/modify\/delete HTTP headers both ways ; - block requests matching a particular pattern ; - hold clients to the right application server depending on application cookies - report detailed status as HTML pages to authenticated users from an URI intercepted from the application. It needs very little resource. Its event-driven architecture allows it to easily handle thousands of simultaneous connections on hundreds of instances without risking the system's stability.","Process Name":"haproxy","Link":"https:\/\/linux.die.net\/man\/1\/haproxy"}},{"Process":{"Description":"This manual page documents hardlink, a program which consolidates duplicate files in one or more directories using hardlinks. hardlink traverses one or more directories searching for duplicate files. When it finds duplicate files, it uses one of them as the master. It then removes all other duplicates and places a hardlink for each one pointing to the master file. This allows for conservation of disk space where multiple directories on a single filesystem contain many duplicate files. Since hard links can only span a single filesystem, hardlink is only useful when all directories specified are on the same filesystem.","Process Name":"hardlink","Link":"https:\/\/linux.die.net\/man\/1\/hardlink"}},{"Process":{"Description":null,"Process Name":"hash","Link":"https:\/\/linux.die.net\/man\/1\/hash"}},{"Process":{"Description":"Computes multiple hashes, or message digests, for any number of files while optionally recursively digging through the directory structure. By default the program computes MD5 and SHA-256 hashes, equivalent to -c md5,sha256. Can also take a list of known hashes and display the filenames of input files whose hashes either do or do not match any of the known hashes. Can also use a list of known hashes to audit a set of FILES. Errors are reported to standard error. If no FILES are specified, reads from standard input. -c <alg1>[,<alg2>...] Computation mode. Compute hashes of FILES using the algorithms specified. Legal values are md5, sha1, sha256, tiger, and whirlpool. -k Load a file of known hashes. This flag is required when using any of the matching or audit modes (i.e. -m, -x, -M, -X, or -a) This flag may be used more than once to add multiple sets of known hashes. Loading sets with different hash algorithms can sometimes generate spurrious hash collisions. For example, let's say we have two hash sets, A and B, which have some overlapping files. For example, the file \/usr\/bin\/bad is in both sets. In A we've recorded the MD5 and SHA-256. In B we've recorded the MD5, SHA-1, and SHA-256. Because these two records are different, they will both be loaded. When the program computes all three hashes and compares them to the set of knowns, we will get an exact match from the record in B and a collision from the record in A. -a Audit mode. Each input file is compared against the set of knowns. An audit is said to pass if each input file is matched against exactly one file in set of knowns. Any collisions, new files, or missing files will make the audit fail. Using this flag alone produces a message, either \"Audit passed\" or \"Audit Failed\". Use the verbose modes, -v, for more details. Using -v prints the number of files in each category. Using -v a second time prints any discrepancies. Using -v a third time prints the results for every file examined and every known file. Due to limitations in the program, any filenames with Unicode characters will appear to have moved during an audit. See the section \"UNICODE SUPPORT\" below. -m Positive matching, requires at least one use of the -k flag. The input files are examined one at a time, and only those files that match the list of known hashes are output. The only acceptable format for known hashes is the output of previous hashdeep runs. If standard input is used with the -m flag, displays \"stdin\" if the input matches one of the hashes in the list of known hashes. If the hash does not match, the program displays no output. This flag may not be used in conjunction with the -x, -X, or -a flags. See the section \"UNICODE SUPPORT\" below. -x Negative matching. Same as the -m flag above, but does negative matching. That is, only those files NOT in the list of known hashes are displayed. This flag may not be used in conjunction with the -m, -M, or -a flags. See the section \"UNICODE SUPPORT\" below. -w When used with positive matching modes (-m,-M) displays the filename of the known hash that matched the input file. See the section \"UNICODE SUPPORT\" below. -M and -X Same as -m and -x above, but displays the hash for each file that does (or does not) match the list of known hashes. -r Enables recursive mode. All subdirectories are traversed. Please note that recursive mode cannot be used to examine all files of a given file extension. For example, calling hashdeep -r *.txt will examine all files in directories that end in .txt. -e Displays a progress indicator and estimate of time remaining for each file being processed. Time estimates for files larger than 4GB are not available on Windows. This mode may not be used with th -p mode. -i <size> Size threshold mode. Only hash files smaller than the given the threshold. Sizes may be specified using multiplers b,k,m,g,t,p, and e. -o <bcpflsd> Enables expert mode. Allows the user specify which (and only which) types of files are processed. Directory processing is still controlled with the -r flag. The expert mode options allowed are: f - Regular files b - Block Devices c - Character Devices p - Named Pipes l - Symbolic Links s - Sockets d - Solaris Doors -s Enables silent mode. All error messages are supressed. -p Piecewise mode. Breaks files into chunks before hashing. Chunks may be specified using multiplers b,k,m,g,t,p, and e. (Never let it be said that the author didn't plan ahead.) -b Enables bare mode. Strips any leading directory information from displayed filenames. This flag may not be used in conjunction with the -l flag. -l Enables relative file paths. Instead of printing the absolute path for each file, displays the relative file path as indicated on the command line. This flag may not be used in conjunction with the -b flag. -v Enables verbose mode. Use again to make the program more verbose. This mostly changes the behvaior of the audit mode, -a. -h Show a help screen and exit. -V Show the version number and exit.","Process Name":"hashdeep","Link":"https:\/\/linux.die.net\/man\/1\/hashdeep"}},{"Process":{"Description":null,"Process Name":"hatimerun","Link":"https:\/\/linux.die.net\/man\/1\/hatimerun"}},{"Process":{"Description":"hattrib permits the alteration of HFS file attributes. In the first form, the MacOS-defined type and creator attributes can be set using the -t and -c flags, respectively. A file's invisible flag can be set or cleared with +i and -i, respectively. Finally, a file's locked flag can be set or cleared with +l and -l. All files mentioned on the command line will receive the specified attributes, regardless of the file's current attributes. Any attribute not mentioned in the command line is left unchanged. In the second form, a single HFS pathname refering to a folder is given with the -b option, causing it to become \"blessed\" as the MacOS System Folder. For this to be useful, the folder should contain valid Macintosh System and Finder files. Note that the locked and \"blessed\" attributes have little consequence to hfsutils.","Process Name":"hattrib","Link":"https:\/\/linux.die.net\/man\/1\/hattrib"}},{"Process":{"Description":"\/usr\/share\/heartbeat\/hb_addnode adds a new node, or multiple nodes, to the cluster configuration. If there is any node in the arguments that is already a cluster member, the command fails and no nodes are added.","Process Name":"hb_addnode","Link":"https:\/\/linux.die.net\/man\/1\/hb_addnode"}},{"Process":{"Description":"\/usr\/share\/heartbeat\/hb_delnode removes a node, or multiple nodes, from the cluster configuration. If there is any node in the arguments that is currently not a cluster member, the command fails and no nodes are removed.","Process Name":"hb_delnode","Link":"https:\/\/linux.die.net\/man\/1\/hb_delnode"}},{"Process":{"Description":null,"Process Name":"hb_standby","Link":"https:\/\/linux.die.net\/man\/1\/hb_standby"}},{"Process":{"Description":"Warning This command is deprecated. It is only suitable for legacy Heartbeat clusters without Pacemaker enabled. In Pacemaker-enabled clusters, the crm(8) shell supports switching individual nodes into standby mode, and replaces hb_takeover. \/usr\/share\/heartbeat\/hb_takeover issues a request to the cluster to move resources to the node where it is invoked, from the other node. Issuing hb_takeover on the current node is equivalent to performing hb_standby on the other node.","Process Name":"hb_takeover","Link":"https:\/\/linux.die.net\/man\/1\/hb_takeover"}},{"Process":{"Description":"CJK bitmap fonts can't be directly used with TeX because the number of characters in such fonts exceeds 256, the limit of a TeX font. Thus it is necessary to split these fonts into subfonts, and this is exactly what hbf2gf does. As the name says, hbf2gf uses CJK fonts in a certain format which is called Hanzi Bitmap Font (HBF) format. It simply consists of the CJK bitmap file(s) and a text file in a format very similar to the BDF format of the X Window System which describes the bitmap font files: the encoding, the size, etc. The produced GF files can then be converted with gftopk into standard PK files. hbf2gf can be called in three modes: hbf2gf [ -q] configuration-file[ .cfg] This call normally creates a set of GF files, one PL file, and a batch file which must be executed after hbf2gf has finished. This script will then call gftopk to convert all GF files into PK files, and it will call pltotf to convert the PL file into a TFM file. Finally it will copy the TFM file so that each PK file has its TFM file (which are all identical). If ofm_file is set to 'yes' in the configuration file, OFM and OVF files will be created too. -q makes hbf2gf quiet. hbf2gf [ -q] [ -p] [ -g] [ -n] subfont-name x-resolution [ y-scale | y-resolution] This mode is intended for use with mktexpk and its derivates. Only one GF file together with a PL file for the given subfont will be computed, taking the horizontal resolution and a vertical scaling factor (if the value is smaller than 10) resp. the vertical resolution (otherwise) from the command line, ignoring the nmb_fonts parameter of the configuration file. The last two characters (which are interpreted as the subfont number) are stripped to get the name for the configuration file (which must end with '.cfg'). No job file will be created. If option -p is set, no PL file is created. If -g is set, no GF file is created. The extension can be controlled with -n; if set, the extension is '.gf', otherwise '.< resolution>gf'. -q makes hbf2gf quiet. hbf2gf -t [ -q] subfont-name This mode is intended for use with scripts like mktexpk; it tests whether the specified subfont name leads to an hbf2gf configuration file. It returns 0 on success and prints out the name of that configuration file (provided the -q switch isn't set). This test isn't a thorough one; it only removes the last two characters and checks whether a configuration file with that name exists. See the next section for more details about configuration files. Specifying the option --version returns the current version of hbf2gf and the used file search library (e.g. kpathsea). Usage information is shown with the --help parameter.","Process Name":"hbf2gf","Link":"https:\/\/linux.die.net\/man\/1\/hbf2gf"}},{"Process":{"Description":null,"Process Name":"hboot","Link":"https:\/\/linux.die.net\/man\/1\/hboot"}},{"Process":{"Description":"The names hcc and hcp have been deprecated in favor of mpicc and mpiCC. Please see their corresponding man pages.","Process Name":"hcc","Link":"https:\/\/linux.die.net\/man\/1\/hcc"}},{"Process":{"Description":null,"Process Name":"hcd","Link":"https:\/\/linux.die.net\/man\/1\/hcd"}},{"Process":{"Description":"hcitool is used to configure Bluetooth connections and send some special command to Bluetooth devices. If no command is given, or if the option -h is used, hcitool prints some usage information and exits.","Process Name":"hcitool","Link":"https:\/\/linux.die.net\/man\/1\/hcitool"}},{"Process":{"Description":"hcopy transfers files from an HFS volume to UNIX or vice versa. The named source files are copied to the named destination target, which must be a directory if multiple files are to be copied. Copies are performed using a translation mode, which must be one of: -m MacBinary II: A popular format for binary file transfer. Both forks of the Macintosh file are preserved. This is the recommended mode for transferring arbitrary Macintosh files. -b BinHex: An alternative format for ASCII file transfer. Both forks of the Macintosh file are preserved. -t Text: Performs end-of-line translation. Only the data fork of the Macintosh file is copied. -r Raw Data: Performs no translation. Only the data fork of the Macintosh file is copied. -a Automatic: A mode will be chosen automatically for each file based on a set of predefined heuristics. If no mode is specified, -a is assumed. If a UNIX source pathname is specified as a single dash (-), hcopy will copy from standard input to the HFS destination. Likewise, a single dash used as a UNIX destination pathname will cause hcopy to copy the HFS source to standard output.","Process Name":"hcopy","Link":"https:\/\/linux.die.net\/man\/1\/hcopy"}},{"Process":{"Description":"The names hcc and hcp have been deprecated in favor of mpicc and mpiCC. Please see their corresponding man pages.","Process Name":"hcp","Link":"https:\/\/linux.die.net\/man\/1\/hcp"}},{"Process":{"Description":null,"Process Name":"hdel","Link":"https:\/\/linux.die.net\/man\/1\/hdel"}},{"Process":{"Description":"HDF is a multi-object file format that facilitates the transfer of various types of scientific data between machines and operating systems. Platforms currently supported include Linux 32 and 64-bit, SunOS 32 and 64-bit, Windows, FreeBSD, and Mac Intel. HDF allows self-definitions of data content and easy extensibility for future enhancements or compatibility with other standard formats. HDF includes Fortran and C calling interfaces, and utilities for manipulating, viewing, and analyzing data in HDF files. The HDF library contains interfaces for storing and retrieving compressed or uncompressed 8-bit and 24-bit raster images with palettes, n-Dimensional scientific datasets and binary tables. An interface is also included that allows arbitrary grouping of other HDF objects. HDF Raster Images HDF supports the storing of both 8-bit and 24-bit raster images. As well as storing information about the dimensions and palette of a raster image, HDF supports raster image compression. In previous versions of HDF, Run-length encoding and Imcomp compression were both supported. With HDF> 3.3 JPEG compression is also available. HDF Scientific Data Sets Scientific Data Sets (SDSs) are useful for storing n-Dimensional gridded data. The actual data in the dataset can be of any of the \"standard\" number types: 8, 16 and 32bit signed and unsigned integers and 32 and 64bit floating point values. In addition, a certain amount of meta-data can be stored with an SDS including: o The coordinate system to use when interpreting or displaying the data.\no Scales to be used for each dimension.\no Labels for each dimension and the dataset as a whole.\no Units for each dimension and the data.\no The valid max and min values for the data.\no Calibration information for the data.\no Fill or missing value information.\no Ability of have more than one file open at a time.\no A more general framework for meta-data within the SDS data-model\n  (allowing 'name = value' styel meta-data).\no Support for an \"unlimited dimension\" in the SDS data-model, making\n  it possible to append planes to an array along one dimension. HDF Annotations Any object in an HDF file can have annotations associated with it. There are a number of types of annotations: o Labels are assumed to be short strings giving the \"name\" of a\n  data object.\no Descriptions are longer text segments that are useful for giving\n  more indepth information about a data object\no File annotations are assumed to apply to all of the objects in a\n  single file. HDF Vset Interfaces The Vset module provides interfaces to two basic HDF building blocks. Vgroups are generic grouping elements allowing a user to associate related objects within an HDF file. As Vgroups can contain other Vgroups, it is possible to build a hierarchical file. Vdatas are data structures made up of fields and records. Data is organized into 'fields' within each Vdata. Each field is identified by a unique 'fieldname'. The type of each field may be any of the basic number types that HDF supports. Fields of different types may exist within the same Vdata. By combining Vdatas in Vgroups it is possible to represent higher level data constructs: mesh data, multi-variate datasets, sparse matrices, finite-element data, spreadsheets, splines, non-Cartesian coordinate data, etc. HDF > 3.3 and netCDF HDF > 3.3 merges in the netCDF library produced by Unidata. The full netCDF library is supported as is a new \"multi-file\" SDS interface. Both of these interfaces can read old netCDF files and HDF files transparently. EXAMPLES All HDF routines require the header \"hdf.h\" to be included in the C source file. If using the SDS routines the header \"mfhdf.h\" should be included instead in in the C source file. Fortran programs should use \"dffunc.inc\" and \"hdf.inc\". To compile a program that makes HDF calls on most Unix platforms. (FORTRAN): {HDFLIBDIR}\/bin\/h4fc myprog.f (C): {HDFLIBDIR}\/bin\/h4cc myprog.c","Process Name":"hdf","Link":"https:\/\/linux.die.net\/man\/1\/hdf"}},{"Process":{"Description":"This program is part of Netpbm(1). hdifftopam undoes what pamtohdiff does.","Process Name":"hdifftopam","Link":"https:\/\/linux.die.net\/man\/1\/hdifftopam"}},{"Process":{"Description":null,"Process Name":"hdir","Link":"https:\/\/linux.die.net\/man\/1\/hdir"}},{"Process":{"Description":"Print the first 10 lines of each FILE to standard output. With more than one FILE, precede each with a header giving the file name. With no FILE, or when FILE is -, read standard input. Mandatory arguments to long options are mandatory for short options too. -c, --bytes= [-]K print the first K bytes of each file; with the leading '-', print all but the last K bytes of each file -n, --lines= [-]K print the first K lines instead of the first 10; with the leading '-', print all but the last K lines of each file -q, --quiet, --silent never print headers giving file names -v, --verbose always print headers giving file names --help display this help and exit --version output version information and exit K may have a multiplier suffix: b 512, kB 1000, K 1024, MB 1000*1000, M 1024*1024, GB 1000*1000*1000, G 1024*1024*1024, and so on for T, P, E, Z, Y.","Process Name":"head","Link":"https:\/\/linux.die.net\/man\/1\/head"}},{"Process":{"Description":"header(1) prints image header fields to stdout.","Process Name":"header","Link":"https:\/\/linux.die.net\/man\/1\/header"}},{"Process":{"Description":"With no arguments, hebcal will print to stdout the dates of the Jewish holidays in the current secular year. Each line is prefixed with a gregorian date of the form mm\/dd\/yyyy. By specifying month, day, or year, output can be limited to a particular month or date in a particular year. Note that year is usually a four-digit integer, So 92 is during the Roman period, not the late twentieth century. In if the hebrew dates option is turned on, this number represents th Jewish calendar year. month is a number from 1..12, or the name of a Jewish calendar month. day is a number from 1..31. For example, the command hebcal 10 1992 will print out the holidays occurring in October of 1992 C.E., while the command hebcal Tish 5752 will print dates of interest in the month of Tishrei in Jewish calendar year 5752. NOTE: hebcal 92 is not the same as hebcal 1992. The year is assumed to be complete, so the former calendar preceeds the latter by nineteen centuries. A few other bells and whistles include the weekly sedra as well as the day of the week, the count of the omer, and the Hebrew date. Output from hebcal can be used to drive calendar(1). Day-to-day use for hebcal is provided for in the -T and -t switches, which print out Jewish calendar entries for the current date. To get a quick-reference on-line help, type hebcal help at the command prompt.","Process Name":"hebcal","Link":"https:\/\/linux.die.net\/man\/1\/hebcal"}},{"Process":{"Description":"The helcor command computes value of heliocentric correction for given julian date and object's coordinates. It may also append the values to a set of measurements stored in a text file. When the -j option is present on the command line, the value of heliocentric correction is printed to the standard output stream. If one or more filenames are present on the command line, each source file given is processed line by line, the program expects the JD value in the first column, which must be divided at least one of common used dividers (semicolon, comma, space, tab char, ...). The JD value can be in full (2453xxx.x) or short (53xxx.x) form. Decimal places must be separated by point, not comma. The julian date is replaced by a corrected date and the value of correction may be optionally appended to the end of the line. If the line starts with the text JD, it is considered to be a table header and it is changed to JDHEL or JDGEO. The text HELCOR is optionally appended to the end of the line. All other lines which do not fit to any of previous rules are copied to the output file without modification.","Process Name":"helcor","Link":"https:\/\/linux.die.net\/man\/1\/helcor"}},{"Process":{"Description":null,"Process Name":"helix","Link":"https:\/\/linux.die.net\/man\/1\/helix"}},{"Process":{"Description":"Print a friendly, customizable greeting. -h, --help display this help and exit -v, --version display version information and exit -t, --traditional use traditional greeting format -n, --next-generation use next-generation greeting format -g, --greeting= TEXT use TEXT as the greeting message","Process Name":"hello","Link":"https:\/\/linux.die.net\/man\/1\/hello"}},{"Process":{"Description":"","Process Name":"help","Link":"https:\/\/linux.die.net\/man\/1\/help"}},{"Process":{"Description":null,"Process Name":"help2man","Link":"https:\/\/linux.die.net\/man\/1\/help2man"}},{"Process":{"Description":"heme is intended to be fast and portable console hex editor for unix systems. It has undo support (number of undo operations is only limited by available memory), ability to fill a range of addresses with the specified byte, ability to search for a single byte or character string. Offsets can be given in hexadecimal, octal or decimal. There are two editing modes: hex and ascii. In hex mode you type hexadecimal digits and in ascii mode you type characters. In the ascii mode cursor is automatically moved to the next byte after typing a character (this behaviour is configurable for hex mode). heme uses curses library for screen and input handling. Colors are supported and they may be configured in the configuration file (see below). If the configuration file isn't present, heme will use defaults. You may also specify which configuration file to use with the -c command line option.","Process Name":"heme","Link":"https:\/\/linux.die.net\/man\/1\/heme"}},{"Process":{"Description":"hesinfo takes two arguments, a name to be resolved and a string, known as a HesiodNameType. It then prints the information returned by the Hesiod nameserver. The value returned by hesinfo is of the type HesiodNameType. hesinfo understands the following options: -l Selects long format. -b Prints the fully-qualified string passed to the nameserver. VALID Hesiod_Names The following types of identifiers may be used in the HesiodName argument to hesinfo. These values will be resolved by accessing the hesiod database. <username> the 8-character-or-less string used to identify users or classes (e.g. joeuser, root, 1.00, etc). Used with the Hesiod_Name_Types passwd, pobox, and filsys. <uid> the id number assigned to a user. <groupid> the id number assigned to a group. <groupname> a name identifying a unique group. <file-system-name> the name of an athena file system. <rvd server>:<pack> the name of an rvd's server and pack seperated by a colon. <nfs server>:<partition> the name of an nfs server and its partition seperated by a colon. <workstation-name> the machine name of an Athena workstation (e.g. E40-343-3). <service-name> name of an Athena service (e.g. Zephyr). <service-type> name of Unix service (valid entries are defined in \/etc\/services). <printer-name> name of a printer. <printer-cluster-name> name of an Athena print cluster. <foo> some hesinfo calls (e.g. prclusterlist ) do not require a specific HesiodName argument. However, you must include a dummy string (e.g. 'foo') for hesinfo to work properly. VALID Hesiod_Name_Types The following symbols are valid substitutions for the HesiodNameType argument to hesinfo. passwd returns string suitable for inclusion in \/etc\/passwd, searching with <username>. pobox returns information on the pobox assigned to the user specified by HesiodName, searching with <username>. uid returns string suitable for inclusion in \/etc\/passwd, searching with <uid>. gid returns string suitable for inclusion in \/etc\/group, searching with <groupid>. group returns string suitable for inclusion in \/etc\/group, searching with <groupname>. grplist returns subgroups included in superset defined by <groupname>. filsys returns file system type, export point, server, mount mode, and import point for the following valid HesiodNames (see above) - <file system name>, <username>, <rvd server>:<pack>, and <nfs server>:<partition> cluster returns information about the local cluster the workstation, specified by <workstation name>. Included is information about the local file and print servers. This information is accesses by clusterinfo at boot time. sloc returns network name of service host for <service-name>. service returns Internet protocol type and protocol service port for <service-type>. pcap returns a valid entry for \/etc\/printcap for <printer-name>. prcluserlist returns a list of print clusters. prcluster returns a list of printers in a cluster specified by <printer-cluster-name>.","Process Name":"hesinfo","Link":"https:\/\/linux.die.net\/man\/1\/hesinfo"}},{"Process":{"Description":"Usage: hexchat [OPTION...] Help Options: -h, --help Show help options --help-all Show all help options --help-gtk Show GTK+ Options Application Options: -a, --no-auto Don't auto connect to servers -d, --cfgdir= PATH Use a different config directory -n, --no-plugins Don't auto load any plugins -p, --plugindir Show plugin auto-load directory -u, --configdir Show user config directory --url= URL Open an irc:\/\/server:port\/channel URL -c, --command= COMMAND Execute command: -e, --existing Open URL or execute command in an existing HexChat --minimize= level Begin minimized. Level 0=Normal 1=Iconified 2=Tray -v, --version Show version information --display= DISPLAY X display to use","Process Name":"hexchat","Link":"https:\/\/linux.die.net\/man\/1\/hexchat"}},{"Process":{"Description":"The hexdump utility is a filter which displays the specified files, or the standard input, if no files are specified, in a user specified format. The options are as follows:       -b'            One-byte octal display.  Display the input offset inhexadecimal, followed by sixteen space-separated, threecolumn, zero-filled, bytes of input data, in octal, per line. -c' One-byte character display. Display the input offset in hexadecimal, followed by sixteen space-separated, three column, space-filled, characters of input data per line. -C' Canonical hex+ASCII display. Display the input offset in hexadecimal, followed by sixteen space-separated, two column, hexadecimal bytes, followed by the same sixteen bytes in %_p format enclosed in ''|'' characters. -d' Two-byte decimal display. Display the input offset in hexadecimal, followed by eight space-separated, five column, zero-filled, two-byte units of input data, in unsigned decimal, per line. -e format_string Specify a format string to be used for displaying data. -f format_file Specify a file that contains one or more newline separated format strings. Empty lines and lines whose first non-blank character is a hash mark (#) are ignored. -n length Interpret only length bytes of input. -o' Two-byte octal display. Display the input offset in hexadecimal, followed by eight space-separated, six column, zero-filled, two byte quantities of input data, in octal, per line. -s offset Skip offset bytes from the beginning of the input. By default, offset is interpreted as a decimal number. With a leading 0x or 0X, offset is interpreted as a hexadecimal number, otherwise, with a leading 0, offset is interpreted as an octal number. Appending the character b, k, or m to offset causes it to be interpreted as a multiple of 512, 1024, or 1048576, respectively. -v' The -v option causes hexdump to display all input data. Without the -v option, any number of groups of output lines, which would be identical to the immediately preceding group of output lines (except for the input offsets), are replaced with a line comprised of a single asterisk. -x' Two-byte hexadecimal display. Display the input offset in hexadecimal, followed by eight, space separated, four column, zero-filled, two-byte quantities of input data, in hexadecimal, per line. For each input file, hexdump sequentially copies the input to standard output, transforming the data according to the format strings specified by the -e and -f options, in the order that they were specified. Formats A format string contains any number of format units, separated by whitespace. A format unit contains up to three items: an iteration count, a byte count, and a format. The iteration count is an optional positive integer, which defaults to one. Each format is applied iteration count times. The byte count is an optional positive integer. If specified it defines the number of bytes to be interpreted by each iteration of the format. If an iteration count and\/or a byte count is specified, a single slash must be placed after the iteration count and\/or before the byte count to disambiguate them. Any whitespace before or after the slash is ignored. The format is required and must be surrounded by double quote (\" \") marks. It is interpreted as a fprintf-style format string (see fprintf(3)), with the following exceptions: \u2022 An asterisk (*) may not be used as a field width or precision. \u2022 A byte count or field precision is required for each ''s'' conversion character (unlike the fprintf(3) default which prints the entire string if the precision is unspecified). \u2022 The conversion characters ''h'', ''l'', ''n'', ''p'' and ''q'' are not supported. \u2022 The single character escape sequences described in the C standard are supported: NUL \\0 <alert character> \\a <backspace> \\b <form-feed> \\f <newline> \\n <carriage return> \\r <tab> \\t <vertical tab> \\v Hexdump also supports the following additional conversion strings:       _a[dox]'            Display the input offset, cumulative across inputfiles, of the next byte to be displayed.  The appendedcharacters d, o, and x specify the display base as decimal,octal or hexadecimal respectively. _A[ dox]' Identical to the _a conversion string except that it is only performed once, when all of the input data has been processed. _c' Output characters in the default character set. Nonprinting characters are displayed in three character, zero-padded octal, except for those representable by standard escape notation (see above), which are displayed as two character strings. _p' Output characters in the default character set. Nonprinting characters are displayed as a single ''.''. _u' Output US ASCII characters, with the exception that control characters are displayed using the following, lower-case, names. Characters greater than 0xff, hexadecimal, are displayed as hexadecimal strings. 000 nul 001 soh 002 stx 003 etx 004 eot 005 enq 006 ack 007 bel 008 bs 009 ht 00A lf 00B vt 00C ff 00D cr 00E so 00F si 010 dle 011 dc1 012 dc2 013 dc3 014 dc4 015 nak 016 syn 017 etb 018 can 019 em 01A sub 01B esc 01C fs 01D gs 01E rs 01F us 0FF del The default and supported byte counts for the conversion characters are as follows: %_c, %_p, %_u, %c'                        One byte counts only. %d, %i, %o, %u, %X, %x Four byte default, one, two and four byte counts supported. %E, %e, %f, %G, %g' Eight byte default, four byte counts supported. The amount of data interpreted by each format string is the sum of the data required by each format unit, which is the iteration count times the byte count, or the iteration count times the number of bytes required by the format if the byte count is not specified. The input is manipulated in ''blocks'', where a block is defined as the largest amount of data specified by any format string. Format strings interpreting less than an input block's worth of data, whose last format unit both interprets some number of bytes and does not have a specified iteration count, have the iteration count incremented until the entire input block has been processed or there is not enough data remaining in the block to satisfy the format string. If, either as a result of user specification or hexdump modifying the iteration count as described above, an iteration count is greater than one, no trailing whitespace characters are output during the last iteration. It is an error to specify a byte count as well as multiple conversion characters or strings unless all but one of the conversion characters or strings is _a or _A. If, as a result of the specification of the -n option or end-of-file being reached, input data only partially satisfies a format string, the input block is zero-padded sufficiently to display all available data (i.e. any format units overlapping the end of data will display some number of the zero bytes). Further output by such format strings is replaced by an equivalent number of spaces. An equivalent number of spaces is defined as the number of spaces output by an s conversion character with the same field width and precision as the original conversion character or conversion string but with any ''+'', '' '', ''#'' conversion flag characters removed, and referencing a NULL string. If no format strings are specified, the default display is equivalent to specifying the -x option. hexdump exits 0 on success and >0 if an error occurred.","Process Name":"hexdump","Link":"https:\/\/linux.die.net\/man\/1\/hexdump"}},{"Process":{"Description":"hexedit shows a file both in ASCII and in hexadecimal. The file can be a device as the file is read a piece at a time. You can modify the file and search through it.","Process Name":"hexedit","Link":"https:\/\/linux.die.net\/man\/1\/hexedit"}},{"Process":{"Description":null,"Process Name":"hf77","Link":"https:\/\/linux.die.net\/man\/1\/hf77"}},{"Process":{"Description":"hformat is used to write a new HFS filesystem to a volume. A UNIX pathname to the volume's destination must be specified. The destination may be either a block device or a regular file, but it must already exist and be writable. An optional label can be specified to name the volume. The name must be between 1-27 characters and cannot contain a colon (:). By default, the volume will be named Untitled. If the destination medium is partitioned, one partition must be selected to receive the filesystem. If there is only one HFS partition on the medium, it will be selected by default. Otherwise, the desired partition number must be specified (as the ordinal nth HFS partition) on the command-line. The size of the partition determines the size of the resulting volume. Partition number 0 can be specified to format the entire medium as a single filesystem without a partition map, erasing any existing partition information. Since this will destroy all the partitions, the -f option must be specified to force this operation if the medium currently contains a partition map. If the medium is not partitioned (or if partition 0 is specified), the size or capacity of the medium determines the size of the resulting volume. The new volume will be empty and will become \"current\" so subsequent commands will refer to it. The current working directory for the volume is set to the root of the volume.","Process Name":"hformat","Link":"https:\/\/linux.die.net\/man\/1\/hformat"}},{"Process":{"Description":null,"Process Name":"hfs","Link":"https:\/\/linux.die.net\/man\/1\/hfs"}},{"Process":{"Description":"hfssh is a Tcl interpreter like tclsh(1) but which also implements the following extensions to support manipulation of Macintosh HFS media: hfs mount path [partno] Mounts the indicated HFS partition from the given path. An HFS volume handle is returned, which may be used for further volume commands described below. hfs zero path nparts The given path is overwritten with a Macintosh partition structure which can accommodate up to nparts partitions. All space on the medium is initially allocated to an empty partition, from which new partitions can be created using hfs mkpart. The number of blocks in this empty space available for partitioning is returned. hfs mkpart path nblocks A new HFS partition is created from the available free space on the specified Macintosh-partitioned medium. The partition is created with a size of nblocks. Any remaining free blocks left in the empty partition space can be further allocated to other new partitions, as long as there are enough partition slots remaining. N.B. When the last remaining partition slot is used, all remaining free space must be allocated to it. It is therefore best to consider this when initially creating the total number of partition slots with hfs zero. hfs nparts path This command returns the number of HFS partitions which exist on the Macintosh-formatted medium specified by path. If path does not appear to have a Macintosh partition map, or if an error occurs, this command will return -1. Otherwise, it will return a number greater than or equal to 0. hfs format path partno vname [bblist] This command creates a new HFS volume by formatting the given path and partition partno and giving it a volume label vname. If it is desired to \"spare\" some blocks from being used by the volume, a list of \"bad block\" numbers can be given, relative to the beginning of the partition. The given blocks will be mapped out of use (if possible) and the size of the resulting volume will be decreased. hfs flushall All pending changes to all open volumes are flushed immediately. This is useful to do periodically to avoid accidental loss of data when volumes are open for long periods of time. hfs chartrans fromset toset string This command translates the given string from the fromset character set to the toset set. Both fromset and toset can be one of latin1 (ISO 8859-1) or macroman (MacOS Standard Roman). A new (translated) string is returned. The translation is not necessarily reversible, since the two character sets do not have a complete one-to-one mapping. hfs version The current running version of hfsutils is returned. hfs copyright A copyright notice is returned. hfs author The name and email address of the author of hfsutils is returned. hfs license A license statement for hfsutils is returned. vol vname The volume name of the given vol handle is returned. This is also the name of the volume's root directory, needed to construct absolute pathnames on the volume. vol size A list of two numbers is returned; the first is the total size of the given vol (in bytes), and the second is the number of free bytes that are currently available. vol crdate The creation date of the given vol is returned, expressed as a number of seconds since 00:00:00 01-Jan-1970 UTC. vol mddate The last modification date of the given vol is returned, expressed as a number of seconds since 00:00:00 01-Jan-1970 UTC. vol islocked A boolean value (either 1 or 0) is returned, indicating whether the given vol handle is locked for read-only access. It may be locked because the medium is physically locked through hardware, or because the medium was opened read-only for special reasons (such as another process also has the medium open). vol umount The indicated vol is unmounted, flushing any unsaved data to the volume and closing the access path to the medium. The vol handle subsequently becomes invalid for further use. vol cwd A numeric value is returned indicating the catalog node ID (CNID) of the current working directory on the given vol. This value can be passed to vol dirinfo to learn the directory's name and parent CNID. vol path A list of directory names is returned, representing the hierarchy between the root and the current directory. These names can be joined with vol sepchar characters (:) to construct an absolute pathname to the current directory. The same information can be acquired by traversing the CNIDs from the current directory to the root using vol dirinfo. (The root directory always has a CNID of 2.) vol dir [path] A list is returned describing the contents of the given directory path (defaulting to the current directory) on the given vol. Each element of the list describes one entry, and contains a set of attribute\/value pairs represented as another list, suitable for assignment to a Tcl array using array set. vol flush All pending changes to the given volume are flushed immediately. vol sepchar The HFS path separator character \":\" is returned. vol cd path vol chdir path The current working directory on the given volume is changed to path, which may be either an absolute or relative path. vol dirinfo cnid A two-element list describing the directory having the given cnid on the given vol is returned. The first element contains the name of the directory, while the second element contains the CNID of the directory's parent. Two CNID values are special: the root directory of the volume has CNID 2, and the \"parent\" of the root directory is returned with CNID 1. vol open path The file on vol having the given path is opened. An HFS file handle is returned, which may be used for further file commands described below. vol stat path Information about the file or directory having the given path is returned in much the same way as vol dir except that only the single argument is described (not its contents). vol mkdir path A new directory on vol having the given path is created. All of the parent directories leading to path must already exist, but path itself must not. vol rmdir path The directory on vol with the given path is removed. The directory must be empty. vol delete path The file on vol with the given path is removed. Both resource and data forks of the file are deleted. vol touch path The modification time for the file or directory specified by path on the given vol is updated to the current time. vol glob pattern The given pattern is treated as a list of globbing patterns, each of which may be expanded to the names of files or directories on the given vol according to the globbing rules described in the hfsutils(1) documentation. The resulting pathnames are returned in a (possibly longer) list. If a pattern does not match any file or directory name, it is returned in the resulting list unchanged. vol bless path The folder named by the given path is \"blessed\" as the MacOS System Folder. For this to be useful, the folder should contain valid Macintosh System and Finder files. vol rename oldpath newpath The existing oldpath on the given vol is renamed to newpath, possibly changing its location at the same time. If newpath already exists, it must be a directory, and the item will simply be moved into it keeping the same name. (In the latter case, there must not be another file or directory already with the same name; in no case will another file or directory be overwritten.) vol create path type creator A new, empty file is created on vol having the given path, and an HFS file handle is returned in the same manner as vol open. The file is given the specified MacOS type and creator codes, which must be 4 character strings. vol copy srcpath dstvol dstpath The given file srcpath located on vol is copied to dstpath located on dstvol (which may be the same as vol). The file and its attributes are copied verbatim; no translation is performed. vol copyin mode srcpath dstpath The specified local (UNIX) srcpath is copied into the given vol as a file having the specified (HFS) dstpath. A translation mode must be given as one of macbinary, binhex, text, or raw. vol copyout mode srcpath dstpath The specified (HFS) srcpath on the given vol is copied out as a local file having the specified (UNIX) dstpath. A translation mode must be given as one of macbinary, binhex, text, or raw. file close The indicated file is closed, all pending changes to the file are flushed, and the file handle becomes invalid for any subsequent operation. file tell A numeric index is returned indicating the character position within file at which the next read or write operation will occur. file stat Information about the given file is returned in much the same way as vol stat. file getfork If the given file is currently performing I\/O on its data fork, the string \"data\" is returned. Otherwise, the string \"rsrc\" is returned. When files are opened, they will default to read\/write on their data fork. The current fork may be changed with file setfork. file setfork fork The current fork of the given file is set to fork (which must be one of data or rsrc), and the current read\/write position is reset to the beginning of the file. file seek pos [from] The character position for the next read or write on file is changed to pos, relative to the indicated from position, which must be one of start, current, or end. The default is to position relative to the start of the file. file read length length bytes are read from the current read\/write position in file, and these bytes are returned as a string. This string may be shorter than length in some circumstances, or may even be empty, indicating the end of the file has been reached. file write string The given string is written to file at the current read\/write position. The number of bytes actually written to the file is returned, and may be less than the length of the string in unusual circumstances (such as when the volume is full).","Process Name":"hfssh","Link":"https:\/\/linux.die.net\/man\/1\/hfssh"}},{"Process":{"Description":"hfsutils is a collection of tools and programs for accessing Macintosh HFS-formatted volumes. See the accompanying man page for each program above for more information.","Process Name":"hfsutils","Link":"https:\/\/linux.die.net\/man\/1\/hfsutils"}},{"Process":{"Description":"The hg command provides a command line interface to the Mercurial system.","Process Name":"hg","Link":"https:\/\/linux.die.net\/man\/1\/hg"}},{"Process":{"Description":"The hgmerge(1) command provides a graphical interface to merge files in the Mercurial system. It is a simple wrapper around kdiff3, merge(1) and tkdiff(1), or simply diff(1) and patch(1) depending on what is present on the system. hgmerge(1) is used by the Mercurial SCM if the environment variable HGMERGE is not set.","Process Name":"hgmerge","Link":"https:\/\/linux.die.net\/man\/1\/hgmerge"}},{"Process":{"Description":null,"Process Name":"hidd","Link":"https:\/\/linux.die.net\/man\/1\/hidd"}},{"Process":{"Description":"Highlight converts sourcecode to HTML, XHTML, RTF, LaTeX, TeX, BBCode, SVG, XTERM or ANSI escape sequences. There are several colour themes available. Highlight recognizes keywords, numbers, strings, comments, symbols and preprocessor directives. It supports about 160 programming languages, which are defined in Lua scripts. It's easily possible to enhance highlight's database of programming languages and colour themes. See the README file for details.","Process Name":"highlight","Link":"https:\/\/linux.die.net\/man\/1\/highlight"}},{"Process":{"Description":null,"Process Name":"hinotes","Link":"https:\/\/linux.die.net\/man\/1\/hinotes"}},{"Process":{"Description":"This program is part of Netpbm(1). hipstopgm reads a HIPS file as input and produces a PGM image as output. If the HIPS file contains more than one frame in sequence, hipstopgm will concatenate all the frames vertically. HIPS is a format developed at the Human Information Processing Laboratory, NYU.","Process Name":"hipstopgm","Link":"https:\/\/linux.die.net\/man\/1\/hipstopgm"}},{"Process":{"Description":"hist2workspace is a utility to create RooFit\/RooStats workspace from histograms","Process Name":"hist2workspace","Link":"https:\/\/linux.die.net\/man\/1\/hist2workspace"}},{"Process":{"Description":"","Process Name":"history","Link":"https:\/\/linux.die.net\/man\/1\/history"}},{"Process":{"Description":"This program navigates through a Windows Registry binary \"hive\" file and extracts either all the (key, value) data pairs stored in that subkey or just the single named data item. In the first form: hivexget hivefile '\\Path\\To\\SubKey' \"hivefile\" is some Windows Registry binary hive, and \"\\Path\\To\\Subkey\" is a path within that hive. NB the path is relative to the top of this hive, and is not the full path as you would use in Windows (eg. \"HKEY_LOCAL_MACHINE\\SYSTEM\" is not a valid path). If the subkey exists, then the output lists all data pairs under this subkey, in a format similar to \"regedit\" in Windows. In the second form: hivexget hivefile '\\Path\\To\\SubKey' name \"hivefile\" and path are as above. \"name\" is the name of the value of interest (use \"@\" for the default value). The corresponding data item is printed \"raw\" (ie. no processing or escaping) except: 1. If it's a string we will convert it from Windows UTF-16 to UTF-8 , if this conversion is possible. The string is printed with a single trailing newline. 2. If it's a multiple-string value, each string is printed on a separate line. 3. If it's a numeric value, it is printed as a decimal number.","Process Name":"hivexget","Link":"https:\/\/linux.die.net\/man\/1\/hivexget"}},{"Process":{"Description":null,"Process Name":"hivexml","Link":"https:\/\/linux.die.net\/man\/1\/hivexml"}},{"Process":{"Description":"Please note hivexregedit is a low-level tool for manipulating hive files directly. To merge or export registry changes to Windows virtual machines it's better to use virt-win-reg(1). Given a local binary (\"hive\") file, there are two modes. \"--merge\" imports (merges) changes from a regedit-format file into the hive. It is similar to using the \"\/s\" switch in Windows regedit.exe. \"--export\" exports a Registry key (recursively) into the regedit format. ENCODING \"hivexregedit\" expects that regedit files have already been re-encoded in the local encoding. Usually on Linux hosts, this means UTF-8 with Unix-style line endings. Since Windows regedit files are often in UTF-16LE with Windows-style line endings, you may need to re-encode the whole file before or after processing. To re-encode a file from Windows format to Linux (before processing it with the \"--merge\" option), you would do something like this: iconv -f utf-16le -t utf-8 < win.reg | dos2unix > linux.reg To go in the opposite direction, after using \"--export\" and before sending the file to a Windows user, do something like this: unix2dos linux.reg | iconv -f utf-8 -t utf-16le > win.reg For more information about encoding, see Win::Hivex::Regedit(3). If you are unsure about the current encoding, use the file(1) command. Recent versions of Windows regedit.exe produce a UTF-16LE file with Windows-style ( CRLF ) line endings, like this: $ file software.reg\nsoftware.reg: Little-endian UTF-16 Unicode text, with very long lines,\nwith CRLF line terminators This file would need conversion before you could \"--merge\" it. SHELL QUOTING Be careful when passing parameters containing \"\\\" (backslash) in the shell. Usually you will have to use 'single quotes' or double backslashes (but not both) to protect them from the shell. CurrentControlSet etc. Registry keys like \"CurrentControlSet\" don't really exist in the Windows Registry at the level of the hive file, and therefore you cannot modify these. \"CurrentControlSet\" is usually an alias for \"ControlSet001\". In some circumstances it might refer to another control set. The way to find out is to look at the \"HKLM\\SYSTEM\\Select\" key: $ hivexregedit --export SYSTEM '\\Select'\n[\\Select]\n\"Current\"=dword:00000001\n\"Default\"=dword:00000001\n\"Failed\"=dword:00000000\n\"LastKnownGood\"=dword:00000002 \"Current\" is the one which Windows will choose when it boots. Similarly, other \"Current...\" keys in the path may need to be replaced.","Process Name":"hivexregedit","Link":"https:\/\/linux.die.net\/man\/1\/hivexregedit"}},{"Process":{"Description":null,"Process Name":"hivexsh","Link":"https:\/\/linux.die.net\/man\/1\/hivexsh"}},{"Process":{"Description":"hk_actionquery executes a query that does not return a result set (that means queries like UPDATE,INSERT,DELETE,CREATE). Use hk_exportxml or hk_exportcsv to execute 'SELECT'-queries. If you don't want to execute a predefined query, use --sql to add a SQL statement","Process Name":"hk_actionquery","Link":"https:\/\/linux.die.net\/man\/1\/hk_actionquery"}},{"Process":{"Description":null,"Process Name":"hk_dbcopy","Link":"https:\/\/linux.die.net\/man\/1\/hk_dbcopy"}},{"Process":{"Description":"hk_exportcsv exports the result set of a query or a table into a comma separated list (CSV). The data will be printed to stdout. To create a table or a query use knoda (http:\/\/www.knoda.org). If the datasource is not a table set the --query option.","Process Name":"hk_exportcsv","Link":"https:\/\/linux.die.net\/man\/1\/hk_exportcsv"}},{"Process":{"Description":"exports the result set of a query or a table to a HTML file. The data will be printed to stdout. To create a table or a query use knoda (http:\/\/www.knoda.org). If the datasource is not a table set the --query option.","Process Name":"hk_exporthtml","Link":"https:\/\/linux.die.net\/man\/1\/hk_exporthtml"}},{"Process":{"Description":"hk_exportxml exports the result set of a query or a table to a XML file. The data will be printed to stdout. To create a table or a query use knoda (http:\/\/www.knoda.org). If the datasource is not a table set the --query option.","Process Name":"hk_exportxml","Link":"https:\/\/linux.die.net\/man\/1\/hk_exportxml"}},{"Process":{"Description":"hk_importcsv imports a csv file and creates the table if necessary.","Process Name":"hk_importcsv","Link":"https:\/\/linux.die.net\/man\/1\/hk_importcsv"}},{"Process":{"Description":null,"Process Name":"hk_report","Link":"https:\/\/linux.die.net\/man\/1\/hk_report"}},{"Process":{"Description":"hlfl is a tool which can produce several types of firewalls from a given set of rules written in a special language also called hlfl (however awkward it is). hlfl attempts to make the best use of the features of the underlying firewall, so that the conversion of a stateless to a stateful requires no modification to the original script","Process Name":"hlfl","Link":"https:\/\/linux.die.net\/man\/1\/hlfl"}},{"Process":{"Description":"clogin is an expect(1) script to automate the process of logging into a Cisco router, catalyst switch, Extreme switch, Juniper ERX\/E-series, Procket Networks, or Redback router. There are complementary scripts for Alteon, Avocent (Cyclades), Bay Networks (nortel), ADC-kentrox EZ-T3 mux, Foundry, HP Procurve Switches and Cisco AGMs, Hitachi Routers, Juniper Networks, MRV optical switch, Netscreen firewalls, Netscaler, Riverstone, Netopia, and Lucent TNT, named alogin, avologin, blogin, elogin, flogin, fnlogin, hlogin, htlogin, jlogin, mrvlogin, nlogin, nslogin, rivlogin, tlogin, and tntlogin, respectively. clogin reads the .cloginrc file for its configuration, then connects and logs into each of the routers specified on the command line in the order listed. Command-line options exist to override some of the directives found in the .cloginrc configuration file. The command-line options are as follows: -S Save the configuration on exit, if the device prompts at logout time. This only has affect when used with -s. -V Prints package name and version strings. -c Command to be run on each router list on the command-line. Multiple commands maybe listed by separating them with semi-colons (;). The argument should be quoted to avoid shell expansion. -d Enable expect debugging. -E Specifies a variable to pass through to scripts (-s). For example, the command-line option -Efoo=bar will produce a global variable by the name Efoo with the initial value \"bar\". -e Specify a password to be supplied when gaining enable privileges on the router(s). Also see the password directive of the .cloginrc file. -f Specifies an alternate configuration file. The default is $HOME\/.cloginrc. -p Specifies a password associated with the user specified by the -u option, user directive of the .cloginrc file, or the Unix username of the user. -s The filename of an expect(1) script which will be sourced after the login is successful and is expected to return control to clogin, with the connection to the router intact, when it is done. Note that clogin disables log_user of expect(1)when -s is used. Example script(s) can be found in share\/rancid\/*.exp. -t Alters the timeout interval; the period that clogin waits for an individual command to return a prompt or the login process to produce a prompt or failure. The argument is in seconds. -u Specifies the username used when prompted. The command-line option overrides any user directive found in .cloginrc. The default is the current Unix username. -v Specifies a vty password, that which is prompted for upon connection to the router. This overrides the vty password of the .cloginrc file's password directive. -w Specifies the username used if prompted when gaining enable privileges. The command-line option overrides any user or enauser directives found in .cloginrc. The default is the current Unix username. -x Similar to the -c option; -x specifies a file with commands to run on each of the routers. The commands must not expect additional input, such as 'copy rcp startup-config' does. For example: show version\nshow logging -y Specifies the encryption algorithm for use with the ssh(1) -c option. The default encryption type is often not supported. See the ssh(1) man page for details. The default is 3des.","Process Name":"hlogin","Link":"https:\/\/linux.die.net\/man\/1\/hlogin"}},{"Process":{"Description":"hls lists files and directories contained in an HFS volume. If one or more arguments are given, each specified file or directory is shown; otherwise, the contents of the current working directory are shown.","Process Name":"hls","Link":"https:\/\/linux.die.net\/man\/1\/hls"}},{"Process":{"Description":null,"Process Name":"hmkdir","Link":"https:\/\/linux.die.net\/man\/1\/hmkdir"}},{"Process":{"Description":"hmount is used to introduce a new HFS volume. A UNIX pathname to the volume's source must be specified. The source may be a block device or a regular file containing an HFS volume image. If the source medium is partitioned, one partition must be selected to be mounted. If there is only one HFS partition on the medium, it will be selected by default. Otherwise, the desired partition number must be specified (as the ordinal nth HFS partition) on the command-line. Partition number 0 can be specified to refer to the entire medium, ignoring what might otherwise be perceived as a partition map, although in practice this is probably only useful if you want this command to fail when the medium is partitioned. The mounted volume becomes \"current\" so subsequent commands will refer to it. The current working directory for the volume is set to the root of the volume. This information is kept in a file named .hcwd in the user's home directory. If the source medium is changed (e.g. floppy or CD-ROM disc exchanged) after hmount has been called, subsequent HFS commands will fail until the original medium is replaced or a different volume is made current. To use the same source path with the different medium, reissue the hmount command.","Process Name":"hmount","Link":"https:\/\/linux.die.net\/man\/1\/hmount"}},{"Process":{"Description":"This manual page documents briefly the hnb hierarchical notebook. hnb is an ncurses program to organize many kinds of data in one place, for example addresses, todo lists, ideas, book reviews or to store snippets of brainstorming, to make a structured packing list or just to take random notes. It can export ascii\/html\/latex\/postscript, supports todo checkboxes, checkbox trees with percentages and searching. hnb uses a simple ascii file for storing your notes. This file can be specified on the command line. If not specified, ~\/.hnb is loaded (or created if missing).","Process Name":"hnb","Link":"https:\/\/linux.die.net\/man\/1\/hnb"}},{"Process":{"Description":"Holland provides a pluggable framework through which to perform database backups. This framework primarily targets MySQL, but there are plans to support other database platforms such as PostgreSQL in the future Holland supports three kinds of plugins currently: \u2022 Backup plugins \u2022 Command plugins \u2022 Library plugins Backup plugins are used for defined backup jobs in order to perform some task. The currently available backup plugins include: \u2022 mysqldump \u2022 mysqlhotcopy (raw file backups for non-transactional tables) \u2022 mysql-lvm (raw file backups using LVM filesystem snasphots) \u2022 maatkit (mk-parallel-dump) Command plugins are used to add additional commands to the holland shell. Currently available commands include: \u2022 backup - run one or more backup jobs \u2022 list-plugins - show known plugins \u2022 list-backups - show completed backup jobs \u2022 mk-config - generate a job config for a given backup plugin Library plugins simply provide support for other plugins. Currently available library plugins include: \u2022 holland.lib.mysql - Core MySQL support \u2022 holland.lib.archive - Standardized access to multiple archive formats \u2022 holland.lib.compression - Standardized access to compression streams","Process Name":"holland","Link":"https:\/\/linux.die.net\/man\/1\/holland"}},{"Process":{"Description":null,"Process Name":"hopalong","Link":"https:\/\/linux.die.net\/man\/1\/hopalong"}},{"Process":{"Description":"host is a simple utility for performing DNS lookups. It is normally used to convert names to IP addresses and vice versa. When no arguments or options are given, host prints a short summary of its command line arguments and options. name is the domain name that is to be looked up. It can also be a dotted-decimal IPv4 address or a colon-delimited IPv6 address, in which case host will by default perform a reverse lookup for that address. server is an optional argument which is either the name or IP address of the name server that host should query instead of the server or servers listed in \/etc\/resolv.conf. The -a (all) option is equivalent to setting the -v option and asking host to make a query of type ANY. When the -C option is used, host will attempt to display the SOA records for zone name from all the listed authoritative name servers for that zone. The list of name servers is defined by the NS records that are found for the zone. The -c option instructs to make a DNS query of class class. This can be used to lookup Hesiod or Chaosnet class resource records. The default class is IN (Internet). Verbose output is generated by host when the -d or -v option is used. The two options are equivalent. They have been provided for backwards compatibility. In previous versions, the -d option switched on debugging traces and -v enabled verbose output. List mode is selected by the -l option. This makes host perform a zone transfer for zone name. Transfer the zone printing out the NS, PTR and address records (A\/AAAA). If combined with -a all records will be printed. The -i option specifies that reverse lookups of IPv6 addresses should use the IP6.INT domain as defined in RFC1886. The default is to use IP6.ARPA. The -N option sets the number of dots that have to be in name for it to be considered absolute. The default value is that defined using the ndots statement in \/etc\/resolv.conf, or 1 if no ndots statement is present. Names with fewer dots are interpreted as relative names and will be searched for in the domains listed in the search or domain directive in \/etc\/resolv.conf. The number of UDP retries for a lookup can be changed with the -R option. number indicates how many times host will repeat a query that does not get answered. The default number of retries is 1. If number is negative or zero, the number of retries will default to 1. Non-recursive queries can be made via the -r option. Setting this option clears the RD - recursion desired - bit in the query which host makes. This should mean that the name server receiving the query will not attempt to resolve name. The -r option enables host to mimic the behavior of a name server by making non-recursive queries and expecting to receive answers to those queries that are usually referrals to other name servers. By default, host uses UDP when making queries. The -T option makes it use a TCP connection when querying the name server. TCP will be automatically selected for queries that require it, such as zone transfer (AXFR) requests. The -4 option forces host to only use IPv4 query transport. The -6 option forces host to only use IPv6 query transport. The -t option is used to select the query type. type can be any recognized query type: CNAME, NS, SOA, SIG, KEY, AXFR, etc. When no query type is specified, host automatically selects an appropriate query type. By default, it looks for A, AAAA, and MX records, but if the -C option was given, queries will be made for SOA records, and if name is a dotted-decimal IPv4 address or colon-delimited IPv6 address, host will query for PTR records. If a query type of IXFR is chosen the starting serial number can be specified by appending an equal followed by the starting serial number (e.g. -t IXFR=12345678). The time to wait for a reply can be controlled through the -W and -w options. The -W option makes host wait for wait seconds. If wait is less than one, the wait interval is set to one second. When the -w option is used, host will effectively wait forever for a reply. The time to wait for a response will be set to the number of seconds given by the hardware's maximum value for an integer quantity. The -s option tells host not to send the query to the next nameserver if any server responds with a SERVFAIL response, which is the reverse of normal stub resolver behavior. The -m can be used to set the memory usage debugging flags record, usage and trace.","Process Name":"host","Link":"https:\/\/linux.die.net\/man\/1\/host"}},{"Process":{"Description":"Print the numeric identifier (in hexadecimal) for the current host. --help display this help and exit --version output version information and exit","Process Name":"hostid","Link":"https:\/\/linux.die.net\/man\/1\/hostid"}},{"Process":{"Description":"Hostname is the program that is used to either set or display the current host, domain or node name of the system. These names are used by many of the networking programs to identify the machine. The domain name is also used by NIS\/YP. Get Name When called without any arguments, the program displays the current names: hostname will print the name of the system as returned by the gethostname(2) function. domainname, nisdomainname, ypdomainname will print the name of the system as returned by the getdomainname(2) function. This is also known as the YP\/NIS domain name of the system. dnsdomainname will print the domain part of the FQDN (Fully Qualified Domain Name). The complete FQDN of the system is returned with hostname --fqdn. The function gethostname(2) is used to get the hostname. When the hostname -a, -d, -f or -i is called will gethostbyname(3) be called. The difference in gethostname(2) and gethostbyname(3) is that gethostbyname(3) is network aware, so it consults \/etc\/nsswitch.conf and \/etc\/host.conf to decide whether to read information in \/etc\/sysconfig\/network or \/etc\/hosts To add another dimension to this, the hostname is also set when the network interface is brought up. Set Name When called with one argument or with the --file option, the commands set the host name, the NIS\/YP domain name or the node name. Note, that only the super-user can change the names. It is not possible to set the FQDN or the DNS domain name with the dnsdomainname command (see THE FQDN below). The host name is usually set once at system startup in \/etc\/rc.d\/rc.inet1 or \/etc\/init.d\/boot (normally by reading the contents of a file which contains the host name, e.g. \/etc\/hostname). the Fqdn You can't change the FQDN (as returned by hostname --fqdn) or the DNS domain name (as returned by dnsdomainname) with this command. The FQDN of the system is the name that the resolver(3) returns for the host name. Technically: The FQDN is the name gethostbyname(2) returns for the host name returned by gethostname(2). The DNS domain name is the part after the first dot. Therefore it depends on the configuration (usually in \/etc\/host.conf) how you can change it. Usually (if the hosts file is parsed before DNS or NIS) you can change it in \/etc\/hosts. If a machine has multiple network interfaces\/addresses or is used in a mobile environment, then it may either have multiple FQDNs\/domain names or none at all. Therefore avoid using hostname --fqdn, hostname --domain and dnsdomainname. hostname --ip-address is subject to the same limitations so it should be avoided as well.","Process Name":"hostname","Link":"https:\/\/linux.die.net\/man\/1\/hostname"}},{"Process":{"Description":null,"Process Name":"hostnameutils","Link":"https:\/\/linux.die.net\/man\/1\/hostnameutils"}},{"Process":{"Description":"Hosts3D is a 3D real-time network monitor, displaying hosts and packet traffic. Features include support for multiple sensors, analysis of packets to gather hostnames and services, configurable layout of subnetworks, recording\/replaying of packet traffic, and the ability to filter packets by hosts, protocol or port. -f Display full screen. Press H key in Hosts3D to show help.","Process Name":"hosts3d","Link":"https:\/\/linux.die.net\/man\/1\/hosts3d"}},{"Process":{"Description":null,"Process Name":"hotspotter","Link":"https:\/\/linux.die.net\/man\/1\/hotspotter"}},{"Process":{"Description":"hotswap allows you to register and unregister hotswappable IDE devices, for example notebook computer modules, with the Linux kernel. It has been developed on and for a Dell Latitude C600, but does not rely on any specific properties of that machine. By default, the command line utility guides you through the hotswapping process interactively. It is also possible to specify an ACTION, for example unregistering the current device, rescanning the IDE bus, &c. This feature is primarily intended for use with scripts and GUI frontends such as xhotswap(1). By using an XML configuration file, system administrators can specify arbitrary shell scripts to be run before and after registering and unregistering devices. This is particularly helpful for automatic configuration of CD-RW drives, which require SCSI emulation and bypass normal access via the IDE subsystem. By default, the configuration file is located at \/etc\/hotswaprc; its syntax is described in hotswaprc(5).","Process Name":"hotswap","Link":"https:\/\/linux.die.net\/man\/1\/hotswap"}},{"Process":{"Description":"hpftodit creates a font file for use with groff -Tlj4 from an HP tagged font metric file. tfm_file is the name of the tagged font metric file for the font. map_file is a file giving the groff names for characters in the font; this file should consist of a sequence of lines of the form: n c1 c2 ... where n is a decimal integer giving the MSL number of the character, and c1, c2,... are the groff names of the character. font is the name of the groff font file. The groff font file is written to font. The -s option should be given if the font is special (a font is special if troff should search it whenever a character is not found in the current font.) If the font is special, it should be listed in the fonts command in the DESC file; if it is not special, there is no need to list it, since troff can automatically mount it when it's first used. If the -i option is used, hpftodit will automatically generate an italic correction, a left italic correction and a subscript correction for each character (the significance of these parameters is explained in groff_font(5)).","Process Name":"hpftodit","Link":"https:\/\/linux.die.net\/man\/1\/hpftodit"}},{"Process":{"Description":null,"Process Name":"hpialarms","Link":"https:\/\/linux.die.net\/man\/1\/hpialarms"}},{"Process":{"Description":null,"Process Name":"hpiel","Link":"https:\/\/linux.die.net\/man\/1\/hpiel"}},{"Process":{"Description":"hpievents polls for events in an opened HPI session. User can specify wait time for the event. User can also select the order between hpi resource (resource event) discovery and hpi event subscription.","Process Name":"hpievents","Link":"https:\/\/linux.die.net\/man\/1\/hpievents"}},{"Process":{"Description":null,"Process Name":"hpifan","Link":"https:\/\/linux.die.net\/man\/1\/hpifan"}},{"Process":{"Description":"hpiinv walks the RPT (Resource Present Table) looking for resources that have Inventory Capability. It displays all inventory records found.","Process Name":"hpiinv","Link":"https:\/\/linux.die.net\/man\/1\/hpiinv"}},{"Process":{"Description":"hpionIBMblade shows how two (2) openhpi plugins can be used to display\nand manage resources of an IBM Blade with Basedboard Management Controller (BMC).\n\n Both the ipmi and snmp_bc plugin have the same IBM Blade target.  Resources from\nboth plugins are combined to show a complete view of the IBM Blade.\n\n @@ WARNING @@ RESTRICTIONS @@ WARNING @@ RESTRICTIONS @@ WARNING @@ RESTRICTIONS @@\n\n This client application is designed to run **only** inband on an IBM Blade with Basedboard\nManagement Controller (BMC)","Process Name":"hpionibmblade","Link":"https:\/\/linux.die.net\/man\/1\/hpionibmblade"}},{"Process":{"Description":null,"Process Name":"hpipower","Link":"https:\/\/linux.die.net\/man\/1\/hpipower"}},{"Process":{"Description":"hpireset searches the Resource Presence Table ( RPT ) for resources with Reset Capability. It sends the requested reset action to all resources with SAHPI_CAPABILITY_RESET .","Process Name":"hpireset","Link":"https:\/\/linux.die.net\/man\/1\/hpireset"}},{"Process":{"Description":null,"Process Name":"hpisensor","Link":"https:\/\/linux.die.net\/man\/1\/hpisensor"}},{"Process":{"Description":"hpisettime sets new date and time for the Event Log clock.","Process Name":"hpisettime","Link":"https:\/\/linux.die.net\/man\/1\/hpisettime"}},{"Process":{"Description":null,"Process Name":"hpithres","Link":"https:\/\/linux.die.net\/man\/1\/hpithres"}},{"Process":{"Description":null,"Process Name":"hpitop","Link":"https:\/\/linux.die.net\/man\/1\/hpitop"}},{"Process":{"Description":"hpitree walks the Resource Presence Table ( RPT ) of the managed openHPI complex, and displays in details the resources (rpt's) and resources' management instruments (rdr's) data structures.","Process Name":"hpitree","Link":"https:\/\/linux.die.net\/man\/1\/hpitree"}},{"Process":{"Description":null,"Process Name":"hpiwdt","Link":"https:\/\/linux.die.net\/man\/1\/hpiwdt"}},{"Process":{"Description":"addr2line translates addresses into file names and line numbers. Given an address in an executable or an offset in a section of a relocatable object, it uses the debugging information to figure out which file name and line number are associated with it. The executable or relocatable object to use is specified with the -e option. The default is the file a.out. The section in the relocatable object to use is specified with the -j option. addr2line has two modes of operation. In the first, hexadecimal addresses are specified on the command line, and addr2line displays the file name and line number for each address. In the second, addr2line reads hexadecimal addresses from standard input, and prints the file name and line number for each address on standard output. In this mode, addr2line may be used in a pipe to convert dynamically chosen addresses. The format of the output is FILENAME:LINENO . The file name and line number for each input address is printed on separate lines. If the -f option is used, then each FILENAME:LINENO line is preceded by FUNCTIONNAME which is the name of the function containing the address. If the -i option is used and the code at the given address is present there because of inlining by the compiler then the { FUNCTIONNAME } FILENAME:LINENO information for the inlining function will be displayed afterwards. This continues recursively until there is no more inlining to report. If the -a option is used then the output is prefixed by the input address. If the -p option is used then the output for each input address is displayed on one, possibly quite long, line. If -p is not used then the output is broken up into multiple lines, based on the paragraphs above. If the file name or function name can not be determined, addr2line will print two question marks in their place. If the line number can not be determined, addr2line will print 0.","Process Name":"hppa64-linux-gnu-addr2line","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-addr2line"}},{"Process":{"Description":null,"Process Name":"hppa64-linux-gnu-ar","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-ar"}},{"Process":{"Description":"GNU as is really a family of assemblers. If you use (or have used) the GNU assembler on one architecture, you should find a fairly similar environment when you use it on another architecture. Each version has much in common with the others, including object file formats, most assembler directives (often called pseudo-ops) and assembler syntax. as is primarily intended to assemble the output of the GNU C compiler \"gcc\" for use by the linker \"ld\". Nevertheless, we've tried to make as assemble correctly everything that other assemblers for the same machine would assemble. Any exceptions are documented explicitly. This doesn't mean as always uses the same syntax as another assembler for the same architecture; for example, we know of several incompatible versions of 680x0 assembly language syntax. Each time you run as it assembles exactly one source program. The source program is made up of one or more files. (The standard input is also a file.) You give as a command line that has zero or more input file names. The input files are read (from left file name to right). A command line argument (in any position) that has no special meaning is taken to be an input file name. If you give as no file names it attempts to read one input file from the as standard input, which is normally your terminal. You may have to type ctl-D to tell as there is no more program to assemble. Use -- if you need to explicitly name the standard input file in your command line. If the source is empty, as produces a small, empty object file. as may write warnings and error messages to the standard error file (usually your terminal). This should not happen when a compiler runs as automatically. Warnings report an assumption made so that as could keep assembling a flawed program; errors report a grave problem that stops the assembly. If you are invoking as via the GNU C compiler, you can use the -Wa option to pass arguments through to the assembler. The assembler arguments must be separated from each other (and the -Wa) by commas. For example: gcc -c -g -O -Wa,-alh,-L file.c This passes two options to the assembler: -alh (emit a listing to standard output with high-level and assembly source) and -L (retain local symbols in the symbol table). Usually you do not need to use this -Wa mechanism, since many compiler command-line options are automatically passed to the assembler by the compiler. (You can call the GNU compiler driver with the -v option to see precisely what options it passes to each compilation pass, including the assembler.)","Process Name":"hppa64-linux-gnu-as","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-as"}},{"Process":{"Description":"The C ++ and Java languages provide function overloading, which means that you can write many functions with the same name, providing that each function takes parameters of different types. In order to be able to distinguish these similarly named functions C ++ and Java encode them into a low-level assembler name which uniquely identifies each different version. This process is known as mangling. The c++filt [1] program does the inverse mapping: it decodes (demangles) low-level names into user-level names so that they can be read. Every alphanumeric word (consisting of letters, digits, underscores, dollars, or periods) seen in the input is a potential mangled name. If the name decodes into a C ++ name, the C ++ name replaces the low-level name in the output, otherwise the original word is output. In this way you can pass an entire assembler source file, containing mangled names, through c++filt and see the same source file containing demangled names. You can also use c++filt to decipher individual symbols by passing them on the command line: c++filt <symbol> If no symbol arguments are given, c++filt reads symbol names from the standard input instead. All the results are printed on the standard output. The difference between reading names from the command line versus reading names from the standard input is that command line arguments are expected to be just mangled names and no checking is performed to separate them from surrounding text. Thus for example: c++filt -n _Z1fv will work and demangle the name to \"f()\" whereas: c++filt -n _Z1fv, will not work. (Note the extra comma at the end of the mangled name which makes it invalid). This command however will work: echo _Z1fv, | c++filt -n and will display \"f(),\", i.e., the demangled name followed by a trailing comma. This behaviour is because when the names are read from the standard input it is expected that they might be part of an assembler source file where there might be extra, extraneous characters trailing after a mangled name. For example: .type   _Z1fv, @function","Process Name":"hppa64-linux-gnu-c++filt","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-c++filt"}},{"Process":{"Description":null,"Process Name":"hppa64-linux-gnu-cpp","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-cpp"}},{"Process":{"Description":"dlltool reads its inputs, which can come from the -d and -b options as well as object files specified on the command line. It then processes these inputs and if the -e option has been specified it creates a exports file. If the -l option has been specified it creates a library file and if the -z option has been specified it creates a def file. Any or all of the -e, -l and -z options can be present in one invocation of dlltool. When creating a DLL , along with the source for the DLL , it is necessary to have three other files. dlltool can help with the creation of these files. The first file is a .def file which specifies which functions are exported from the DLL , which functions the DLL imports, and so on. This is a text file and can be created by hand, or dlltool can be used to create it using the -z option. In this case dlltool will scan the object files specified on its command line looking for those functions which have been specially marked as being exported and put entries for them in the .def file it creates. In order to mark a function as being exported from a DLL , it needs to have an -export:<name_of_function> entry in the .drectve section of the object file. This can be done in C by using the asm() operator: asm (\".section .drectve\");\nasm (\".ascii \\\"-export:my_func\\\"\");\n\nint my_func (void) { ... } The second file needed for DLL creation is an exports file. This file is linked with the object files that make up the body of the DLL and it handles the interface between the DLL and the outside world. This is a binary file and it can be created by giving the -e option to dlltool when it is creating or reading in a .def file. The third file needed for DLL creation is the library file that programs will link with in order to access the functions in the DLL (an 'import library'). This file can be created by giving the -l option to dlltool when it is creating or reading in a .def file. If the -y option is specified, dlltool generates a delay-import library that can be used instead of the normal import library to allow a program to link to the dll only as soon as an imported function is called for the first time. The resulting executable will need to be linked to the static delayimp library containing __delayLoadHelper2(), which in turn will import LoadLibraryA and GetProcAddress from kernel32. dlltool builds the library file by hand, but it builds the exports file by creating temporary files containing assembler statements and then assembling these. The -S command line option can be used to specify the path to the assembler that dlltool will use, and the -f option can be used to pass specific flags to that assembler. The -n can be used to prevent dlltool from deleting these temporary assembler files when it is done, and if -n is specified twice then this will prevent dlltool from deleting the temporary object files it used to build the library. Here is an example of creating a DLL from a source file dll.c and also creating a program (from an object file called program.o) that uses that DLL: gcc -c dll.c\ndlltool -e exports.o -l dll.lib dll.o\ngcc dll.o exports.o -o dll.dll\ngcc program.o dll.lib -o program dlltool may also be used to query an existing import library to determine the name of the DLL to which it is associated. See the description of the -I or --identify option.","Process Name":"hppa64-linux-gnu-dlltool","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-dlltool"}},{"Process":{"Description":"elfedit updates the ELF header of ELF files which have the matching ELF machine and file types. The options control how and which fields in the ELF header should be updated. elffile... are the ELF files to be updated. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files.","Process Name":"hppa64-linux-gnu-elfedit","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-elfedit"}},{"Process":{"Description":null,"Process Name":"hppa64-linux-gnu-gcc","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-gcc"}},{"Process":{"Description":"gcov is a test coverage program. Use it in concert with GCC to analyze your programs to help create more efficient, faster running code and to discover untested parts of your program. You can use gcov as a profiling tool to help discover where your optimization efforts will best affect your code. You can also use gcov along with the other profiling tool, gprof, to assess which parts of your code use the greatest amount of computing time. Profiling tools help you analyze your code's performance. Using a profiler such as gcov or gprof, you can find out some basic performance statistics, such as: \u2022 how often each line of code executes \u2022 what lines of code are actually executed \u2022 how much computing time each section of code uses Once you know these things about how your code works when compiled, you can look at each module to see which modules should be optimized. gcov helps you determine where to work on optimization. Software developers also use coverage testing in concert with testsuites, to make sure software is actually good enough for a release. Testsuites can verify that a program works as expected; a coverage program tests to see how much of the program is exercised by the testsuite. Developers can then determine what kinds of test cases need to be added to the testsuites to create both better testing and a better final product. You should compile your code without optimization if you plan to use gcov because the optimization, by combining some lines of code into one function, may not give you as much information as you need to look for 'hot spots' where the code is using a great deal of computer time. Likewise, because gcov accumulates statistics by line (at the lowest resolution), it works best with a programming style that places only one statement on each line. If you use complicated macros that expand to loops or to other control structures, the statistics are less helpful---they only report on the line where the macro call appears. If your complex macros behave like functions, you can replace them with inline functions to solve this problem. gcov creates a logfile called sourcefile.gcov which indicates how many times each line of a source file sourcefile.c has executed. You can use these logfiles along with gprof to aid in fine-tuning the performance of your programs. gprof gives timing information you can use along with the information you get from gcov. gcov works only on code compiled with GCC . It is not compatible with any other profiling or test coverage mechanism.","Process Name":"hppa64-linux-gnu-gcov","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-gcov"}},{"Process":{"Description":"\"gprof\" produces an execution profile of C, Pascal, or Fortran77 programs. The effect of called routines is incorporated in the profile of each caller. The profile data is taken from the call graph profile file (gmon.out default) which is created by programs that are compiled with the -pg option of \"cc\", \"pc\", and \"f77\". The -pg option also links in versions of the library routines that are compiled for profiling. \"Gprof\" reads the given object file (the default is \"a.out\") and establishes the relation between its symbol table and the call graph profile from gmon.out. If more than one profile file is specified, the \"gprof\" output shows the sum of the profile information in the given profile files. \"Gprof\" calculates the amount of time spent in each routine. Next, these times are propagated along the edges of the call graph. Cycles are discovered, and calls into a cycle are made to share the time of the cycle. Several forms of output are available from the analysis. The flat profile shows how much time your program spent in each function, and how many times that function was called. If you simply want to know which functions burn most of the cycles, it is stated concisely here. The call graph shows, for each function, which functions called it, which other functions it called, and how many times. There is also an estimate of how much time was spent in the subroutines of each function. This can suggest places where you might try to eliminate function calls that use a lot of time. The annotated source listing is a copy of the program's source code, labeled with the number of times each line of the program was executed.","Process Name":"hppa64-linux-gnu-gprof","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-gprof"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"hppa64-linux-gnu-ld","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-ld"}},{"Process":{"Description":"nlmconv converts the relocatable i386 object file infile into the NetWare Loadable Module outfile, optionally reading headerfile for NLM header information. For instructions on writing the NLM command file language used in header files, see the linkers section, NLMLINK in particular, of the NLM Development and Tools Overview, which is part of the NLM Software Developer's Kit (\" NLM SDK \"), available from Novell, Inc. nlmconv uses the GNU Binary File Descriptor library to read infile; nlmconv can perform a link step. In other words, you can list more than one object file for input if you list them in the definitions file (rather than simply specifying one input file on the command line). In this case, nlmconv calls the linker for you.","Process Name":"hppa64-linux-gnu-nlmconv","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-nlmconv"}},{"Process":{"Description":"GNU nm lists the symbols from object files objfile.... If no object files are listed as arguments, nm assumes the file a.out. For each symbol, nm shows: \u2022 The symbol value, in the radix selected by options (see below), or hexadecimal by default. \u2022 The symbol type. At least the following types are used; others are, as well, depending on the object file format. If lowercase, the symbol is usually local; if uppercase, the symbol is global (external). There are however a few lowercase symbols that are shown for special global symbols (\"u\", \"v\" and \"w\"). \"A\" The symbol's value is absolute, and will not be changed by further linking. \"B\" \"b\" The symbol is in the uninitialized data section (known as BSS ). \"C\" The symbol is common. Common symbols are uninitialized data. When linking, multiple common symbols may appear with the same name. If the symbol is defined anywhere, the common symbols are treated as undefined references. \"D\" \"d\" The symbol is in the initialized data section. \"G\" \"g\" The symbol is in an initialized data section for small objects. Some object file formats permit more efficient access to small data objects, such as a global int variable as opposed to a large global array. \"i\" For PE format files this indicates that the symbol is in a section specific to the implementation of DLLs. For ELF format files this indicates that the symbol is an indirect function. This is a GNU extension to the standard set of ELF symbol types. It indicates a symbol which if referenced by a relocation does not evaluate to its address, but instead must be invoked at runtime. The runtime execution will then return the value to be used in the relocation. \"N\" The symbol is a debugging symbol. \"p\" The symbols is in a stack unwind section. \"R\" \"r\" The symbol is in a read only data section. \"S\" \"s\" The symbol is in an uninitialized data section for small objects. \"T\" \"t\" The symbol is in the text (code) section. \"U\" The symbol is undefined. \"u\" The symbol is a unique global symbol. This is a GNU extension to the standard set of ELF symbol bindings. For such a symbol the dynamic linker will make sure that in the entire process there is just one symbol with this name and type in use. \"V\" \"v\" The symbol is a weak object. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the weak symbol becomes zero with no error. On some systems, uppercase indicates that a default value has been specified. \"W\" \"w\" The symbol is a weak symbol that has not been specifically tagged as a weak object symbol. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the symbol is determined in a system-specific manner without error. On some systems, uppercase indicates that a default value has been specified. \"-\" The symbol is a stabs symbol in an a.out object file. In this case, the next values printed are the stabs other field, the stabs desc field, and the stab type. Stabs symbols are used to hold debugging information. \"?\" The symbol type is unknown, or object file format specific. \u2022 The symbol name.","Process Name":"hppa64-linux-gnu-nm","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-nm"}},{"Process":{"Description":null,"Process Name":"hppa64-linux-gnu-objcopy","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-objcopy"}},{"Process":{"Description":"objdump displays information about one or more object files. The options control what particular information to display. This information is mostly useful to programmers who are working on the compilation tools, as opposed to programmers who just want their program to compile and work. objfile... are the object files to be examined. When you specify archives, objdump shows information on each of the member object files.","Process Name":"hppa64-linux-gnu-objdump","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-objdump"}},{"Process":{"Description":"ranlib generates an index to the contents of an archive and stores it in the archive. The index lists each symbol defined by a member of an archive that is a relocatable object file. You may use nm -s or nm --print-armap to list this index. An archive with such an index speeds up linking to the library and allows routines in the library to call each other without regard to their placement in the archive. The GNU ranlib program is another form of GNU ar; running ranlib is completely equivalent to executing ar -s.","Process Name":"hppa64-linux-gnu-ranlib","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-ranlib"}},{"Process":{"Description":"readelf displays information about one or more ELF format object files. The options control what particular information to display. elffile... are the object files to be examined. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files. This program performs a similar function to objdump but it goes into more detail and it exists independently of the BFD library, so if there is a bug in BFD then readelf will not be affected.","Process Name":"hppa64-linux-gnu-readelf","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-readelf"}},{"Process":{"Description":"The GNU size utility lists the section sizes---and the total size---for each of the object or archive files objfile in its argument list. By default, one line of output is generated for each object file or each module in an archive. objfile... are the object files to be examined. If none are specified, the file \"a.out\" will be used.","Process Name":"hppa64-linux-gnu-size","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-size"}},{"Process":{"Description":"For each file given, GNU strings prints the printable character sequences that are at least 4 characters long (or the number given with the options below) and are followed by an unprintable character. By default, it only prints the strings from the initialized and loaded sections of object files; for other types of files, it prints the strings from the whole file. strings is mainly useful for determining the contents of non-text files.","Process Name":"hppa64-linux-gnu-strings","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-strings"}},{"Process":{"Description":null,"Process Name":"hppa64-linux-gnu-strip","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-strip"}},{"Process":{"Description":"windmc reads message definitions from an input file (.mc) and translate them into a set of output files. The output files may be of four kinds: \"h\" A C header file containing the message definitions. \"rc\" A resource file compilable by the windres tool. \"bin\" One or more binary files containing the resource data for a specific message language. \"dbg\" A C include file that maps message id's to their symbolic name. The exact description of these different formats is available in documentation from Microsoft. When windmc converts from the \"mc\" format to the \"bin\" format, \"rc\", \"h\", and optional \"dbg\" it is acting like the Windows Message Compiler.","Process Name":"hppa64-linux-gnu-windmc","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-windmc"}},{"Process":{"Description":null,"Process Name":"hppa64-linux-gnu-windres","Link":"https:\/\/linux.die.net\/man\/1\/hppa64-linux-gnu-windres"}},{"Process":{"Description":"hpwd displays the complete (absolute) HFS pathname to the current working directory on the current volume. The current working directory can be changed with the hcd command.","Process Name":"hpwd","Link":"https:\/\/linux.die.net\/man\/1\/hpwd"}},{"Process":{"Description":null,"Process Name":"hqx2bin","Link":"https:\/\/linux.die.net\/man\/1\/hqx2bin"}},{"Process":{"Description":"rancid is a perl(1) script which uses the login scripts (see clogin(1)) to login to a device, execute commands to display the configuration, etc, then filters the output for formatting, security, and so on. rancid's product is a file with the name of it's last argument plus the suffix .new. For example, hostname.new. There are complementary scripts for other platforms and\/or manufacturers that are supported by rancid(1). Briefly, these are: agmrancid Cisco Anomaly Guard Module (AGM) arancid Alteon WebOS switches arrancid Arista Networks devices brancid Bay Networks (nortel) cat5rancid Cisco catalyst switches cssrancid Cisco content services switches erancid ADC-kentrox EZ-T3 mux f10rancid Force10 f5rancid F5 BigIPs fnrancid Fortinet Firewalls francid Foundry and HP procurve OEMs of Foundry hrancid HP Procurve Switches htranicd Hitachi Routers jerancid Juniper Networks E-series jrancid Juniper Networks mrancid MRTd mrvrancid MRV optical switches nrancid Netscreen firewalls nsrancid Netscaler nxrancid Cisco Nexus boxes prancid Procket Networks rivrancid Riverstone rrancid Redback srancid SMC switch (some Dell OEMs) trancid Netopia sDSL\/T1 routers tntrancid Lucent TNT xrancid Extreme switches xrrancid Cisco IOS-XR boxes zrancid Zebra routing software The command-line options are as follows: -V Prints package name and version strings. -d Display debugging information. -l Display somewhat less debugging information. -f rancid should interpret the next argument as a filename which contains the output it would normally collect from the device ( hostname) with clogin(1).","Process Name":"hrancid","Link":"https:\/\/linux.die.net\/man\/1\/hrancid"}},{"Process":{"Description":null,"Process Name":"hrename","Link":"https:\/\/linux.die.net\/man\/1\/hrename"}},{"Process":{"Description":"hrmdir deletes directories (folders) from the current HFS volume. Each named directory must already be empty.","Process Name":"hrmdir","Link":"https:\/\/linux.die.net\/man\/1\/hrmdir"}},{"Process":{"Description":"A very powerful rpm rebuilder using perl-RPM4","Process Name":"hrpmreb","Link":"https:\/\/linux.die.net\/man\/1\/hrpmreb"}},{"Process":{"Description":"hsen is a packet capture daemon which reads and sends packet header information to Hosts3D, locally or remotely. hsen also equates hostname to IP by reading DNS packets (UDP type A class IN standard query response). Multiple sensors can send information to multiple computers running Hosts3D on the same subnet via broadcast. id Identify packets from a specific hsen when multiple exist (1-255). interface Listen on interface (eth0, eth1, ppp0, wlan0, etc.); or file Read packets from pcap file. Standard input is used if file is \"-\". destination Hosts3D IP or broadcast address (default localhost). -p Enable promiscuous mode (default disable).","Process Name":"hsen","Link":"https:\/\/linux.die.net\/man\/1\/hsen"}},{"Process":{"Description":"hspell tries to find incorrectly spelled Hebrew words in its input files. Like the traditional Unix spell(1), hspell outputs the sorted list of incorrect words, and does not (yet) have a more friendly interface for making corrections for you. However, unlike spell(1), hspell can suggest possible corrections for some spelling errors - such suggestions are enabled with the -c (correct) and -n (notes) options. Hspell currently expects ISO-8859-8-encoded input files. Non-Hebrew characters in the input files are ignored, allowing the easy spellchecking of Hebrew-English texts, as well as HTML or TeX files. If files using a different encoding (e.g., UTF8) are to be checked, they must be converted first to ISO-8859-8 (e.g., see iconv(1), recode(1)). The output will also be in ISO-8859-8 encoding, in so-called \"logical order\", so it is normally useful to pipe it to bidiv(1) before viewing, as in: hspell -c filename | bidiv | less If no input file is given, hspell reads from its standard input.","Process Name":"hspell","Link":"https:\/\/linux.die.net\/man\/1\/hspell"}},{"Process":{"Description":null,"Process Name":"htc","Link":"https:\/\/linux.die.net\/man\/1\/htc"}},{"Process":{"Description":"htcopy is a HTTP client that implements some extensions of the protocol used by the HTTP\/DAV front-end of LCGDM. The command is designed to work without user interaction. It allows to copy from local to remote, from remote to local, and it can also trigger third party copies. It supports multiple streams for uploads and downloads, improving the performance of the transfer.","Process Name":"htcopy","Link":"https:\/\/linux.die.net\/man\/1\/htcopy"}},{"Process":{"Description":null,"Process Name":"htcp","Link":"https:\/\/linux.die.net\/man\/1\/htcp"}},{"Process":{"Description":"","Process Name":"htdbm","Link":"https:\/\/linux.die.net\/man\/1\/htdbm"}},{"Process":{"Description":null,"Process Name":"htdig","Link":"https:\/\/linux.die.net\/man\/1\/htdig"}},{"Process":{"Description":null,"Process Name":"htdig-pdfparser","Link":"https:\/\/linux.die.net\/man\/1\/htdig-pdfparser"}},{"Process":{"Description":"","Process Name":"htdigest","Link":"https:\/\/linux.die.net\/man\/1\/htdigest"}},{"Process":{"Description":"Htdump writes out an ASCII-text version of the document database in the same form as the -t option of htdig.","Process Name":"htdump","Link":"https:\/\/linux.die.net\/man\/1\/htdump"}},{"Process":{"Description":"htcp is a client to fetch files or directory listings from remote servers using HTTP or HTTPS, or to put or delete files or directories onto remote servers using HTTPS. htcp is similar to scp(1), but uses HTTP\/HTTPS rather than ssh as its transfer protocol. htcp can also use the HTCP protocol to query HTTP(S) fileservers via SiteCast. When talking to a fileserver with HTTPS, htcp can run \"anonymously\", with a standard X.509 user certificate and key, or with a GSI Proxy. This makes htcp very useful in Grid environments where many users have certificates and where jobs and users have access to GSI proxies.","Process Name":"htfind","Link":"https:\/\/linux.die.net\/man\/1\/htfind"}},{"Process":{"Description":"Description to follow here","Process Name":"htfuzzy","Link":"https:\/\/linux.die.net\/man\/1\/htfuzzy"}},{"Process":{"Description":"htcp is a client to fetch files or directory listings from remote servers using HTTP or HTTPS, or to put or delete files or directories onto remote servers using HTTPS. htcp is similar to scp(1), but uses HTTP\/HTTPS rather than ssh as its transfer protocol. htcp can also use the HTCP protocol to query HTTP(S) fileservers via SiteCast. When talking to a fileserver with HTTPS, htcp can run \"anonymously\", with a standard X.509 user certificate and key, or with a GSI Proxy. This makes htcp very useful in Grid environments where many users have certificates and where jobs and users have access to GSI proxies.","Process Name":"htll","Link":"https:\/\/linux.die.net\/man\/1\/htll"}},{"Process":{"Description":"Htload reads in an ASCII-text version of the document database in the same form as the -t option of htdig and htdump. Note that this will overwrite data in your databases, so this should be used with great care.","Process Name":"htload","Link":"https:\/\/linux.die.net\/man\/1\/htload"}},{"Process":{"Description":"clogin is an expect(1) script to automate the process of logging into a Cisco router, catalyst switch, Extreme switch, Juniper ERX\/E-series, Procket Networks, or Redback router. There are complementary scripts for Alteon, Avocent (Cyclades), Bay Networks (nortel), ADC-kentrox EZ-T3 mux, Foundry, HP Procurve Switches and Cisco AGMs, Hitachi Routers, Juniper Networks, MRV optical switch, Netscreen firewalls, Netscaler, Riverstone, Netopia, and Lucent TNT, named alogin, avologin, blogin, elogin, flogin, fnlogin, hlogin, htlogin, jlogin, mrvlogin, nlogin, nslogin, rivlogin, tlogin, and tntlogin, respectively. clogin reads the .cloginrc file for its configuration, then connects and logs into each of the routers specified on the command line in the order listed. Command-line options exist to override some of the directives found in the .cloginrc configuration file. The command-line options are as follows: -S Save the configuration on exit, if the device prompts at logout time. This only has affect when used with -s. -V Prints package name and version strings. -c Command to be run on each router list on the command-line. Multiple commands maybe listed by separating them with semi-colons (;). The argument should be quoted to avoid shell expansion. -d Enable expect debugging. -E Specifies a variable to pass through to scripts (-s). For example, the command-line option -Efoo=bar will produce a global variable by the name Efoo with the initial value \"bar\". -e Specify a password to be supplied when gaining enable privileges on the router(s). Also see the password directive of the .cloginrc file. -f Specifies an alternate configuration file. The default is $HOME\/.cloginrc. -p Specifies a password associated with the user specified by the -u option, user directive of the .cloginrc file, or the Unix username of the user. -s The filename of an expect(1) script which will be sourced after the login is successful and is expected to return control to clogin, with the connection to the router intact, when it is done. Note that clogin disables log_user of expect(1)when -s is used. Example script(s) can be found in share\/rancid\/*.exp. -t Alters the timeout interval; the period that clogin waits for an individual command to return a prompt or the login process to produce a prompt or failure. The argument is in seconds. -u Specifies the username used when prompted. The command-line option overrides any user directive found in .cloginrc. The default is the current Unix username. -v Specifies a vty password, that which is prompted for upon connection to the router. This overrides the vty password of the .cloginrc file's password directive. -w Specifies the username used if prompted when gaining enable privileges. The command-line option overrides any user or enauser directives found in .cloginrc. The default is the current Unix username. -x Similar to the -c option; -x specifies a file with commands to run on each of the routers. The commands must not expect additional input, such as 'copy rcp startup-config' does. For example: show version\nshow logging -y Specifies the encryption algorithm for use with the ssh(1) -c option. The default encryption type is often not supported. See the ssh(1) man page for details. The default is 3des.","Process Name":"htlogin","Link":"https:\/\/linux.die.net\/man\/1\/htlogin"}},{"Process":{"Description":"htcp is a client to fetch files or directory listings from remote servers using HTTP or HTTPS, or to put or delete files or directories onto remote servers using HTTPS. htcp is similar to scp(1), but uses HTTP\/HTTPS rather than ssh as its transfer protocol. htcp can also use the HTCP protocol to query HTTP(S) fileservers via SiteCast. When talking to a fileserver with HTTPS, htcp can run \"anonymously\", with a standard X.509 user certificate and key, or with a GSI Proxy. This makes htcp very useful in Grid environments where many users have certificates and where jobs and users have access to GSI proxies.","Process Name":"htls","Link":"https:\/\/linux.die.net\/man\/1\/htls"}},{"Process":{"Description":"Htmerge is used to create a document index and word database from the files that were created by htdig. These databases are then used by htsearch to perform the actual searched.","Process Name":"htmerge","Link":"https:\/\/linux.die.net\/man\/1\/htmerge"}},{"Process":{"Description":"htcp is a client to fetch files or directory listings from remote servers using HTTP or HTTPS, or to put or delete files or directories onto remote servers using HTTPS. htcp is similar to scp(1), but uses HTTP\/HTTPS rather than ssh as its transfer protocol. htcp can also use the HTCP protocol to query HTTP(S) fileservers via SiteCast. When talking to a fileserver with HTTPS, htcp can run \"anonymously\", with a standard X.509 user certificate and key, or with a GSI Proxy. This makes htcp very useful in Grid environments where many users have certificates and where jobs and users have access to GSI proxies.","Process Name":"htmkdir","Link":"https:\/\/linux.die.net\/man\/1\/htmkdir"}},{"Process":{"Description":"See: http:\/\/translate.sourceforge.net\/wiki\/toolkit\/html2po for examples and usage instructions","Process Name":"html2po","Link":"https:\/\/linux.die.net\/man\/1\/html2po"}},{"Process":{"Description":"The program html2ps converts HTML to PostScript. The HTML code can be retrieved from one or more URL:s or local files, specified as parameters on the command line. If no parameter is given, html2ps reads from standard input. Note: To avoid unnecessary network traffic, one can rebuild an already generated PostScript file with new options. This is done by running html2ps with the new options, and with the old PostScript file as input (not applicable for all options).","Process Name":"html2ps","Link":"https:\/\/linux.die.net\/man\/1\/html2ps"}},{"Process":{"Description":"html2text reads HTML documents from the input-urls, formats each of them into a stream of plain text characters, and writes the result to standard output (or into output-file, if the -o command line option is used). Documents that are specified by a URL (RFC 1738) that begins with \"http:\" are retrieved with the Hypertext Transfer Protocol (RFC 1945). URLs that begin with \"file:\" and URLs that do not contain a colon specify local files. All other URLs are invalid. If no input-urls are specified on the command line, html2text reads from standard input. A dash as the input-url is an alternate way to specify standard input. html2text understands all HTML 3.2 constructs, but can render only part of them due to the limitations of the text output format. However, the program attempts to provide good substitutes for the elements it cannot render. html2text parses HTML 4 input, too, but not always as successful as other HTML processors. It also accepts syntactically incorrect input, and attempts to interpret it \"reasonably\". The way html2text formats the HTML documents is controlled by formatting properties read from an RC file. html2text attempts to read $HOME\/.html2textrc (or the file specified by the -rcfile command line option); if that file cannot be read, html2text attempts to read \/etc\/html2textrc. If no RC file can be read (or if the RC file does not override all formatting properties), then \"reasonable\" defaults are assumed. The RC file format is described in the html2textrc(5) manual page.","Process Name":"html2text","Link":"https:\/\/linux.die.net\/man\/1\/html2text"}},{"Process":{"Description":"This program provides a command-line interface to the HTML::Clean module, which can help you to provide more compatible, smaller HTML files at the expense of reducing the human readability of the HTML code. In some cases you may be able to reduce the size of your HTML by up to 50%! The HTML::Clean library provides a number of features that improve your HTML for browsing and serving: htmlclean passes each file given on the command line to the library and writes out the new HTML according to the specified options. The default is to create a backup file and replace the file with cleaned HTML . Removing whitespace, Comments and other useless or redundant constructs Insuring that font tags work across multiple operating systems For full details see the documentations for HTML::Clean itself.","Process Name":"htmlclean","Link":"https:\/\/linux.die.net\/man\/1\/htmlclean"}},{"Process":{"Description":"HTMLDOC converts HTML source files into indexed HTML, PostScript, or Portable Document Format (PDF) files that can be viewed online or printed. With no options a HTML document is produced on stdout. The second form of HTMLDOC reads HTML source from stdin, which allows you to use HTMLDOC as a filter. The third form of HTMLDOC launches a graphical interface that allows you to change options and generate documents interactively.","Process Name":"htmldoc","Link":"https:\/\/linux.die.net\/man\/1\/htmldoc"}},{"Process":{"Description":"htcp is a client to fetch files or directory listings from remote servers using HTTP or HTTPS, or to put or delete files or directories onto remote servers using HTTPS. htcp is similar to scp(1), but uses HTTP\/HTTPS rather than ssh as its transfer protocol. htcp can also use the HTCP protocol to query HTTP(S) fileservers via SiteCast. When talking to a fileserver with HTTPS, htcp can run \"anonymously\", with a standard X.509 user certificate and key, or with a GSI Proxy. This makes htcp very useful in Grid environments where many users have certificates and where jobs and users have access to GSI proxies.","Process Name":"htmv","Link":"https:\/\/linux.die.net\/man\/1\/htmv"}},{"Process":{"Description":"Htnotify scans the document database created by htmerge and sends an email message for every page that is out of date. Please have a look at the ht:\/\/Dig notification manual for instructions on how to set up this service.","Process Name":"htnotify","Link":"https:\/\/linux.die.net\/man\/1\/htnotify"}},{"Process":{"Description":"Generates a .icl and .dcl file for a c header file. This makes it possible to call C functions from within a Clean program.","Process Name":"htoclean","Link":"https:\/\/linux.die.net\/man\/1\/htoclean"}},{"Process":{"Description":"Htop is a free (GPL) ncurses-based process viewer for Linux. It is similar to top, but allows you to scroll vertically and horizontally, so you can see all the processes running on the system, along with their full command lines. Tasks related to processes (killing, renicing) can be done without entering their PIDs.","Process Name":"htop","Link":"https:\/\/linux.die.net\/man\/1\/htop"}},{"Process":{"Description":"","Process Name":"htpasswd","Link":"https:\/\/linux.die.net\/man\/1\/htpasswd"}},{"Process":{"Description":"Sets a user's password in an httpd-style password file. The -c flag creates a new file.","Process Name":"htpasswd.thttpd","Link":"https:\/\/linux.die.net\/man\/1\/htpasswd.thttpd"}},{"Process":{"Description":"htcp is a client to fetch files or directory listings from remote servers using HTTP or HTTPS, or to put or delete files or directories onto remote servers using HTTPS. htcp is similar to scp(1), but uses HTTP\/HTTPS rather than ssh as its transfer protocol. htcp can also use the HTCP protocol to query HTTP(S) fileservers via SiteCast. When talking to a fileserver with HTTPS, htcp can run \"anonymously\", with a standard X.509 user certificate and key, or with a GSI Proxy. This makes htcp very useful in Grid environments where many users have certificates and where jobs and users have access to GSI proxies.","Process Name":"htping","Link":"https:\/\/linux.die.net\/man\/1\/htping"}},{"Process":{"Description":"htproxyput is a client to perform GSI proxy delegations using the GridSite\/gLite delegation Web Service portType. The gridsite-delegation(8) CGI program is the complementary server-side implementation. htproxyinfo examines a local copy of a GSI proxy, and outputs a summary of its X.509 and VOMS contents.","Process Name":"htproxydestroy","Link":"https:\/\/linux.die.net\/man\/1\/htproxydestroy"}},{"Process":{"Description":"htproxyput is a client to perform GSI proxy delegations using the GridSite\/gLite delegation Web Service portType. The gridsite-delegation(8) CGI program is the complementary server-side implementation. htproxyinfo examines a local copy of a GSI proxy, and outputs a summary of its X.509 and VOMS contents.","Process Name":"htproxyinfo","Link":"https:\/\/linux.die.net\/man\/1\/htproxyinfo"}},{"Process":{"Description":"htproxyput is a client to perform GSI proxy delegations using the GridSite\/gLite delegation Web Service portType. The gridsite-delegation(8) CGI program is the complementary server-side implementation. htproxyinfo examines a local copy of a GSI proxy, and outputs a summary of its X.509 and VOMS contents.","Process Name":"htproxyput","Link":"https:\/\/linux.die.net\/man\/1\/htproxyput"}},{"Process":{"Description":"htproxyput is a client to perform GSI proxy delegations using the GridSite\/gLite delegation Web Service portType. The gridsite-delegation(8) CGI program is the complementary server-side implementation. htproxyinfo examines a local copy of a GSI proxy, and outputs a summary of its X.509 and VOMS contents.","Process Name":"htproxyrenew","Link":"https:\/\/linux.die.net\/man\/1\/htproxyrenew"}},{"Process":{"Description":"htproxyput is a client to perform GSI proxy delegations using the GridSite\/gLite delegation Web Service portType. The gridsite-delegation(8) CGI program is the complementary server-side implementation. htproxyinfo examines a local copy of a GSI proxy, and outputs a summary of its X.509 and VOMS contents.","Process Name":"htproxytime","Link":"https:\/\/linux.die.net\/man\/1\/htproxytime"}},{"Process":{"Description":"htproxyput is a client to perform GSI proxy delegations using the GridSite\/gLite delegation Web Service portType. The gridsite-delegation(8) CGI program is the complementary server-side implementation. htproxyinfo examines a local copy of a GSI proxy, and outputs a summary of its X.509 and VOMS contents.","Process Name":"htproxyunixtime","Link":"https:\/\/linux.die.net\/man\/1\/htproxyunixtime"}},{"Process":{"Description":"Htpurge functions to remove specified URLs from the databases as well as bad URLs, unretrieved URLs, obsolete documents, etc. It is recommended that htpurge be run after htdig to clean out any documents of this sort.","Process Name":"htpurge","Link":"https:\/\/linux.die.net\/man\/1\/htpurge"}},{"Process":{"Description":"rancid is a perl(1) script which uses the login scripts (see clogin(1)) to login to a device, execute commands to display the configuration, etc, then filters the output for formatting, security, and so on. rancid's product is a file with the name of it's last argument plus the suffix .new. For example, hostname.new. There are complementary scripts for other platforms and\/or manufacturers that are supported by rancid(1). Briefly, these are: agmrancid Cisco Anomaly Guard Module (AGM) arancid Alteon WebOS switches arrancid Arista Networks devices brancid Bay Networks (nortel) cat5rancid Cisco catalyst switches cssrancid Cisco content services switches erancid ADC-kentrox EZ-T3 mux f10rancid Force10 f5rancid F5 BigIPs fnrancid Fortinet Firewalls francid Foundry and HP procurve OEMs of Foundry hrancid HP Procurve Switches htranicd Hitachi Routers jerancid Juniper Networks E-series jrancid Juniper Networks mrancid MRTd mrvrancid MRV optical switches nrancid Netscreen firewalls nsrancid Netscaler nxrancid Cisco Nexus boxes prancid Procket Networks rivrancid Riverstone rrancid Redback srancid SMC switch (some Dell OEMs) trancid Netopia sDSL\/T1 routers tntrancid Lucent TNT xrancid Extreme switches xrrancid Cisco IOS-XR boxes zrancid Zebra routing software The command-line options are as follows: -V Prints package name and version strings. -d Display debugging information. -l Display somewhat less debugging information. -f rancid should interpret the next argument as a filename which contains the output it would normally collect from the device ( hostname) with clogin(1).","Process Name":"htrancid","Link":"https:\/\/linux.die.net\/man\/1\/htrancid"}},{"Process":{"Description":"htcp is a client to fetch files or directory listings from remote servers using HTTP or HTTPS, or to put or delete files or directories onto remote servers using HTTPS. htcp is similar to scp(1), but uses HTTP\/HTTPS rather than ssh as its transfer protocol. htcp can also use the HTCP protocol to query HTTP(S) fileservers via SiteCast. When talking to a fileserver with HTTPS, htcp can run \"anonymously\", with a standard X.509 user certificate and key, or with a GSI Proxy. This makes htcp very useful in Grid environments where many users have certificates and where jobs and users have access to GSI proxies.","Process Name":"htrm","Link":"https:\/\/linux.die.net\/man\/1\/htrm"}},{"Process":{"Description":"This manual page briefly documents the hts command. hts listens for incoming httptunnel connections at PORT (default port is 8888). When a connection is made, I\/O is redirected to the destination specified by the --device or --forward-port switch.","Process Name":"hts","Link":"https:\/\/linux.die.net\/man\/1\/hts"}},{"Process":{"Description":"Htsearch is used to search in de databases created by htdig for content. is the actual search engine of the htdig search system. It is a CGI program that is expected to be invoked by an HTML form. It will accept both the GET and POST methods of passing data to the CGI program.","Process Name":"htsearch","Link":"https:\/\/linux.die.net\/man\/1\/htsearch"}},{"Process":{"Description":"The htsslpass program is used to prompt for SSL private key passwords mod_ssl; it can be configured as an SSLPassPhraseDialog pipe filter in the mod_ssl configuration, \/etc\/httpd\/conf.d\/ssl.conf. The program write any input from stdin to \/dev\/tty, and writes any input from \/dev\/tty to stdout. If no input is received after five minutes, the program times out.","Process Name":"htsslpass","Link":"https:\/\/linux.die.net\/man\/1\/htsslpass"}},{"Process":{"Description":"Htdig retrieves HTML documents using the HTTP protocol and gathers information from these documents which can later be used to search these documents. This program can be referred to as the search robot.","Process Name":"htstat","Link":"https:\/\/linux.die.net\/man\/1\/htstat"}},{"Process":{"Description":"This manual page documents briefly the http-replicator command. Replicator is a replicating HTTP proxy server. Files that are downloaded through the proxy are transparently stored in a private cache, so an exact copy of accessed remote files is created on the local machine. Please see the README or the website for more information: http:\/\/gertjan.freezope.org\/replicator","Process Name":"http-replicator","Link":"https:\/\/linux.die.net\/man\/1\/http-replicator"}},{"Process":{"Description":"This manual page documents briefly the http-replicator_maintenance command. This maintenance script is meant to be run as a cron task to keep a debian package cache built with http-replicator(1) down in size. It does this by deleting the oldest versions of each package in cache. The number of packages to keep should be specified as a command line option. Please see the README or the website for more information: http:\/\/gertjan.freezope.org\/replicator","Process Name":"http-replicator_maintenance","Link":"https:\/\/linux.die.net\/man\/1\/http-replicator_maintenance"}},{"Process":{"Description":"This tool is used to manage the certificates used by the HttpListener embeddable server class when the HttpListener is configured as an HTTPS server instead of an HTTP server. You must select one of the possible actions: add, delete or list. When adding a certificate (-add), you must provide the following information: a certificate (with the -cert flag) a Private Key file (with the -pvk argument, the filename is typically \"key\") and a port number (with the -port flag). When deleting a certificate (-del or -delete) you need to provide the port number.","Process Name":"httpcfg","Link":"https:\/\/linux.die.net\/man\/1\/httpcfg"}},{"Process":{"Description":"httperf is a tool to measure web server performance. It speaks the HTTP protocol both in its HTTP\/1.0 and HTTP\/1.1 flavors and offers a variety of workload generators. While running, it keeps track of a number of performance metrics that are summarized in the form of statistics that are printed at the end of a test run. The most basic operation of httperf is to generate a fixed number of HTTP GET requests and to measure how many replies (responses) came back from the server and at what rate the responses arrived. IMPORTANT: To obtain correct results, it is necessary to run at most one httperf process per client machine. Also, there should be as few background processes as possible both on the client and server machines.","Process Name":"httperf","Link":"https:\/\/linux.die.net\/man\/1\/httperf"}},{"Process":{"Description":"The program httping lets you measure the latency of a webserver. Since version 1.0.6 also the throughput can be measured.","Process Name":"httping","Link":"https:\/\/linux.die.net\/man\/1\/httping"}},{"Process":{"Description":"httpry is a tool designed for displaying and logging HTTP traffic. It is not designed to perform analysis itself, but instead to capture, parse and log the traffic for later analysis. It can be run in real-time displaying the live traffic on the wire, or as a daemon process that logs to an output file.","Process Name":"httpry","Link":"https:\/\/linux.die.net\/man\/1\/httpry"}},{"Process":{"Description":"","Process Name":"huge-combine.pl","Link":"https:\/\/linux.die.net\/man\/1\/huge-combine.pl"}},{"Process":{"Description":"Runs count.pl efficiently on large amounts of data by splitting the data into separate files, and counting up each file separately, and then merging them to get overall results. Two output files are created. destination-dir\/huge-count.output contains the bigram counts after applying --remove and --remove. destination-dir\/complete-huge-count.output provides the bigram counts as if no --uremove or --remove cutoff were provided.","Process Name":"huge-count.pl","Link":"https:\/\/linux.die.net\/man\/1\/huge-count.pl"}},{"Process":{"Description":"See perldoc huge-delete.pl","Process Name":"huge-delete.pl","Link":"https:\/\/linux.die.net\/man\/1\/huge-delete.pl"}},{"Process":{"Description":"Combine the sorted bigram files generated by huge-sort.pl efficiently. This program is used internally by huge-count.pl.","Process Name":"huge-merge.pl","Link":"https:\/\/linux.die.net\/man\/1\/huge-merge.pl"}},{"Process":{"Description":"huge-sort.pl takes as input a duplicate bigram file generate by count.pl with --tokenlist option, counts the frequency of each bigram and sorts them in alphabetical order. The output file will be found in input-file.sorted. This program is used internally by huge-count.pl.","Process Name":"huge-sort.pl","Link":"https:\/\/linux.die.net\/man\/1\/huge-sort.pl"}},{"Process":{"Description":"See perldoc huge-split.pl","Process Name":"huge-split.pl","Link":"https:\/\/linux.die.net\/man\/1\/huge-split.pl"}},{"Process":{"Description":"humount is used to forget about an HFS volume previously mounted with hmount. Either the volume's name or the UNIX path to the volume may be used to specify the volume. If no name or path is given, the current volume is assumed.","Process Name":"humount","Link":"https:\/\/linux.die.net\/man\/1\/humount"}},{"Process":{"Description":"Hunspell is fashioned after the Ispell program. The most common usage is \"hunspell\" or \"hunspell filename\". Without filename parameter, hunspell checks the standard input. Typing \"cat\" and \"exsample\" in two input lines, we got an asterisk (it means \"cat\" is a correct word) and a line with corrections: $ hunspell -d en_US\nHunspell 1.2.3\n*\n& exsample 4 0: example, examples, ex sample, ex-sample Correct words signed with an '*', '+' or '-', unrecognized words signed with '#' or '&' in output lines (see later). (Close the standard input with Ctrl-d on Unix\/Linux and Ctrl-Z Enter or Ctrl-C on Windows.) With filename parameters, hunspell will display each word of the files which does not appear in the dictionary at the top of the screen and allow you to change it. If there are \"near misses\" in the dictionary, then they are also displayed on following lines. Finally, the line containing the word and the previous line are printed at the bottom of the screen. If your terminal can display in reverse video, the word itself is highlighted. You have the option of replacing the word completely, or choosing one of the suggested words. Commands are single characters as follows (case is ignored): R Replace the misspelled word completely. Space Accept the word this time only. A Accept the word for the rest of this hunspell session. I Accept the word, capitalized as it is in the file, and update private dictionary. U Accept the word, and add an uncapitalized (actually, all lower-case) version to the private dictionary. S Ask a stem and a model word and store them in the private dictionary. The stem will be accepted also with the affixes of the model word. 0-n Replace with one of the suggested words. X Write the rest of this file, ignoring misspellings, and start next file. Q Exit immediately and leave the file unchanged. ^Z Suspend hunspell. ? Give help screen.","Process Name":"hunspell","Link":"https:\/\/linux.die.net\/man\/1\/hunspell"}},{"Process":{"Description":"This manual page documents briefly the hunt command. This manual page was written for the Debian GNU\/Linux distribution because the original program does not have a manual page. Instead, it has documentation in the GNU Info format; see below.","Process Name":"hunt","Link":"https:\/\/linux.die.net\/man\/1\/hunt"}},{"Process":{"Description":"Hunzip is the decompression and decryption program of hzip format.","Process Name":"hunzip","Link":"https:\/\/linux.die.net\/man\/1\/hunzip"}},{"Process":{"Description":"Hvectext generates a Geomview\/OOGL vector-text object, of given height or total length (default -s .25). It aligns the given 3-D point (default -at 0 0 0) with the given corner of the text (default -align c). Alignment specs are like geographic directions; -align sw puts the ''at'' point at the ''southwest'' corner of the text, so that its lower left corner is at that coordinate. The corresponding VECT object is written to standard output. If no non-blank text characters are given, hvectext produces a null geom object (\"{ }\") as output. The special option -- marks the end of options; it's useful in case the text itself begins with a hyphen. Hvectext accepts Ghostscript Hershey fonts; the built-in default is Hershey-Plain-Simplex a.k.a. hrpl_s.gsf. Others of interest: hrsy_r.gsf (Symbol), hrpl_t.gsf (Triplex). For a complete list, see all the Ghostscript font files of nontrivial size whose names begin with \"hr\". Although there's no way to switch fonts within the string, the first line of output from hvectext is an OOGL comment resembling: # Continue with: -align sw -plane xy -s 0.25 -at 1.24378 0 0 giving options for hvectext to continue the string where it left off.","Process Name":"hvectext","Link":"https:\/\/linux.die.net\/man\/1\/hvectext"}},{"Process":{"Description":"With no arguments, hvol displays the name and path to the \"current\" HFS volume as well as the names and paths of all previously mounted (\"known\") volumes. With an argument, hvol changes the current volume to be the one specified. Either the name of the volume or the path to its UNIX source can be specified. The volume must have been previously mounted using hmount. A separate \"current working directory\" is maintained for each mounted volume. This information is kept in a file named .hcwd in the user's home directory.","Process Name":"hvol","Link":"https:\/\/linux.die.net\/man\/1\/hvol"}},{"Process":{"Description":"hwloc-assembler combines the input XML topologies and exports the resulting global topologies to a new XML file. All inputs are inserting as children of the global root object. Each input topology root is annotated with info attributes before insertion. AssemblerIndex is set to the index within the list of inputs. AssemblerName is set to the name given with --name if any. hwloc-assembler-remote offers a frontend for assembling remote nodes topologies without having to manually gather and transfer each of them.","Process Name":"hwloc-assembler","Link":"https:\/\/linux.die.net\/man\/1\/hwloc-assembler"}},{"Process":{"Description":"hwloc-assembler-remote is a frontend to hwloc-assembler. It takes care of retrieving the remote nodes' topologies before assembling them with hwloc-assembler.","Process Name":"hwloc-assembler-remote","Link":"https:\/\/linux.die.net\/man\/1\/hwloc-assembler-remote"}},{"Process":{"Description":"hwloc-bind execs an executable (with optional command line arguments) that is bound to the specified location (or list of locations). Upon successful execution, hwloc-bind simply sets bindings and then execs the executable over itself. NOTE: It is highly recommended that you read the hwloc(7) overview page before reading this man page. Most of the concepts described in hwloc(7) directly apply to the hwloc-bind utility.","Process Name":"hwloc-bind","Link":"https:\/\/linux.die.net\/man\/1\/hwloc-bind"}},{"Process":{"Description":"hwloc-calc generates and manipulates CPU mask strings or objects. Both input and output may be either objects (with physical or logical indexes), CPU lists (with physical or logical indexes), or CPU mask strings (always physically indexed). If objects or CPU mask strings are given on the command-line, they are combined and a single output is printed. If no object or CPU mask strings are given on the command-line, the program will read the standard input. It will combine multiple objects or CPU mask strings that are given on the same line of the standard input line with spaces as separators. Different input lines will be processed separately. Command-line arguments and options are processed in order. For instance, it means that changing the type of input indexes with --li or changing the input topology with -i only affects the processing the following arguments. NOTE: It is highly recommended that you read the hwloc(7) overview page before reading this man page. Most of the concepts described in hwloc(7) directly apply to the hwloc-calc utility.","Process Name":"hwloc-calc","Link":"https:\/\/linux.die.net\/man\/1\/hwloc-calc"}},{"Process":{"Description":"hwloc-distances displays also distance matrices attached to the topology. A breadth-first traversal of the topology is performed starting from the root to find all distance matrices. NOTE: lstopo may also display distance matrices in its verbose textual output. However lstopo only prints matrices that cover the entire topology while hwloc-distances also displays matrices that ignore part of the topology.","Process Name":"hwloc-distances","Link":"https:\/\/linux.die.net\/man\/1\/hwloc-distances"}},{"Process":{"Description":"hwloc-distrib generates a series of CPU masks corresponding to a distribution of a given number of elements over the topology of the machine. The distribution is done recursively from the top of the hierarchy (or from the level specified by option --from) down to the bottom of the hierarchy (or down to the level specified by option --to, or until only one element remains), splitting the number of elements at each encountered hierarchy level not ignored by options --ignore. This can e.g. be used to distribute a set of processes hierarchically according to the topology of a machine. These masks can be used with hwloc-bind(1). NOTE: It is highly recommended that you read the hwloc(7) overview page before reading this man page. Most of the concepts described in hwloc(7) directly apply to the hwloc-bind utility.","Process Name":"hwloc-distrib","Link":"https:\/\/linux.die.net\/man\/1\/hwloc-distrib"}},{"Process":{"Description":"hwloc-gather-topology saves all the relevant topology files into an archive (<path>.tar.bz2) and the lstopo output (<path>.output). The utility for example stores the \/proc\/cpuinfo file and the entire \/sys\/devices\/system\/node\/ directory tree. These files can be used later to explore the machine topology offline. Once the tarball has been extracted, it may for instance be given to some hwloc command-line utilities through their --input option. It is also possible to override the default topology that the hwloc library will read by setting the extracted path in the HWLOC_FSROOT environment variable. Both archive and lstopo output may also be submitted to hwloc developers to debug issues remotely. hwloc-gather-topology is a Linux specific tool, it is not installed on other operating systems. NOTE: It is highly recommended that you read the hwloc(7) overview page before reading this man page.","Process Name":"hwloc-gather-topology","Link":"https:\/\/linux.die.net\/man\/1\/hwloc-gather-topology"}},{"Process":{"Description":"lstopo and lstopo-no-graphics are capable of displaying a topological map of the system in a variety of different output formats. The only difference between lstopo and lstopo-no-graphics is that graphical outputs are only supported by lstopo, to reduce dependencies on external libraries. If no filename is specified and the DISPLAY environment variable is set, lstopo displays the map in a graphical window. If no filename is specified and the DISPLAY environment variable is not set, a text summary is displayed. The filename specified directly implies the output format that will be used; see the OUTPUT FORMATS section, below. Output formats that support color will indicate specific characteristics about individual CPUs by their color; see the COLORS section, below.","Process Name":"hwloc-info","Link":"https:\/\/linux.die.net\/man\/1\/hwloc-info"}},{"Process":{"Description":"lstopo and lstopo-no-graphics are capable of displaying a topological map of the system in a variety of different output formats. The only difference between lstopo and lstopo-no-graphics is that graphical outputs are only supported by lstopo, to reduce dependencies on external libraries. If no filename is specified and the DISPLAY environment variable is set, lstopo displays the map in a graphical window. If no filename is specified and the DISPLAY environment variable is not set, a text summary is displayed. The filename specified directly implies the output format that will be used; see the OUTPUT FORMATS section, below. Output formats that support color will indicate specific characteristics about individual CPUs by their color; see the COLORS section, below.","Process Name":"hwloc-ls","Link":"https:\/\/linux.die.net\/man\/1\/hwloc-ls"}},{"Process":{"Description":"hwloc-calc generates and manipulates CPU mask strings or objects. Both input and output may be either objects (with physical or logical indexes), CPU lists (with physical or logical indexes), or CPU mask strings (always physically indexed). If objects or CPU mask strings are given on the command-line, they are combined and a single output is printed. If no object or CPU mask strings are given on the command-line, the program will read the standard input. It will combine multiple objects or CPU mask strings that are given on the same line of the standard input line with spaces as separators. Different input lines will be processed separately. Command-line arguments and options are processed in order. For instance, it means that changing the type of input indexes with --li or changing the input topology with -i only affects the processing the following arguments. NOTE: It is highly recommended that you read the hwloc(7) overview page before reading this man page. Most of the concepts described in hwloc(7) directly apply to the hwloc-calc utility.","Process Name":"hwloc-mask","Link":"https:\/\/linux.die.net\/man\/1\/hwloc-mask"}},{"Process":{"Description":"By default, hwloc-ps lists only those currently-running processes that are bound. If -t is given, processes that are not bound but contain at least one bound thread are also displayed, as well as all their threads. hwloc-ps displays process identifier, command-line and binding. The binding may be reported as objects or cpusets. By default, process bindings are restricted to the currently available topology. If some processes are bound to processors that are not available to the current process, they are ignored unless --whole-system is given. The output is a plain list. If you wish to annotate the hierarchical topology with processes so as to see how they are actual distributed on the machine, you might want to use lstopo --ps instead (which also only shows processes that are bound). The -a switch can be used to show all processes, if desired.","Process Name":"hwloc-ps","Link":"https:\/\/linux.die.net\/man\/1\/hwloc-ps"}},{"Process":{"Description":"Hyla FAX is a telecommunication system for UNIX  systems. Among the features of Hyla FAX are: \u2022 Hyla FAX runs as a network service; this means a modem may be effectively shared by a large number of users. \u2022 Hyla FAX can be configured to work with a wide variety of modems on a wide variety of systems. \u2022 Access to the system can be restricted by the administrator to selected hosts and\/or users. \u2022 Transmission requests may be processed immediately (default) or queued for processing at a later time, in the manner of the at(1) command. \u2022 Remote facsimile machines may be polled to retrieve publicly available documents. \u2022 P OST S CRIPT , PDF, and TIFF Class F documents are passed directly to the fax server for transmission; the system attempts to convert other file formats to either P OST S CRIPT or TIFF through the use of an extensible file typing and conversion facility. In normal operation ASCII -text, troff(1) output, and Silicon Graphics images are automatically converted. Additional file formats can be added; see typerules(5F). \u2022 The faxcover(1) program can be automatically invoked to create a cover page for each facsimile, using information deduced by the sendfax command. Alternatively, users may supply their own cover pages using their preferred tools. \u2022 Facsimile are normally imaged in a system-default page size (usually letter-size pages, 8.5\" by 11\", for sites in North America). Alternate page sizes can be specified with a -s option to all Hyla FAX programs. Well known page sizes include: ISO A3, ISO A4, ISO A5, ISO A6, ISO B4, North American Letter, American Legal, American Ledger, American Executive, Japanese Letter, and Japanese Legal. Note that it may not be permissible to image into the full page area; the guaranteed reproducible area for a page is typically smaller. Also, note that while arbitrary page sizes can be specified, only a limited number of page dimensions are supported by the facsimile protocol. Thus if an odd-size facsimile is submitted for transmission it may not be possible to determine if it can be sent until the fax server establishes communication with the remote facsimile machine. \u2022 Facsimile can be sent at low resolution (98 lines\/inch) or medium resolution (196 lines\/inch)-often called fine mode. Documents with mixed resolution pages are handled correctly. \u2022 Users are notified by electronic mail if a job can not be transmitted. It is also possible to receive notification by mail when a job has been completed successfully and each time that the job is requeued for retransmission. Any untransmitted documents are returned to the sender by electronic mail in a form suitable for re-submission. \u2022 Support is provided for broadcasting facsimile. The Hyla FAX server software optimizes preparation of broadcast documents and the client applications support the notion of a job group which permits a group of jobs to be manipulated together. \u2022 Support is provided for transmitting alpha-numeric messages to pager devices or GSM mobiles using the Simple Network Paging Protocol ( SNPP ) and the IXO or UCP protocol (for message delivery). The Hyla FAX software is divided into two packages: software used on client machines and software used on machines where one or more modems reside. Client software includes: \u2022 sendfax, a program to submit outgoing facsimile; \u2022 sendpage, a program to submit alpha-numeric messages to SNPP servers; \u2022 faxstat, a program obtain status information about Hyla FAX servers; \u2022 faxrm, a program to remove jobs and documents; \u2022 faxalter, a program to change parameters of queued jobs; and \u2022 fax2ps, a program that converts facsimile documents to P OST S CRIPT so that they may be viewed with a P OST S CRIPT previewer or printed on a P OST S CRIPT printer (this program is actually part of the companion TIFF distribution that is used by Hyla FAX ). Many systems also support submission of outgoing facsimile by electronic mail and\/or graphical interfaces to the sendfax program. Such facilities are site-dependent; consult local documentation for more information.","Process Name":"hylafax-client","Link":"https:\/\/linux.die.net\/man\/1\/hylafax-client"}},{"Process":{"Description":"The hyperball program displays a wireframe projection of a hyperball which is rotating at user-specified rates around any or all of its four axes.","Process Name":"hyperball","Link":"https:\/\/linux.die.net\/man\/1\/hyperball"}},{"Process":{"Description":"The hypercube program displays a wireframe projection of a hypercube which is rotating at user-specified rates around any or all of its four axes.","Process Name":"hypercube","Link":"https:\/\/linux.die.net\/man\/1\/hypercube"}},{"Process":{"Description":"The hypertorus program shows the Clifford torus as it rotates in 4d. The Clifford torus is a torus lies on the \"surface\" of the hypersphere in 4d. The program projects the 4d torus to 3d using either a perspective or an orthographic projection. Of the two alternatives, the perspective projection looks much more appealing. In orthographic projections the torus degenerates into a doubly covered cylinder for some angles. The projected 3d torus can then be projected to the screen either perspectively or orthographically. There are three display modes for the torus: mesh (wireframe), solid, or transparent. Furthermore, the appearance of the torus can be as a solid object or as a set of see-through bands. Finally, the colors with with the torus is drawn can be set to either two-sided or to a color wheel. In the first case, the torus is drawn with red on the outside and green on the inside. This mode enables you to see that the torus turns inside-out as it rotates in 4d. The second mode draws the torus with a fully saturated color wheel. This gives a very nice effect when combined with the see-through bands mode. The rotation speed for each of the six planes around which the torus rotates can be chosen. This program is very much inspired by Thomas Banchoff's book \"Beyond the Third Dimension: Geometry, Computer Graphics, and Higher Dimensions\", Scientific American Library, 1990.","Process Name":"hypertorus","Link":"https:\/\/linux.die.net\/man\/1\/hypertorus"}},{"Process":{"Description":"hunzip is a small utility for text file compression and encryption, especially for sorted dictionaries. \"hunspell filename\" creates the compressed file \"filename.hz\" without removing the original file. The compression algorithm uses 16-bit Huffman encoding and line-oriented prefix-suffix compression. It has good compression ratio for huge sorted word lists.","Process Name":"hzip","Link":"https:\/\/linux.die.net\/man\/1\/hzip"}}]