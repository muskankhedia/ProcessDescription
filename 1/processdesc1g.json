[{"Process":{"Description":null,"Process Name":"g++","Link":"https:\/\/linux.die.net\/man\/1\/g++"}},{"Process":{"Description":null,"Process Name":"g2root","Link":"https:\/\/linux.die.net\/man\/1\/g2root"}},{"Process":{"Description":"Reads a Group 3 fax file (raw or digifax) as input. If no filename is given, stdin is used. Produces a portable bitmap as output.","Process Name":"g32pbm","Link":"https:\/\/linux.die.net\/man\/1\/g32pbm"}},{"Process":{"Description":null,"Process Name":"g3cat","Link":"https:\/\/linux.die.net\/man\/1\/g3cat"}},{"Process":{"Description":"This program is part of Netpbm(1). g3topbm reads a Group 3 fax file in MH format as input and produces a PBM image as output. g3topbm tolerates various deviations from the standard, so as to recover some of the image if there was a transmission error. One thing it tolerates is lines of varying length. The standard requires all the lines to be the same length; g3topbm makes the output image as wide as the longest line in the input and pads the others on the right. It warns you when it does this. You can use the stop_error option to make g3topbm insist on valid input. There is no Netpbm program that understands the other command fax formats, MR and MMR. There are subformats of TIFF that use the Group 3 fax encodings inside. See tifftopnm.","Process Name":"g3topbm","Link":"https:\/\/linux.die.net\/man\/1\/g3topbm"}},{"Process":{"Description":"The g77 command supports all the options supported by the gcc command. All gcc and g77 options are accepted both by g77 and by gcc (as well as any other drivers built at the same time, such as g++), since adding g77 to the gcc distribution enables acceptance of g77 options by all of the relevant drivers. In some cases, options have positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"g77","Link":"https:\/\/linux.die.net\/man\/1\/g77"}},{"Process":{"Description":null,"Process Name":"g_anadock","Link":"https:\/\/linux.die.net\/man\/1\/g_anadock"}},{"Process":{"Description":"g_anaeig analyzes eigenvectors. The eigenvectors can be of a covariance matrix ( g_covar) or of a Normal Modes analysis ( g_nmeig). When a trajectory is projected on eigenvectors, all structures are fitted to the structure in the eigenvector file, if present, otherwise to the structure in the structure file. When no run input file is supplied, periodicity will not be taken into account. Most analyses are performed on eigenvectors -first to -last, but when -first is set to -1 you will be prompted for a selection. -comp: plot the vector components per atom of eigenvectors -first to -last. -rmsf: plot the RMS fluctuation per atom of eigenvectors -first to -last (requires -eig). -proj: calculate projections of a trajectory on eigenvectors -first to -last. The projections of a trajectory on the eigenvectors of its covariance matrix are called principal components (pc's). It is often useful to check the cosine content of the pc's, since the pc's of random diffusion are cosines with the number of periods equal to half the pc index. The cosine content of the pc's can be calculated with the program g_analyze. -2d: calculate a 2d projection of a trajectory on eigenvectors -first and -last. -3d: calculate a 3d projection of a trajectory on the first three selected eigenvectors. -filt: filter the trajectory to show only the motion along eigenvectors -first to -last. -extr: calculate the two extreme projections along a trajectory on the average structure and interpolate -nframes frames between them, or set your own extremes with -max. The eigenvector -first will be written unless -first and -last have been set explicitly, in which case all eigenvectors will be written to separate files. Chain identifiers will be added when writing a .pdb file with two or three structures (you can use rasmol -nmrpdb to view such a .pdb file). Overlap calculations between covariance analysis: Note: the analysis should use the same fitting structure -over: calculate the subspace overlap of the eigenvectors in file -v2 with eigenvectors -first to -last in file -v. -inpr: calculate a matrix of inner-products between eigenvectors in files -v and -v2. All eigenvectors of both files will be used unless -first and -last have been set explicitly. When -v, -eig, -v2 and -eig2 are given, a single number for the overlap between the covariance matrices is generated. The formulas are: difference = sqrt(tr((sqrt(M1) - sqrt(M2))2)) normalized overlap = 1 - difference\/sqrt(tr(M1) + tr(M2)) shape overlap = 1 - sqrt(tr((sqrt(M1\/tr(M1)) - sqrt(M2\/tr(M2)))2)) where M1 and M2 are the two covariance matrices and tr is the trace of a matrix. The numbers are proportional to the overlap of the square root of the fluctuations. The normalized overlap is the most useful number, it is 1 for identical matrices and 0 when the sampled subspaces are orthogonal. When the -entropy flag is given an entropy estimate will be computed based on the Quasiharmonic approach and based on Schlitter's formula.","Process Name":"g_anaeig","Link":"https:\/\/linux.die.net\/man\/1\/g_anaeig"}},{"Process":{"Description":"g_analyze reads an ASCII file and analyzes data sets. A line in the input file may start with a time (see option -time) and any number of y-values may follow. Multiple sets can also be read when they are separated by & (option -n); in this case only one y-value is read from each line. All lines starting with and @ are skipped. All analyses can also be done for the derivative of a set (option -d). All options, except for -av and -power, assume that the points are equidistant in time. g_analyze always shows the average and standard deviation of each set, as well as the relative deviation of the third and fourth cumulant from those of a Gaussian distribution with the same standard deviation. Option -ac produces the autocorrelation function(s). Option -cc plots the resemblance of set i with a cosine of i\/2 periods. The formula is: 2 (int0-T y(t) cos(i pi t) dt)2 \/ int0-T y(t) y(t) dt This is useful for principal components obtained from covariance analysis, since the principal components of random diffusion are pure cosines. Option -msd produces the mean square displacement(s). Option -dist produces distribution plot(s). Option -av produces the average over the sets. Error bars can be added with the option -errbar. The errorbars can represent the standard deviation, the error (assuming the points are independent) or the interval containing 90% of the points, by discarding 5% of the points at the top and the bottom. Option -ee produces error estimates using block averaging. A set is divided in a number of blocks and averages are calculated for each block. The error for the total average is calculated from the variance between averages of the m blocks B_i as follows: error2 = Sum (B_i - B)2 \/ (m*(m-1)). These errors are plotted as a function of the block size. Also an analytical block average curve is plotted, assuming that the autocorrelation is a sum of two exponentials. The analytical curve for the block average is: f(t) = sigma *sqrt(2\/T ( alpha (tau1 ((exp(-t\/tau1) - 1) tau1\/t + 1)) + (1-alpha) (tau2 ((exp(-t\/tau2) - 1) tau2\/t + 1)))), where T is the total time. alpha, tau1 and tau2 are obtained by fitting f2(t) to error2. When the actual block average is very close to the analytical curve, the error is sigma *sqrt(2\/T (a tau1 + (1-a) tau2)). The complete derivation is given in B. Hess, J. Chem. Phys. 116:209-217, 2002. Option -bal finds and subtracts the ultrafast \"ballistic\" component from a hydrogen bond autocorrelation function by the fitting of a sum of exponentials, as described in e.g. O. Markovitch, J. Chem. Phys. 129:084505, 2008. The fastest term is the one with the most negative coefficient in the exponential, or with -d, the one with most negative time derivative at time 0. -nbalexp sets the number of exponentials to fit. Option -gem fits bimolecular rate constants ka and kb (and optionally kD) to the hydrogen bond autocorrelation function according to the reversible geminate recombination model. Removal of the ballistic component first is strongly adviced. The model is presented in O. Markovitch, J. Chem. Phys. 129:084505, 2008. Option -filter prints the RMS high-frequency fluctuation of each set and over all sets with respect to a filtered average. The filter is proportional to cos(pi t\/len) where t goes from -len\/2 to len\/2. len is supplied with the option -filter. This filter reduces oscillations with period len\/2 and len by a factor of 0.79 and 0.33 respectively. Option -g fits the data to the function given with option -fitfn. Option -power fits the data to b ta, which is accomplished by fitting to a t + b on log-log scale. All points after the first zero or with a negative value are ignored. Option -luzar performs a Luzar & Chandler kinetics analysis on output from g_hbond. The input file can be taken directly from g_hbond -ac, and then the same result should be produced.","Process Name":"g_analyze","Link":"https:\/\/linux.die.net\/man\/1\/g_analyze"}},{"Process":{"Description":null,"Process Name":"g_angle","Link":"https:\/\/linux.die.net\/man\/1\/g_angle"}},{"Process":{"Description":"g_bar calculates free energy difference estimates through Bennett's acceptance ratio method (BAR). It also automatically adds series of individual free energies obtained with BAR into a combined free energy estimate. Every individual BAR free energy difference relies on two simulations at different states: say state A and state B, as controlled by a parameter, lambda (see the .mdp parameter init_lambda). The BAR method calculates a ratio of weighted average of the Hamiltonian difference of state B given state A and vice versa. If the Hamiltonian does not depend linearly on lambda (in which case we can extrapolate the derivative of the Hamiltonian with respect to lambda, as is the default when free_energy is on), the energy differences to the other state need to be calculated explicitly during the simulation. This can be controlled with the .mdp option foreign_lambda. Input option -f expects multiple dhdl.xvg files. Two types of input files are supported: * Files with only one y-value, for such files it is assumed that the y-value is dH\/dlambda and that the Hamiltonian depends linearly on lambda. The lambda value of the simulation is inferred from the subtitle (if present), otherwise from a number in the subdirectory in the file name. * Files with more than one y-value. The files should have columns with dH\/dlambda and Deltalambda. The lambda values are inferred from the legends: lambda of the simulation from the legend of dH\/dlambda and the foreign lambda values from the legends of Delta H. The lambda of the simulation is parsed from dhdl.xvg file's legend containing the string 'dH', the foreign lambda values from the legend containing the capitalized letters 'D' and 'H'. The temperature is parsed from the legend line containing 'T ='. The input option -g expects multiple .edr files. These can contain either lists of energy differences (see the .mdp option separate_dhdl_file), or a series of histograms (see the .mdp options dh_hist_size and dh_hist_spacing). The temperature and lambda values are automatically deduced from the ener.edr file. The free energy estimates are determined using BAR with bisection, with the precision of the output set with -prec. An error estimate taking into account time correlations is made by splitting the data into blocks and determining the free energy differences over those blocks and assuming the blocks are independent. The final error estimate is determined from the average variance over 5 blocks. A range of block numbers for error estimation can be provided with the options -nbmin and -nbmax. g_bar tries to aggregate samples with the same 'native' and 'foreign' lambda values, but always assumes independent samples. Note that when aggregating energy differences\/derivatives with different sampling intervals, this is almost certainly not correct. Usually subsequent energies are correlated and different time intervals mean different degrees of correlation between samples. The results are split in two parts: the last part contains the final results in kJ\/mol, together with the error estimate for each part and the total. The first part contains detailed free energy difference estimates and phase space overlap measures in units of kT (together with their computed error estimate). The printed values are: * lam_A: the lambda values for point A. * lam_B: the lambda values for point B. * DG: the free energy estimate. * s_A: an estimate of the relative entropy of B in A. * s_A: an estimate of the relative entropy of A in B. * stdev: an estimate expected per-sample standard deviation. The relative entropy of both states in each other's ensemble can be interpreted as a measure of phase space overlap: the relative entropy s_A of the work samples of lambda_B in the ensemble of lambda_A (and vice versa for s_B), is a measure of the 'distance' between Boltzmann distributions of the two states, that goes to zero for identical distributions. See Wu & Kofke, J. Chem. Phys. 123 084109 (2005) for more information. The estimate of the expected per-sample standard deviation, as given in Bennett's original BAR paper: Bennett, J. Comp. Phys. 22, p 245 (1976). Eq. 10 therein gives an estimate of the quality of sampling (not directly of the actual statistical error, because it assumes independent samples). To get a visual estimate of the phase space overlap, use the -oh option to write series of histograms, together with the -nbin option.","Process Name":"g_bar","Link":"https:\/\/linux.die.net\/man\/1\/g_bar"}},{"Process":{"Description":"g_bond makes a distribution of bond lengths. If all is well a Gaussian distribution should be made when using a harmonic potential. Bonds are read from a single group in the index file in order i1-j1 i2-j2 through in-jn. -tol gives the half-width of the distribution as a fraction of the bondlength ( -blen). That means, for a bond of 0.2 a tol of 0.1 gives a distribution from 0.18 to 0.22. Option -d plots all the distances as a function of time. This requires a structure file for the atom and residue names in the output. If however the option -averdist is given (as well or separately) the average bond length is plotted instead.","Process Name":"g_bond","Link":"https:\/\/linux.die.net\/man\/1\/g_bond"}},{"Process":{"Description":null,"Process Name":"g_bundle","Link":"https:\/\/linux.die.net\/man\/1\/g_bundle"}},{"Process":{"Description":"g_chi computes phi, psi, omega, and chi dihedrals for all your amino acid backbone and sidechains. It can compute dihedral angle as a function of time, and as histogram distributions. The distributions (histo-(dihedral)(RESIDUE).xvg) are cumulative over all residues of each type. If option -corr is given, the program will calculate dihedral autocorrelation functions. The function used is C(t) = cos(chi(tau)) cos(chi(tau+t)) . The use of cosines rather than angles themselves, resolves the problem of periodicity. (Van der Spoel & Berendsen (1997), Biophys. J. 72, 2032-2041). Separate files for each dihedral of each residue (corr(dihedral)(RESIDUE)(nresnr).xvg) are output, as well as a file containing the information for all residues (argument of -corr). With option -all, the angles themselves as a function of time for each residue are printed to separate files (dihedral)(RESIDUE)(nresnr).xvg. These can be in radians or degrees. A log file (argument -g) is also written. This contains (a) information about the number of residues of each type. (b) The NMR 3J coupling constants from the Karplus equation. (c) a table for each residue of the number of transitions between rotamers per nanosecond, and the order parameter S2 of each dihedral. (d) a table for each residue of the rotamer occupancy. All rotamers are taken as 3-fold, except for omega and chi dihedrals to planar groups (i.e. chi2 of aromatics, Asp and Asn; chi3 of Glu and Gln; and chi4 of Arg), which are 2-fold. \"rotamer 0\" means that the dihedral was not in the core region of each rotamer. The width of the core region can be set with -core_rotamer The S2 order parameters are also output to an .xvg file (argument -o ) and optionally as a .pdb file with the S2 values as B-factor (argument -p). The total number of rotamer transitions per timestep (argument -ot), the number of transitions per rotamer (argument -rt), and the 3J couplings (argument -jc), can also be written to .xvg files. If -chi_prod is set (and -maxchi 0), cumulative rotamers, e.g. 1+9(chi1-1)+3(chi2-1)+(chi3-1) (if the residue has three 3-fold dihedrals and -maxchi = 3) are calculated. As before, if any dihedral is not in the core region, the rotamer is taken to be 0. The occupancies of these cumulative rotamers (starting with rotamer 0) are written to the file that is the argument of -cp, and if the -all flag is given, the rotamers as functions of time are written to chiproduct(RESIDUE)(nresnr).xvg and their occupancies to histo-chiproduct(RESIDUE)(nresnr).xvg. The option -r generates a contour plot of the average omega angle as a function of the phi and psi angles, that is, in a Ramachandran plot the average omega angle is plotted using color coding.","Process Name":"g_chi","Link":"https:\/\/linux.die.net\/man\/1\/g_chi"}},{"Process":{"Description":null,"Process Name":"g_cluster","Link":"https:\/\/linux.die.net\/man\/1\/g_cluster"}},{"Process":{"Description":"This program computes the size distributions of molecular\/atomic clusters in the gas phase. The output is given in the form of an .xpm file. The total number of clusters is written to an .xvg file. When the -mol option is given clusters will be made out of molecules rather than atoms, which allows clustering of large molecules. In this case an index file would still contain atom numbers or your calculation will die with a SEGV. When velocities are present in your trajectory, the temperature of the largest cluster will be printed in a separate .xvg file assuming that the particles are free to move. If you are using constraints, please correct the temperature. For instance water simulated with SHAKE or SETTLE will yield a temperature that is 1.5 times too low. You can compensate for this with the -ndf option. Remember to take the removal of center of mass motion into account. The -mc option will produce an index file containing the atom numbers of the largest cluster.","Process Name":"g_clustsize","Link":"https:\/\/linux.die.net\/man\/1\/g_clustsize"}},{"Process":{"Description":"g_confrms computes the root mean square deviation (RMSD) of two structures after least-squares fitting the second structure on the first one. The two structures do NOT need to have the same number of atoms, only the two index groups used for the fit need to be identical. With -name only matching atom names from the selected groups will be used for the fit and RMSD calculation. This can be useful when comparing mutants of a protein. The superimposed structures are written to file. In a .pdb file the two structures will be written as separate models (use rasmol -nmrpdb). Also in a .pdb file, B-factors calculated from the atomic MSD values can be written with -bfac.","Process Name":"g_confrms","Link":"https:\/\/linux.die.net\/man\/1\/g_confrms"}},{"Process":{"Description":null,"Process Name":"g_covar","Link":"https:\/\/linux.die.net\/man\/1\/g_covar"}},{"Process":{"Description":"g_current is a tool for calculating the current autocorrelation function, the correlation of the rotational and translational dipole moment of the system, and the resulting static dielectric constant. To obtain a reasonable result, the index group has to be neutral. Furthermore, the routine is capable of extracting the static conductivity from the current autocorrelation function, if velocities are given. Additionally, an Einstein-Helfand fit can be used to obtain the static conductivity. The flag -caf is for the output of the current autocorrelation function and -mc writes the correlation of the rotational and translational part of the dipole moment in the corresponding file. However, this option is only available for trajectories containing velocities. Options -sh and -tr are responsible for the averaging and integration of the autocorrelation functions. Since averaging proceeds by shifting the starting point through the trajectory, the shift can be modified with -sh to enable the choice of uncorrelated starting points. Towards the end, statistical inaccuracy grows and integrating the correlation function only yields reliable values until a certain point, depending on the number of frames. The option -tr controls the region of the integral taken into account for calculating the static dielectric constant. Option -temp sets the temperature required for the computation of the static dielectric constant. Option -eps controls the dielectric constant of the surrounding medium for simulations using a Reaction Field or dipole corrections of the Ewald summation (eps=0 corresponds to tin-foil boundary conditions). -[no]nojump unfolds the coordinates to allow free diffusion. This is required to get a continuous translational dipole moment, required for the Einstein-Helfand fit. The results from the fit allow the determination of the dielectric constant for system of charged molecules. However, it is also possible to extract the dielectric constant from the fluctuations of the total dipole moment in folded coordinates. But this option has to be used with care, since only very short time spans fulfill the approximation that the density of the molecules is approximately constant and the averages are already converged. To be on the safe side, the dielectric constant should be calculated with the help of the Einstein-Helfand method for the translational part of the dielectric constant.","Process Name":"g_current","Link":"https:\/\/linux.die.net\/man\/1\/g_current"}},{"Process":{"Description":"Compute partial densities across the box, using an index file. For the total density of NPT simulations, use g_energy instead. Densities are in kg\/m3, and number densities or electron densities can also be calculated. For electron densities, a file describing the number of electrons for each type of atom should be provided using -ei. It should look like: 2 atomname = nrelectrons atomname = nrelectrons The first line contains the number of lines to read from the file. There should be one line for each unique atom name in your system. The number of electrons for each atom is modified by its atomic partial charge.","Process Name":"g_density","Link":"https:\/\/linux.die.net\/man\/1\/g_density"}},{"Process":{"Description":null,"Process Name":"g_densmap","Link":"https:\/\/linux.die.net\/man\/1\/g_densmap"}},{"Process":{"Description":"A small program to reduce a two-phase density distribution along an axis, computed over a MD trajectory to 2D surfaces fluctuating in time, by a fit to a functional profile for interfacial densities A time-averaged spatial representation of the interfaces can be output with the option -tavg","Process Name":"g_densorder","Link":"https:\/\/linux.die.net\/man\/1\/g_densorder"}},{"Process":{"Description":null,"Process Name":"g_dielectric","Link":"https:\/\/linux.die.net\/man\/1\/g_dielectric"}},{"Process":{"Description":"g_dih can do two things. The default is to analyze dihedral transitions by merely computing all the dihedral angles defined in your topology for the whole trajectory. When a dihedral flips over to another minimum an angle\/time plot is made. The opther option is to discretize the dihedral space into a number of bins, and group each conformation in dihedral space in the appropriate bin. The output is then given as a number of dihedral conformations sorted according to occupancy.","Process Name":"g_dih","Link":"https:\/\/linux.die.net\/man\/1\/g_dih"}},{"Process":{"Description":"g_dipoles computes the total dipole plus fluctuations of a simulation system. From this you can compute e.g. the dielectric constant for low-dielectric media. For molecules with a net charge, the net charge is subtracted at center of mass of the molecule. The file Mtot.xvg contains the total dipole moment of a frame, the components as well as the norm of the vector. The file aver.xvg contains |Mu|2 and | Mu |2 during the simulation. The file dipdist.xvg contains the distribution of dipole moments during the simulation The value of -mumax is used as the highest value in the distribution graph. Furthermore, the dipole autocorrelation function will be computed when option -corr is used. The output file name is given with the -c option. The correlation functions can be averaged over all molecules ( mol), plotted per molecule separately ( molsep) or it can be computed over the total dipole moment of the simulation box ( total). Option -g produces a plot of the distance dependent Kirkwood G-factor, as well as the average cosine of the angle between the dipoles as a function of the distance. The plot also includes gOO and hOO according to Nymand & Linse, J. Chem. Phys. 112 (2000) pp 6386-6395. In the same plot, we also include the energy per scale computed by taking the inner product of the dipoles divided by the distance to the third power. EXAMPLES g_dipoles -corr mol -P1 -o dip_sqr -mu 2.273 -mumax 5.0 -nofft This will calculate the autocorrelation function of the molecular dipoles using a first order Legendre polynomial of the angle of the dipole vector and itself a time t later. For this calculation 1001 frames will be used. Further, the dielectric constant will be calculated using an epsilonRF of infinity (default), temperature of 300 K (default) and an average dipole moment of the molecule of 2.273 (SPC). For the distribution function a maximum of 5.0 will be used.","Process Name":"g_dipoles","Link":"https:\/\/linux.die.net\/man\/1\/g_dipoles"}},{"Process":{"Description":null,"Process Name":"g_disre","Link":"https:\/\/linux.die.net\/man\/1\/g_disre"}},{"Process":{"Description":"g_dist can calculate the distance between the centers of mass of two groups of atoms as a function of time. The total distance and its x-, y-, and z-components are plotted. Or when -dist is set, print all the atoms in group 2 that are closer than a certain distance to the center of mass of group 1. With options -lt and -dist the number of contacts of all atoms in group 2 that are closer than a certain distance to the center of mass of group 1 are plotted as a function of the time that the contact was continously present. Other programs that calculate distances are g_mindist and g_bond.","Process Name":"g_dist","Link":"https:\/\/linux.die.net\/man\/1\/g_dist"}},{"Process":{"Description":null,"Process Name":"g_do_dssp","Link":"https:\/\/linux.die.net\/man\/1\/g_do_dssp"}},{"Process":{"Description":"g_dyndom reads a .pdb file output from DynDom ( http:\/\/www.cmp.uea.ac.uk\/dyndom\/). It reads the coordinates, the coordinates of the rotation axis, and an index file containing the domains. Furthermore, it takes the first and last atom of the arrow file as command line arguments (head and tail) and finally it takes the translation vector (given in DynDom info file) and the angle of rotation (also as command line arguments). If the angle determined by DynDom is given, one should be able to recover the second structure used for generating the DynDom output. Because of limited numerical accuracy this should be verified by computing an all-atom RMSD (using g_confrms) rather than by file comparison (using diff). The purpose of this program is to interpolate and extrapolate the rotation as found by DynDom. As a result unphysical structures with long or short bonds, or overlapping atoms may be produced. Visual inspection, and energy minimization may be necessary to validate the structure.","Process Name":"g_dyndom","Link":"https:\/\/linux.die.net\/man\/1\/g_dyndom"}},{"Process":{"Description":"editconf converts generic structure format to .gro, .g96 or .pdb. The box can be modified with options -box, -d and -angles. Both -box and -d will center the system in the box, unless -noc is used. Option -bt determines the box type: triclinic is a triclinic box, cubic is a rectangular box with all sides equal dodecahedron represents a rhombic dodecahedron and octahedron is a truncated octahedron. The last two are special cases of a triclinic box. The length of the three box vectors of the truncated octahedron is the shortest distance between two opposite hexagons. Relative to a cubic box with some periodic image distance, the volume of a dodecahedron with this same periodic distance is 0.71 times that of the cube, and that of a truncated octahedron is 0.77 times. Option -box requires only one value for a cubic, rhombic dodecahedral, or truncated octahedral box. With -d and a triclinic box the size of the system in the x-, y-, and z-directions is used. With -d and cubic, dodecahedron or octahedron boxes, the dimensions are set to the diameter of the system (largest distance between atoms) plus twice the specified distance. Option -angles is only meaningful with option -box and a triclinic box and cannot be used with option -d. When -n or -ndef is set, a group can be selected for calculating the size and the geometric center, otherwise the whole system is used. -rotate rotates the coordinates and velocities. -princ aligns the principal axes of the system along the coordinate axes, with the longest axis aligned with the x-axis. This may allow you to decrease the box volume, but beware that molecules can rotate significantly in a nanosecond. Scaling is applied before any of the other operations are performed. Boxes and coordinates can be scaled to give a certain density (option -density). Note that this may be inaccurate in case a .gro file is given as input. A special feature of the scaling option is that when the factor -1 is given in one dimension, one obtains a mirror image, mirrored in one of the planes. When one uses -1 in three dimensions, a point-mirror image is obtained. Groups are selected after all operations have been applied. Periodicity can be removed in a crude manner. It is important that the box vectors at the bottom of your input file are correct when the periodicity is to be removed. When writing .pdb files, B-factors can be added with the -bf option. B-factors are read from a file with with following format: first line states number of entries in the file, next lines state an index followed by a B-factor. The B-factors will be attached per residue unless an index is larger than the number of residues or unless the -atom option is set. Obviously, any type of numeric data can be added instead of B-factors. -legend will produce a row of CA atoms with B-factors ranging from the minimum to the maximum value found, effectively making a legend for viewing. With the option -mead a special .pdb ( .pqr) file for the MEAD electrostatics program (Poisson-Boltzmann solver) can be made. A further prerequisite is that the input file is a run input file. The B-factor field is then filled with the Van der Waals radius of the atoms while the occupancy field will hold the charge. The option -grasp is similar, but it puts the charges in the B-factor and the radius in the occupancy. Option -align allows alignment of the principal axis of a specified group against the given vector, with an optional center of rotation specified by -aligncenter. Finally, with option -label, editconf can add a chain identifier to a .pdb file, which can be useful for analysis with e.g. Rasmol. To convert a truncated octrahedron file produced by a package which uses a cubic box with the corners cut off (such as GROMOS), use: editconf -f in -rotate 0 45 35.264 -bt o -box veclen -o out where veclen is the size of the cubic box times sqrt(3)\/2.","Process Name":"g_editconf","Link":"https:\/\/linux.die.net\/man\/1\/g_editconf"}},{"Process":{"Description":"With multiple files specified for the -f option: Concatenates several energy files in sorted order. In the case of double time frames, the one in the later file is used. By specifying -settime you will be asked for the start time of each file. The input files are taken from the command line, such that the command eneconv -f *.edr -o fixed.edr should do the trick. With one file specified for -f: Reads one energy file and writes another, applying the -dt, -offset, -t0 and -settime options and converting to a different format if necessary (indicated by file extentions). -settime is applied first, then -dt\/ -offset followed by -b and -e to select which frames to write.","Process Name":"g_eneconv","Link":"https:\/\/linux.die.net\/man\/1\/g_eneconv"}},{"Process":{"Description":null,"Process Name":"g_enemat","Link":"https:\/\/linux.die.net\/man\/1\/g_enemat"}},{"Process":{"Description":"g_energy extracts energy components or distance restraint data from an energy file. The user is prompted to interactively select the desired energy terms. Average, RMSD, and drift are calculated with full precision from the simulation (see printed manual). Drift is calculated by performing a least-squares fit of the data to a straight line. The reported total drift is the difference of the fit at the first and last point. An error estimate of the average is given based on a block averages over 5 blocks using the full-precision averages. The error estimate can be performed over multiple block lengths with the options -nbmin and -nbmax. Note that in most cases the energy files contains averages over all MD steps, or over many more points than the number of frames in energy file. This makes the g_energy statistics output more accurate than the .xvg output. When exact averages are not present in the energy file, the statistics mentioned above are simply over the single, per-frame energy values. The term fluctuation gives the RMSD around the least-squares fit. When the -viol option is set, the time averaged violations are plotted and the running time-averaged and instantaneous sum of violations are recalculated. Additionally running time-averaged and instantaneous distances between selected pairs can be plotted with the -pairs option. Options -ora, -ort, -oda, -odr and -odt are used for analyzing orientation restraint data. The first two options plot the orientation, the last three the deviations of the orientations from the experimental values. The options that end on an 'a' plot the average over time as a function of restraint. The options that end on a 't' prompt the user for restraint label numbers and plot the data as a function of time. Option -odr plots the RMS deviation as a function of restraint. When the run used time or ensemble averaged orientation restraints, option -orinst can be used to analyse the instantaneous, not ensemble-averaged orientations and deviations instead of the time and ensemble averages. Option -oten plots the eigenvalues of the molecular order tensor for each orientation restraint experiment. With option -ovec also the eigenvectors are plotted. Option -odh extracts and plots the free energy data (Hamiltoian differences and\/or the Hamiltonian derivative dhdl) from the ener.edr file. With -fee an estimate is calculated for the free-energy difference with an ideal gas state: Delta A = A(N,V,T) - A_idgas(N,V,T) = kT ln e(Upot\/kT) Delta G = G(N,p,T) - G_idgas(N,p,T) = kT ln e(Upot\/kT) where k is Boltzmann's constant, T is set by -fetemp and the average is over the ensemble (or time in a trajectory). Note that this is in principle only correct when averaging over the whole (Boltzmann) ensemble and using the potential energy. This also allows for an entropy estimate using: Delta S(N,V,T) = S(N,V,T) - S_idgas(N,V,T) = (Upot - Delta A)\/T Delta S(N,p,T) = S(N,p,T) - S_idgas(N,p,T) = (Upot + pV - Delta G)\/T When a second energy file is specified ( -f2), a free energy difference is calculated dF = -kT ln e -(EB-EA)\/kT A , where EA and EB are the energies from the first and second energy files, and the average is over the ensemble A. The running average of the free energy difference is printed to a file specified by -ravg. Note that the energies must both be calculated from the same trajectory.","Process Name":"g_energy","Link":"https:\/\/linux.die.net\/man\/1\/g_energy"}},{"Process":{"Description":"g_filter performs frequency filtering on a trajectory. The filter shape is cos(pi t\/A) + 1 from -A to +A, where A is given by the option -nf times the time step in the input trajectory. This filter reduces fluctuations with period A by 85%, with period 2*A by 50% and with period 3*A by 17% for low-pass filtering. Both a low-pass and high-pass filtered trajectory can be written. Option -ol writes a low-pass filtered trajectory. A frame is written every -nf input frames. This ratio of filter length and output interval ensures a good suppression of aliasing of high-frequency motion, which is useful for making smooth movies. Also averages of properties which are linear in the coordinates are preserved, since all input frames are weighted equally in the output. When all frames are needed, use the -all option. Option -oh writes a high-pass filtered trajectory. The high-pass filtered coordinates are added to the coordinates from the structure file. When using high-pass filtering use -fit or make sure you use a trajectory that has been fitted on the coordinates in the structure file.","Process Name":"g_filter","Link":"https:\/\/linux.die.net\/man\/1\/g_filter"}},{"Process":{"Description":"genbox can do one of 3 things: 1) Generate a box of solvent. Specify -cs and -box. Or specify -cs and -cp with a structure file with a box, but without atoms. 2) Solvate a solute configuration, e.g. a protein, in a bath of solvent molecules. Specify -cp (solute) and -cs (solvent). The box specified in the solute coordinate file ( -cp) is used, unless -box is set. If you want the solute to be centered in the box, the program editconf has sophisticated options to change the box dimensions and center the solute. Solvent molecules are removed from the box where the distance between any atom of the solute molecule(s) and any atom of the solvent molecule is less than the sum of the van der Waals radii of both atoms. A database ( vdwradii.dat) of van der Waals radii is read by the program, and atoms not in the database are assigned a default distance -vdwd. Note that this option will also influence the distances between solvent molecules if they contain atoms that are not in the database. 3) Insert a number ( -nmol) of extra molecules ( -ci) at random positions. The program iterates until nmol molecules have been inserted in the box. To test whether an insertion is successful the same van der Waals criterium is used as for removal of solvent molecules. When no appropriately-sized holes (holes that can hold an extra molecule) are available, the program tries for -nmol * -try times before giving up. Increase -try if you have several small holes to fill. If you need to do more than one of the above operations, it can be best to call genbox separately for each operation, so that you are sure of the order in which the operations occur. The default solvent is Simple Point Charge water (SPC), with coordinates from $GMXLIB\/spc216.gro. These coordinates can also be used for other 3-site water models, since a short equibilibration will remove the small differences between the models. Other solvents are also supported, as well as mixed solvents. The only restriction to solvent types is that a solvent molecule consists of exactly one residue. The residue information in the coordinate files is used, and should therefore be more or less consistent. In practice this means that two subsequent solvent molecules in the solvent coordinate file should have different residue number. The box of solute is built by stacking the coordinates read from the coordinate file. This means that these coordinates should be equlibrated in periodic boundary conditions to ensure a good alignment of molecules on the stacking interfaces. The -maxsol option simply adds only the first -maxsol solvent molecules and leaves out the rest would have fit into the box. The program can optionally rotate the solute molecule to align the longest molecule axis along a box edge. This way the amount of solvent molecules necessary is reduced. It should be kept in mind that this only works for short simulations, as e.g. an alpha-helical peptide in solution can rotate over 90 degrees, within 500 ps. In general it is therefore better to make a more or less cubic box. Setting -shell larger than zero will place a layer of water of the specified thickness (nm) around the solute. Hint: it is a good idea to put the protein in the center of a box first (using editconf). Finally, genbox will optionally remove lines from your topology file in which a number of solvent molecules is already added, and adds a line with the total number of solvent molecules in your coordinate file.","Process Name":"g_genbox","Link":"https:\/\/linux.die.net\/man\/1\/g_genbox"}},{"Process":{"Description":"genconf multiplies a given coordinate file by simply stacking them on top of each other, like a small child playing with wooden blocks. The program makes a grid of user-defined proportions ( -nbox), and interspaces the grid point with an extra space -dist. When option -rot is used the program does not check for overlap between molecules on grid points. It is recommended to make the box in the input file at least as big as the coordinates + van der Waals radius. If the optional trajectory file is given, conformations are not generated, but read from this file and translated appropriately to build the grid.","Process Name":"g_genconf","Link":"https:\/\/linux.die.net\/man\/1\/g_genconf"}},{"Process":{"Description":null,"Process Name":"g_genion","Link":"https:\/\/linux.die.net\/man\/1\/g_genion"}},{"Process":{"Description":"genrestr produces an include file for a topology containing a list of atom numbers and three force constants for the x-, y-, and z-direction. A single isotropic force constant may be given on the command line instead of three components. WARNING: position restraints only work for the one molecule at a time. Position restraints are interactions within molecules, therefore they should be included within the correct [ moleculetype ] block in the topology. Since the atom numbers in every moleculetype in the topology start at 1 and the numbers in the input file for genrestr number consecutively from 1, genrestr will only produce a useful file for the first molecule. The -of option produces an index file that can be used for freezing atoms. In this case, the input file must be a .pdb file. With the -disre option, half a matrix of distance restraints is generated instead of position restraints. With this matrix, that one typically would apply to Calpha atoms in a protein, one can maintain the overall conformation of a protein without tieing it to a specific position (as with position restraints).","Process Name":"g_genrestr","Link":"https:\/\/linux.die.net\/man\/1\/g_genrestr"}},{"Process":{"Description":"gmxcheck reads a trajectory ( .trj, .trr or .xtc), an energy file ( .ene or .edr) or an index file ( .ndx) and prints out useful information about them. Option -c checks for presence of coordinates, velocities and box in the file, for close contacts (smaller than -vdwfac and not bonded, i.e. not between -bonlo and -bonhi, all relative to the sum of both Van der Waals radii) and atoms outside the box (these may occur often and are no problem). If velocities are present, an estimated temperature will be calculated from them. If an index file, is given its contents will be summarized. If both a trajectory and a .tpr file are given (with -s1) the program will check whether the bond lengths defined in the tpr file are indeed correct in the trajectory. If not you may have non-matching files due to e.g. deshuffling or due to problems with virtual sites. With these flags, gmxcheck provides a quick check for such problems. The program can compare two run input ( .tpr, .tpb or .tpa) files when both -s1 and -s2 are supplied. Similarly a pair of trajectory files can be compared (using the -f2 option), or a pair of energy files (using the -e2 option). For free energy simulations the A and B state topology from one run input file can be compared with options -s1 and -ab. In case the -m flag is given a LaTeX file will be written consisting of a rough outline for a methods section for a paper.","Process Name":"g_gmxcheck","Link":"https:\/\/linux.die.net\/man\/1\/g_gmxcheck"}},{"Process":{"Description":"gmxdump reads a run input file ( .tpa\/ .tpr\/ .tpb), a trajectory ( .trj\/ .trr\/ .xtc), an energy file ( .ene\/ .edr), or a checkpoint file ( .cpt) and prints that to standard output in a readable format. This program is essential for checking your run input file in case of problems. The program can also preprocess a topology to help finding problems. Note that currently setting GMXLIB is the only way to customize directories used for searching include files.","Process Name":"g_gmxdump","Link":"https:\/\/linux.die.net\/man\/1\/g_gmxdump"}},{"Process":{"Description":null,"Process Name":"g_grompp","Link":"https:\/\/linux.die.net\/man\/1\/g_grompp"}},{"Process":{"Description":"g_gyrate computes the radius of gyration of a group of atoms and the radii of gyration about the x-, y- and z-axes, as a function of time. The atoms are explicitly mass weighted. With the -nmol option the radius of gyration will be calculated for multiple molecules by splitting the analysis group in equally sized parts. With the option -nz 2D radii of gyration in the x-y plane of slices along the z-axis are calculated.","Process Name":"g_gyrate","Link":"https:\/\/linux.die.net\/man\/1\/g_gyrate"}},{"Process":{"Description":"g_h2order computes the orientation of water molecules with respect to the normal of the box. The program determines the average cosine of the angle between the dipole moment of water and an axis of the box. The box is divided in slices and the average orientation per slice is printed. Each water molecule is assigned to a slice, per time frame, based on the position of the oxygen. When -nm is used, the angle between the water dipole and the axis from the center of mass to the oxygen is calculated instead of the angle between the dipole and a box axis.","Process Name":"g_h2order","Link":"https:\/\/linux.die.net\/man\/1\/g_h2order"}},{"Process":{"Description":"g_hbond computes and analyzes hydrogen bonds. Hydrogen bonds are determined based on cutoffs for the angle Acceptor - Donor - Hydrogen (zero is extended) and the distance Hydrogen - Acceptor. OH and NH groups are regarded as donors, O is an acceptor always, N is an acceptor by default, but this can be switched using -nitacc. Dummy hydrogen atoms are assumed to be connected to the first preceding non-hydrogen atom. You need to specify two groups for analysis, which must be either identical or non-overlapping. All hydrogen bonds between the two groups are analyzed. If you set -shell, you will be asked for an additional index group which should contain exactly one atom. In this case, only hydrogen bonds between atoms within the shell distance from the one atom are considered. [ selected ] 20 21 24 25 26 29 1 3 6 Note that the triplets need not be on separate lines. Each atom triplet specifies a hydrogen bond to be analyzed, note also that no check is made for the types of atoms. Output: -num: number of hydrogen bonds as a function of time. -ac: average over all autocorrelations of the existence functions (either 0 or 1) of all hydrogen bonds. -dist: distance distribution of all hydrogen bonds. -ang: angle distribution of all hydrogen bonds. -hx: the number of n-n+i hydrogen bonds as a function of time where n and n+i stand for residue numbers and i ranges from 0 to 6. This includes the n-n+3, n-n+4 and n-n+5 hydrogen bonds associated with helices in proteins. -hbn: all selected groups, donors, hydrogens and acceptors for selected groups, all hydrogen bonded atoms from all groups and all solvent atoms involved in insertion. -hbm: existence matrix for all hydrogen bonds over all frames, this also contains information on solvent insertion into hydrogen bonds. Ordering is identical to that in -hbn index file. -dan: write out the number of donors and acceptors analyzed for each timeframe. This is especially useful when using -shell. -nhbdist: compute the number of HBonds per hydrogen in order to compare results to Raman Spectroscopy. Note: options -ac, -life, -hbn and -hbm require an amount of memory proportional to the total numbers of donors times the total number of acceptors in the selected group(s).","Process Name":"g_hbond","Link":"https:\/\/linux.die.net\/man\/1\/g_hbond"}},{"Process":{"Description":null,"Process Name":"g_helix","Link":"https:\/\/linux.die.net\/man\/1\/g_helix"}},{"Process":{"Description":"g_helixorient calculates the coordinates and direction of the average axis inside an alpha helix, and the direction\/vectors of both the Calpha and (optionally) a sidechain atom relative to the axis. As input, you need to specify an index group with Calpha atoms corresponding to an alpha-helix of continuous residues. Sidechain directions require a second index group of the same size, containing the heavy atom in each residue that should represent the sidechain. Note that this program does not do any fitting of structures. We need four Calpha coordinates to define the local direction of the helix axis. The tilt\/rotation is calculated from Euler rotations, where we define the helix axis as the local x-axis, the residues\/Calpha vector as y, and the z-axis from their cross product. We use the Euler Y-Z-X rotation, meaning we first tilt the helix axis (1) around and (2) orthogonal to the residues vector, and finally apply the (3) rotation around it. For debugging or other purposes, we also write out the actual Euler rotation angles as theta[1-3].xvg","Process Name":"g_helixorient","Link":"https:\/\/linux.die.net\/man\/1\/g_helixorient"}},{"Process":{"Description":null,"Process Name":"g_hydorder","Link":"https:\/\/linux.die.net\/man\/1\/g_hydorder"}},{"Process":{"Description":"g_lie computes a free energy estimate based on an energy analysis from. One needs an energy file with the following components: Coul (A-B) LJ-SR (A-B) etc.","Process Name":"g_lie","Link":"https:\/\/linux.die.net\/man\/1\/g_lie"}},{"Process":{"Description":"make_edi generates an essential dynamics (ED) sampling input file to be used with mdrun based on eigenvectors of a covariance matrix ( g_covar) or from a normal modes anaysis ( g_nmeig). ED sampling can be used to manipulate the position along collective coordinates (eigenvectors) of (biological) macromolecules during a simulation. Particularly, it may be used to enhance the sampling efficiency of MD simulations by stimulating the system to explore new regions along these collective coordinates. A number of different algorithms are implemented to drive the system along the eigenvectors ( -linfix, -linacc, -radfix, -radacc, -radcon), to keep the position along a certain (set of) coordinate(s) fixed ( -linfix), or to only monitor the projections of the positions onto these coordinates ( -mon). References: A. Amadei, A.B.M. Linssen, B.L. de Groot, D.M.F. van Aalten and H.J.C. Berendsen; An efficient method for sampling the essential subspace of proteins., J. Biomol. Struct. Dyn. 13:615-626 (1996) B.L. de Groot, A. Amadei, D.M.F. van Aalten and H.J.C. Berendsen; Towards an exhaustive sampling of the configurational spaces of the two forms of the peptide hormone guanylin, J. Biomol. Struct. Dyn. 13 : 741-751 (1996) B.L. de Groot, A.Amadei, R.M. Scheek, N.A.J. van Nuland and H.J.C. Berendsen; An extended sampling of the configurational space of HPr from E. coli Proteins: Struct. Funct. Gen. 26: 314-322 (1996) You will be prompted for one or more index groups that correspond to the eigenvectors, reference structure, target positions, etc. -mon: monitor projections of the coordinates onto selected eigenvectors. -linfix: perform fixed-step linear expansion along selected eigenvectors. -linacc: perform acceptance linear expansion along selected eigenvectors. (steps in the desired directions will be accepted, others will be rejected). -radfix: perform fixed-step radius expansion along selected eigenvectors. -radacc: perform acceptance radius expansion along selected eigenvectors. (steps in the desired direction will be accepted, others will be rejected). Note: by default the starting MD structure will be taken as origin of the first expansion cycle for radius expansion. If -ori is specified, you will be able to read in a structure file that defines an external origin. -radcon: perform acceptance radius contraction along selected eigenvectors towards a target structure specified with -tar. NOTE: each eigenvector can be selected only once. -outfrq: frequency (in steps) of writing out projections etc. to .edo file -slope: minimal slope in acceptance radius expansion. A new expansion cycle will be started if the spontaneous increase of the radius (in nm\/step) is less than the value specified. -maxedsteps: maximum number of steps per cycle in radius expansion before a new cycle is started. Note on the parallel implementation: since ED sampling is a 'global' thing (collective coordinates etc.), at least on the 'protein' side, ED sampling is not very parallel-friendly from an implentation point of view. Because parallel ED requires some extra communication, expect the performance to be lower as in a free MD simulation, especially on a large number of nodes. All output of mdrun (specify with -eo) is written to a .edo file. In the output file, per OUTFRQ step the following information is present: * the step number * the number of the ED dataset. ( Note that you can impose multiple ED constraints in a single simulation (on different molecules) if several .edi files were concatenated first. The constraints are applied in the order they appear in the .edi file.) * RMSD (for atoms involved in fitting prior to calculating the ED constraints) * projections of the positions onto selected eigenvectors FLOODING: with -flood, you can specify which eigenvectors are used to compute a flooding potential, which will lead to extra forces expelling the structure out of the region described by the covariance matrix. If you switch -restrain the potential is inverted and the structure is kept in that region. The origin is normally the average structure stored in the eigvec.trr file. It can be changed with -ori to an arbitrary position in configurational space. With -tau, -deltaF0, and -Eflnull you control the flooding behaviour. Efl is the flooding strength, it is updated according to the rule of adaptive flooding. Tau is the time constant of adaptive flooding, high tau means slow adaption (i.e. growth). DeltaF0 is the flooding strength you want to reach after tau ps of simulation. To use constant Efl set -tau to zero. -alpha is a fudge parameter to control the width of the flooding potential. A value of 2 has been found to give good results for most standard cases in flooding of proteins. alpha basically accounts for incomplete sampling, if you sampled further the width of the ensemble would increase, this is mimicked by alpha 1. For restraining, alpha 1 can give you smaller width in the restraining potential. RESTART and FLOODING: If you want to restart a crashed flooding simulation please find the values deltaF and Efl in the output file and manually put them into the .edi file under DELTA_F0 and EFL_NULL.","Process Name":"g_make_edi","Link":"https:\/\/linux.die.net\/man\/1\/g_make_edi"}},{"Process":{"Description":null,"Process Name":"g_make_ndx","Link":"https:\/\/linux.die.net\/man\/1\/g_make_ndx"}},{"Process":{"Description":"g_mdmat makes distance matrices consisting of the smallest distance between residue pairs. With -frames, these distance matrices can be stored in order to see differences in tertiary structure as a function of time. If you choose your options unwisely, this may generate a large output file. By default, only an averaged matrix over the whole trajectory is output. Also a count of the number of different atomic contacts between residues over the whole trajectory can be made. The output can be processed with xpm2ps to make a PostScript (tm) plot.","Process Name":"g_mdmat","Link":"https:\/\/linux.die.net\/man\/1\/g_mdmat"}},{"Process":{"Description":null,"Process Name":"g_mdrun","Link":"https:\/\/linux.die.net\/man\/1\/g_mdrun"}},{"Process":{"Description":null,"Process Name":"g_membed","Link":"https:\/\/linux.die.net\/man\/1\/g_membed"}},{"Process":{"Description":null,"Process Name":"g_mindist","Link":"https:\/\/linux.die.net\/man\/1\/g_mindist"}},{"Process":{"Description":"mk_angndx makes an index file for calculation of angle distributions etc. It uses a run input file ( .tpx) for the definitions of the angles, dihedrals etc.","Process Name":"g_mk_angndx","Link":"https:\/\/linux.die.net\/man\/1\/g_mk_angndx"}},{"Process":{"Description":null,"Process Name":"g_morph","Link":"https:\/\/linux.die.net\/man\/1\/g_morph"}},{"Process":{"Description":"g_msd computes the mean square displacement (MSD) of atoms from a set of initial positions. This provides an easy way to compute the diffusion constant using the Einstein relation. The time between the reference points for the MSD calculation is set with -trestart. The diffusion constant is calculated by least squares fitting a straight line (D*t + c) through the MSD(t) from -beginfit to -endfit (note that t is time from the reference positions, not simulation time). An error estimate given, which is the difference of the diffusion coefficients obtained from fits over the two halves of the fit interval. There are three, mutually exclusive, options to determine different types of mean square displacement: -type, -lateral and -ten. Option -ten writes the full MSD tensor for each group, the order in the output is: trace xx yy zz yx zx zy. If -mol is set, g_msd plots the MSD for individual molecules (including making molecules whole across periodic boundaries): for each individual molecule a diffusion constant is computed for its center of mass. The chosen index group will be split into molecules. The default way to calculate a MSD is by using mass-weighted averages. This can be turned off with -nomw. With the option -rmcomm, the center of mass motion of a specific group can be removed. For trajectories produced with GROMACS this is usually not necessary, as mdrun usually already removes the center of mass motion. When you use this option be sure that the whole system is stored in the trajectory file. The diffusion coefficient is determined by linear regression of the MSD, where, unlike for the normal output of D, the times are weighted according to the number of reference points, i.e. short times have a higher weight. Also when -beginfit=-1,fitting starts at 10% and when -endfit=-1, fitting goes to 90%. Using this option one also gets an accurate error estimate based on the statistics between individual molecules. Note that this diffusion coefficient and error estimate are only accurate when the MSD is completely linear between -beginfit and -endfit. Option -pdb writes a .pdb file with the coordinates of the frame at time -tpdb with in the B-factor field the square root of the diffusion coefficient of the molecule. This option implies option -mol.","Process Name":"g_msd","Link":"https:\/\/linux.die.net\/man\/1\/g_msd"}},{"Process":{"Description":"g_nmeig calculates the eigenvectors\/values of a (Hessian) matrix, which can be calculated with mdrun. The eigenvectors are written to a trajectory file ( -v). The structure is written first with t=0. The eigenvectors are written as frames with the eigenvector number as timestamp. The eigenvectors can be analyzed with g_anaeig. An ensemble of structures can be generated from the eigenvectors with g_nmens. When mass weighting is used, the generated eigenvectors will be scaled back to plain Cartesian coordinates before generating the output. In this case, they will no longer be exactly orthogonal in the standard Cartesian norm, but in the mass-weighted norm they would be. This program can be optionally used to compute quantum corrections to heat capacity and enthalpy by providing an extra file argument -qcorr. See the GROMACS manual, Chapter 1, for details. The result includes subtracting a harmonic degree of freedom at the given temperature. The total correction is printed on the terminal screen. The recommended way of getting the corrections out is: g_nmeig -s topol.tpr -f nm.mtx -first 7 -last 10000 -T 300 -qc [-constr] The -constr option should be used when bond constraints were used during the simulation for all the covalent bonds. If this is not the case, you need to analyze the quant_corr.xvg file yourself. To make things more flexible, the program can also take virtual sites into account when computing quantum corrections. When selecting -constr and -qc, the -begin and -end options will be set automatically as well. Again, if you think you know it better, please check the eigenfreq.xvg output.","Process Name":"g_nmeig","Link":"https:\/\/linux.die.net\/man\/1\/g_nmeig"}},{"Process":{"Description":null,"Process Name":"g_nmens","Link":"https:\/\/linux.die.net\/man\/1\/g_nmens"}},{"Process":{"Description":"g_nmtraj generates an virtual trajectory from an eigenvector, corresponding to a harmonic Cartesian oscillation around the average structure. The eigenvectors should normally be mass-weighted, but you can use non-weighted eigenvectors to generate orthogonal motions. The output frames are written as a trajectory file covering an entire period, and the first frame is the average structure. If you write the trajectory in (or convert to) PDB format you can view it directly in PyMol and also render a photorealistic movie. Motion amplitudes are calculated from the eigenvalues and a preset temperature, assuming equipartition of the energy over all modes. To make the motion clearly visible in PyMol you might want to amplify it by setting an unrealistically high temperature. However, be aware that both the linear Cartesian displacements and mass weighting will lead to serious structure deformation for high amplitudes - this is is simply a limitation of the Cartesian normal mode model. By default the selected eigenvector is set to 7, since the first six normal modes are the translational and rotational degrees of freedom.","Process Name":"g_nmtraj","Link":"https:\/\/linux.die.net\/man\/1\/g_nmtraj"}},{"Process":{"Description":"GROMACS programs have some standard options, of which some are hidden by default:","Process Name":"g_options","Link":"https:\/\/linux.die.net\/man\/1\/g_options"}},{"Process":{"Description":"Compute the order parameter per atom for carbon tails. For atom i the vector i-1, i+1 is used together with an axis. The index file should contain only the groups to be used for calculations, with each group of equivalent carbons along the relevant acyl chain in its own group. There should not be any generic groups (like System, Protein) in the index file to avoid confusing the program (this is not relevant to tetrahedral order parameters however, which only work for water anyway). The program can also give all diagonal elements of the order tensor and even calculate the deuterium order parameter Scd (default). If the option -szonly is given, only one order tensor component (specified by the -d option) is given and the order parameter per slice is calculated as well. If -szonly is not selected, all diagonal elements and the deuterium order parameter is given. The tetrahedrality order parameters can be determined around an atom. Both angle an distance order parameters are calculated. See P.-L. Chau and A.J. Hardwick, Mol. Phys., 93, (1998), 511-518. for more details.","Process Name":"g_order","Link":"https:\/\/linux.die.net\/man\/1\/g_order"}},{"Process":{"Description":null,"Process Name":"g_pdb2gmx","Link":"https:\/\/linux.die.net\/man\/1\/g_pdb2gmx"}},{"Process":{"Description":"g_pme_error estimates the error of the electrostatic forces if using the sPME algorithm. The flag -tune will determine the splitting parameter such that the error is equally distributed over the real and reciprocal space part. The part of the error that stems from self interaction of the particles is computationally demanding. However, a good a approximation is to just use a fraction of the particles for this term which can be indicated by the flag -self.","Process Name":"g_pme_error","Link":"https:\/\/linux.die.net\/man\/1\/g_pme_error"}},{"Process":{"Description":null,"Process Name":"g_polystat","Link":"https:\/\/linux.die.net\/man\/1\/g_polystat"}},{"Process":{"Description":"g_potential computes the electrostatical potential across the box. The potential is calculated by first summing the charges per slice and then integrating twice of this charge distribution. Periodic boundaries are not taken into account. Reference of potential is taken to be the left side of the box. It is also possible to calculate the potential in spherical coordinates as function of r by calculating a charge distribution in spherical slices and twice integrating them. epsilon_r is taken as 1, but 2 is more appropriate in many cases.","Process Name":"g_potential","Link":"https:\/\/linux.die.net\/man\/1\/g_potential"}},{"Process":{"Description":null,"Process Name":"g_principal","Link":"https:\/\/linux.die.net\/man\/1\/g_principal"}},{"Process":{"Description":null,"Process Name":"g_protonate","Link":"https:\/\/linux.die.net\/man\/1\/g_protonate"}},{"Process":{"Description":"g_rama selects the phi\/psi dihedral combinations from your topology file and computes these as a function of time. Using simple Unix tools such as grep you can select out specific residues.","Process Name":"g_rama","Link":"https:\/\/linux.die.net\/man\/1\/g_rama"}},{"Process":{"Description":"The structure of liquids can be studied by either neutron or X-ray scattering. The most common way to describe liquid structure is by a radial distribution function. However, this is not easy to obtain from a scattering experiment. g_rdf calculates radial distribution functions in different ways. The normal method is around a (set of) particle(s), the other methods are around the center of mass of a set of particles ( -com) or to the closest particle in a set ( -surf). With all methods, the RDF can also be calculated around axes parallel to the z-axis with option -xy. With option -surf normalization can not be used. The option -rdf sets the type of RDF to be computed. Default is for atoms or particles, but one can also select center of mass or geometry of molecules or residues. In all cases, only the atoms in the index groups are taken into account. For molecules and\/or the center of mass option, a run input file is required. Weighting other than COM or COG can currently only be achieved by providing a run input file with different masses. Options -com and -surf also work in conjunction with -rdf. If a run input file is supplied ( -s) and -rdf is set to atom, exclusions defined in that file are taken into account when calculating the RDF. The option -cut is meant as an alternative way to avoid intramolecular peaks in the RDF plot. It is however better to supply a run input file with a higher number of exclusions. For e.g. benzene a topology, setting nrexcl to 5 would eliminate all intramolecular contributions to the RDF. Note that all atoms in the selected groups are used, also the ones that don't have Lennard-Jones interactions. Option -cn produces the cumulative number RDF, i.e. the average number of particles within a distance r. To bridge the gap between theory and experiment structure factors can be computed (option -sq). The algorithm uses FFT, the grid spacing of which is determined by option -grid.","Process Name":"g_rdf","Link":"https:\/\/linux.die.net\/man\/1\/g_rdf"}},{"Process":{"Description":"g_rms compares two structures by computing the root mean square deviation (RMSD), the size-independent rho similarity parameter ( rho) or the scaled rho ( rhosc), see Maiorov & Crippen, Proteins 22, 273 (1995). This is selected by -what. Each structure from a trajectory ( -f) is compared to a reference structure. The reference structure is taken from the structure file ( -s). With option -mir also a comparison with the mirror image of the reference structure is calculated. This is useful as a reference for 'significant' values, see Maiorov & Crippen, Proteins 22, 273 (1995). Option -prev produces the comparison with a previous frame the specified number of frames ago. Option -m produces a matrix in .xpm format of comparison values of each structure in the trajectory with respect to each other structure. This file can be visualized with for instance xv and can be converted to postscript with xpm2ps. Option -fit controls the least-squares fitting of the structures on top of each other: complete fit (rotation and translation), translation only, or no fitting at all. Option -mw controls whether mass weighting is done or not. If you select the option (default) and supply a valid .tpr file masses will be taken from there, otherwise the masses will be deduced from the atommass.dat file in GMXLIB. This is fine for proteins, but not necessarily for other molecules. A default mass of 12.011 amu (carbon) is assigned to unknown atoms. You can check whether this happend by turning on the -debug flag and inspecting the log file. With -f2, the 'other structures' are taken from a second trajectory, this generates a comparison matrix of one trajectory versus the other. Option -bin does a binary dump of the comparison matrix. Option -bm produces a matrix of average bond angle deviations analogously to the -m option. Only bonds between atoms in the comparison group are considered.","Process Name":"g_rms","Link":"https:\/\/linux.die.net\/man\/1\/g_rms"}},{"Process":{"Description":"g_rmsdist computes the root mean square deviation of atom distances, which has the advantage that no fit is needed like in standard RMS deviation as computed by g_rms. The reference structure is taken from the structure file. The RMSD at time t is calculated as the RMS of the differences in distance between atom-pairs in the reference structure and the structure at time t. g_rmsdist can also produce matrices of the rms distances, rms distances scaled with the mean distance and the mean distances and matrices with NMR averaged distances (1\/r3 and 1\/r6 averaging). Finally, lists of atom pairs with 1\/r3 and 1\/r6 averaged distance below the maximum distance ( -max, which will default to 0.6 in this case) can be generated, by default averaging over equivalent hydrogens (all triplets of hydrogens named *[123]). Additionally a list of equivalent atoms can be supplied ( -equiv), each line containing a set of equivalent atoms specified as residue number and name and atom name; e.g.: 3 SER HB1 3 SER HB2 Residue and atom names must exactly match those in the structure file, including case. Specifying non-sequential atoms is undefined.","Process Name":"g_rmsdist","Link":"https:\/\/linux.die.net\/man\/1\/g_rmsdist"}},{"Process":{"Description":null,"Process Name":"g_rmsf","Link":"https:\/\/linux.die.net\/man\/1\/g_rmsf"}},{"Process":{"Description":null,"Process Name":"g_rotacf","Link":"https:\/\/linux.die.net\/man\/1\/g_rotacf"}},{"Process":{"Description":"g_rotmat plots the rotation matrix required for least squares fitting a conformation onto the reference conformation provided with -s. Translation is removed before fitting. The output are the three vectors that give the new directions of the x, y and z directions of the reference conformation, for example: (zx,zy,zz) is the orientation of the reference z-axis in the trajectory frame. This tool is useful for, for instance, determining the orientation of a molecule at an interface, possibly on a trajectory produced with trjconv -fit rotxy+transxy to remove the rotation in the x-y plane. Option -ref determines a reference structure for fitting, instead of using the structure from -s. The structure with the lowest sum of RMSD's to all other structures is used. Since the computational cost of this procedure grows with the square of the number of frames, the -skip option can be useful. A full fit or only a fit in the x-y plane can be performed. Option -fitxy fits in the x-y plane before determining the rotation matrix.","Process Name":"g_rotmat","Link":"https:\/\/linux.die.net\/man\/1\/g_rotmat"}},{"Process":{"Description":null,"Process Name":"g_saltbr","Link":"https:\/\/linux.die.net\/man\/1\/g_saltbr"}},{"Process":{"Description":"g_sas computes hydrophobic, hydrophilic and total solvent accessible surface area. As a side effect, the Connolly surface can be generated as well in a .pdb file where the nodes are represented as atoms and the vertices connecting the nearest nodes as CONECT records. The program will ask for a group for the surface calculation and a group for the output. The calculation group should always consists of all the non-solvent atoms in the system. The output group can be the whole or part of the calculation group. The average and standard deviation of the area over the trajectory can be plotted per residue and atom as well (options -or and -oa). In combination with the latter option an .itp file can be generated (option -i) which can be used to restrain surface atoms. By default, periodic boundary conditions are taken into account, this can be turned off using the -nopbc option. With the -tv option the total volume and density of the molecule can be computed. Please consider whether the normal probe radius is appropriate in this case or whether you would rather use e.g. 0. It is good to keep in mind that the results for volume and density are very approximate. For example, in ice Ih, one can easily fit water molecules in the pores which would yield a volume that is too low, and surface area and density that are both too high.","Process Name":"g_sas","Link":"https:\/\/linux.die.net\/man\/1\/g_sas"}},{"Process":{"Description":"g_select writes out basic data about dynamic selections. It can be used for some simple analyses, or the output can be combined with output from other programs and\/or external analysis programs to calculate more complex things. Any combination of the output options is possible, but note that -om only operates on the first selection. With -os, calculates the number of positions in each selection for each frame. With -norm, the output is between 0 and 1 and describes the fraction from the maximum number of positions (e.g., for selection 'resname RA and x 5' the maximum number of positions is the number of atoms in RA residues). With -cfnorm, the output is divided by the fraction covered by the selection. -norm and -cfnorm can be specified independently of one another. With -oc, the fraction covered by each selection is written out as a function of time. With -oi, the selected atoms\/residues\/molecules are written out as a function of time. In the output, the first column contains the frame time, the second contains the number of positions, followed by the atom\/residue\/molecule numbers. If more than one selection is specified, the size of the second group immediately follows the last number of the first group and so on. With -dump, the frame time and the number of positions is omitted from the output. In this case, only one selection can be given. With -on, the selected atoms are written as a index file compatible with make_ndx and the analyzing tools. Each selection is written as a selection group and for dynamic selections a group is written for each frame. For residue numbers, the output of -oi can be controlled with -resnr: number (default) prints the residue numbers as they appear in the input file, while index prints unique numbers assigned to the residues in the order they appear in the input file, starting with 1. The former is more intuitive, but if the input contains multiple residues with the same number, the output can be less useful. With -om, a mask is printed for the first selection as a function of time. Each line in the output corresponds to one frame, and contains either 0\/1 for each atom\/residue\/molecule possibly selected. 1 stands for the atom\/residue\/molecule being selected for the current frame, 0 for not selected. With -dump, the frame time is omitted from the output.","Process Name":"g_select","Link":"https:\/\/linux.die.net\/man\/1\/g_select"}},{"Process":{"Description":"Compute the angle and distance between two groups. The groups are defined by a number of atoms given in an index file and may be two or three atoms in size. If -one is set, only one group should be specified in the index file and the angle between this group at time 0 and t will be computed. The angles calculated depend on the order in which the atoms are given. Giving, for instance, 5 6 will rotate the vector 5-6 with 180 degrees compared to giving 6 5. If three atoms are given, the normal on the plane spanned by those three atoms will be calculated, using the formula P1P2 x P1P3. The cos of the angle is calculated, using the inproduct of the two normalized vectors. Here is what some of the file options do: -oa: Angle between the two groups specified in the index file. If a group contains three atoms the normal to the plane defined by those three atoms will be used. If a group contains two atoms, the vector defined by those two atoms will be used. -od: Distance between two groups. Distance is taken from the center of one group to the center of the other group. -od1: If one plane and one vector is given, the distances for each of the atoms from the center of the plane is given separately. -od2: For two planes this option has no meaning.","Process Name":"g_sgangle","Link":"https:\/\/linux.die.net\/man\/1\/g_sgangle"}},{"Process":{"Description":"g_sham makes multi-dimensional free-energy, enthalpy and entropy plots. g_sham reads one or more .xvg files and analyzes data sets. The basic purpose of g_sham is to plot Gibbs free energy landscapes (option -ls) by Bolzmann inverting multi-dimensional histograms (option -lp), but it can also make enthalpy (option -lsh) and entropy (option -lss) plots. The histograms can be made for any quantities the user supplies. A line in the input file may start with a time (see option -time) and any number of y-values may follow. Multiple sets can also be read when they are separated by & (option -n), in this case only one y-value is read from each line. All lines starting with and @ are skipped. Option -ge can be used to supply a file with free energies when the ensemble is not a Boltzmann ensemble, but needs to be biased by this free energy. One free energy value is required for each (multi-dimensional) data point in the -f input. Option -ene can be used to supply a file with energies. These energies are used as a weighting function in the single histogram analysis method by Kumar et al. When temperatures are supplied (as a second column in the file), an experimental weighting scheme is applied. In addition the vales are used for making enthalpy and entropy plots. With option -dim, dimensions can be gives for distances. When a distance is 2- or 3-dimensional, the circumference or surface sampled by two particles increases with increasing distance. Depending on what one would like to show, one can choose to correct the histogram and free-energy for this volume effect. The probability is normalized by r and r2 for dimensions of 2 and 3, respectively. A value of -1 is used to indicate an angle in degrees between two vectors: a sin(angle) normalization will be applied. Note that for angles between vectors the inner-product or cosine is the natural quantity to use, as it will produce bins of the same volume.","Process Name":"g_sham","Link":"https:\/\/linux.die.net\/man\/1\/g_sham"}},{"Process":{"Description":"g_sigeps is a simple utility that converts C6\/C12 or C6\/Cn combinations to sigma and epsilon, or vice versa. It can also plot the potential in file. In addition, it makes an approximation of a Buckingham potential to a Lennard-Jones potential.","Process Name":"g_sigeps","Link":"https:\/\/linux.die.net\/man\/1\/g_sigeps"}},{"Process":{"Description":null,"Process Name":"g_sorient","Link":"https:\/\/linux.die.net\/man\/1\/g_sorient"}},{"Process":{"Description":"g_spatial calculates the spatial distribution function and outputs it in a form that can be read by VMD as Gaussian98 cube format. This was developed from template.c (GROMACS-3.3). For a system of 32,000 atoms and a 50 ns trajectory, the SDF can be generated in about 30 minutes, with most of the time dedicated to the two runs through trjconv that are required to center everything properly. This also takes a whole bunch of space (3 copies of the .xtc file). Still, the pictures are pretty and very informative when the fitted selection is properly made. 3-4 atoms in a widely mobile group (like a free amino acid in solution) works well, or select the protein backbone in a stable folded structure to get the SDF of solvent and look at the time-averaged solvation shell. It is also possible using this program to generate the SDF based on some arbitrary Cartesian coordinate. To do that, simply omit the preliminary trjconv steps. USAGE: 1. Use make_ndx to create a group containing the atoms around which you want the SDF 2. trjconv -s a.tpr -f a.xtc -o b.xtc -center tric -ur compact -pbc none 3. trjconv -s a.tpr -f b.xtc -o c.xtc -fit rot+trans 4. run g_spatial on the .xtc output of step 3. 5. Load grid.cube into VMD and view as an isosurface. Note that systems such as micelles will require trjconv -pbc cluster between steps 1 and 2 WARNINGS: The SDF will be generated for a cube that contains all bins that have some non-zero occupancy. However, the preparatory -fit rot+trans option to trjconv implies that your system will be rotating and translating in space (in order that the selected group does not). Therefore the values that are returned will only be valid for some region around your central group\/coordinate that has full overlap with system volume throughout the entire translated\/rotated system over the course of the trajectory. It is up to the user to ensure that this is the case. BUGS: When the allocated memory is not large enough, a segmentation fault may occur. This is usually detected and the program is halted prior to the fault while displaying a warning message suggesting the use of the -nab (Number of Additional Bins) option. However, the program does not detect all such events. If you encounter a segmentation fault, run it again with an increased -nab value. RISKY OPTIONS: To reduce the amount of space and time required, you can output only the coords that are going to be used in the first and subsequent run through trjconv. However, be sure to set the -nab option to a sufficiently high value since memory is allocated for cube bins based on the initial coordinates and the -nab option value.","Process Name":"g_spatial","Link":"https:\/\/linux.die.net\/man\/1\/g_spatial"}},{"Process":{"Description":"g_spol analyzes dipoles around a solute; it is especially useful for polarizable water. A group of reference atoms, or a center of mass reference (option -com) and a group of solvent atoms is required. The program splits the group of solvent atoms into molecules. For each solvent molecule the distance to the closest atom in reference group or to the COM is determined. A cumulative distribution of these distances is plotted. For each distance between -rmin and -rmax the inner product of the distance vector and the dipole of the solvent molecule is determined. For solvent molecules with net charge (ions), the net charge of the ion is subtracted evenly from all atoms in the selection of each ion. The average of these dipole components is printed. The same is done for the polarization, where the average dipole is subtracted from the instantaneous dipole. The magnitude of the average dipole is set with the option -dip, the direction is defined by the vector from the first atom in the selected solvent group to the midpoint between the second and the third atom.","Process Name":"g_spol","Link":"https:\/\/linux.die.net\/man\/1\/g_spol"}},{"Process":{"Description":"g_tcaf computes tranverse current autocorrelations. These are used to estimate the shear viscosity, eta. For details see: Palmer, Phys. Rev. E 49 (1994) pp 359-366. Transverse currents are calculated using the k-vectors (1,0,0) and (2,0,0) each also in the y- and z-direction, (1,1,0) and (1,-1,0) each also in the 2 other planes (these vectors are not independent) and (1,1,1) and the 3 other box diagonals (also not independent). For each k-vector the sine and cosine are used, in combination with the velocity in 2 perpendicular directions. This gives a total of 16*2*2=64 transverse currents. One autocorrelation is calculated fitted for each k-vector, which gives 16 TCAF's. Each of these TCAF's is fitted to f(t) = exp(-v)(cosh(Wv) + 1\/W sinh(Wv)), v = -t\/(2 tau), W = sqrt(1 - 4 tau eta\/rho k2), which gives 16 values of tau and eta. The fit weights decay with time as exp(-t\/wt), and the TCAF and fit are calculated up to time 5*wt. The eta values should be fitted to 1 - a eta(k) k2, from which one can estimate the shear viscosity at k=0. When the box is cubic, one can use the option -oc, which averages the TCAF's over all k-vectors with the same length. This results in more accurate tcaf's. Both the cubic TCAF's and fits are written to -oc The cubic eta estimates are also written to -ov. With option -mol, the transverse current is determined of molecules instead of atoms. In this case, the index group should consist of molecule numbers instead of atom numbers. The k-dependent viscosities in the -ov file should be fitted to eta(k) = eta0 (1 - a k2) to obtain the viscosity at infinite wavelength. Note: make sure you write coordinates and velocities often enough. The initial, non-exponential, part of the autocorrelation function is very important for obtaining a good fit.","Process Name":"g_tcaf","Link":"https:\/\/linux.die.net\/man\/1\/g_tcaf"}},{"Process":{"Description":null,"Process Name":"g_tpbconv","Link":"https:\/\/linux.die.net\/man\/1\/g_tpbconv"}},{"Process":{"Description":"g_traj plots coordinates, velocities, forces and\/or the box. With -com the coordinates, velocities and forces are calculated for the center of mass of each group. When -mol is set, the numbers in the index file are interpreted as molecule numbers and the same procedure as with -com is used for each molecule. Option -ot plots the temperature of each group, provided velocities are present in the trajectory file. No corrections are made for constrained degrees of freedom! This implies -com. Options -ekt and -ekr plot the translational and rotational kinetic energy of each group, provided velocities are present in the trajectory file. This implies -com. Options -cv and -cf write the average velocities and average forces as temperature factors to a .pdb file with the average coordinates or the coordinates at -ctime. The temperature factors are scaled such that the maximum is 10. The scaling can be changed with the option -scale. To get the velocities or forces of one frame set both -b and -e to the time of desired frame. When averaging over frames you might need to use the -nojump option to obtain the correct average coordinates. If you select either of these option the average force and velocity for each atom are written to an .xvg file as well (specified with -av or -af). Option -vd computes a velocity distribution, i.e. the norm of the vector is plotted. In addition in the same graph the kinetic energy distribution is given.","Process Name":"g_traj","Link":"https:\/\/linux.die.net\/man\/1\/g_traj"}},{"Process":{"Description":null,"Process Name":"g_trjcat","Link":"https:\/\/linux.die.net\/man\/1\/g_trjcat"}},{"Process":{"Description":"trjconv can convert trajectory files in many ways: 1. from one format to another 2. select a subset of atoms 3. change the periodicity representation 4. keep multimeric molecules together 5. center atoms in the box 6. fit atoms to reference structure 7. reduce the number of frames 8. change the timestamps of the frames ( -t0 and -timestep) 9. cut the trajectory in small subtrajectories according to information in an index file. This allows subsequent analysis of the subtrajectories that could, for example, be the result of a cluster analysis. Use option -sub. This assumes that the entries in the index file are frame numbers and dumps each group in the index file to a separate trajectory file. 10. select frames within a certain range of a quantity given in an .xvg file. The program trjcat is better suited for concatenating multiple trajectory files. Currently seven formats are supported for input and output: .xtc, .trr, .trj, .gro, .g96, .pdb and .g87. The file formats are detected from the file extension. The precision of .xtc and .gro output is taken from the input file for .xtc, .gro and .pdb, and from the -ndec option for other input formats. The precision is always taken from -ndec, when this option is set. All other formats have fixed precision. .trr and .trj output can be single or double precision, depending on the precision of the trjconv binary. Note that velocities are only supported in .trr, .trj, .gro and .g96 files. Option -app can be used to append output to an existing trajectory file. No checks are performed to ensure integrity of the resulting combined trajectory file. Option -sep can be used to write every frame to a separate .gro, .g96 or .pdb file. By default, all frames all written to one file. .pdb files with all frames concatenated can be viewed with rasmol -nmrpdb. It is possible to select part of your trajectory and write it out to a new trajectory file in order to save disk space, e.g. for leaving out the water from a trajectory of a protein in water. ALWAYS put the original trajectory on tape! We recommend to use the portable .xtc format for your analysis to save disk space and to have portable files. There are two options for fitting the trajectory to a reference either for essential dynamics analysis, etc. The first option is just plain fitting to a reference structure in the structure file. The second option is a progressive fit in which the first timeframe is fitted to the reference structure in the structure file to obtain and each subsequent timeframe is fitted to the previously fitted structure. This way a continuous trajectory is generated, which might not be the case when using the regular fit method, e.g. when your protein undergoes large conformational transitions. Option -pbc sets the type of periodic boundary condition treatment: * mol puts the center of mass of molecules in the box. * res puts the center of mass of residues in the box. * atom puts all the atoms in the box. * nojump checks if atoms jump across the box and then puts them back. This has the effect that all molecules will remain whole (provided they were whole in the initial conformation). Note that this ensures a continuous trajectory but molecules may diffuse out of the box. The starting configuration for this procedure is taken from the structure file, if one is supplied, otherwise it is the first frame. * cluster clusters all the atoms in the selected index such that they are all closest to the center of mass of the cluster, which is iteratively updated. Note that this will only give meaningful results if you in fact have a cluster. Luckily that can be checked afterwards using a trajectory viewer. Note also that if your molecules are broken this will not work either. The separate option -clustercenter can be used to specify an approximate center for the cluster. This is useful e.g. if you have two big vesicles, and you want to maintain their relative positions. * whole only makes broken molecules whole. Option -ur sets the unit cell representation for options mol, res and atom of -pbc. All three options give different results for triclinic boxes and identical results for rectangular boxes. rect is the ordinary brick shape. tric is the triclinic unit cell. compact puts all atoms at the closest distance from the center of the box. This can be useful for visualizing e.g. truncated octahedra. The center for options tric and compact is tric (see below), unless the option -boxcenter is set differently. Option -center centers the system in the box. The user can select the group which is used to determine the geometrical center. Option -boxcenter sets the location of the center of the box for options -pbc and -center. The center options are: tric: half of the sum of the box vectors, rect: half of the box diagonal, zero: zero. Use option -pbc mol in addition to -center when you want all molecules in the box after the centering. With -dt, it is possible to reduce the number of frames in the output. This option relies on the accuracy of the times in your input trajectory, so if these are inaccurate use the -timestep option to modify the time (this can be done simultaneously). For making smooth movies, the program g_filter can reduce the number of frames while using low-pass frequency filtering, this reduces aliasing of high frequency motions. Using -trunc trjconv can truncate .trj in place, i.e. without copying the file. This is useful when a run has crashed during disk I\/O (i.e. full disk), or when two contiguous trajectories must be concatenated without having double frames. Option -dump can be used to extract a frame at or near one specific time from your trajectory. Option -drop reads an .xvg file with times and values. When options -dropunder and\/or -dropover are set, frames with a value below and above the value of the respective options will not be written.","Process Name":"g_trjconv","Link":"https:\/\/linux.die.net\/man\/1\/g_trjconv"}},{"Process":{"Description":"trjorder orders molecules according to the smallest distance to atoms in a reference group or on z-coordinate (with option -z). With distance ordering, it will ask for a group of reference atoms and a group of molecules. For each frame of the trajectory the selected molecules will be reordered according to the shortest distance between atom number -da in the molecule and all the atoms in the reference group. The center of mass of the molecules can be used instead of a reference atom by setting -da to 0. All atoms in the trajectory are written to the output trajectory. trjorder can be useful for e.g. analyzing the n waters closest to a protein. In that case the reference group would be the protein and the group of molecules would consist of all the water atoms. When an index group of the first n waters is made, the ordered trajectory can be used with any Gromacs program to analyze the n closest waters. If the output file is a .pdb file, the distance to the reference target will be stored in the B-factor field in order to color with e.g. Rasmol. With option -nshell the number of molecules within a shell of radius -r around the reference group are printed.","Process Name":"g_trjorder","Link":"https:\/\/linux.die.net\/man\/1\/g_trjorder"}},{"Process":{"Description":"For a given number -np or -nt of processors\/threads, this program systematically times mdrun with various numbers of PME-only nodes and determines which setting is fastest. It will also test whether performance can be enhanced by shifting load from the reciprocal to the real space part of the Ewald sum. Simply pass your .tpr file to g_tune_pme together with other options for mdrun as needed. Which executables are used can be set in the environment variables MPIRUN and MDRUN. If these are not present, 'mpirun' and 'mdrun' will be used as defaults. Note that for certain MPI frameworks you need to provide a machine- or hostfile. This can also be passed via the MPIRUN variable, e.g. export MPIRUN=\"\/usr\/local\/mpirun -machinefile hosts\" Please call g_tune_pme with the normal options you would pass to mdrun and add -np for the number of processors to perform the tests on, or -nt for the number of threads. You can also add -r to repeat each test several times to get better statistics. g_tune_pme can test various real space \/ reciprocal space workloads for you. With -ntpr you control how many extra .tpr files will be written with enlarged cutoffs and smaller fourier grids respectively. Typically, the first test (number 0) will be with the settings from the input .tpr file; the last test (number ntpr) will have cutoffs multiplied by (and at the same time fourier grid dimensions divided by) the scaling factor -fac (default 1.2). The remaining .tpr files will have about equally-spaced values in between these extremes. Note that you can set -ntpr to 1 if you just want to find the optimal number of PME-only nodes; in that case your input .tpr file will remain unchanged. For the benchmark runs, the default of 1000 time steps should suffice for most MD systems. The dynamic load balancing needs about 100 time steps to adapt to local load imbalances, therefore the time step counters are by default reset after 100 steps. For large systems (1M atoms) you may have to set -resetstep to a higher value. From the 'DD' load imbalance entries in the md.log output file you can tell after how many steps the load is sufficiently balanced. Example call: g_tune_pme -np 64 -s protein.tpr -launch After calling mdrun several times, detailed performance information is available in the output file perf.out. Note that during the benchmarks, a couple of temporary files are written (options -b*), these will be automatically deleted after each test. If you want the simulation to be started automatically with the optimized parameters, use the command line option -launch.","Process Name":"g_tune_pme","Link":"https:\/\/linux.die.net\/man\/1\/g_tune_pme"}},{"Process":{"Description":null,"Process Name":"g_vanhove","Link":"https:\/\/linux.die.net\/man\/1\/g_vanhove"}},{"Process":{"Description":"g_velacc computes the velocity autocorrelation function. When the -m option is used, the momentum autocorrelation function is calculated. With option -mol the velocity autocorrelation function of molecules is calculated. In this case the index group should consist of molecule numbers instead of atom numbers.","Process Name":"g_velacc","Link":"https:\/\/linux.die.net\/man\/1\/g_velacc"}},{"Process":{"Description":"This is an analysis program that implements the Weighted Histogram Analysis Method (WHAM). It is intended to analyze output files generated by umbrella sampling simulations to compute a potential of mean force (PMF). At present, three input modes are supported. * With option -it, the user provides a file which contains the file names of the umbrella simulation run-input files ( .tpr files), AND, with option -ix, a file which contains file names of the pullx mdrun output files. The .tpr and pullx files must be in corresponding order, i.e. the first .tpr created the first pullx, etc. * Same as the previous input mode, except that the the user provides the pull force output file names ( pullf.xvg) with option -if. From the pull force the position in the umbrella potential is computed. This does not work with tabulated umbrella potentials. * With option -ip, the user provides file names of (gzipped) .pdo files, i.e. the GROMACS 3.3 umbrella output files. If you have some unusual reaction coordinate you may also generate your own .pdo files and feed them with the -ip option into to g_wham. The .pdo file header must be similar to the following: UMBRELLA 3.0 Component selection: 0 0 1 nSkip 1 Ref. Group 'TestAtom' Nr. of pull groups 2 Group 1 'GR1' Umb. Pos. 5.0 Umb. Cons. 1000.0 Group 2 'GR2' Umb. Pos. 2.0 Umb. Cons. 500.0 The number of pull groups, umbrella positions, force constants, and names may (of course) differ. Following the header, a time column and a data column for each pull group follows (i.e. the displacement with respect to the umbrella center). Up to four pull groups are possible per .pdo file at present. By default, the output files are -o PMF output file -hist Histograms output file Always check whether the histograms sufficiently overlap. The umbrella potential is assumed to be harmonic and the force constants are read from the .tpr or .pdo files. If a non-harmonic umbrella force was applied a tabulated potential can be provided with -tab. WHAM OPTIONS ------------ -bins Number of bins used in analysis -temp Temperature in the simulations -tol Stop iteration if profile (probability) changed less than tolerance -auto Automatic determination of boundaries -min,-max Boundaries of the profile The data points that are used to compute the profile can be restricted with options -b, -e, and -dt. Adjust -b to ensure sufficient equilibration in each umbrella window. With -log (default) the profile is written in energy units, otherwise (with -nolog) as probability. The unit can be specified with -unit. With energy output, the energy in the first bin is defined to be zero. If you want the free energy at a different position to be zero, set -zprof0 (useful with bootstrapping, see below). For cyclic or periodic reaction coordinates (dihedral angle, channel PMF without osmotic gradient), the option -cycl is useful. g_wham will make use of the periodicity of the system and generate a periodic PMF. The first and the last bin of the reaction coordinate will assumed be be neighbors. Option -sym symmetrizes the profile around z=0 before output, which may be useful for, e.g. membranes. AUTOCORRELATIONS ---------------- With -ac, g_wham estimates the integrated autocorrelation time (IACT) tau for each umbrella window and weights the respective window with 1\/[1+2*tau\/dt]. The IACTs are written to the file defined with -oiact. In verbose mode, all autocorrelation functions (ACFs) are written to hist_autocorr.xvg. Because the IACTs can be severely underestimated in case of limited sampling, option -acsig allows to smooth the IACTs along the reaction coordinate with a Gaussian (sigma provided with -acsig, see output in iact.xvg). Note that the IACTs are estimated by simple integration of the ACFs while the ACFs are larger 0.05. If you prefer to compute the IACTs by a more sophisticated (but possibly less robust) method such as fitting to a double exponential, you can compute the IACTs with g_analyze and provide them to g_wham with the file iact-in.dat (option -iiact), which should contain one line per input file ( .pdo or pullx\/f file) and one column per pull group in the respective file. ERROR ANALYSIS -------------- Statistical errors may be estimated with bootstrap analysis. Use it with care, otherwise the statistical error may be substantially underestimated. More background and examples for the bootstrap technique can be found in Hub, de Groot and Van der Spoel, JCTC (2010) 6: 3713-3720. -nBootstrap defines the number of bootstraps (use, e.g., 100). Four bootstrapping methods are supported and selected with -bs-method. (1) b-hist Default: complete histograms are considered as independent data points, and the bootstrap is carried out by assigning random weights to the histograms (\"Bayesian bootstrap\"). Note that each point along the reaction coordinate must be covered by multiple independent histograms (e.g. 10 histograms), otherwise the statistical error is underestimated. (2) hist Complete histograms are considered as independent data points. For each bootstrap, N histograms are randomly chosen from the N given histograms (allowing duplication, i.e. sampling with replacement). To avoid gaps without data along the reaction coordinate blocks of histograms ( -histbs-block) may be defined. In that case, the given histograms are divided into blocks and only histograms within each block are mixed. Note that the histograms within each block must be representative for all possible histograms, otherwise the statistical error is underestimated. (3) traj The given histograms are used to generate new random trajectories, such that the generated data points are distributed according the given histograms and properly autocorrelated. The autocorrelation time (ACT) for each window must be known, so use -ac or provide the ACT with -iiact. If the ACT of all windows are identical (and known), you can also provide them with -bs-tau. Note that this method may severely underestimate the error in case of limited sampling, that is if individual histograms do not represent the complete phase space at the respective positions. (4) traj-gauss The same as method traj, but the trajectories are not bootstrapped from the umbrella histograms but from Gaussians with the average and width of the umbrella histograms. That method yields similar error estimates like method traj. Bootstrapping output: -bsres Average profile and standard deviations -bsprof All bootstrapping profiles With -vbs (verbose bootstrapping), the histograms of each bootstrap are written, and, with bootstrap method traj, the cumulative distribution functions of the histograms.","Process Name":"g_wham","Link":"https:\/\/linux.die.net\/man\/1\/g_wham"}},{"Process":{"Description":"g_wheel plots a helical wheel representation of your sequence. The input sequence is in the .dat file where the first line contains the number of residues and each consecutive line contains a residue name.","Process Name":"g_wheel","Link":"https:\/\/linux.die.net\/man\/1\/g_wheel"}},{"Process":{"Description":"g_x2top generates a primitive topology from a coordinate file. The program assumes all hydrogens are present when defining the hybridization from the atom name and the number of bonds. The program can also make an .rtp entry, which you can then add to the .rtp database. When -param is set, equilibrium distances and angles and force constants will be printed in the topology for all interactions. The equilibrium distances and angles are taken from the input coordinates, the force constant are set with command line options. The force fields somewhat supported currently are: G53a5 GROMOS96 53a5 Forcefield (official distribution) oplsaa OPLS-AA\/L all-atom force field (2001 aminoacid dihedrals) The corresponding data files can be found in the library directory with name atomname2type.n2t. Check Chapter 5 of the manual for more information about file formats. By default, the force field selection is interactive, but you can use the -ff option to specify one of the short names above on the command line instead. In that case g_x2top just looks for the corresponding file.","Process Name":"g_x2top","Link":"https:\/\/linux.die.net\/man\/1\/g_x2top"}},{"Process":{"Description":"xpm2ps makes a beautiful color plot of an XPixelMap file. Labels and axis can be displayed, when they are supplied in the correct matrix format. Matrix data may be generated by programs such as do_dssp, g_rms or g_mdmat. Parameters are set in the .m2p file optionally supplied with -di. Reasonable defaults are provided. Settings for the y-axis default to those for the x-axis. Font names have a defaulting hierarchy: titlefont - legendfont; titlefont - (xfont - yfont - ytickfont) - xtickfont, e.g. setting titlefont sets all fonts, setting xfont sets yfont, ytickfont and xtickfont. When no .m2p file is supplied, many settings are taken from command line options. The most important option is -size, which sets the size of the whole matrix in postscript units. This option can be overridden with the -bx and -by options (and the corresponding parameters in the .m2p file), which set the size of a single matrix element. With -f2 a second matrix file can be supplied. Both matrix files will be read simultaneously and the upper left half of the first one ( -f) is plotted together with the lower right half of the second one ( -f2). The diagonal will contain values from the matrix file selected with -diag. Plotting of the diagonal values can be suppressed altogether by setting -diag to none. In this case, a new color map will be generated with a red gradient for negative numbers and a blue for positive. If the color coding and legend labels of both matrices are identical, only one legend will be displayed, else two separate legends are displayed. With -combine, an alternative operation can be selected to combine the matrices. The output range is automatically set to the actual range of the combined matrix. This can be overridden with -cmin and -cmax. -title can be set to none to suppress the title, or to ylabel to show the title in the Y-label position (alongside the y-axis). With the -rainbow option, dull grayscale matrices can be turned into attractive color pictures. Merged or rainbowed matrices can be written to an XPixelMap file with the -xpm option.","Process Name":"g_xpm2ps","Link":"https:\/\/linux.die.net\/man\/1\/g_xpm2ps"}},{"Process":{"Description":"g_xrama shows a Ramachandran movie, that is, it shows the Phi\/Psi angles as a function of time in an X-Window. Static Phi\/Psi plots for printing can be made with g_rama. Some of the more common X command line options can be used: -bg, -fg change colors, -font fontname, changes the font.","Process Name":"g_xrama","Link":"https:\/\/linux.die.net\/man\/1\/g_xrama"}},{"Process":{"Description":null,"Process Name":"gacutil","Link":"https:\/\/linux.die.net\/man\/1\/gacutil"}},{"Process":{"Description":"Gajim is a jabber client written in PyGTK and released under the GNU GPL. For more information on jabber, see http:\/\/www.jabber.org and on Gajim see http:\/\/www.gajim.org","Process Name":"gajim","Link":"https:\/\/linux.die.net\/man\/1\/gajim"}},{"Process":{"Description":"Gajim-history-manager is a tool to manage (do some cleanup) log file of Gajim jabber client.","Process Name":"gajim-history-manager","Link":"https:\/\/linux.die.net\/man\/1\/gajim-history-manager"}},{"Process":{"Description":null,"Process Name":"gajim-remote","Link":"https:\/\/linux.die.net\/man\/1\/gajim-remote"}},{"Process":{"Description":"The galaxy program draws spinning galaxies.","Process Name":"galaxy","Link":"https:\/\/linux.die.net\/man\/1\/galaxy"}},{"Process":{"Description":null,"Process Name":"galculator","Link":"https:\/\/linux.die.net\/man\/1\/galculator"}},{"Process":{"Description":"This program is a tool for Nokia and other mobile phones. Parameters, which allow to control debug level: nothing - no debug level text - transmission dump in text format textall - all possible info in text format errors - errors in text format binary - transmission dump in binary format Please put them between Gammu name and \"real\" OPTIONS.","Process Name":"gammu","Link":"https:\/\/linux.die.net\/man\/1\/gammu"}},{"Process":{"Description":"gamt provides access to the serial-over-lan port of Intel AMT managed machines. host is the hostname or IP address of the machine gamt should connect to. For more inforamtions on Intel AMT check amt-howto(7).","Process Name":"gamt","Link":"https:\/\/linux.die.net\/man\/1\/gamt"}},{"Process":{"Description":"gate is the static version of the program transforming a file produced by glade(1), the Gtk GUI builder, into an Ada program. It will generate a set of Ada files that, when compiled, will recreate the interface you just designed in glade(1). The most important file created by Gate is called callback_ <project_name> .adb and contains stubs for all the callbacks you declared in Glade. Note that you can easily go back to Glade any time, modify your interface, and have gate regenerate a set of files. All your modifications will be kept in the new files. For that, gate creates a directory .gate in the current directory. Please do not delete it if you want gate to be able to keep your changes from one version to the next. Also note that to be able to keep track of your modifications, gate relies on patch and diff being available on your system. If you don't have a working set of diff\/patch, configure will simply replace them by null operations.","Process Name":"gate","Link":"https:\/\/linux.die.net\/man\/1\/gate"}},{"Process":{"Description":"This utility script aggregates profiling logs generated using Python's hotshot profiler. The sole command-line argument is the full path to the directory containing the profiling logfiles.","Process Name":"gather_profile_stats","Link":"https:\/\/linux.die.net\/man\/1\/gather_profile_stats"}},{"Process":{"Description":null,"Process Name":"gawk","Link":"https:\/\/linux.die.net\/man\/1\/gawk"}},{"Process":{"Description":"gc is a graph analogue to wc in that it prints to standard output the number of nodes, edges, connected components or clusters contained in the input files. It also prints a total count for all graphs if more than one graph is given.","Process Name":"gc","Link":"https:\/\/linux.die.net\/man\/1\/gc"}},{"Process":{"Description":null,"Process Name":"gcalctool","Link":"https:\/\/linux.die.net\/man\/1\/gcalctool"}},{"Process":{"Description":null,"Process Name":"gcaps","Link":"https:\/\/linux.die.net\/man\/1\/gcaps"}},{"Process":{"Description":"When you invoke GCC , it normally does preprocessing, compilation, assembly and linking. The \"overall options\" allow you to stop this process at an intermediate stage. For example, the -c option says not to run the linker. Then the output consists of object files output by the assembler. Other options are passed on to one stage of processing. Some options control the preprocessor and others the compiler itself. Yet other options control the assembler and linker; most of these are not documented here, since you rarely need to use any of them. Most of the command line options that you can use with GCC are useful for C programs; when an option is only useful with another language (usually C ++ ), the explanation says so explicitly. If the description for a particular option does not mention a source language, you can use that option with all supported languages. The gcc program accepts options and file names as operands. Many options have multi-letter names; therefore multiple single-letter options may not be grouped: -dv is very different from -d -v. You can mix options and other arguments. For the most part, the order you use doesn't matter. Order does matter when you use several options of the same kind; for example, if you specify -L more than once, the directories are searched in the order specified. Also, the placement of the -l option is significant. Many options have long names starting with -f or with -W---for example, -fmove-loop-invariants, -Wformat and so on. Most of these have both positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"gcc","Link":"https:\/\/linux.die.net\/man\/1\/gcc"}},{"Process":{"Description":"The gccmakedep program calls 'gcc -M' to output makefile rules describing the dependencies of each sourcefile, so that make(1) knows which object files must be recompiled when a dependency has changed. By default, gccmakedep places its output in the file named makefile if it exists, otherwise Makefile. An alternate makefile may be specified with the -f option. It first searches the makefile for a line beginning with # DO NOT DELETE or one provided with the -s option, as a delimiter for the dependency output. If it finds it, it will delete everything following this up to the end of the makefile and put the output after this line. If it doesn't find it, the program will append the string to the makefile and place the output after that.","Process Name":"gccmakedep","Link":"https:\/\/linux.die.net\/man\/1\/gccmakedep"}},{"Process":{"Description":"GCC-XML parses a C++ source file as it is seen by the compiler when it is built. An easy-to-parse XML representation of the class, function, and namespace declarations is dumped to a specified file. Full C preprocessing transforms the file into a C++ translation unit as seen by the compiler. This means that GCC-XML should make use of the same standard library and other header files as the compiler. GCC-XML can be configured to simulate any of several popular compilers.","Process Name":"gccxml","Link":"https:\/\/linux.die.net\/man\/1\/gccxml"}},{"Process":{"Description":"gcdmaster allows the creation of toc-files for cdrdao and can control the recording process. Its main application is the composition of audio CDs from one or more audio files. It supports PQ-channel editing, entry of meta data like ISRC codes\/CD-TEXT and non destructive cut of the audio data. If a toc-file is specified it will be read and the referenced audio data will be displayed. It is also possible to specify a \".cue\" file. The GUI periodically polls all configured CD-ROM and CD-recorder devices (see section DEVICE CONFIGURE DIALOG) to retrieve their status (ready, busy, no disk). This is done by sending a TEST UNIT READY command to the devices. Following problems may arise: o Some devices (e.g. the Philips CDD2600) block the SCSI bus when a TEST UNIT READY is issued while it logs in a new medium. This will cause a buffer under run for all currently recording devices that are connected to the same bus. o The GUI cannot detect if a device is used by another program. It will continue to poll the device which may disturb the operation of the other program. The GUI supports recording of the same or different projects on multiple devices in parallel. However, there are some caveats and your system must meat some prerequisites: o Under Linux a kernel version >= 2.2.6 should be used. The generic SCSI device of older kernels does not support parallel access to multiple devices. o The bandwidths of the disks that contain the source data and the involved busses must be big enough to serve all recorder devices. o Some recorder devices may block the SCSI bus when the disk is ejected by the software after the burning process (e.g. the Plextor PX-R412). For this reason a warning message will be displayed if the \"Eject\" button is checked in the \"Record\" dialog. Manually ejecting a disk seems not to be a problem. o ATAPI devices supported by the 'generic-mmc' driver will block the IDE bus while writing the lead-in and lead-out. Thus parallel writing with such devices connected to the same IDE channel will not work. There is a way to avoid this blocking with some ATAPI devices but it is not implemented, yet.","Process Name":"gcdmaster","Link":"https:\/\/linux.die.net\/man\/1\/gcdmaster"}},{"Process":{"Description":"gcin is an input method(IM) server, which focused mainly on Traditional Chinese. It is also very useful for Simplified Chinese, Japanese, and many other languages. IM table format of gcin is almost as same as those of scim and xcin. Users may append their IMs to gcin very easily. The \"g\" of gcin means that it features a better GTK+ user interface. If you are looking for configuration tool of gcin, please see the manpage of gcin-setup.","Process Name":"gcin","Link":"https:\/\/linux.die.net\/man\/1\/gcin"}},{"Process":{"Description":"gcin-gb-toggle toggles gcin's output between traditional Chinese and simplified Chinese.","Process Name":"gcin-gb-toggle","Link":"https:\/\/linux.die.net\/man\/1\/gcin-gb-toggle"}},{"Process":{"Description":"gcin-kbm-toggle toggles gcin's virtual keyboard.","Process Name":"gcin-kbm-toggle","Link":"https:\/\/linux.die.net\/man\/1\/gcin-kbm-toggle"}},{"Process":{"Description":"gcin-message displays notification image and\/or text. It is useful for filters to have interaction with users.","Process Name":"gcin-message","Link":"https:\/\/linux.die.net\/man\/1\/gcin-message"}},{"Process":{"Description":null,"Process Name":"gcin-setup","Link":"https:\/\/linux.die.net\/man\/1\/gcin-setup"}},{"Process":{"Description":"gcin2tab converts plain text input method table to gcin's .gtab binary format.","Process Name":"gcin2tab","Link":"https:\/\/linux.die.net\/man\/1\/gcin2tab"}},{"Process":{"Description":null,"Process Name":"gcipher","Link":"https:\/\/linux.die.net\/man\/1\/gcipher"}},{"Process":{"Description":"As gcj is just another front end to gcc, it supports many of the same options as gcc. This manual only documents the options specific to gcj.","Process Name":"gcj","Link":"https:\/\/linux.die.net\/man\/1\/gcj"}},{"Process":{"Description":"\"gcj-dbtool\" is a tool for creating and manipulating class file mapping databases. \"libgcj\" can use these databases to find a shared library corresponding to the bytecode representation of a class. This functionality is useful for ahead-of-time compilation of a program that has no knowledge of \"gcj\". \"gcj-dbtool\" works best if all the jar files added to it are compiled using \"-findirect-dispatch\". Note that \"gcj-dbtool\" is currently available as \"preview technology\". We believe it is a reasonable way to allow application-transparent ahead-of-time compilation, but this is an unexplored area. We welcome your comments.","Process Name":"gcj-dbtool","Link":"https:\/\/linux.die.net\/man\/1\/gcj-dbtool"}},{"Process":{"Description":"The \"gcjh\" program is used to generate header files from class files. It can generate both CNI and JNI header files, as well as stub implementation files which can be used as a basis for implementing the required native methods. It is similar to \"javah\" but has slightly different command line options, and defaults to CNI .","Process Name":"gcjh","Link":"https:\/\/linux.die.net\/man\/1\/gcjh"}},{"Process":{"Description":"gclose terminates an application from the command line","Process Name":"gclose","Link":"https:\/\/linux.die.net\/man\/1\/gclose"}},{"Process":{"Description":null,"Process Name":"gcloseall","Link":"https:\/\/linux.die.net\/man\/1\/gcloseall"}},{"Process":{"Description":"GConf-Editor is a tool used for editing the GConf configuration database. It might be useful when the proper configuration utility for some software provides no way of changing some option.","Process Name":"gconf-editor","Link":"https:\/\/linux.die.net\/man\/1\/gconf-editor"}},{"Process":{"Description":"Each preference in the GConf repository is expressed as a key-value pair. A GConf preference key is an element in the GConf repository that corresponds to an application prefer-ence. Preference keys typically have simple values such as strings, integers, or lists of strings and integers. The default key-value pairs are installed on the system, and are known as schemas. gconftool-2 is the command-line interface tool that enables you to set the values of keys, display the values of keys, and install schemas from schema definition files when you install an application. The GConf preference keys are stored and managed by the GConf daemon (gconfd-2). These keys are cached in memory, and saved to disk in XML format when appropriate.","Process Name":"gconftool-2","Link":"https:\/\/linux.die.net\/man\/1\/gconftool-2"}},{"Process":{"Description":"gcov is a test coverage program. Use it in concert with GCC to analyze your programs to help create more efficient, faster running code and to discover untested parts of your program. You can use gcov as a profiling tool to help discover where your optimization efforts will best affect your code. You can also use gcov along with the other profiling tool, gprof, to assess which parts of your code use the greatest amount of computing time. Profiling tools help you analyze your code's performance. Using a profiler such as gcov or gprof, you can find out some basic performance statistics, such as: \u2022 how often each line of code executes \u2022 what lines of code are actually executed \u2022 how much computing time each section of code uses Once you know these things about how your code works when compiled, you can look at each module to see which modules should be optimized. gcov helps you determine where to work on optimization. Software developers also use coverage testing in concert with testsuites, to make sure software is actually good enough for a release. Testsuites can verify that a program works as expected; a coverage program tests to see how much of the program is exercised by the testsuite. Developers can then determine what kinds of test cases need to be added to the testsuites to create both better testing and a better final product. You should compile your code without optimization if you plan to use gcov because the optimization, by combining some lines of code into one function, may not give you as much information as you need to look for 'hot spots' where the code is using a great deal of computer time. Likewise, because gcov accumulates statistics by line (at the lowest resolution), it works best with a programming style that places only one statement on each line. If you use complicated macros that expand to loops or to other control structures, the statistics are less helpful---they only report on the line where the macro call appears. If your complex macros behave like functions, you can replace them with inline functions to solve this problem. gcov creates a logfile called sourcefile.gcov which indicates how many times each line of a source file sourcefile.c has executed. You can use these logfiles along with gprof to aid in fine-tuning the performance of your programs. gprof gives timing information you can use along with the information you get from gcov. gcov works only on code compiled with GCC . It is not compatible with any other profiling or test coverage mechanism.","Process Name":"gcov","Link":"https:\/\/linux.die.net\/man\/1\/gcov"}},{"Process":{"Description":"Convert gcov files to Devel::Cover databases.","Process Name":"gcov2perl","Link":"https:\/\/linux.die.net\/man\/1\/gcov2perl"}},{"Process":{"Description":null,"Process Name":"gctags","Link":"https:\/\/linux.die.net\/man\/1\/gctags"}},{"Process":{"Description":"This utility script (available on Unix systems) can be used to determine various information about a GDAL installation. It is normally just used by configure scripts for applications using GDAL but can be queried by an end user. --prefix: the top level directory for the GDAL installation. --libs: The libraries and link directives required to use GDAL. --cflags: The include and macro definition required to compiled modules using GDAL. --version: Reports the GDAL version. --ogr-enabled: Reports 'yes' or 'no' to standard output depending on whether OGR is built into GDAL. --formats: Reports which formats are configured into GDAL to stdout. Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"gdal-config","Link":"https:\/\/linux.die.net\/man\/1\/gdal-config"}},{"Process":{"Description":"This utility generates a directory with small tiles and metadata, following OSGeo Tile Map Service Specification. Simple web pages with viewers based on Google Maps and OpenLayers are generated as well - so anybody can comfortably explore your maps on-line and you do not need to install or configure any special software (like mapserver) and the map displays very fast in the webbrowser. You only need to upload generated directory into a web server. GDAL2Tiles creates also necessary metadata for Google Earth (KML SuperOverlay), in case the supplied map uses EPSG:4326 projection. World files and embedded georeference is used during tile generation, but you can publish a picture without proper georeference too. -title 'Title' : Title used for generated metadata, web viewers and KML files. -publishurl http:\/\/yourserver\/dir\/ : Address of a directory into which you are going to upload the result. It should end with slash. -nogooglemaps: Do not generate Google Maps based html page. -noopenlayers: Do not generate OpenLayers based html page. -nokml: Do not generate KML files for Google Earth. -googlemapskey KEY : Key for your domain generated on Google Maps API web page ( http:\/\/www.google.com\/apis\/maps\/signup.html). -forcekml Force generating of KML files. Input file must use EPSG:4326 coordinates! -v Generate verbose output of tile generation. NOTE: gdal2tiles.py is a Python script that needs to be run against 'new generation' Python GDAL binding.","Process Name":"gdal2tiles","Link":"https:\/\/linux.die.net\/man\/1\/gdal2tiles"}},{"Process":{"Description":"This program generates a vector contour file from the input raster elevation model (DEM). Starting from version 1.7 the contour line-strings will be oriented consistently. The high side will be on the right, i.e. a line string goes clockwise around a top. -b band : picks a particular band to get the DEM from. Defaults to band 1. -a name : provides a name for the attribute in which to put the elevation. If not provided no elevation attribute is attached. -3d: Force production of 3D vectors instead of 2D. Includes elevation at every vertex. -inodata: Ignore any nodata value implied in the dataset - treat all values as valid. -snodata value : Input pixel value to treat as 'nodata'. -f format : create output in a particular format, default is shapefiles. -i interval : elevation interval between contours. -off offset : Offset from zero relative to which to interpret intervals. -fl level : Name one or more 'fixed levels' to extract. -nln outlayername : Provide a name for the output vector layer. Defaults to 'contour'.","Process Name":"gdal_contour","Link":"https:\/\/linux.die.net\/man\/1\/gdal_contour"}},{"Process":{"Description":"The gdal_nodatafill.py script fills selection regions (usually nodata areas) by interpolating from valid pixels around the edges of the area. Additional details on the algorithm are available in the GDALFillNodata() docs. -q: The script runs in quiet mode. The progress monitor is supressed and routine messages are not displayed. -md max_distance : The maximum distance (in pixels) that the algorithm will search out for values to interpolate. -si smooth_iterations : The number of 3x3 average filter smoothing iterations to run after the interpolation to dampen artifacts. The default is zero smoothing iterations. -o name=value : Specify a special argument to the algorithm. Currently none are supported. -b band : The band to operate on, by default the first band is operated on. srcfile The source raster file used to identify target pixels. Only one band is used. -nomask: Do not use the default validity mask for the input band (such as nodata, or alpha masks). -mask filename : Use the first band of the specified file as a validity mask (zero is invalid, non-zero is valid). dstfile The new file to create with the interpolated result. If not provided, the source band is updated in place. -of format : Select the output format. The default is GeoTIFF (GTiff). Use the short format name.","Process Name":"gdal_fillnodata","Link":"https:\/\/linux.die.net\/man\/1\/gdal_fillnodata"}},{"Process":{"Description":"This program creates regular grid (raster) from the scattered data read from the OGR datasource. Input data will be interpolated to fill grid nodes with values, you can choose from various interpolation methods. -ot type : For the output bands to be of the indicated data type. -of format : Select the output format. The default is GeoTIFF (GTiff). Use the short format name. -txe xmin xmax : Set georeferenced X extents of output file to be created. -tye ymin ymax : Set georeferenced Y extents of output file to be created. -outsize xsize ysize : Set the size of the output file in pixels and lines. -a_srs srs_def : Override the projection for the output file. The srs_def may be any of the usual GDAL\/OGR forms, complete WKT, PROJ.4, EPSG:n or a file containing the WKT. -zfield field_name : Identifies an attribute field on the features to be used to get a Z value from. This value overrides Z value read from feature geometry record (naturally, if you have a Z value in geometry, otherwise you have no choice and should specify a field name containing Z value). -a [algorithm[:parameter1=value1][:parameter2=value2]...] : Set the interpolation algorithm or data metric name and (optionally) its parameters. See INTERPOLATION ALGORITHMS and DATA METRICS sections for further discussion of available options. -spat xmin ymin xmax ymax : Adds a spatial filter to select only features contained within the bounding box described by (xmin, ymin) - (xmax, ymax). -clipsrc [xmin ymin xmax ymax]|WKT|datasource|spat_extent : Adds a spatial filter to select only features contained within the specified bounding box (expressed in source SRS), WKT geometry (POLYGON or MULTIPOLYGON), from a datasource or to the spatial extent of the -spat option if you use the spat_extent keyword. When specifying a datasource, you will generally want to use it in combination of the -clipsrclayer, -clipsrcwhere or -clipsrcsql options. -clipsrcsql sql_statement : Select desired geometries using an SQL query instead. -clipsrclayer layername : Select the named layer from the source clip datasource. -clipsrcwhere expression : Restrict desired geometries based on attribute query. -l layername : Indicates the layer(s) from the datasource that will be used for input features. May be specified multiple times, but at least one layer name or a -sql option must be specified. -where expression : An optional SQL WHERE style query expression to be applied to select features to process from the input layer(s). -sql select_statement : An SQL statement to be evaluated against the datasource to produce a virtual layer of features to be processed. -co 'NAME=VALUE' : Passes a creation option to the output format driver. Multiple -co options may be listed. See format specific documentation for legal creation options for each format. -q: Suppress progress monitor and other non-error output. src_datasource : Any OGR supported readable datasource. dst_filename : The GDAL supported output file.","Process Name":"gdal_grid","Link":"https:\/\/linux.die.net\/man\/1\/gdal_grid"}},{"Process":{"Description":"This utility will automatically mosaic a set of images. All the images must be in the same coordinate system and have a matching number of bands, but they may be overlapping, and at different resolutions. In areas of overlap, the last image will be copied over earlier ones. -o out_filename : The name of the output file, which will be created if it does not already exist (defaults to 'out.tif'). -of format : Output format, defaults to GeoTIFF (GTiff). -co NAME=VALUE : Creation option for output file. Multiple options can be specified. -ot datatype : Force the output image bands to have a specific type. Use type names (ie. Byte, Int16,...) -ps pixelsize_x pixelsize_y : Pixel size to be used for the output file. If not specified the resolution of the first input file will be used. -ul_lr ulx uly lrx lry : The extents of the output file. If not specified the aggregate extents of all input files will be used. -v: Generate verbose output of mosaicing operations as they are done. -separate: Place each input file into a separate stacked band. -pct: Grab a pseudocolor table from the first input image, and use it for the output. Merging pseudocolored images this way assumes that all input files use the same color table. -n nodata_value : Ignore pixels from files being merged in with this pixel value. -init value : Pre-initialize the output image bands with these values. However, it is not marked as the nodata value in the output file. If only one value is given, the same value is used in all the bands. -createonly: The output file is created (and potentially pre-initialized) but no input image data is copied into it. NOTE: gdal_merge.py is a Python script, and will only work if GDAL was built with Python support.","Process Name":"gdal_merge","Link":"https:\/\/linux.die.net\/man\/1\/gdal_merge"}},{"Process":{"Description":"This program burns vector geometries (points, lines and polygons) into the raster band(s) of a raster image. Vectors are read from OGR supported vector formats. Note that the vector data must in the same coordinate system as the raster data, on the fly reprojection is not provided. -b band : The band(s) to burn values into. Multiple -b arguments may be used to burn into a list of bands. The default is to burn into band 1. -i: Invert rasterization. Burn the fixed burn value, or the burn value associated with the first feature into all parts of the image not inside the provided a polygon. -at: Enables the ALL_TOUCHED rasterization option so that all pixels touched by lines or polygons will be updated not just those one the line render path, or whose center point is within the polygon. Defaults to disabled for normal rendering rules. -burn value : A fixed value to burn into a band for all objects. A list of -burn options can be supplied, one per band being written to. -a attribute_name : Identifies an attribute field on the features to be used for a burn in value. The value will be burned into all output bands. -3d: Indicates that a burn value should be extracted from the 'Z' values of the feature. These values are adjusted by the burn value given by '-burn value' or '-a attribute_name' if provided. As of now, only points and lines are drawn in 3D. -l layername : Indicates the layer(s) from the datasource that will be used for input features. May be specified multiple times, but at least one layer name or a -sql option must be specified. -where expression : An optional SQL WHERE style query expression to be applied to select features to burn in from the input layer(s). -sql select_statement : An SQL statement to be evaluated against the datasource to produce a virtual layer of features to be burned in. src_datasource : Any OGR supported readable datasource. dst_filename : The GDAL supported output file. Must support update mode access. Currently gdal_rasterize cannot create new output files though that may be added eventually.","Process Name":"gdal_rasterize","Link":"https:\/\/linux.die.net\/man\/1\/gdal_rasterize"}},{"Process":{"Description":null,"Process Name":"gdal_retile","Link":"https:\/\/linux.die.net\/man\/1\/gdal_retile"}},{"Process":{"Description":"The gdal_sieve.py script removes raster polygons smaller than a provided threshold size (in pixels) and replaces replaces them with the pixel value of the largest neighbour polygon. The result can be written back to the existing raster band, or copied into a new file. Additional details on the algorithm are available in the GDALSieveFilter() docs. -q: The script runs in quiet mode. The progress monitor is supressed and routine messages are not displayed. -st threshold : Set the size threshold in pixels. Only raster polygons smaller than this size will be removed. -o name=value : Specify a special argument to the algorithm. Currently none are supported. -4: Four connectedness should be used when determining polygons. That is diagonal pixels are not considered directly connected. This is the default. -8: Eight connectedness should be used when determining polygons. That is diagonal pixels are considered directly connected. srcfile The source raster file used to identify target pixels. Only the first band is used. -nomask: Do not use the default validity mask for the input band (such as nodata, or alpha masks). -mask filename : Use the first band of the specified file as a validity mask (zero is invalid, non-zero is valid). dstfile The new file to create with the filtered result. If not provided, the source band is updated in place. -of format : Select the output format. The default is GeoTIFF (GTiff). Use the short format name.","Process Name":"gdal_sieve","Link":"https:\/\/linux.die.net\/man\/1\/gdal_sieve"}},{"Process":{"Description":"The gdal_translate utility can be used to convert raster data between different formats, potentially performing some operations like subsettings, resampling, and rescaling pixels in the process. -ot: type For the output bands to be of the indicated data type. -strict: Do'nt be forgiving of mismatches and lost data when translating to the output format. -of format : Select the output format. The default is GeoTIFF (GTiff). Use the short format name. -b band : Select an input band band for output. Bands are numbered from 1 Multiple -b switches may be used to select a set of input bands to write to the output file, or to reorder bands. -expand gray|rgb|rgba : (From GDAL 1.6.0) To expose a dataset with 1 band with a color table as a dataset with 3 (RGB) or 4 (RGBA) bands. Usefull for output drivers such as JPEG, JPEG2000, MrSID, ECW that don't support color indexed datasets. The 'gray' value (from GDAL 1.7.0) enables to expand a dataset with a color table that only contains gray levels to a gray indexed dataset. -outsize xsize[%] ysize[%] : Set the size of the output file. Outsize is in pixels and lines unless '' is attached in which case it is as a fraction of the input image size. -scale [src_min src_max [dst_min dst_max]] : Rescale the input pixels values from the range src_min to src_max to the range dst_min to dst_max. If omitted the output range is 0 to 255. If omitted the input range is automatically computed from the source data. -unscale: Apply the scale\/offset metadata for the bands to convert scaled values to unscaled values. It is also often necessary to reset the output datatype with the -ot switch. -srcwin xoff yoff xsize ysize : Selects a subwindow from the source image for copying based on pixel\/line location. -projwin ulx uly lrx lry : Selects a subwindow from the source image for copying (like -srcwin) but with the corners given in georeferenced coordinates. -a_srs srs_def : Override the projection for the output file. The srs_def may be any of the usual GDAL\/OGR forms, complete WKT, PROJ.4, EPSG:n or a file containing the WKT. -a_ullr ulx uly lrx lry : Assign\/override the georeferenced bounds of the output file. This assigns georeferenced bounds to the output file, ignoring what would have been derived from the source file. -a_nodata value : Assign a specified nodata value to output bands. -mo 'META-TAG=VALUE' : Passes a metadata key and value to set on the output dataset if possible. -co 'NAME=VALUE' : Passes a creation option to the output format driver. Multiple -co options may be listed. See format specific documentation for legal creation options for each format. -gcp pixel line easting northing elevation : Add the indicated ground control point to the output dataset. This option may be provided multiple times to provide a set of GCPs. -q: Suppress progress monitor and other non-error output. -sds: Copy all subdatasets of this file to individual output files. Use with formats like HDF or OGDI that have subdatasets. src_dataset : The source dataset name. It can be either file name, URL of data source or subdataset name for multi-dataset files. dst_dataset : The destination file name.","Process Name":"gdal_translate","Link":"https:\/\/linux.die.net\/man\/1\/gdal_translate"}},{"Process":{"Description":"","Process Name":"gdal_utilities","Link":"https:\/\/linux.die.net\/man\/1\/gdal_utilities"}},{"Process":{"Description":"The gdaladdo utility can be used to build or rebuild overview images for most supported file formats with one over several downsampling algorithms. -r {nearest (default),average,gauss,cubic,average_mp,average_magphase,mode} : Select a resampling algorithm. -ro: (available from GDAL 1.6.0) open the dataset in read-only mode, in order to generate external overview (for GeoTIFF especially). -clean: (available from GDAL 1.7.0) remove all overviews. filename : The file to build overviews for (or whose overviews must be removed). levels : A list of integral overview levels to build. Ignored with -clean option. Mode (available from GDAL 1.6.0) selects the value which appears most often of all the sampled points. average_mp is unsuitable for use. Average_magphase averages complex data in mag\/phase space. Nearest and average are applicable to normal image data. Nearest applies a nearest neighbour (simple sampling) resampler, while average computes the average of all non-NODATA contributing pixels. Cubic resampling (available from GDAL 1.7.0) applies a 4x4 approximate cubic convolution kernel. Gauss resampling (available from GDAL 1.6.0) applies a Gaussian kernel before computing the overview, which can lead to better results than simple averaging in e.g case of sharp edges with high contrast or noisy patterns. The advised level values should be 2, 4, 8, ... so that a 3x3 resampling Gaussian kernel is selected. gdaladdo will honour properly NODATA_VALUES tuples (special dataset metadata) so that only a given RGB triplet (in case of a RGB image) will be considered as the nodata value and not each value of the triplet independantly per band. Selecting a level value like 2 causes an overview level that is 1\/2 the resolution (in each dimension) of the base layer to be computed. If the file has existing overview levels at a level selected, those levels will be recomputed and rewritten in place. Some format drivers do not support overviews at all. Many format drivers store overviews in a secondary file with the extension .ovr that is actually in TIFF format. By default, the GeoTIFF driver stores overviews internally to the file operated on (if it is writable), unless the -ro flag is specified. External overviews created in TIFF format may be compressed using the COMPRESS_OVERVIEW configuration option. All compression methods, supported by the GeoTIFF driver, available here. (eg --config COMPRESS_OVERVIEW DEFLATE). The photometric interpretation can be set with --config PHOTOMETRIC_OVERVIEW {RGB,YCBCR,...}, and the interleaving with --config INTERLEAVE_OVERVIEW {PIXEL|BAND}. To produce the smallest possible JPEG-In-TIFF overviews, you should use : --config COMPRESS_OVERVIEW JPEG --config PHOTOMETRIC_OVERVIEW YCBCR --config INTERLEAVE_OVERVIEW PIXEL Starting with GDAL 1.7.0, external overviews can be created in the BigTIFF format by using the BIGTIFF_OVERVIEW configuration option : --config BIGTIFF_OVERVIEW {IF_NEEDED|IF_SAFER|YES|NO}. The default value is IF_NEEDED. The behaviour of this option is exactly the same as the BIGTIFF creation option documented in the GeoTIFF driver documentation. \u2022 YES forces BigTIFF. \u2022 NO forces classic TIFF. \u2022 IF_NEEDED will only create a BigTIFF if it is clearly needed (uncompressed, and overviews larger than 4GB). \u2022 IF_SAFER will create BigTIFF if the resulting file *might* exceed 4GB. Most drivers also support an alternate overview format using Erdas Imagine format. To trigger this use the USE_RRD=YES configuration option. This will place the overviews in an associated .aux file suitable for direct use with Imagine or ArcGIS as well as GDAL applications. (eg --config USE_RRD YES)","Process Name":"gdaladdo","Link":"https:\/\/linux.die.net\/man\/1\/gdaladdo"}},{"Process":{"Description":"This program builds a VRT (Virtual Dataset) that is a mosaic of the list of input gdal datasets. The list of input gdal datasets can be specified at the end of the command line, or put in a text file (one filename per line) for very long lists, or it can be a MapServer tileindex (see gdaltindex utility). In the later case, all entries in the tile index will be added to the VRT. With -separate, each files goes into a separate stacked band in the VRT band. Otherwise, the files are considered as tiles of a larger mosaic and the VRT file has as many bands as one of the input files. If one GDAL dataset is made of several subdatasets and has 0 raster bands, all the subdatasets will be added to the VRT rather than the dataset itself. gdalbuildvrt does some amount of checks to assure that all files that will be put in the resulting VRT have similar characteristics : number of bands, projection, color interpretation... If not, files that do not match the common characteristics will be skipped. (This is only true in the default mode, and not when using the -separate option) If there is some amount of spatial overlapping between files, the order may depend on the order they are inserted in the VRT file, but this behaviour should not be relied on. This utility is somehow equivalent to the gdal_vrtmerge.py utility and is build by default in GDAL 1.6.1. -tileindex: Use the specified value as the tile index field, instead of the default value with is 'location'. -resolution {highest|lowest|average|user}: In case the resolution of all input files is not the same, the -resolution flag enables the user to control the way the output resolution is computed. 'average' is the default. 'highest' will pick the smallest values of pixel dimensions within the set of source rasters. 'lowest' will pick the largest values of pixel dimensions within the set of source rasters. 'average' will compute an average of pixel dimensions within the set of source rasters. 'user' is new in GDAL 1.7.0 and must be used in combination with the -tr option to specify the target resolution. -tr xres yres : (starting with GDAL 1.7.0) set target resolution. The values must be expressed in georeferenced units. Both must be positive values. Specifying those values is of curse incompatible with highest|lowest|average values for -resolution option. -te xmin ymin xmax ymax : (starting with GDAL 1.7.0) set georeferenced extents of VRT file. The values must be expressed in georeferenced units. If not specified, the extent of the VRT is the minimum bounding box of the set of source rasters. -addalpha: (starting with GDAL 1.7.0) Adds an alpha mask band to the VRT when the source raster have none. Mainly useful for RGB sources (or grey-level sources). The alpha band is filled on-the-fly with the value 0 in areas without any source raster, and with value 255 in areas with source raster. The effect is that a RGBA viewer will render the areas without source rasters as transparent and areas with source rasters as opaque. This option is not compatible with -separate. -hidenodata: (starting with GDAL 1.7.0) Even if any band contains nodata value, giving this option makes the VRT band not report the NoData. Useful when you want to control the background color of the dataset. By using along with the -addalpha option, you can prepare a dataset which doesn't report nodata value but is transparent in areas with no data. -srcnodata value [value...] : (starting with GDAL 1.7.0) Set nodata values for input bands (different values can be supplied for each band). If more than one value is supplied all values should be quoted to keep them together as a single operating system argument. If the option is not specified, the instrinsic nodata settings on the source datasets will be used (if they exist). The value set by this option is written in the NODATA element of each ComplexSource element. Use a value of None to ignore intrinsic nodata settings on the source datasets. -vrtnodata value [value...] : (starting with GDAL 1.7.0) Set nodata values at the VRT band level (different values can be supplied for each band). If more than one value is supplied all values should be quoted to keep them together as a single operating system argument. If the option is not specified, instrinsic nodata settings on the first dataset will be used (if they exist). The value set by this option is written in the NoDataValue element of each VRTRasterBand element. Use a value of None to ignore intrinsic nodata settings on the source datasets. -separate: (starting with GDAL 1.7.0) Place each input file into a separate stacked band. In that case, only the first band of each dataset will be placed into a new band. Contrary to the default mode, it is not required that all bands have the same datatype. -allow_projection_difference: (starting with GDAL 1.7.0) When this option is specified, the utility will accept to make a VRT even if the input datasets have not the same projection. Note: this does not mean that they will be reprojected. Their projection will just be ignored. -input_file_list: To specify a text file with an input filename on each line -q: (starting with GDAL 1.7.0) To disable the progress bar on the console","Process Name":"gdalbuildvrt","Link":"https:\/\/linux.die.net\/man\/1\/gdalbuildvrt"}},{"Process":{"Description":"This utility has 7 different modes : hillshade to generate a shaded relief map from any GDAL-supported elevation raster slope to generate a slope map from any GDAL-supported elevation raster aspect to generate an aspect map from any GDAL-supported elevation raster color-relief to generate a color relief map from any GDAL-supported elevation raster TRI to generate a map of Terrain Ruggedness Index from any GDAL-supported elevation raster TPI to generate a map of Topographic Position Index from any GDAL-supported elevation raster roughness to generate a map of roughness from any GDAL-supported elevation raster The following general options are available : input_dem : The input DEM raster to be processed output_xxx_map : The output raster produced -of format : Select the output format. The default is GeoTIFF (GTiff). Use the short format name. -b band : Select an input band to be processed. Bands are numbered from 1. -co 'NAME=VALUE' : Passes a creation option to the output format driver. Multiple -co options may be listed. See format specific documentation for legal creation options for each format. -q: Suppress progress monitor and other non-error output.","Process Name":"gdaldem","Link":"https:\/\/linux.die.net\/man\/1\/gdaldem"}},{"Process":{"Description":"The gdalinfo program lists various information about a GDAL supported raster dataset. -mm Force computation of the actual min\/max values for each band in the dataset. -stats Read and display image statistics. Force computation if no statistics are stored in an image. -nogcp Suppress ground control points list printing. It may be useful for datasets with huge amount of GCPs, such as L1B AVHRR or HDF4 MODIS which contain thousands of the ones. -nomd Suppress metadata printing. Some datasets may contain a lot of metadata strings. -noct Suppress printing of color table. -checksum Force computation of the checksum for each band in the dataset. -mdd domain Report metadata for the specified domain The gdalinfo will report all of the following (if known): \u2022 The format driver used to access the file. \u2022 Raster size (in pixels and lines). \u2022 The coordinate system for the file (in OGC WKT). \u2022 The geotransform associated with the file (rotational coefficients are currently not reported). \u2022 Corner coordinates in georeferenced, and if possible lat\/long based on the full geotransform (but not GCPs). \u2022 Ground control points. \u2022 File wide (including subdatasets) metadata. \u2022 Band data types. \u2022 Band color interpretations. \u2022 Band block size. \u2022 Band descriptions. \u2022 Band min\/max values (internally known and possibly computed). \u2022 Band checksum (if computation asked). \u2022 Band NODATA value. \u2022 Band overview resolutions available. \u2022 Band unit type (i.e.. 'meters' or 'feet' for elevation bands). \u2022 Band pseudo-color tables.","Process Name":"gdalinfo","Link":"https:\/\/linux.die.net\/man\/1\/gdalinfo"}},{"Process":{"Description":"This program builds a shapefile with a record for each input raster file, an attribute containing the filename, and a polygon geometry outlining the raster. This output is suitable for use with MapServer as a raster tileindex. \u2022 The shapefile (index_file) will be created if it doesn't already exist, otherwise it will append to the existing file. \u2022 The default tile index field is 'location'. \u2022 Raster filenames will be put in the file exactly as they are specified on the commandline unless the option -write_absolute_path is used. \u2022 If -skip_different_projection is specified, only files with same projection ref as files already inserted in the tileindex will be inserted. \u2022 Simple rectangular polygons are generated in the same coordinate system as the rasters.","Process Name":"gdaltindex","Link":"https:\/\/linux.die.net\/man\/1\/gdaltindex"}},{"Process":{"Description":"The gdaltransform utility reprojects a list of coordinates into any supported projection,including GCP-based transformations. -s_srs srs def : source spatial reference set. The coordinate systems that can be passed are anything supported by the OGRSpatialReference.SetFromUserInput() call, which includes EPSG PCS and GCSes (ie. EPSG:4296), PROJ.4 declarations (as above), or the name of a .prf file containing well known text. -t_srs srs_def : target spatial reference set. The coordinate systems that can be passed are anything supported by the OGRSpatialReference.SetFromUserInput() call, which includes EPSG PCS and GCSes (ie. EPSG:4296), PROJ.4 declarations (as above), or the name of a .prf file containing well known text. -to NAME=VALUE : set a transformer option suitable to pass to GDALCreateGenImgProjTransformer2(). -order n : order of polynomial used for warping (1 to 3). The default is to select a polynomial order based on the number of GCPs. -tps: Force use of thin plate spline transformer based on available GCPs. -rpc: Force use of RPCs. -geoloc: Force use of Geolocation Arrays. -i Inverse transformation: from destination to source. -gcp pixel line easting northing [elevation] : Provide a GCP to be used for transformation (generally three or more are required) srcfile : File with source projection definition or GCP's. If not given, source projection is read from the command-line -s_srs or -gcp parameters dstfile : File with destination projection definition. Coordinates are read as pairs (or triples) of numbers per line from standard input, transformed, and written out to standard output in the same way. All transformations offered by gdalwarp are handled, including gcp-based ones. Note that input and output must always be in decimal form. There is currently no support for DMS input or output. If an input image file is provided, input is in pixel\/line coordinates on that image. If an output file is provided, output is in pixel\/line coordinates on that image.","Process Name":"gdaltransform","Link":"https:\/\/linux.die.net\/man\/1\/gdaltransform"}},{"Process":{"Description":"The gdalwarp utility is an image mosaicing, reprojection and warping utility. The program can reproject to any supported projection, and can also apply GCPs stored with the image if the image is 'raw' with control information. -s_srs srs def : source spatial reference set. The coordinate systems that can be passed are anything supported by the OGRSpatialReference.SetFromUserInput() call, which includes EPSG PCS and GCSes (ie. EPSG:4296), PROJ.4 declarations (as above), or the name of a .prf file containing well known text. -t_srs srs_def : target spatial reference set. The coordinate systems that can be passed are anything supported by the OGRSpatialReference.SetFromUserInput() call, which includes EPSG PCS and GCSes (ie. EPSG:4296), PROJ.4 declarations (as above), or the name of a .prf file containing well known text. -to NAME=VALUE : set a transformer option suitable to pass to GDALCreateGenImgProjTransformer2(). -order n : order of polynomial used for warping (1 to 3). The default is to select a polynomial order based on the number of GCPs. -tps: Force use of thin plate spline transformer based on available GCPs. -rpc: Force use of RPCs. -geoloc: Force use of Geolocation Arrays. -et err_threshold : error threshold for transformation approximation (in pixel units - defaults to 0.125). -te xmin ymin xmax ymax : set georeferenced extents of output file to be created (in target SRS). -tr xres yres : set output file resolution (in target georeferenced units) -ts width height : set output file size in pixels and lines. If width or height is set to 0, the other dimension will be guessed from the computed resolution. Note that -ts cannot be used with -tr -wo 'NAME=VALUE' : Set a warp options. The GDALWarpOptions::papszWarpOptions docs show all options. Multiple -wo options may be listed. -ot type : For the output bands to be of the indicated data type. -wt type : Working pixel data type. The data type of pixels in the source image and destination image buffers. -r resampling_method : Resampling method to use. Available methods are: near: nearest neighbour resampling (default, fastest algorithm, worst interpolation quality). bilinear: bilinear resampling. cubic: cubic resampling. cubicspline: cubic spline resampling. lanczos: Lanczos windowed sinc resampling. -srcnodata value [value...] : Set nodata masking values for input bands (different values can be supplied for each band). If more than one value is supplied all values should be quoted to keep them together as a single operating system argument. Masked values will not be used in interpolation. Use a value of None to ignore intrinsic nodata settings on the source dataset. -dstnodata value [value...] : Set nodata values for output bands (different values can be supplied for each band). If more than one value is supplied all values should be quoted to keep them together as a single operating system argument. New files will be initialized to this value and if possible the nodata value will be recorded in the output file. -dstalpha: Create an output alpha band to identify nodata (unset\/transparent) pixels. -wm memory_in_mb : Set the amount of memory (in megabytes) that the warp API is allowed to use for caching. -multi: Use multithreaded warping implementation. Multiple threads will be used to process chunks of image and perform input\/output operation simultaneously. -q: Be quiet. -of format : Select the output format. The default is GeoTIFF (GTiff). Use the short format name. -co 'NAME=VALUE' : passes a creation option to the output format driver. Multiple -co options may be listed. See format specific documentation for legal creation options for each format. -cutline datasource : Enable use of a blend cutline from the name OGR support datasource. -cl layername : Select the named layer from the cutline datasource. -cwhere expression : Restrict desired cutline features based on attribute query. -csql query : Select cutline features using an SQL query instead of from a layer with -cl. -cblend distance : Set a blend distance to use to blend over cutlines (in pixels). srcfile : The source file name(s). dstfile : The destination file name. Mosaicing into an existing output file is supported if the output file already exists. The spatial extent of the existing file will not be modified to accomodate new data, so you may have to remove it in that case. Polygon cutlines may be used to restrict the the area of the destination file that may be updated, including blending. Cutline features must be in the georeferenced units of the destination file.","Process Name":"gdalwarp","Link":"https:\/\/linux.die.net\/man\/1\/gdalwarp"}},{"Process":{"Description":"The purpose of a debugger such as GDB is to allow you to see what is going on ''inside'' another program while it executes-or what another program was doing at the moment it crashed. GDB can do four main kinds of things (plus other things in support of these) to help you catch bugs in the act: \u2022 Start your program, specifying anything that might affect its behavior. \u2022 Make your program stop on specified conditions. \u2022 Examine what has happened, when your program has stopped. \u2022 Change things in your program, so you can experiment with correcting the effects of one bug and go on to learn about another. You can use GDB to debug programs written in C, C++, and Modula-2. Fortran support will be added when a GNU Fortran compiler is ready. GDB is invoked with the shell command gdb. Once started, it reads commands from the terminal until you tell it to exit with the GDB command quit. You can get online help from gdb itself by using the command help. You can run gdb with no arguments or options; but the most usual way to start GDB is with one argument or two, specifying an executable program as the argument: gdb program You can also start with both an executable program and a core file specified: gdb program core You can, instead, specify a process ID as a second argument, if you want to debug a running process: gdb program 1234 would attach GDB to process 1234 (unless you also have a file named '1234'; GDB does check for a core file first). Here are some of the most frequently needed GDB commands: break [ file :] function Set a breakpoint at function (in file). run [ arglist] Start your program (with arglist, if specified). bt Backtrace: display the program stack. print expr Display the value of an expression. c Continue running your program (after stopping, e.g. at a breakpoint). next Execute next program line (after stopping); step over any function calls in the line. edit [ file :] function look at the program line where it is presently stopped. list [ file :] function type the text of the program in the vicinity of where it is presently stopped. step Execute next program line (after stopping); step into any function calls in the line. help [ name] Show information about GDB command name, or general information about using GDB. quit Exit from GDB. For full details on GDB, see Using GDB: A Guide to the GNU Source-Level Debugger, by Richard M. Stallman and Roland H. Pesch. The same text is available online as the gdb entry in the info program.","Process Name":"gdb","Link":"https:\/\/linux.die.net\/man\/1\/gdb"}},{"Process":{"Description":null,"Process Name":"gdbserver","Link":"https:\/\/linux.die.net\/man\/1\/gdbserver"}},{"Process":{"Description":"The purpose of a debugger such as GDB is to allow you to see what is going on ''inside'' another program while it executes-or what another program was doing at the moment it crashed. GDB can do four main kinds of things (plus other things in support of these) to help you catch bugs in the act: \u2022 Start your program, specifying anything that might affect its behavior. \u2022 Make your program stop on specified conditions. \u2022 Examine what has happened, when your program has stopped. \u2022 Change things in your program, so you can experiment with correcting the effects of one bug and go on to learn about another. You can use GDB to debug programs written in C, C++, and Modula-2. Fortran support will be added when a GNU Fortran compiler is ready. GDB is invoked with the shell command gdb. Once started, it reads commands from the terminal until you tell it to exit with the GDB command quit. You can get online help from gdb itself by using the command help. You can run gdb with no arguments or options; but the most usual way to start GDB is with one argument or two, specifying an executable program as the argument: gdb program You can also start with both an executable program and a core file specified: gdb program core You can, instead, specify a process ID as a second argument, if you want to debug a running process: gdb program 1234 would attach GDB to process 1234 (unless you also have a file named '1234'; GDB does check for a core file first). Here are some of the most frequently needed GDB commands: break [ file :] function Set a breakpoint at function (in file). run [ arglist] Start your program (with arglist, if specified). bt Backtrace: display the program stack. print expr Display the value of an expression. c Continue running your program (after stopping, e.g. at a breakpoint). next Execute next program line (after stopping); step over any function calls in the line. edit [ file :] function look at the program line where it is presently stopped. list [ file :] function type the text of the program in the vicinity of where it is presently stopped. step Execute next program line (after stopping); step into any function calls in the line. help [ name] Show information about GDB command name, or general information about using GDB. quit Exit from GDB. For full details on GDB, see Using GDB: A Guide to the GNU Source-Level Debugger, by Richard M. Stallman and Roland H. Pesch. The same text is available online as the gdb entry in the info program.","Process Name":"gdbtui","Link":"https:\/\/linux.die.net\/man\/1\/gdbtui"}},{"Process":{"Description":null,"Process Name":"gdc","Link":"https:\/\/linux.die.net\/man\/1\/gdc"}},{"Process":{"Description":"gdk-pixbuf-csource is a small utility that generates C code containing images, useful for compiling images directly into programs.","Process Name":"gdk-pixbuf-csource","Link":"https:\/\/linux.die.net\/man\/1\/gdk-pixbuf-csource"}},{"Process":{"Description":null,"Process Name":"gdk-pixbuf-query-loaders","Link":"https:\/\/linux.die.net\/man\/1\/gdk-pixbuf-query-loaders"}},{"Process":{"Description":"gdl is an incremental compiler for the GNU Data Language (GDL). GDL is being developed with the aim of providing an open-source drop-in replacement for ITTVIS Interactive Data Language (IDL). It is also partially compatible with Visual Numerics (VN) PV-WAVE. IDL and PV-WAVE are registered trademarks of ITTVIS and VN, respectively. GDL and its library routines are designed as a tool for numerical data analysis and visualisation. GDL is dynamically typed, vectorized and has object-oriented programming capabilities. The library routines handle numerical calculations, data visualisation, signal\/image processing, file input\/output (incl. graphical and scientific data formats such as TIFF, PNG, netCDF, HDF etc) and interaction with host OS. Information on GDL\/IDL\/PV-WAVE syntax and library routines can be found in e.g.: - GDL documentation draft at: http:\/\/gnudatalanguage.sf.net\/gdl.pdf - IDL manual on-line @ NASA.gov: http:\/\/idlastro.gsfc.nasa.gov\/idl_html_help\/home.html - ITTVIS (ittvis.com) and Visual Numerics (vni.com) websites - comp.lang.idl-pvwave newsgroup A vast part of GDL library routines is implemented using open-source libraries including GSL, plPlot, and optionally: readline, FFTW, ImageMagick, netCDF, HDF4, HDF5, libproj4, UDUNITS-2, libps and others.","Process Name":"gdl","Link":"https:\/\/linux.die.net\/man\/1\/gdl"}},{"Process":{"Description":"GDM is a replacement for XDM, the X Display Manager. Unlike its competitors (X3DM, KDM, WDM) GDM was written from scratch and does not contain any original XDM \/ X Consortium code. GDM runs and manages the X servers for both local and remote logins (using XDMCP). See http:\/\/www.gnome.org\/projects\/gdm\/ for more details. gdm is just a script that runs the actual gdm-binary executable. gdm-stop is a script that stops the current running daemon immediately, gdm-restart restarts the current daemon immediately and gdm-safe-restart restarts the current daemon after everyone has logged out. gdmsetup is a graphical tool for easily changing the most commonly used options. For full documentation see the GNOME help browser under the GNOME \/ System section.","Process Name":"gdm","Link":"https:\/\/linux.die.net\/man\/1\/gdm"}},{"Process":{"Description":"The gdnc daemon is used by GNUstep programs to send notifications and messages to one another. By default it uses private inter-process communications accessible only to the curtrent user on the machine on which it is running (where the operating system supports them). Every user needs to have his own instance of gdnc running. While gdnc will be started automatically as soon as it is needed, it is recommended to start gdnc in a personal login script like ~\/.bashrc or ~\/.cshrc. Alternatively (if you have no command-line tools which use distributed notifications) you can launch gdnc when your windowing system or the window manager is started. For example, on systems with X11 you can launch gdnc from your .xinitrc script or alternatively - if you are running Window Maker - put it in Window Maker's autostart script. See the GNUstep Build Guide for a sample startup script.","Process Name":"gdnc","Link":"https:\/\/linux.die.net\/man\/1\/gdnc"}},{"Process":{"Description":"Geany is a small and fast editor with basic features of an integrated development environment. Some of its features: syntax highlighting, code completion, code folding, symbol\/tag lists and many supported filetypes like C(++), Java, PHP, HTML, DocBook, Perl and more. Homepage: http:\/\/www.geany.org\/","Process Name":"geany","Link":"https:\/\/linux.die.net\/man\/1\/geany"}},{"Process":{"Description":"This tool allows you to run both Gearman clients and workers from the command line. Client mode is the default, with worker mode triggered with the -w option. More documentation on this tool coming soon!","Process Name":"gearman","Link":"https:\/\/linux.die.net\/man\/1\/gearman"}},{"Process":{"Description":"This draws sets of turning, interlocking gears, rotating in three dimensions.","Process Name":"gears","Link":"https:\/\/linux.die.net\/man\/1\/gears"}},{"Process":{"Description":null,"Process Name":"gedit","Link":"https:\/\/linux.die.net\/man\/1\/gedit"}},{"Process":{"Description":"","Process Name":"geekdec","Link":"https:\/\/linux.die.net\/man\/1\/geekdec"}},{"Process":{"Description":"","Process Name":"geekgen","Link":"https:\/\/linux.die.net\/man\/1\/geekgen"}},{"Process":{"Description":"This program is part of Netpbm(1). gemtopbm was replaced in Netpbm 9.1 (May 2000) by gemtopnm(1). gemtopnm is backward compatible with gemtopbm, but works on color images as well.","Process Name":"gemtopbm","Link":"https:\/\/linux.die.net\/man\/1\/gemtopbm"}},{"Process":{"Description":null,"Process Name":"gemtopnm","Link":"https:\/\/linux.die.net\/man\/1\/gemtopnm"}},{"Process":{"Description":"gen-cam-menu is a shell script which creates Enlightenment user menu entries for the bookmarks stored by feh-cam(1) in ~\/.enlightenment\/.","Process Name":"gen-cam-menu","Link":"https:\/\/linux.die.net\/man\/1\/gen-cam-menu"}},{"Process":{"Description":null,"Process Name":"genau","Link":"https:\/\/linux.die.net\/man\/1\/genau"}},{"Process":{"Description":"genbackupdata generates test data sets for performance testing of backup software. It creates a directory tree filled with files of different sizes. The total size and the distribution of sizes between small and big are configurable. The program can also modify an existing directory tree by creating new files, and deleting, renaming, or modifying existing files. This can be used to generate test data for successive generations of backups. The program is deterministic: with a given set of parameters (and a given pre-existing directory tree), it always creates the same output. This way, it is possible to reproduce backup tests exactly, without having to distribute the potentially very large test sets. The data set consists of plain files and directories. Files are either small text files or big binary files. Text files contain the \"lorem ipsum\" stanza, binary files contain randomly generated byte streams. The percentage of file data that is small text or big binary files can be set, as can the sizes of the respective file types. Files and directories are named \"fileXXXX\" or \"dirXXXX\", where \"XXXX\" is a successive integer, separate successions for files and directories. There is an upper limit to how many files a directory may contain. After the file limit is reached, a new sub-directory is created. The first set of files go into the root directory of the test set. You have to give one of the options --create, --delete, --rename, or --modify for the program to do anything. You can, however, give more than one of them, if DIR already exists. (Giving the same option more than once means that only the last instance is counted.) (DIR) is created if it doesn't exist already.","Process Name":"genbackupdata","Link":"https:\/\/linux.die.net\/man\/1\/genbackupdata"}},{"Process":{"Description":null,"Process Name":"genbrk","Link":"https:\/\/linux.die.net\/man\/1\/genbrk"}},{"Process":{"Description":"gencases is used to generate test 802.11b packets that are encrypted with weak initialization vectors as described by Fluhrer, Mantin, and Shamir (FMS). The packets are saved to a user specified file in libpcap compatible format for later use with the airsnort(1) program. The intent is to provide airsnort with the packets required to demonstrate the FMS attack. Arguments to gencases allow the user to specify the WEP key that will be used to encrypt the packets as well as the number of packets to be generated for each byte of the wep key. The generated packets represent UDP echo requests from host 192.168.0.2 (on a wired net) to host 192.168.0.10 (on a wireless net) and the corresponding replies. The wireless client has MAC 00:02:2d:01:23:45, the wired host has MAC 00:04:5A:AB:CD:EF, and the access point has BSSID 00:02:2D:98:76:54. The generated packet file can be examined with standard packet analysis tools such as tcpdump or ethereal. It can also be loaded into airsnort(1) which, given that a sufficient number of cases were generated for each key byte, will produce a cracked WEP key. Running the generated file through the decrypt(1) utility will produce the decrypted versions of the packets which should reflect the echo request\/reply traffic described above, and which can be verified using tcpdump or ethereal.","Process Name":"gencases","Link":"https:\/\/linux.die.net\/man\/1\/gencases"}},{"Process":{"Description":null,"Process Name":"gencat","Link":"https:\/\/linux.die.net\/man\/1\/gencat"}},{"Process":{"Description":"Creates headers with C macros and structures for Prima core module object definitions.","Process Name":"gencls","Link":"https:\/\/linux.die.net\/man\/1\/gencls"}},{"Process":{"Description":null,"Process Name":"gencnval","Link":"https:\/\/linux.die.net\/man\/1\/gencnval"}},{"Process":{"Description":"genctd reads the word list from dictionary-file and creates a compact trie dictionary file. Normally this data file has the .ctd extension. Words begin at the beginning of a line and are terminated by the first whitespace. Lines that begin with whitespace are ignored.","Process Name":"genctd","Link":"https:\/\/linux.die.net\/man\/1\/genctd"}},{"Process":{"Description":"Convert plain text test case descriptions into a format as understood by genhtml. inputfile needs to observe the following format: For each test case: - one line containing the test case name beginning at the start of the line - one or more lines containing the test case description indented with at least one whitespace character (tab or space) Example input file: test01 An example test case description. Description continued test42 Supposedly the answer to most of your questions Note: valid test names can consist of letters, decimal digits and the underscore character ('_').","Process Name":"gendesc","Link":"https:\/\/linux.die.net\/man\/1\/gendesc"}},{"Process":{"Description":null,"Process Name":"gendiff","Link":"https:\/\/linux.die.net\/man\/1\/gendiff"}},{"Process":{"Description":"The gendsa command generates a DSA private key from a DSA parameter file (which will be typically generated by the openssl dsaparam command).","Process Name":"gendsa","Link":"https:\/\/linux.die.net\/man\/1\/gendsa"}},{"Process":{"Description":"Produces a file comprised of random integers in network-byte-order from the rand(3) library call.","Process Name":"generate_randfile","Link":"https:\/\/linux.die.net\/man\/1\/generate_randfile"}},{"Process":{"Description":null,"Process Name":"genericseq.pl","Link":"https:\/\/linux.die.net\/man\/1\/genericseq.pl"}},{"Process":{"Description":"GNU gengetopt 2.22.5 This program generates a C function that uses getopt_long function to parse the command line options, validate them and fill a struct. -h, --help Print help and exit --detailed-help Print help, including all details and hidden options, and exit -V, --version Print version and exit Main options: -i, --input= filename input file (default std input) -f, --func-name= name name of generated function (default='cmdline_parser') -a, --arg-struct-name= name name of generated args info struct (default='gengetopt_args_info') -F, --file-name= name name of generated file (default='cmdline') --output-dir= path output directory if this option is not specified, the files are generated in the current directory. --header-output-dir= path header output directory --src-output-dir= path source output directory -c, --c-extension= ext extension of c file (default='c') -H, --header-extension= ext extension of header file (default='h') -l, --long-help long usage line in help The usage line will print all the options, e.g., sample1 -iINT|--int-opt=INT [-h|--help] --default-optional by default, an option is considered optional if not specified otherwise -u, --unamed-opts[= STRING] accept options without names (e.g., file names) (default='FILES') The parser generated is thought to be used to parse the command line arguments. However, you can also generate parsers for configuration files, or strings that contain the arguments to parse, by using the following two options. -C, --conf-parser generate a config file parser -S, --string-parser generate a string parser (the string contains the command line) Additional options: -G, --include-getopt adds the code for getopt_long in the generated C file -n, --no-handle-help do not handle --help|-h automatically If --no-handle-help is specified, the command line option --help|-h will not be handled automatically, so the programmer will be able to print some other information; then the function for printing the standard help output can be used; this function is called <parser-name>_print_help. Notice that, although the programmer can handle --help|-h manually, the parser will return after finding such option: the other command line options, if any, will be ignored. In case you want to have full control on --help|-h, you should use --ho-help. --no-help do not add --help|-h automatically With this option you can disable the automatic addition of options --help|-h. The programmer will then be able to add this option in the input file and handle it as he sees fit. Notice that --no-help will also disable the automatic options --detailed-help and --full-help. The programmer can still define options with short character h as he wants, but he cannot define options help, unless he specifies --no-help (otherwise an error will be printed). -N, --no-handle-version do not handle --version|-V automatically --no-version do not add --version|-V automatically See above the details about --no-handle-help and --no-help, respectively. -e, --no-handle-error do not exit on errors With this option, if the generated parser encounters an error (e.g., an unknown option) it does not make the main program exit; instead, the parser function returns a value different 0, and the main program can print a help message. --show-required[= STRING] in the output of help will specify which options are mandatory, by using the optional passed string (default='(mandatory)') -g, --gen-version put gengetopt version in the generated file (default=on) --set-package= STRING set the package name (override package defined in the .ggo file) --set-version= STRING set the version number (override version defined in the .ggo file) --show-help show the output of --help instead of generating code --show-full-help show the output of --full-help (i.e., including hidden options) instead of generating code --show-detailed-help show the output of --detailed-help (i.e., including details and hidden options) instead of generating code --show-version show the output of --version instead of generating code Please refer to the info manual for further explanations. Maintained by Lorenzo Bettini <http:\/\/www.lorenzobettini.it>","Process Name":"gengetopt","Link":"https:\/\/linux.die.net\/man\/1\/gengetopt"}},{"Process":{"Description":null,"Process Name":"genhash","Link":"https:\/\/linux.die.net\/man\/1\/genhash"}},{"Process":{"Description":"genhostid generates a random hostid and stores it in \/etc\/hostid, if \/etc\/hostid does not already exist.","Process Name":"genhostid","Link":"https:\/\/linux.die.net\/man\/1\/genhostid"}},{"Process":{"Description":null,"Process Name":"genhtml","Link":"https:\/\/linux.die.net\/man\/1\/genhtml"}},{"Process":{"Description":"geninfo converts all GCOV coverage data files found in directory into tracefiles, which the genhtml tool can convert to HTML output. Unless the --output-filename option is specified, geninfo writes its output to one file per .da file, the name of which is generated by simply appending \".info\" to the respective .da file name. Note that the current user needs write access to both directory as well as to the original source code location. This is necessary because some temporary files have to be created there during the conversion process. Note also that geninfo is called from within lcov, so that there is usually no need to call it directly.","Process Name":"geninfo","Link":"https:\/\/linux.die.net\/man\/1\/geninfo"}},{"Process":{"Description":"genior generates a stringified object reference from the arguments supplied to it. If an object key argument isn't supplied, it will use an object key generated by omniORB2.","Process Name":"genior","Link":"https:\/\/linux.die.net\/man\/1\/genior"}},{"Process":{"Description":"genisoimage is a pre-mastering program to generate ISO9660\/Joliet\/HFS hybrid filesystems. genisoimage is capable of generating the System Use Sharing Protocol records (SUSP) specified by the Rock Ridge Interchange Protocol. This is used to further describe the files in the ISO9660 filesystem to a Unix host, and provides information such as long filenames, UID\/GID, POSIX permissions, symbolic links, and block and character device files. If Joliet or HFS hybrid command line options are specified, genisoimage will create the additional filesystem metadata needed for Joliet or HFS. Otherwise genisoimage will generate a pure ISO9660 filesystem. genisoimage can generate a true (or shared) HFS hybrid filesystem. The same files are seen as HFS files when accessed from a Macintosh and as ISO9660 files when accessed from other machines. HFS stands for Hierarchical File System and is the native filesystem used on Macintosh computers. As an alternative, genisoimage can generate the Apple Extensions to ISO9660 for each file. These extensions provide each file with CREATOR, TYPE and certain Finder flags when accessed from a Macintosh. See the HFS MACINTOSH FILE FORMATS section below. genisoimage takes a snapshot of a given directory tree, and generates a binary image which will correspond to an ISO9660 and\/or HFS filesystem when written to a block device. Each file written to the ISO9660 filesystem must have a filename in the 8.3 format (up to 8 characters, period, up to 3 characters, all uppercase), even if Rock Ridge is in use. This filename is used on systems that are not able to make use of the Rock Ridge extensions (such as MS-DOS), and each filename in each directory must be different from the other filenames in the same directory. genisoimage generally tries to form correct names by forcing the Unix filename to uppercase and truncating as required, but often this yields unsatisfactory results when the truncated names are not all unique. genisoimage assigns weightings to each filename, and if two names that are otherwise the same are found, the name with the lower priority is renamed to include a 3-digit number (guaranteed to be unique). For example, the two files foo.bar and foo.bar.~1~ could be rendered as FOO.BAR;1 and FOO000.BAR;1. When used with various HFS options, genisoimage will attempt to recognise files stored in a number of Apple\/Unix file formats and will copy the data and resource forks as well as any relevant Finder information. See the HFS MACINTOSH FILE FORMATS section below for more about formats genisoimage supports. Note that genisoimage is not designed to communicate with the writer directly. Most writers have proprietary command sets which vary from one manufacturer to another, and you need a specialized tool to actually burn the disc. wodim is one such tool. The latest version of wodim is available from http:\/\/www.cdrkit.org\/. pathspec is the path of the directory tree to be copied into the ISO9660 filesystem. Multiple paths can be specified, and genisoimage will merge the files found in all of the specified path components to form the filesystem image. If the option -graft-points has been specified, it is possible to graft the paths at points other than the root directory, and it is possible to graft files or directories onto the cdrom image with names different than what they have in the source filesystem. This is easiest to illustrate with a couple of examples. Let's start by assuming that a local file ..\/old.lis exists, and you wish to include it in the cdrom image. foo\/bar\/=..\/old.lis will include old.lis in the cdrom image at \/foo\/bar\/old.lis, while foo\/bar\/xxx=..\/old.lis will include old.lis in the cdrom image at \/foo\/bar\/xxx. The same sort of syntax can be used with directories as well. genisoimage will create any directories required such that the graft points exist on the cdrom image -- the directories do not need to appear in one of the paths. By default, any directories that are created on the fly like this will have permissions 0555 and appear to be owned by the person running genisoimage. If you wish other permissions or owners of the intermediate directories, see -uid, -gid, -dir-mode, -file-mode and -new-dir-mode. genisoimage will also run on Windows machines when compiled with Cygnus' cygwin (available from http:\/\/www.cygwin.com\/). Therefore most references in this man page to Unix can be replaced with Win32.","Process Name":"genisoimage","Link":"https:\/\/linux.die.net\/man\/1\/genisoimage"}},{"Process":{"Description":"genkey is an interactive command-line tool which can be used to generate SSL certificates or Certificate Signing Requests (CSR). Generated certificates are stored in the directory \/etc\/pki\/tls\/certs\/, and the corresponding private key in \/etc\/pki\/tls\/private\/. When using mod_nss the private key is stored in the nss database. Consult the nss.conf file in \/etc\/httpd\/conf.d\/ for the location of the database. genkey will prompt for the size of key desired; whether or not to generate a CSR; whether or not an encrypted private key is desired; the certificate subject DN details. genkey generates random data for the private key using the truerand library and also by prompting the user for entry of random text. nss indicates that mod_nss database should be used to store keys and certificates.","Process Name":"genkey","Link":"https:\/\/linux.die.net\/man\/1\/genkey"}},{"Process":{"Description":"genkrf generates a keyrec file from KSK and\/or ZSK files. It generates new KSK and ZSK keys if needed. The name of the keyrec file to be generated is given by the -krfile option. If this option is not specified, zone-name.krf is used as the name of the keyrec file. If the keyrec file already exists, it will be overwritten with new keyrec definitions. The zone-file argument is required. It specifies the name of the zone file from which the signed zone file was created. The optional signed-zone-file argument specifies the name of the signed zone file. If it is not given, then it defaults to zone-file.signed. The signed zone file field is, in effect, a dummy field as the zone file is not actually signed.","Process Name":"genkrf","Link":"https:\/\/linux.die.net\/man\/1\/genkrf"}},{"Process":{"Description":"genlib is a set of C functions dedicated to procedural generation purposes. From a user point of view, genlib is a circuit's description language that allows standard C programming flow control, variable use, and specialized functions in order to handle vlsi objects. Based upon the Alliance mbk data structures, the genlib language gives the user the ability to describe both netlist and layout views, thus allowing both standard cell and full custom approachs. Netlist Capture It is a hierachical structural description of a circuit in terms of connectors (I\/Os), signals (nets), and instances. The function calls used to handle the netlist view are : genlib_def_lofig(3) genlib_save_lofig(3) genlib_loins(3) genlib_locon(3) genlib_losig(3) genlib_flatten_lofig(3) Some facilities, in order to create vectors are also available : genlib_bus(3) genlib_elm(3) Standard Cell Placement The following functions allows to define a placement file for a standard cell design. This file can be used by the standard cell router ocr(1) : genlib_def_phsc(3) genlib_save_phsc(3) genlib_sc_place(3) genlib_sc_right(3) genlib_sc_top(3) genlib_sc_left(3) genlib_sc_bottom(3) Full Custom Symbolic Layout Those functions are dedicated to optimized full custom procedural layout. In order to provide some process independance, Alliance uses a symbolic layout approach (fixed grid without compaction). The symbolic objects are segments (wires), vias (contacts), connectors (I\/Os), references and instances. For more informations, see phseg(1), phvia(1), phcon(1), phref(1), phins(1) and alc(1). genlib_def_phfig(3) genlib_save_phfig(3) genlib_def_ab(3) genlib_def_phins(3) genlib_phcon(3) genlib_copy_up_con(3) genlib_copy_up_con_face(3) genlib_copy_up_all_con(3) genlib_phseg(3) genlib_copy_up_seg(3) GENLIB_THRU_H(3) GENLIB_THRU_V(3) GENLIB_THRU_CON_H(3) GENLIB_THRU_CON_V(3) genlib_wire1(3) genlib_wire2(3) genlib_wire3(3) genlib_phvia(3) genlib_place(3) genlib_place_right(3) genlib_place_top(3) genlib_place_left(3) genlib_place_bottom(3) genlib_place_on(3) genlib_phref(3) genlib_copy_up_ref(3) genlib_copy_up_all_ref(3) genlib_place_via_ref(3) genlib_place_con_ref(3) genlib_place_seg_ref(3) genlib_flatten_phfig(3) genlib_get_ref_x(3) genlib_get_ref_y(3) genlib_get_con_x(3) genlib_get_con_y(3) genlib_height(3) GENLIB_WIDTH(3) In order to have information about each of these functions, use the online documentation with man(1), as in man function-name. It is strongly recommended to read some books on C programming, in order to take full advantage of the C flow control possibilities, as it may greatly reduce the size of a genlib source code.","Process Name":"genlib","Link":"https:\/\/linux.die.net\/man\/1\/genlib"}},{"Process":{"Description":null,"Process Name":"genmap","Link":"https:\/\/linux.die.net\/man\/1\/genmap"}},{"Process":{"Description":"Genpat is a set of C fonctions that allows a procedural description of input pattern file for the logic simulator ASIMUT. The Unix genpat command accepts a C file as input and produces a pattern description file as output. The extension \".c\" is not to be given. The file generated by genpat is in pat format, so IT IS STRONGLY RECOMMENDED TO SEE pat(5) BEFORE THIS MANUAL.","Process Name":"genpat","Link":"https:\/\/linux.die.net\/man\/1\/genpat"}},{"Process":{"Description":"The genpkey command generates a private key.","Process Name":"genpkey","Link":"https:\/\/linux.die.net\/man\/1\/genpkey"}},{"Process":{"Description":null,"Process Name":"genpmda","Link":"https:\/\/linux.die.net\/man\/1\/genpmda"}},{"Process":{"Description":"genpng creates an overview image for a given source code file of either plain text or .gcov file format. Note that the GD.pm PERL module has to be installed for this script to work (it may be obtained from http:\/\/www.cpan.org). Note also that genpng is called from within genhtml so that there is usually no need to call it directly.","Process Name":"genpng","Link":"https:\/\/linux.die.net\/man\/1\/genpng"}},{"Process":{"Description":null,"Process Name":"genrb","Link":"https:\/\/linux.die.net\/man\/1\/genrb"}},{"Process":{"Description":"Generates the LCG dictionary file for each header file","Process Name":"genreflex","Link":"https:\/\/linux.die.net\/man\/1\/genreflex"}},{"Process":{"Description":"genreflex-rootcint is called by rootcint(1) if the option -gccxml is specified, and gccxml(1) is found.","Process Name":"genreflex-rootcint","Link":"https:\/\/linux.die.net\/man\/1\/genreflex-rootcint"}},{"Process":{"Description":"This manual page documents briefly the genresscript command. This manual page was written for the Debian GNU distribution because the original program does not have a manual page. Genresscript takes any number of PE or NE binaries, and outputs a resource script with all resources from the binaries listed. (Everything but the destination filename will be filled in in the script.) This is useful when you already have extracted images and want to create a resource script of them.","Process Name":"genresscript","Link":"https:\/\/linux.die.net\/man\/1\/genresscript"}},{"Process":{"Description":"The genrsa command generates an RSA private key.","Process Name":"genrsa","Link":"https:\/\/linux.die.net\/man\/1\/genrsa"}},{"Process":{"Description":"Script to generate the server.pem and client.pem SSL certificate files to enable the Small-Footprint CIM Broker to handle https requests from CIM clients.","Process Name":"gensslcert","Link":"https:\/\/linux.die.net\/man\/1\/gensslcert"}},{"Process":{"Description":"gentest is a utility for detecting differences in behaviour between SMB servers. It will run a random set of generic operations against \/\/server1\/share1 and then the same random set against \/\/server2\/share2 and display the differences in the responses it gets. This utility is used by the Samba team to find differences in behaviour between Samba and Windows servers.","Process Name":"gentest","Link":"https:\/\/linux.die.net\/man\/1\/gentest"}},{"Process":{"Description":null,"Process Name":"gentoo","Link":"https:\/\/linux.die.net\/man\/1\/gentoo"}},{"Process":{"Description":"genxs is a tool for generating custom XML serialization writers and readers for classes. configurationFile is configuration file which specifies several information, such as the class for which to generate the reader and writer, the name and namespace of the classes to generate, and a collection of hooks to apply. By using hooks it is possible to customize the behavior of the serializer without needing to modify the generated file, so you can safely regenerate it if the source class is modified. destinationFolder specifies the folder where the files will be generated. NOTE: This tool only runs in the Mono runtime, since it uses some internal classes not available in other runtimes.","Process Name":"genxs","Link":"https:\/\/linux.die.net\/man\/1\/genxs"}},{"Process":{"Description":"Geod (direct) and invgeod (inverse) perform geodesic (Great Circle) computations for determining latitude, longitude and back azimuth of a terminus point given a initial point latitude, longitude, azimuth and distance (direct) or the forward and back azimuths and distance between an initial and terminus point latitudes and longitudes (inverse). The following runline control parameters can appear in any order: -I Specifies that the inverse geodesic computation is to be performed. May be used with execution of goed as an alternative to invgeod execution. -a Latitude and longitudes of the initial and terminal points, forward and back azimuths and distance are output. -ta A specifies a character employed as the first character to denote a control line to be passed through without processing. -le Gives a listing of all the ellipsoids that may be selected with the +ellps= option. -lu Gives a listing of all the units that may be selected with the +units= option. -[f|F] format Format is a printf format string to control the output form of the geographic coordinate values (f) or distance value (F). The default mode is DMS for geographic coordinates and \"%.3f\" for distance. -[w|W] n N is the number of significant fractional digits to employ for seconds output (when the option is not specified, -w3 is assumed). When -W is employed the fields will be constant width with leading zeroes. -p This option causes the azimuthal values to be output as unsigned DMS numbers between 0 and 360 degrees. Also note -f. The +args run-line arguments are associated with geodetic parameters for specifying the ellipsoidal or sphere to use. See proj documentation for full list of these parameters and controls. The options are processed in left to right order from the run line. Reentry of an option is ignored with the first occurrence assumed to be the desired value. One or more files (processed in left to right order) specify the source of data to be transformed. A - will specify the location of processing standard input. If no files are specified, the input is assumed to be from stdin. For direct determinations input data must be in latitude, longitude, azimuth and distance order and output will be latitude, longitude and back azimuth of the terminus point. Latitude, longitude of the initial and terminus point are input for the inverse mode and respective forward and back azimuth from the initial and terminus points are output along with the distance between the points. Input geographic coordinates (latitude and longitude) and azimuthal data must be in DMS format and input distance data must be in units consistent with the ellipsoid major axis or sphere radius units. Output geographic coordinates will be in DMS (if the -f switch is not employed) to 0.001\" with trailing, zero-valued minute-second fields deleted. Output distance data will be in the same units as the ellipsoid or sphere radius. The Earth's ellipsoidal figure may be selected in the same manner as program proj by using +ellps=, +a=, +es=, etc. Geod may also be used to determine intermediate points along either a geodesic line between two points or along an arc of specified distance from a geographic point. In both cases an initial point must be specified with +lat_1=lat and +lon_1=lon parameters and either a terminus point +lat_2=lat and +lon_2=lon or a distance and azimuth from the initial point with +S=distance and +A=azimuth must be specified. If points along a geodesic are to be determined then either +n_S=integer specifying the number of intermediate points and\/or +del_S=distance specifying the incremental distance between points must be specified. To determine points along an arc equidistant from the initial point both +del_A=angle and +n_A=integer must be specified which determine the respective angular increments and number of points to be determined.","Process Name":"geod","Link":"https:\/\/linux.die.net\/man\/1\/geod"}},{"Process":{"Description":"The geoip-lookup program will return the country for the IP address or hostname given as the first command line argument. It queries the GeoIP Country database in \"\/usr\/local\/share\/GeoIP\/GeoIP.dat\". By default it prints the ISO 3166 country code. Use the \"-l\" option to print the country name.","Process Name":"geoip-lookup","Link":"https:\/\/linux.die.net\/man\/1\/geoip-lookup"}},{"Process":{"Description":"geoiplookup uses the GeoIP library and database to find the Country that an IP address or hostname originates from. For example geoiplookup 80.60.233.195 will find the Country that 80.60.233.195 originates from, in the following format: NL, Netherlands","Process Name":"geoiplookup","Link":"https:\/\/linux.die.net\/man\/1\/geoiplookup"}},{"Process":{"Description":null,"Process Name":"geoiplookup6","Link":"https:\/\/linux.die.net\/man\/1\/geoiplookup6"}},{"Process":{"Description":"geoipupdate automatically updates the GeoIP database for GeoIP subscribers. It connects to the MaxMind GeoIP Update server and checks for an updated database. If it finds an updated database, then it downloads it, uncompresses it, and installs it. If you are running a firewall, it requires that the DNS and HTTP (80) ports be open. For example geoipupdate -v Performs the update in verbose mode.","Process Name":"geoipupdate","Link":"https:\/\/linux.die.net\/man\/1\/geoipupdate"}},{"Process":{"Description":"If available, geome queries NetworkManager for its wifi information, such as ESSID's and signal strength, and together with its IP address, sends this information to the Google Location service to obtain your geographic location.","Process Name":"geome","Link":"https:\/\/linux.die.net\/man\/1\/geome"}},{"Process":{"Description":"Geomview is an interactive geometry viewer written at the Geometry Center. It displays the objects in the files given on the command line and allows the user to view and manipulate them interactively. The present version (1.7) of geomview runs on Silicon Graphics Irises, and X Window System workstations. We are very interested in hearing about any problems you may have using it; see below for information on how to contact us. In addition to providing interactive control over a world of objects via the mouse and keyboard, geomview has an interpreted language of commands for controlling almost every aspect of its display. It can execute files containing statements in this language, and it can communicate with other programs using the language. See geomview(5), e.g. the file man\/cat5\/geomview.5 in the geomview distribution, for more details of the language.","Process Name":"geomview","Link":"https:\/\/linux.die.net\/man\/1\/geomview"}},{"Process":{"Description":"This manual page describes the GNU version of eqn, which is part of the groff document formatting system. eqn compiles descriptions of equations embedded within troff input files into commands that are understood by troff. Normally, it should be invoked using the -e option of groff. The syntax is quite compatible with Unix eqn. The output of GNU eqn cannot be processed with Unix troff; it must be processed with GNU troff. If no files are given on the command line, the standard input will be read. A filename of - will cause the standard input to be read. eqn searches for the file eqnrc in the directories given with the -M option first, then in \/usr\/lib64\/groff\/site-tmac, \/usr\/share\/groff\/site-tmac, and finally in the standard macro directory \/usr\/share\/groff\/1.18.1.4\/tmac. If it exists, eqn will process it before the other input files. The -R option prevents this. GNU eqn does not provide the functionality of neqn: it does not support low-resolution, typewriter-like devices (although it may work adequately for very simple input).","Process Name":"geqn","Link":"https:\/\/linux.die.net\/man\/1\/geqn"}},{"Process":{"Description":"gerbv is a viewer for RS274-X, commonly known as Gerber, files. RS274-X files are generated from different PCB CAD programs and are used in the printed circuit board manufacturing process. gerbv also supports Excellon\/NC drill files as well as XY (centroid) files produced by the program PCB (http:\/\/pcb.sf.net).","Process Name":"gerbv","Link":"https:\/\/linux.die.net\/man\/1\/gerbv"}},{"Process":{"Description":null,"Process Name":"get","Link":"https:\/\/linux.die.net\/man\/1\/get"}},{"Process":{"Description":"get-iab fetches the Ethernet IAB file from the IEEE website, and saves it in the format used by arp-scan. The IAB file contains all of the IABs (Individual Address Blocks) that have been registered with IEEE. Each IAB entry in the file specifies the first 36-bits of the 48-bit Ethernet hardware address, leaving the remaining 12-bits for use by the registering organisation. For example the IAB entry \"0050C2003\", registered to Microsoft, applies to any Ethernet hardware address from 00:50:c2:00:30:00 to 00:50:c2:00:3f:ff inclusive. Each IAB assignment represents a total of 2^12 (4,096) Ethernet addresses. Major Ethernet hardware vendors typically use an OUI registration rather than an IAB registration. See get-oui(1) for details. This script can be used to update the arp-scan IAB file from the latest data on the IEEE website. It is relatively rare to see Ethernet addresses from IAB registrations, so the IAB file is not as important as the OUI file. The IAB data is fetched from the URL http:\/\/standards.ieee.org\/regauth\/oui\/iab.txt and the output file is saved to the file ieee-iab.txt in the current directory. The URL to fetch the data from can be changed with the -u option, and the output file name can be changed with the -f option. The ieee-iab.txt file that is produced by this script is used by arp-scan to determine the Ethernet card vendor from its hardware address. The directory that arp-scan will look for the ieee-iab.txt file depends on the options used when it was built. If it was built using the default options, then it will look in \/usr\/local\/share\/arp-scan.","Process Name":"get-iab","Link":"https:\/\/linux.die.net\/man\/1\/get-iab"}},{"Process":{"Description":"get-oui fetches the Ethernet OUI file from the IEEE website, and saves it in the format used by arp-scan. The OUI file contains all of the OUIs (Organizationally Unique Identifiers) that have been registered with IEEE. Each OUI entry in the file specifies the first 24-bits of the 48-bit Ethernet hardware address, leaving the remaining 24-bits for use by the registering organisation. For example the OUI entry \"080020\", registered to Sun Microsystems, applies to any Ethernet hardware address from 08:00:20:00:00:00 to 08:00:20:ff:ff:ff inclusive. Each OUI assignment represents a total of 2^24 (16,777,216) Ethernet addresses. Every major Ethernet hardware vendor registers an OUI for their equipment, and larger vendors will need to register more than one. For example, 3Com have a total of 37 OUI entries. Organisations that only produce a small number of Ethernet devices will often obtain an IAB registration instead. See get-iab(1) for details. This script can be used to update the arp-scan OUI file from the latest data on the IEEE website. Most of the Ethernet addresses in use belong to an OUI registration, so this is the most important of the files that arp-scan uses to decode Ethernet hardware addresses. You should therefore run get-oui occasionally to keep the arp-scan OUI file up to date. The OUI data is fetched from the URL http:\/\/standards.ieee.org\/regauth\/oui\/oui.txt and the output file is saved to the file ieee-oui.txt in the current directory. The URL to fetch the data from can be changed with the -u option, and the output file name can be changed with the -f option. The ieee-oui.txt file that is produced by this script is used by arp-scan to determine the Ethernet card vendor from its hardware address. The directory that arp-scan will look for the ieee-oui.txt file depends on the options used when it was built. If it was built using the default options, then it will look in \/usr\/local\/share\/arp-scan.","Process Name":"get-oui","Link":"https:\/\/linux.die.net\/man\/1\/get-oui"}},{"Process":{"Description":"This utility is a command-line wrapper around the val_getaddrinfo() function. It invokes the val_getaddrinfo() operation for the given command-line arguments and displays the return code from the function and the contents of the addrinfo structure. The exit status for this program is 1 if the answer received is trusted, 2 if the answer received is validated, and -1 for an error. If no validation is performed, or the result is neither trusted nor validated, a value of 0 is returned. The trusted and validated status values are obtained using the val_istrusted() and val_isvalidated() functions from libval(3).","Process Name":"getaddr","Link":"https:\/\/linux.die.net\/man\/1\/getaddr"}},{"Process":{"Description":null,"Process Name":"getafm","Link":"https:\/\/linux.die.net\/man\/1\/getafm"}},{"Process":{"Description":"The getcert tool issues requests to a org.fedorahosted.certmonger service on behalf of the invoking user. It can ask the service to begin enrollment, optionally generating a key pair to use, it can ask the service to begin monitoring a certificate in a specified location for expiration, and optionally to refresh it when expiration nears, it can list the set of certificates that the service is already monitoring, or it can list the set of CAs that the service is capable of using. If no command is given as the first command-line argument, getcert will print short usage information for each of its functions.","Process Name":"getcert","Link":"https:\/\/linux.die.net\/man\/1\/getcert"}},{"Process":{"Description":null,"Process Name":"getcert-list","Link":"https:\/\/linux.die.net\/man\/1\/getcert-list"}},{"Process":{"Description":"Queries certmonger for a list of known CAs.","Process Name":"getcert-list-cas","Link":"https:\/\/linux.die.net\/man\/1\/getcert-list-cas"}},{"Process":{"Description":null,"Process Name":"getcert-request","Link":"https:\/\/linux.die.net\/man\/1\/getcert-request"}},{"Process":{"Description":"Tells certmonger to generate (or regenerate) a signing request and submit (or resubmit) the signing request to a CA for signing.","Process Name":"getcert-resubmit","Link":"https:\/\/linux.die.net\/man\/1\/getcert-resubmit"}},{"Process":{"Description":"Tells certmonger to monitor an already-issued certificate. Optionally, when the certificate nears expiration, use an existing key pair (or to generate one if one is not already found in the specified location), to generate a signing request using the key pair and to submit them for signing to a CA.","Process Name":"getcert-start-tracking","Link":"https:\/\/linux.die.net\/man\/1\/getcert-start-tracking"}},{"Process":{"Description":null,"Process Name":"getcert-stop-tracking","Link":"https:\/\/linux.die.net\/man\/1\/getcert-stop-tracking"}},{"Process":{"Description":"This tool is part of the cifs-utils suite. getcifsacl is a userspace helper program for the Linux CIFS client file system. It is intended to display a security descriptor including ACL for a file system object. It is best utilized when an option of cifsacl is specified when mounting a cifs share in conjunction with winbind facility of Samba suite. Fields of an ACE such as SID, type, flags, and mask are displayed separated by \/. Numeric values of type, flags, and mask are displayed in hexadecimal format.","Process Name":"getcifsacl","Link":"https:\/\/linux.die.net\/man\/1\/getcifsacl"}},{"Process":{"Description":null,"Process Name":"getcol","Link":"https:\/\/linux.die.net\/man\/1\/getcol"}},{"Process":{"Description":"In the first synopsis form, the getconf utility shall write to the standard output the value of the variable specified by the system_var operand. In the second synopsis form, the getconf utility shall write to the standard output the value of the variable specified by the path_var operand for the path specified by the pathname operand. The value of each configuration variable shall be determined as if it were obtained by calling the function from which it is defined to be available by this volume of IEEE Std 1003.1-2001 or by the System Interfaces volume of IEEE Std 1003.1-2001 (see the OPERANDS section). The value shall reflect conditions in the current operating environment.","Process Name":"getconf","Link":"https:\/\/linux.die.net\/man\/1\/getconf"}},{"Process":{"Description":null,"Process Name":"getconfig","Link":"https:\/\/linux.die.net\/man\/1\/getconfig"}},{"Process":{"Description":"getdnskeys manages lists of DNSKEYs from DNS zones. It may be used to retrieve and compare DNSKEYs. The output from getdnskeys may be included (directly or indirectly) in a named.conf file.","Process Name":"getdnskeys","Link":"https:\/\/linux.die.net\/man\/1\/getdnskeys"}},{"Process":{"Description":"getds will create a DS record from DNSKEYs for the specified DNS domain. It does this by converting DNSKEYs to DS records using the specified hashing algorithm. The results can then be passed to upstream DNSSEC-supporting parents or to DLV registries. getds will also pull the parent's published DS records and compare them against the existing keys. It will then list any DS records not published in the parent, as well as any DS records that are published in the parent but which don't match an existing key.","Process Name":"getds","Link":"https:\/\/linux.die.net\/man\/1\/getds"}},{"Process":{"Description":"The getent command displays entries from databases supported by the Name Service Switch libraries, which are configured in \/etc\/nsswitch.conf. If one or more key arguments are provided, then only the entries that match the supplied keys will be displayed. Otherwise, if no key is provided, all entries will be displayed (unless the database does not support enumeration). The database may be any of those supported by the GNU C Library, listed below: ahosts When no key is provided, use sethostent(3), gethostent(3), and endhostent(3) to enumerate the hosts database. This is identical to using hosts. When one or more key arguments are provided, pass each key in succession to getaddrinfo(3) with the address family AF_UNSPEC, enumerating each socket address structure returned. ahostsv4 Same as ahosts, but use the address family AF_INET. ahostsv6 Same as ahosts, but use the address family AF_INET6. The call to getaddrinfo(3) in this case includes the AI_V4MAPPED flag. aliases When no key is provided, use setaliasent(3), getaliasent(3), and endaliasent(3) to enumerate the aliases database. When one or more key arguments are provided, pass each key in succession to getaliasbyname(3) and display the result. ethers When one or more key arguments are provided, pass each key in succession to ether_aton(3) and ether_hostton(3) until a result is obtained, and display the result. Enumeration is not supported on ethers, so a key must be provided. group When no key is provided, use setgrent(3), getgrent(3), and endgrent(3) to enumerate the group database. When one or more key arguments are provided, pass each numeric key to getgrgid(3) and each nonnumeric key to getgrnam(3) and display the result. gshadow When no key is provided, use setsgent(3), getsgent(3), and endsgent(3) to enumerate the gshadow database. When one or more key arguments are provided, pass each key in succession to getsgnam(3) and display the result. hosts When no key is provided, use sethostent(3), gethostent(3), and endhostent(3) to enumerate the hosts database. When one or more key arguments are provided, pass each key to gethostbyaddr(3) or gethostbyname2(3), depending on whether a call to inet_pton(3) indicates that the key is an IPv6 or IPv4 address or not, and display the result. initgroups When one or more key arguments are provided, pass each key in succession to getgrouplist(3) and display the result. Enumeration is not supported on initgroups, so a key must be provided. netgroup When one key is provided, pass the key to setnetgrent(3) and, using getnetgrent(3) display the resulting string triple ( hostname, username, domainname). Alternatively, three keys may be provided, which are interpreted as the hostname, username and domainname to match to a netgroup name via innetgr(3). Enumeration is not supported on netgroup, so either one or three keys must be provided. networks When no key is provided, use setnetent(3), getnetent(3), and endnetent(3) to enumerate the networks database. When one or more key arguments are provided, pass each numeric key to getnetbyaddr(3) and each nonnumeric key to getnetbyname(3) and display the result. passwd When no key is provided, use setpwent(3), getpwent(3), and endpwent(3) to enumerate the passwd database. When one or more key arguments are provided, pass each numeric key to getpwuid(3) and each nonnumeric key to getpwnam(3) and display the result. protocols When no key is provided, use setprotoent(3), getprotoent(3), and endprotoent(3) to enumerate the protocols database. When one or more key arguments are provided, pass each numeric key to getprotobynumber(3) and each nonnumeric key to getprotobyname(3) and display the result. rpc When no key is provided, use setrpcent(3), getrpcent(3), and endrpcent(3) to enumerate the rpc database. When one or more key arguments are provided, pass each numeric key to getrpcbynumber(3) and each nonnumeric key to getrpcbyname(3) and display the result. services When no key is provided, use setservent(3), getservent(3), and endservent(3) to enumerate the services database. When one or more key arguments are provided, pass each numeric key to getservbynumber(3) and each nonnumeric key to getservbyname(3) and display the result. shadow When no key is provided, use setspent(3), getspent(3), and endspent(3) to enumerate the shadow database. When one or more key arguments are provided, pass each key in succession to getspnam(3) and display the result.","Process Name":"getent","Link":"https:\/\/linux.die.net\/man\/1\/getent"}},{"Process":{"Description":"For each file, getfacl displays the file name, owner, the group, and the Access Control List (ACL). If a directory has a default ACL, getfacl also displays the default ACL. Non-directories cannot have default ACLs. If getfacl is used on a file system that does not support ACLs, getfacl displays the access permissions defined by the traditional file mode permission bits. The output format of getfacl is as follows:  1:  # file: somedir\/\n 2:  # owner: lisa\n 3:  # group: staff\n 4:  # flags: -s-\n 5:  user::rwx\n 6:  user:joe:rwx               #effective:r-x\n 7:  group::rwx                 #effective:r-x\n 8:  group:cool:r-x\n 9:  mask::r-x\n10:  other::r-x\n11:  default:user::rwx\n12:  default:user:joe:rwx       #effective:r-x\n13:  default:group::r-x\n14:  default:mask::r-x\n15:  default:other::--- Lines 1--3 indicate the file name, owner, and owning group. Line 4 indicates the setuid (s), setgid (s), and sticky (t) bits: either the letter representing the bit, or else a dash (-). This line is included if any of those bits is set and left out otherwise, so it will not be shown for most files. (See CONFORMANCE TO POSIX 1003.1e DRAFT STANDARD 17 below.) Lines 5, 7 and 10 correspond to the user, group and other fields of the file mode permission bits. These three are called the base ACL entries. Lines 6 and 8 are named user and named group entries. Line 9 is the effective rights mask. This entry limits the effective rights granted to all groups and to named users. (The file owner and others permissions are not affected by the effective rights mask; all other entries are.) Lines 11--15 display the default ACL associated with this directory. Directories may have a default ACL. Regular files never have a default ACL. The default behavior for getfacl is to display both the ACL and the default ACL, and to include an effective rights comment for lines where the rights of the entry differ from the effective rights. If output is to a terminal, the effective rights comment is aligned to column 40. Otherwise, a single tab character separates the ACL entry and the effective rights comment. The ACL listings of multiple files are separated by blank lines. The output of getfacl can also be used as input to setfacl. PERMISSIONS Process with search access to a file (i.e., processes with read access to the containing directory of a file) are also granted read access to the file's ACLs. This is analogous to the permissions required for accessing the file mode. OPTIONS -a, --access Display the file access control list. -d, --default Display the default access control list. -c, --omit-header Do not display the comment header (the first three lines of each file's output). -e, --all-effective Print all effective rights comments, even if identical to the rights defined by the ACL entry. -E, --no-effective Do not print effective rights comments. -s, --skip-base Skip files that only have the base ACL entries (owner, group, others). -R, --recursive List the ACLs of all files and directories recursively. -L, --logical Logical walk, follow symbolic links to directories. The default behavior is to follow symbolic link arguments, and skip symbolic links encountered in subdirectories. Only effective in combination with -R. -P, --physical Physical walk, do not follow symbolic links to directories. This also skips symbolic link arguments. Only effective in combination with -R. -t, --tabular Use an alternative tabular output format. The ACL and the default ACL are displayed side by side. Permissions that are ineffective due to the ACL mask entry are displayed capitalized. The entry tag names for the ACL_USER_OBJ and ACL_GROUP_OBJ entries are also displayed in capital letters, which helps in spotting those entries. -p, --absolute-names Do not strip leading slash characters ('\/'). The default behavior is to strip leading slash characters. -n, --numeric List numeric user and group IDs -v, --version Print the version of getfacl and exit. -h, --help Print help explaining the command line options. -- End of command line options. All remaining parameters are interpreted as file names, even if they start with a dash character. - If the file name parameter is a single dash character, getfacl reads a list of files from standard input.","Process Name":"getfacl","Link":"https:\/\/linux.die.net\/man\/1\/getfacl"}},{"Process":{"Description":"For each file, getfattr displays the file name, and the set of extended attribute names (and optionally values) which are associated with that file. The output format of getfattr -d is as follows: 1:  # file: somedir\/\n2:  user.name0=\"value0\"\n3:  user.name1=\"value1\"\n4:  user.name2=\"value2\"\n5:  ... Line 1 identifies the file name for which the following lines are being reported. The remaining lines (lines 2 to 4 above) show the name and value pairs associated with the specified file. OPTIONS -n name, --name= name Dump the value of the named extended attribute extended attribute. -d, --dump Dump the values of all extended attributes associated with pathname. -e en, --encoding= en Encode values after retrieving them. Valid values of en are \"text\", \"hex\", and \"base64\". Values encoded as text strings are enclosed in double quotes (\"), while strings encoded as hexidecimal and base64 are prefixed with 0x and 0s, respectively. -h, --no-dereference Do not dereference symlinks. Instead of the file a symlink refers to, the symlink itself is examined. Unless doing a logical (-L) traversal, do not traverse symlinks to directories. -m pattern, --match= pattern Only include attributes with names matching the regular expression pattern. The default value for pattern is \"^user\\\\.\", which includes all the attributes in the user namespace. Specify \"-\" for including all attributes. Refer to attr(5) for a more detailed discussion of namespaces. --absolute-names Do not strip leading slash characters ('\/'). The default behaviour is to strip leading slash characters. --only-values Dump out the extended attribute value(s) only. -R, --recursive List the attributes of all files and directories recursively. -L, --logical Logical walk, follow symbolic links to directories. The default behaviour is to follow symbolic link arguments unless --no-dereference is given, and to skip symbolic links encountered in subdirectories. Only effective in combination with -R. -P, --physical Physical walk, do not follow symbolic links to directories. This also skips symbolic link arguments. Only effective in combination with -R. --version Print the version of getfattr and exit. --help Print help explaining the command line options. -- End of command line options. All remaining parameters are interpreted as file names, even if they start with a dash character.","Process Name":"getfattr","Link":"https:\/\/linux.die.net\/man\/1\/getfattr"}},{"Process":{"Description":null,"Process Name":"gethead","Link":"https:\/\/linux.die.net\/man\/1\/gethead"}},{"Process":{"Description":null,"Process Name":"gethost","Link":"https:\/\/linux.die.net\/man\/1\/gethost"}},{"Process":{"Description":"This manual page documents briefly the gethostip command. The gethostip utility converts the given hostname or IP address into a variety formats. It is provided by the syslinux package to make it easier to calculate the appropriate names for pxelinux configuration files. These filenames can be the complete hexadecimal representation for a given IP address, or a partial hexadecimal representation to match a range of IP addresses.","Process Name":"gethostip","Link":"https:\/\/linux.die.net\/man\/1\/gethostip"}},{"Process":{"Description":null,"Process Name":"geticonset","Link":"https:\/\/linux.die.net\/man\/1\/geticonset"}},{"Process":{"Description":"getkey waits until one of KEYS is pressed. If KEYS are not specified, any key is accepted. KEYS are matched case-insensitive.","Process Name":"getkey","Link":"https:\/\/linux.die.net\/man\/1\/getkey"}},{"Process":{"Description":null,"Process Name":"getlist","Link":"https:\/\/linux.die.net\/man\/1\/getlist"}},{"Process":{"Description":"getltscfg gets the configuration values defined in \/etc\/lts.conf, and outputs those values in in a way meant to be evaluated in a shell script.","Process Name":"getltscfg","Link":"https:\/\/linux.die.net\/man\/1\/getltscfg"}},{"Process":{"Description":null,"Process Name":"getmail","Link":"https:\/\/linux.die.net\/man\/1\/getmail"}},{"Process":{"Description":null,"Process Name":"getmail_fetch","Link":"https:\/\/linux.die.net\/man\/1\/getmail_fetch"}},{"Process":{"Description":"Deliver a mail message from standard input, to the maildir named PATH. PATH must start with a dot or a slash and end with a slash. getmail_maildir uses the SENDER environment variable to construct a Return-Path: header field and the contents of the RECIPIENT environment variable to construct a Delivered-To: header field at the top of the message. --verbose, -v print a status message on success","Process Name":"getmail_maildir","Link":"https:\/\/linux.die.net\/man\/1\/getmail_maildir"}},{"Process":{"Description":"Deliver a mail message from standard input, to the mboxrd-format mbox file named PATH. PATH must start with a dot or a slash and must not end with a slash. fcntl-type locking is used; if your system requires another type of locking (such as flock or dotlock ), use an MDA configured for that style of locking instead. getmail_mbox uses the SENDER environment variable to construct a Return-Path: header field and the contents of the RECIPIENT environment variable to construct a Delivered-To: header field at the top of the message. SENDER is also used in creating the mbox From_ line. --verbose, -v print a status message on success","Process Name":"getmail_mbox","Link":"https:\/\/linux.die.net\/man\/1\/getmail_mbox"}},{"Process":{"Description":"This utility is a command-line wrapper around the val_getnameinfo() function. It invokes the val_getnameinfo() function with the given command-line arguments and displays the returned host and service information for the given node name, with a validation status value. The exit status for this program is 1 if the answer received is trusted, 2 if the answer received is validated, and -1 for an error. The program returns 0 if no validation was performed or if the result was neither trusted nor validated. The trusted and validated status values are obtained using the val_istrusted() and val_isvalidated() functions from libval(3).","Process Name":"getname","Link":"https:\/\/linux.die.net\/man\/1\/getname"}},{"Process":{"Description":"getopt is used to break up ( parse) options in command lines for easy parsing by shell procedures, and to check for legal options. It uses the GNU getopt(3) routines to do this. The parameters getopt is called with can be divided into two parts: options which modify the way getopt will parse (options and -o|--options optstring in the SYNOPSIS), and the parameters which are to be parsed (parameters in the SYNOPSIS). The second part will start at the first non-option parameter that is not an option argument, or after the first occurrence of '--'. If no '-o' or '--options' option is found in the first part, the first parameter of the second part is used as the short options string. If the environment variable GETOPT_COMPATIBLE is set, or if its first parameter is not an option (does not start with a '-', this is the first format in the SYNOPSIS), getopt will generate output that is compatible with that of other versions of getopt(1). It will still do parameter shuffling and recognize optional arguments (see section COMPATIBILITY for more information). Traditional implementations of getopt(1) are unable to cope with whitespace and other (shell-specific) special characters in arguments and non-option parameters. To solve this problem, this implementation can generate quoted output which must once again be interpreted by the shell (usually by using the eval command). This has the effect of preserving those characters, but you must call getopt in a way that is no longer compatible with other versions (the second or third format in the SYNOPSIS). To determine whether this enhanced version of getopt(1) is installed, a special test option (-T) can be used.","Process Name":"getopt","Link":"https:\/\/linux.die.net\/man\/1\/getopt"}},{"Process":{"Description":"","Process Name":"getopts","Link":"https:\/\/linux.die.net\/man\/1\/getopts"}},{"Process":{"Description":"getpeername prints the IP address and service name (port number) of the remote peer connected to standard input. The IP address and the service name are printed on the same line, separated by one space.","Process Name":"getpeername","Link":"https:\/\/linux.die.net\/man\/1\/getpeername"}},{"Process":{"Description":"This utility is a command-line wrapper around the val_res_query() function. It invokes the val_res_query() function with the given command-line arguments and displays the returned data as a DNS-style response. It also displays the validation status value for the records returned for the query. The exit status for this program is 1 if the answer received is trusted, 2 if the answer received is validated, and -1 for an error. The program returns 0 if the result was neither trusted nor validated. The trusted and validated status values are obtained using the val_istrusted() and val_isvalidated() functions from libval(3).","Process Name":"getquery","Link":"https:\/\/linux.die.net\/man\/1\/getquery"}},{"Process":{"Description":"This utility is a command-line wrapper around the val_get_rrset() function. It invokes the val_get_rrset() function with the given command-line arguments and displays the returned data and the validated, trusted and non-existence status for each resource record set returned in the response. Any information about name aliases are also displayed. The trusted and validated status values are obtained using the val_istrusted() and val_isvalidated() functions from libval(3). This program returns 0 on success and -1 on failure.","Process Name":"getrrset","Link":"https:\/\/linux.die.net\/man\/1\/getrrset"}},{"Process":{"Description":"getSchema is a small script to automatically downlaod and install the latest CIM Schema from the DMTF website. The CIM Schema is required by the Small-Footprint CIM Broker (sfcb) to be able to install and register new classes. getSchema uses curl to fetch the CIM Schema tarball.","Process Name":"getschema","Link":"https:\/\/linux.die.net\/man\/1\/getschema"}},{"Process":{"Description":"getstyle can either dump the current Window Maker style related configuration information to a file\/stdout or create a self-contained theme pack. A theme pack is a directory that contains everything that is needed by a redistributable theme, includig the style information and pixmaps used by it. Note that style information stored in the global configuration of the system is not read. The following options are stored by default: IconBack, IconBack, TitleJustify, WindowTitleFont, MenuTitleFont, MenuTextFont, IconTitleFont, DisplayFont, HighlightColor, HighlightTextColor, IconTitleColor, CClipTitleColor, IconBackColor, FTitleColor, PTitleColor, UTitleColor, FTitleBack, PTitleBack, UTitleBack, MenuTitleColor, MenuTextColor, MenuDisabledColor, MenuTitleBack and MenuTextBack. If either -t or --theme-options is specified, in addition to the previous options, WorkspaceBack is also stored, along with any user-definable mouse cursor settings (NormalCursor, ArrowCursor, MoveCursor, TopLeftResizeCursor, TopRightResizeCursor, BottomLeftResizeCursor, BottomRightResizeCursor, VerticalResizeCursor, HorizontalResizeCursor, WaitCursor, QuestionCursor, TextCursor, SelectCursor) that are present.","Process Name":"getstyle","Link":"https:\/\/linux.die.net\/man\/1\/getstyle"}},{"Process":{"Description":"The gettext program translates a natural language message into the user's language, by looking up the translation in a message catalog. Display native language translation of a textual message. -d, --domain= TEXTDOMAIN retrieve translated messages from TEXTDOMAIN -e enable expansion of some escape sequences -E (ignored for compatibility) -h, --help display this help and exit -n suppress trailing newline -V, --version display version information and exit [TEXTDOMAIN] MSGID retrieve translated message corresponding to MSGID from TEXTDOMAIN If the TEXTDOMAIN parameter is not given, the domain is determined from the environment variable TEXTDOMAIN. If the message catalog is not found in the regular directory, another location can be specified with the environment variable TEXTDOMAINDIR. When used with the -s option the program behaves like the 'echo' command. But it does not simply copy its arguments to stdout. Instead those messages found in the selected catalog are translated. Standard search directory: \/usr\/share\/locale","Process Name":"gettext","Link":"https:\/\/linux.die.net\/man\/1\/gettext"}},{"Process":{"Description":null,"Process Name":"gettextize","Link":"https:\/\/linux.die.net\/man\/1\/gettextize"}},{"Process":{"Description":"Simple utility printing the current version of the gfal2 library. Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"gfal2_version","Link":"https:\/\/linux.die.net\/man\/1\/gfal2_version"}},{"Process":{"Description":null,"Process Name":"gfalfs","Link":"https:\/\/linux.die.net\/man\/1\/gfalfs"}},{"Process":{"Description":"","Process Name":"gfdl","Link":"https:\/\/linux.die.net\/man\/1\/gfdl"}},{"Process":{"Description":"gfloppy is a simple floppy formatter for Linux. It supports the DOS format and the ext2 file system. In order to create DOS floppies you need to have the mtools package installed. For full documentation see the Floppy Formatter online help.","Process Name":"gfloppy","Link":"https:\/\/linux.die.net\/man\/1\/gfloppy"}},{"Process":{"Description":null,"Process Name":"gflux","Link":"https:\/\/linux.die.net\/man\/1\/gflux"}},{"Process":{"Description":"The gfortran command supports all the options supported by the gcc command. Only options specific to GNU Fortran are documented here. All GCC and GNU Fortran options are accepted both by gfortran and by gcc (as well as any other drivers built at the same time, such as g++), since adding GNU Fortran to the GCC distribution enables acceptance of GNU Fortran options by all of the relevant drivers. In some cases, options have positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"gfortran","Link":"https:\/\/linux.die.net\/man\/1\/gfortran"}},{"Process":{"Description":"This manual page is not meant to be exhaustive. The complete documentation for this version of TeX can be found in the info file or manual Web2C: A TeX implementation. The gftodvi program converts a generic font (gf) file output by, for example, mf(1), to a device independent (DVI) file (that can then be typeset using the same software that has already been written for TeX). The characters in the gf file will appear one per page, with labels, titles, and annotations as specified in Appendix H (Hardcopy Proofs) of The Metafontbook. gftodvi uses other fonts in addition to the main gf file. A 'gray' font is used to typeset the pixels that actually make up the character. (We wouldn't want all the pixels to be simply black, since then labels, key points, and other information would be lost.) A 'title' font is used for the information at the top of the page. A 'label' font is used for the labels on key points of the figure. A 'slant' font is used to typeset diagonal lines, which otherwise have to be simulated using horizontal and vertical rules. The default gray, title, and label fonts are gray, cmr8, and cmtt10, respectively; there is no default slant font. To change the default fonts, you can give special commands in your Metafont source file, or you can change the fonts online. An online dialog ensues if you end the gf_file_name with a '\/'. For example, gftodvi cmr10.300gf\/ Special font substitution: grayfont black OK; any more? grayfontarea \/home\/art\/don\/ OK; any more? slantfont \/home\/fonts\/slantimagen6 OK; any more? <RET> will use \/home\/art\/don\/black as the 'gray' font and \/home\/fonts\/slantimagen6 as the 'slant' font (this name indicates a font for lines with slope 1\/6 at the resolution of an Imagen printer). The gf_file_name on the command line must be complete. (The program prompts you for it if you don't give it.) Because the resolution is part of the extension, it would not make sense to append a default extension as is done with TeX or DVI-reading software. The output file name defaults to the same root as the gf file, with the dvi extension added. For example, the input file cmr10.2602gf would become cmr10.dvi.","Process Name":"gftodvi","Link":"https:\/\/linux.die.net\/man\/1\/gftodvi"}},{"Process":{"Description":"This manual page is not meant to be exhaustive. The complete documentation for this version of TeX can be found in the info file or manual Web2C: A TeX implementation. The gftopk program converts a generic font file output by, for example, mf(1), to a packed font file for use by DVI-reading programs. Packed font files (pk files) are much smaller than the corresponding gf files, so they are generally the font format of choice. The gf_file_name on the command line must be complete. Because the resolution is part of the extension, it would not make sense to append a default extension as is done with TeX or DVI-reading software. The pk_file_name defaults to the same (stripped) name as gf_file_name, and it is placed in the current working directory with the pk suffix replacing gf. For example, the input file cmr10.300gf would become cmr10.300pk.","Process Name":"gftopk","Link":"https:\/\/linux.die.net\/man\/1\/gftopk"}},{"Process":{"Description":null,"Process Name":"gftp","Link":"https:\/\/linux.die.net\/man\/1\/gftp"}},{"Process":{"Description":"This manual page is not meant to be exhaustive. The complete documentation for this version of TeX can be found in the info file or manual Web2C: A TeX implementation. The gftype program translates a gf (generic font) file output by, for example, mf(1), to a file that humans can read. It also serves as a gf file-validating program (i.e., if gftype can read it, it's correct) and as an example of a gf-reading program for other software that wants to read gf files. The gf_file_name on the command line must be complete. Because the resolution is part of the extension, it would not make sense to append a default extension as is done with TeX or DVI-reading software. If no output_file_name is specified, the output goes to stdout.","Process Name":"gftype","Link":"https:\/\/linux.die.net\/man\/1\/gftype"}},{"Process":{"Description":"This manual page documents briefly the ghc and ghci commands. Note that ghci is not yet available on all architectures. Extensive documentation is available in various other formats including DVI, PostScript and HTML; see below. Each of GHC's command line options is classified as either static or dynamic. A static flag may only be specified on the command line, whereas a dynamic flag may also be given in an OPTIONS pragma in a source file or set from the GHCi command-line with :set. As a rule of thumb, all the language options are dynamic, as are the warning options and the debugging options. The rest are static, with the notable exceptions of -v, -cpp, -fasm, -fvia-C, -fllvm, and \" -#include . The OPTIONS sections lists the status of each flag. Common suffixes of file names for Haskell are: .hs Haskell source code; preprocess, compile .lhs literate Haskell source; unlit, preprocess, compile .hi Interface file; contains information about exported symbols .hc intermediate C files .x_o way x object files; common ways are: p, u, s .x_hi way x interface files","Process Name":"ghc","Link":"https:\/\/linux.die.net\/man\/1\/ghc"}},{"Process":{"Description":"This manual page documents briefly the ghdl command. This manual page was written for user of man, but is not as complete as the reference documentation. Instead, users should read the GHDL texinfo manual","Process Name":"ghdl","Link":"https:\/\/linux.die.net\/man\/1\/ghdl"}},{"Process":{"Description":null,"Process Name":"ghosts","Link":"https:\/\/linux.die.net\/man\/1\/ghosts"}},{"Process":{"Description":"The gs (gswin32c, gswin32, gsos2) command invokes Ghostscript, an interpreter of Adobe Systems' PostScript(tm) and Portable Document Format (PDF) languages. gs reads \"files\" in sequence and executes them as Ghostscript programs. After doing this, it reads further input from the standard input stream (normally the keyboard), interpreting each line separately. The interpreter exits gracefully when it encounters the \"quit\" command (either in a file or from the keyboard), at end-of-file, or at an interrupt signal (such as Control-C at the keyboard). The interpreter recognizes many option switches, some of which are described below. Please see the usage documentation for complete information. Switches may appear anywhere in the command line and apply to all files thereafter. Invoking Ghostscript with the -h or -? switch produces a message which shows several useful switches, all the devices known to that executable, and the search path for fonts; on Unix it also shows the location of detailed documentation. Ghostscript may be built to use many different output devices. To see which devices your executable includes, run \"gs -h\". Unless you specify a particular device, Ghostscript normally opens the first one of those and directs output to it, so if the first one in the list is the one you want to use, just issue the command gs myfile.ps You can also check the set of available devices from within Ghostscript: invoke Ghostscript and type devicenames == but the first device on the resulting list may not be the default device you determine with \" gs -h\". To specify \"AbcXyz\" as the initial output device, include the switch -sDEVICE=AbcXyz For example, for output to an Epson printer you might use the command gs -sDEVICE=epson myfile.ps The \"-sDEVICE=\" switch must precede the first mention of a file to print, and only the switch's first use has any effect. Finally, you can specify a default device in the environment variable GS_DEVICE. The order of precedence for these alternatives from highest to lowest (Ghostscript uses the device defined highest in the list) is: Some devices can support different resolutions (densities). To specify the resolution on such a printer, use the \"-r\" switch: gs -sDEVICE=<device> -r<xres>x<yres> For example, on a 9-pin Epson-compatible printer, you get the lowest-density (fastest) mode with gs -sDEVICE=epson -r60x72 and the highest-density (best output quality) mode with gs -sDEVICE=epson -r240x72. If you select a printer as the output device, Ghostscript also allows you to choose where Ghostscript sends the output -- on Unix systems, usually to a temporary file. To send the output to a file \"foo.xyz\", use the switch -sOutputFile=foo.xyz You might want to print each page separately. To do this, send the output to a series of files \"foo1.xyz, foo2.xyz, ...\" using the \"-sOutputFile=\" switch with \"%d\" in a filename template: -sOutputFile=foo%d.xyz Each resulting file receives one page of output, and the files are numbered in sequence. \"%d\" is a printf format specification; you can also use a variant like \"%02d\". On Unix and MS Windows systems you can also send output to a pipe. For example, to pipe output to the \"lpr\" command (which, on many Unix systems, directs it to a printer), use the option -sOutputFile=%pipe%lpr Note that the '%' characters need to be doubled on MS Windows to avoid mangling by the command interpreter. You can also send output to standard output: -sOutputFile=- or -sOutputFile=%stdout% In this case you must also use the -q switch, to prevent Ghostscript from writing messages to standard output. To select a specific paper size, use the command line switch -sPAPERSIZE=<paper_size> for instance -sPAPERSIZE=a4 or -sPAPERSIZE=legal Most ISO and US paper sizes are recognized. See the usage documentation for a full list, or the definitions in the initialization file \"gs_statd.ps\". Ghostscript can do many things other than print or view PostScript and PDF files. For example, if you want to know the bounding box of a PostScript (or EPS) file, Ghostscript provides a special \"device\" that just prints out this information. For example, using one of the example files distributed with Ghostscript, gs -sDEVICE=bbox golfer.ps prints out %%BoundingBox: 0 25 583 732 %%HiResBoundingBox: 0.808497 25.009496 582.994503 731.809445","Process Name":"ghostscript","Link":"https:\/\/linux.die.net\/man\/1\/ghostscript"}},{"Process":{"Description":"Dumps information contained in GHW files for debugging purposes.","Process Name":"ghwdump","Link":"https:\/\/linux.die.net\/man\/1\/ghwdump"}},{"Process":{"Description":null,"Process Name":"gif2swf","Link":"https:\/\/linux.die.net\/man\/1\/gif2swf"}},{"Process":{"Description":"Gif2tiff converts a file in the GIF87 format to TIFF. The TIFF image is created as a palette image, with samples compressed with the Lempel-Ziv & Welch algorithm (Compression=5). These characteristics can overridden, or explicitly specified with the options described below.","Process Name":"gif2tiff","Link":"https:\/\/linux.die.net\/man\/1\/gif2tiff"}},{"Process":{"Description":"gifdiff compares two GIF files and determines if they appear identical. Differences that don't affect appearance (like colormap ordering or how much an animation is optimized) are not reported. gifdiff prints details of any differences it finds. If the GIFs are the same, it prints nothing. It exits with status 0 if there were no differences, 1 if there were some differences, and 2 if there was trouble.","Process Name":"gifdiff","Link":"https:\/\/linux.die.net\/man\/1\/gifdiff"}},{"Process":{"Description":"gifsicle is a powerful command-line program for creating, editing, manipulating, and getting information about GIF images and animations.","Process Name":"gifsicle","Link":"https:\/\/linux.die.net\/man\/1\/gifsicle"}},{"Process":{"Description":null,"Process Name":"giftcurs","Link":"https:\/\/linux.die.net\/man\/1\/giftcurs"}},{"Process":{"Description":"giftd is a modular deamon capable of abstracting the communication between the end user and specific filesharing protocols (peer-to-peer or otherwise). This giFT daemon is the main core of the system, but will not be useful without a controlling interface (known as a giFT client or interface) and a protocol plugin. Please note that the bulk of the run-time options are controlled through the daemon and plugin configuration files. See the FILES section for more information.","Process Name":"giftd","Link":"https:\/\/linux.die.net\/man\/1\/giftd"}},{"Process":{"Description":"This program is part of Netpbm(1). This is a graphics format converter from the GIF format to the PNM (i.e. PBM, PGM, or PPM) format. If the image contains only black and maximally bright white, the output is PBM. If the image contains more than those two colors, but only grays, the output is PGM. If the image contains other colors, the output is PPM. A GIF image contains rectangular pixels. They all have the same aspect ratio, but may not be square (it's actually quite unusual for them not to be square, but it could happen). The pixels of a Netpbm image are always square. Because of the engineering complexity to do otherwise, giftopnm converts a GIF image to a Netpbm image pixel-for-pixel. This means if the GIF pixels are not square, the Netpbm output image has the wrong aspect ratio. In this case, giftopnm issues an informational message telling you to run pamscale to correct the output.","Process Name":"giftopnm","Link":"https:\/\/linux.die.net\/man\/1\/giftopnm"}},{"Process":{"Description":"This manual page documents briefly the giFToxic command. This manual page was written for the Debian GNU\/Linux distribution because the original program does not have a manual page.","Process Name":"giftoxic","Link":"https:\/\/linux.die.net\/man\/1\/giftoxic"}},{"Process":{"Description":null,"Process Name":"gifview","Link":"https:\/\/linux.die.net\/man\/1\/gifview"}},{"Process":{"Description":"Gigolo is a frontend to easily manage connections to remote filesystems using GIO\/GVfs. It allows you to quickly mount a remote filesystem and manage bookmarks to such. Homepage: http:\/\/www.uvena.de\/gigolo\/","Process Name":"gigolo","Link":"https:\/\/linux.die.net\/man\/1\/gigolo"}},{"Process":{"Description":null,"Process Name":"gij","Link":"https:\/\/linux.die.net\/man\/1\/gij"}},{"Process":{"Description":"GIMP is the GNU Image Manipulation Program. It is used to edit and manipulate images. It can load and save a variety of image formats and can be used to convert between formats. GIMP can also be used as a paint program. It features a set of drawing and painting tools such as airbrush, clone, pencil, and paint brush. Painting and drawing tools can be applied to an image with a variety of paint modes. It also offers an extensive array of selection tools like rectangle, ellipse, fuzzy select, bezier select, intelligent scissors, and select by color. GIMP offers a variety of plug-ins that perform a variety of image manipulations. Examples include bumpmap, edge detect, gaussian blur, and many others. In addition, GIMP has several scripting extension which allow for advanced non-interactive processing and creation of images. GIMP ships with a second binary called gimp-console. This binary is a console-only version and behaves as if gimp was called with the --no-interface command-line option. On platforms with the D-Bus message bus system, GIMP will by default check if an instance is already running in this user session. If it detects that, it will pass all filenames given on the command-line to the already running GIMP instance and quit.","Process Name":"gimp","Link":"https:\/\/linux.die.net\/man\/1\/gimp"}},{"Process":{"Description":"The GIMP is the GNU Image Manipulation Program. It is used to edit and manipulate images. It can load and save a variety of image formats and can be used to convert between formats. GIMP can also be used as a paint program. It features a set of drawing and painting tools such as airbrush, clone, pencil, and paint brush. Painting and drawing tools can be applied to an image with a variety of paint modes. It also offers an extensive array of selection tools like rectangle, ellipse, fuzzy select, bezier select, intelligent scissors, and select by color. GIMP offers a variety of plugins that perform a variety of image manipulations. Examples include bumpmap, edge detect, gaussian blur, and many others. In addition, GIMP has several scripting extension which allow for advanced non-interactive processing and creation of images.","Process Name":"gimp-2.0","Link":"https:\/\/linux.die.net\/man\/1\/gimp-2.0"}},{"Process":{"Description":"The GIMP is the GNU Image Manipulation Program. It is used to edit and manipulate images. It can load and save a variety of image formats and can be used to convert between formats. GIMP can also be used as a paint program. It features a set of drawing and painting tools such as airbrush, clone, pencil, and paint brush. Painting and drawing tools can be applied to an image with a variety of paint modes. It also offers an extensive array of selection tools like rectangle, ellipse, fuzzy select, bezier select, intelligent scissors, and select by color. GIMP offers a variety of plugins that perform a variety of image manipulations. Examples include bumpmap, edge detect, gaussian blur, and many others. In addition, GIMP has several scripting extension which allow for advanced non-interactive processing and creation of images.","Process Name":"gimp-2.2","Link":"https:\/\/linux.die.net\/man\/1\/gimp-2.2"}},{"Process":{"Description":"GIMP is the GNU Image Manipulation Program. It is used to edit and manipulate images. It can load and save a variety of image formats and can be used to convert between formats. GIMP can also be used as a paint program. It features a set of drawing and painting tools such as airbrush, clone, pencil, and paint brush. Painting and drawing tools can be applied to an image with a variety of paint modes. It also offers an extensive array of selection tools like rectangle, ellipse, fuzzy select, bezier select, intelligent scissors, and select by color. GIMP offers a variety of plug-ins that perform a variety of image manipulations. Examples include bumpmap, edge detect, gaussian blur, and many others. In addition, GIMP has several scripting extension which allow for advanced non-interactive processing and creation of images. GIMP ships with a second binary called gimp-console. This binary is a console-only version and behaves as if gimp was called with the --no-interface command-line option. On platforms with the D-Bus message bus system, GIMP will by default check if an instance is already running in this user session. If it detects that, it will pass all filenames given on the command-line to the already running GIMP instance and quit.","Process Name":"gimp-2.6","Link":"https:\/\/linux.die.net\/man\/1\/gimp-2.6"}},{"Process":{"Description":"GIMP is the GNU Image Manipulation Program. It is used to edit and manipulate images. It can load and save a variety of image formats and can be used to convert between formats. GIMP can also be used as a paint program. It features a set of drawing and painting tools such as airbrush, clone, pencil, and paint brush. Painting and drawing tools can be applied to an image with a variety of paint modes. It also offers an extensive array of selection tools like rectangle, ellipse, fuzzy select, bezier select, intelligent scissors, and select by color. GIMP offers a variety of plug-ins that perform a variety of image manipulations. Examples include bumpmap, edge detect, gaussian blur, and many others. In addition, GIMP has several scripting extension which allow for advanced non-interactive processing and creation of images. GIMP ships with a second binary called gimp-console. This binary is a console-only version and behaves as if gimp was called with the --no-interface command-line option. On platforms with the D-Bus message bus system, GIMP will by default check if an instance is already running in this user session. If it detects that, it will pass all filenames given on the command-line to the already running GIMP instance and quit.","Process Name":"gimp-console","Link":"https:\/\/linux.die.net\/man\/1\/gimp-console"}},{"Process":{"Description":"GIMP is the GNU Image Manipulation Program. It is used to edit and manipulate images. It can load and save a variety of image formats and can be used to convert between formats. GIMP can also be used as a paint program. It features a set of drawing and painting tools such as airbrush, clone, pencil, and paint brush. Painting and drawing tools can be applied to an image with a variety of paint modes. It also offers an extensive array of selection tools like rectangle, ellipse, fuzzy select, bezier select, intelligent scissors, and select by color. GIMP offers a variety of plug-ins that perform a variety of image manipulations. Examples include bumpmap, edge detect, gaussian blur, and many others. In addition, GIMP has several scripting extension which allow for advanced non-interactive processing and creation of images. GIMP ships with a second binary called gimp-console. This binary is a console-only version and behaves as if gimp was called with the --no-interface command-line option. On platforms with the D-Bus message bus system, GIMP will by default check if an instance is already running in this user session. If it detects that, it will pass all filenames given on the command-line to the already running GIMP instance and quit.","Process Name":"gimp-console-2.6","Link":"https:\/\/linux.die.net\/man\/1\/gimp-console-2.6"}},{"Process":{"Description":"gimp-remote is a small utility that tells a running GIMP to open one or more (local or remote) image files. It does so by searching for a GIMP toolbox on the active display. If it can find a GIMP toolbox, a synthetic drop event is created which makes GIMP think the files would have been dropped onto the toolbox. More than one filename or URL can be specified on the commandline. If no GIMP window is found, gimp-remote will start a new GIMP instance and ask it to load the specified images. If no filename or URL is given, gimp-remote will start a new GIMP. This behaviour can be altered using the command-line options described below. If you are using GIMP on Linux or another platform with the D-Bus message bus system, chances are good that this functionality is already built into the main GIMP executable and that you will not need to use gimp-remote.","Process Name":"gimp-remote","Link":"https:\/\/linux.die.net\/man\/1\/gimp-remote"}},{"Process":{"Description":null,"Process Name":"gimp-remote-2.0","Link":"https:\/\/linux.die.net\/man\/1\/gimp-remote-2.0"}},{"Process":{"Description":"gimp-remote is a small utility that tells a running GIMP to open one or more (local or remote) image files. It does so by searching for a GIMP toolbox on the active display. If it can find a GIMP toolbox, a synthetic drop event is created which makes GIMP think the files would have been dropped onto the toolbox. More than one filename or URL can be specified on the commandline. If no GIMP window is found, gimp-remote will start a new GIMP instance and ask it to load the specified images. If no filename or URL is given, gimp-remote will start a new GIMP. This behaviour can be altered using the command-line options described below.","Process Name":"gimp-remote-2.2","Link":"https:\/\/linux.die.net\/man\/1\/gimp-remote-2.2"}},{"Process":{"Description":"gimp-remote is a small utility that tells a running GIMP to open one or more (local or remote) image files. It does so by searching for a GIMP toolbox on the active display. If it can find a GIMP toolbox, a synthetic drop event is created which makes GIMP think the files would have been dropped onto the toolbox. More than one filename or URL can be specified on the commandline. If no GIMP window is found, gimp-remote will start a new GIMP instance and ask it to load the specified images. If no filename or URL is given, gimp-remote will start a new GIMP. This behaviour can be altered using the command-line options described below. If you are using GIMP on Linux or another platform with the D-Bus message bus system, chances are good that this functionality is already built into the main GIMP executable and that you will not need to use gimp-remote.","Process Name":"gimp-remote-2.6","Link":"https:\/\/linux.die.net\/man\/1\/gimp-remote-2.6"}},{"Process":{"Description":null,"Process Name":"gimpprint-config","Link":"https:\/\/linux.die.net\/man\/1\/gimpprint-config"}},{"Process":{"Description":null,"Process Name":"gimptool","Link":"https:\/\/linux.die.net\/man\/1\/gimptool"}},{"Process":{"Description":"gimptool-2.0 is a tool that can, among other things, build plug-ins or scripts and install them if they are distributed in one source file. gimptool-2.0 can also be used by programs that need to know what libraries and include-paths GIMP was compiled with. gimptool-2.0 uses pkg-config for this task. For use in Makefiles, it is recommended that you use pkg-config directly instead of calling gimptool-2.0.","Process Name":"gimptool-2.0","Link":"https:\/\/linux.die.net\/man\/1\/gimptool-2.0"}},{"Process":{"Description":"indxbib makes an inverted index for the bibliographic databases in filename... for use with refer(1), lookbib(1), and lkbib(1). The index will be named filename.i; the index is written to a temporary file which is then renamed to this. If no filenames are given on the command line because the -f option has been used, and no -o option is given, the index will be named Ind.i. Bibliographic databases are divided into records by blank lines. Within a record, each fields starts with a % character at the beginning of a line. Fields have a one letter name which follows the % character. The values set by the -c, -n, -l and -t options are stored in the index; when the index is searched, keys will be discarded and truncated in a manner appropriate to these options; the original keys will be used for verifying that any record found using the index actually contains the keys. This means that a user of an index need not know whether these options were used in the creation of the index, provided that not all the keys to be searched for would have been discarded during indexing and that the user supplies at least the part of each key that would have remained after being truncated during indexing. The value set by the -i option is also stored in the index and will be used in verifying records found using the index.","Process Name":"gindxbib","Link":"https:\/\/linux.die.net\/man\/1\/gindxbib"}},{"Process":{"Description":"List service URLs and other attributes.","Process Name":"ginfo","Link":"https:\/\/linux.die.net\/man\/1\/ginfo"}},{"Process":{"Description":"ginsh is an interactive frontend for the GiNaC symbolic computation framework. It is intended as a tool for testing and experimenting with GiNaC's features, not as a replacement for traditional interactive computer algebra systems. Although it can do many things these traditional systems can do, ginsh provides no programming constructs like loops or conditional expressions. If you need this functionality you are advised to write your program in C++, using the \"native\" GiNaC class framework.","Process Name":"ginsh","Link":"https:\/\/linux.die.net\/man\/1\/ginsh"}},{"Process":{"Description":"Git is a fast, scalable, distributed revision control system with an unusually rich command set that provides both high-level operations and full access to internals. See gittutorial(7) to get started, then see Everyday Git [1] for a useful minimum set of commands, and \"man git-commandname\" for documentation of each command. CVS users may also want to read gitcvs-migration(7). See the Git User's Manual [2] for a more in-depth introduction. The COMMAND is either a name of a Git command (see below) or an alias as defined in the configuration file (see git-config(1)). Formatted and hyperlinked version of the latest git documentation can be viewed at http:\/\/www.kernel.org\/pub\/software\/scm\/git\/docs\/.","Process Name":"git","Link":"https:\/\/linux.die.net\/man\/1\/git"}},{"Process":{"Description":"This command updates the index using the current content found in the working tree, to prepare the content staged for the next commit. It typically adds the current content of existing paths as a whole, but with some options it can also be used to add content with only part of the changes made to the working tree files applied, or remove paths that do not exist in the working tree anymore. The \"index\" holds a snapshot of the content of the working tree, and it is this snapshot that is taken as the contents of the next commit. Thus after making any changes to the working directory, and before running the commit command, you must use the add command to add any new or modified files to the index. This command can be performed multiple times before a commit. It only adds the content of the specified file(s) at the time the add command is run; if you want subsequent changes included in the next commit, then you must run git add again to add the new content to the index. The git status command can be used to obtain a summary of which files have changes that are staged for the next commit. The git add command will not add ignored files by default. If any ignored files were explicitly specified on the command line, git add will fail with a list of ignored files. Ignored files reached by directory recursion or filename globbing performed by Git (quote your globs before the shell) will be silently ignored. The git add command can be used to add ignored files with the -f (force) option. Please see git-commit(1) for alternative ways to add content to a commit.","Process Name":"git-add","Link":"https:\/\/linux.die.net\/man\/1\/git-add"}},{"Process":{"Description":"Splits mail messages in a mailbox into commit log message, authorship information and patches, and applies them to the current branch.","Process Name":"git-am","Link":"https:\/\/linux.die.net\/man\/1\/git-am"}},{"Process":{"Description":"git-annex allows managing files with git, without checking the file contents into git. While that may seem paradoxical, it is useful when dealing with files larger than git can currently easily handle, whether due to limitations in memory, checksumming time, or disk space. Even without file content tracking, being able to manage files with git, move files around and delete files with versioned directory trees, and use branches and distributed clones, are all very handy reasons to use git. And annexed files can co-exist in the same git repository with regularly versioned files, which is convenient for maintaining documents, Makefiles, etc that are associated with annexed files but that benefit from full revision control. When a file is annexed, its content is moved into a key-value store, and a symlink is made that points to the content. These symlinks are checked into git and versioned like regular files. You can move them around, delete them, and so on. Pushing to another git repository will make git-annex there aware of the annexed file, and it can be used to retrieve its content from the key-value store.","Process Name":"git-annex","Link":"https:\/\/linux.die.net\/man\/1\/git-annex"}},{"Process":{"Description":"git-annex-shell is a restricted shell, similar to git-shell, which can be used as a login shell for SSH accounts. Since its syntax is identical to git-shell's, it can be used as a drop-in replacement anywhere git-shell is used. For example it can be used as a user's restricted login shell.","Process Name":"git-annex-shell","Link":"https:\/\/linux.die.net\/man\/1\/git-annex-shell"}},{"Process":{"Description":"Annotates each line in the given file with information from the commit which introduced the line. Optionally annotates from a given revision. The only difference between this command and git-blame(1) is that they use slightly different output formats, and this command exists only for backward compatibility to support existing scripts, and provide a more familiar command name for people coming from other SCM systems.","Process Name":"git-annotate","Link":"https:\/\/linux.die.net\/man\/1\/git-annotate"}},{"Process":{"Description":"Reads the supplied diff output (i.e. \"a patch\") and applies it to files. With the --index option the patch is also applied to the index, and with the --cache option the patch is only applied to the index. Without these options, the command applies the patch only to files, and does not require them to be in a git repository.","Process Name":"git-apply","Link":"https:\/\/linux.die.net\/man\/1\/git-apply"}},{"Process":{"Description":null,"Process Name":"git-archimport","Link":"https:\/\/linux.die.net\/man\/1\/git-archimport"}},{"Process":{"Description":"Creates an archive of the specified format containing the tree structure for the named tree, and writes it out to the standard output. If <prefix> is specified it is prepended to the filenames in the archive. git archive behaves differently when given a tree ID versus when given a commit ID or tag ID. In the first case the current time is used as the modification time of each file in the archive. In the latter case the commit time as recorded in the referenced commit object is used instead. Additionally the commit ID is stored in a global extended pax header if the tar format is used; it can be extracted using git get-tar-commit-id. In ZIP files it is stored as a file comment.","Process Name":"git-archive","Link":"https:\/\/linux.die.net\/man\/1\/git-archive"}},{"Process":{"Description":null,"Process Name":"git-bisect","Link":"https:\/\/linux.die.net\/man\/1\/git-bisect"}},{"Process":{"Description":"Annotates each line in the given file with information from the revision which last modified the line. Optionally, start annotating from the given revision. The command can also limit the range of lines annotated. The report does not tell you anything about lines which have been deleted or replaced; you need to use a tool such as git diff or the \"pickaxe\" interface briefly mentioned in the following paragraph. Apart from supporting file annotation, git also supports searching the development history for when a code snippet occurred in a change. This makes it possible to track when a code snippet was added to a file, moved or copied between files, and eventually deleted or replaced. It works by searching for a text string in the diff. A small example: $ git log --pretty=oneline -S'blame_usage'\n5040f17eba15504bad66b14a645bddd9b015ebb7 blame -S <ancestry-file>\nea4c7f9bf69e781dd0cd88d2bccb2bf5cc15c9a7 git-blame: Make the output","Process Name":"git-blame","Link":"https:\/\/linux.die.net\/man\/1\/git-blame"}},{"Process":{"Description":"With no arguments, existing branches are listed and the current branch will be highlighted with an asterisk. Option -r causes the remote-tracking branches to be listed, and option -a shows both. With --contains, shows only the branches that contain the named commit (in other words, the branches whose tip commits are descendants of the named commit). With --merged, only branches merged into the named commit (i.e. the branches whose tip commits are reachable from the named commit) will be listed. With --no-merged only branches not merged into the named commit will be listed. If the <commit> argument is missing it defaults to HEAD (i.e. the tip of the current branch). The command's second form creates a new branch head named <branchname> which points to the current HEAD, or <start-point> if given. Note that this will create the new branch, but it will not switch the working tree to it; use \"git checkout <newbranch>\" to switch to the new branch. When a local branch is started off a remote branch, git sets up the branch so that git pull will appropriately merge from the remote branch. This behavior may be changed via the global branch.autosetupmerge configuration flag. That setting can be overridden by using the --track and --no-track options. With a -m or -M option, <oldbranch> will be renamed to <newbranch>. If <oldbranch> had a corresponding reflog, it is renamed to match <newbranch>, and a reflog entry is created to remember the branch renaming. If <newbranch> exists, -M must be used to force the rename to happen. With a -d or -D option, <branchname> will be deleted. You may specify more than one branch for deletion. If the branch currently has a reflog then the reflog will also be deleted. Use -r together with -d to delete remote-tracking branches. Note, that it only makes sense to delete remote-tracking branches if they no longer exist in the remote repository or if git fetch was configured not to fetch them again. See also the prune subcommand of git-remote(1) for a way to clean up all obsolete remote-tracking branches.","Process Name":"git-branch","Link":"https:\/\/linux.die.net\/man\/1\/git-branch"}},{"Process":{"Description":null,"Process Name":"git-bundle","Link":"https:\/\/linux.die.net\/man\/1\/git-bundle"}},{"Process":{"Description":"In its first form, the command provides the content or the type of an object in the repository. The type is required unless -t or -p is used to find the object type, or -s is used to find the object size. In the second form, a list of objects (separated by linefeeds) is provided on stdin, and the SHA1, type, and size of each object is printed on stdout.","Process Name":"git-cat-file","Link":"https:\/\/linux.die.net\/man\/1\/git-cat-file"}},{"Process":{"Description":null,"Process Name":"git-check-attr","Link":"https:\/\/linux.die.net\/man\/1\/git-check-attr"}},{"Process":{"Description":null,"Process Name":"git-check-ref-format","Link":"https:\/\/linux.die.net\/man\/1\/git-check-ref-format"}},{"Process":{"Description":"When <paths> are not given, this command switches branches by updating the index, working tree, and HEAD to reflect the specified branch. If -b is given, a new branch is created and checked out, as if git-branch(1) were called; in this case you can use the --track or --no-track options, which will be passed to git branch. As a convenience, --track without -b implies branch creation; see the description of --track below. When <paths> or --patch are given, this command does not switch branches. It updates the named paths in the working tree from the index file, or from a named <tree-ish> (most often a commit). In this case, the -b and --track options are meaningless and giving either of them results in an error. The <tree-ish> argument can be used to specify a specific tree-ish (i.e. commit, tag or tree) to update the index for the given paths before updating the working tree. The index may contain unmerged entries after a failed merge. By default, if you try to check out such an entry from the index, the checkout operation will fail and nothing will be checked out. Using -f will ignore these unmerged entries. The contents from a specific side of the merge can be checked out of the index by using --ours or --theirs. With -m, changes made to the working tree file can be discarded to recreate the original conflicted merge result.","Process Name":"git-checkout","Link":"https:\/\/linux.die.net\/man\/1\/git-checkout"}},{"Process":{"Description":"Will copy all files listed from the index to the working directory (not overwriting existing files).","Process Name":"git-checkout-index","Link":"https:\/\/linux.die.net\/man\/1\/git-checkout-index"}},{"Process":{"Description":"The changeset (or \"diff\") of each commit between the fork-point and <head> is compared against each commit between the fork-point and <upstream>. The commits are compared with their patch id, obtained from the git patch-id program. Every commit that doesn't exist in the <upstream> branch has its id (sha1) reported, prefixed by a symbol. The ones that have equivalent change already in the <upstream> branch are prefixed with a minus (-) sign, and those that only exist in the <head> branch are prefixed with a plus (+) symbol:            __*__*__*__*__> <upstream>\n          \/\nfork-point\n          \\__+__+__-__+__+__-__+__> <head> If a <limit> has been given then the commits along the <head> branch up to and including <limit> are not reported:            __*__*__*__*__> <upstream>\n          \/\nfork-point\n          \\__*__*__<limit>__-__+__> <head> Because git cherry compares the changeset rather than the commit id (sha1), you can use git cherry to find out if a commit you made locally has been applied <upstream> under a different commit id. For example, this will happen if you're feeding patches <upstream> via email rather than pushing or pulling commits directly.","Process Name":"git-cherry","Link":"https:\/\/linux.die.net\/man\/1\/git-cherry"}},{"Process":{"Description":null,"Process Name":"git-cherry-pick","Link":"https:\/\/linux.die.net\/man\/1\/git-cherry-pick"}},{"Process":{"Description":null,"Process Name":"git-citool","Link":"https:\/\/linux.die.net\/man\/1\/git-citool"}},{"Process":{"Description":"Cleans the working tree by recursively removing files that are not under version control, starting from the current directory. Normally, only files unknown to git are removed, but if the -x option is specified, ignored files are also removed. This can, for example, be useful to remove all build products. If any optional <path>... arguments are given, only those paths are affected.","Process Name":"git-clean","Link":"https:\/\/linux.die.net\/man\/1\/git-clean"}},{"Process":{"Description":"Clones a repository into a newly created directory, creates remote-tracking branches for each branch in the cloned repository (visible using git branch -r), and creates and checks out an initial branch that is forked from the cloned repository's currently active branch. After the clone, a plain git fetch without arguments will update all the remote-tracking branches, and a git pull without arguments will in addition merge the remote master branch into the current master branch, if any. This default configuration is achieved by creating references to the remote branch heads under refs\/remotes\/origin and by initializing remote.origin.url and remote.origin.fetch configuration variables.","Process Name":"git-clone","Link":"https:\/\/linux.die.net\/man\/1\/git-clone"}},{"Process":{"Description":"Stores the current contents of the index in a new commit along with a log message from the user describing the changes. The content to be added can be specified in several ways: 1. by using git add to incrementally \"add\" changes to the index before using the commit command (Note: even modified files must be \"added\"); 2. by using git rm to remove files from the working tree and the index, again before using the commit command; 3. by listing files as arguments to the commit command, in which case the commit will ignore changes staged in the index, and instead record the current content of the listed files (which must already be known to git); 4. by using the -a switch with the commit command to automatically \"add\" changes from all known files (i.e. all files that are already listed in the index) and to automatically \"rm\" files in the index that have been removed from the working tree, and then perform the actual commit; 5. by using the --interactive switch with the commit command to decide one by one which files should be part of the commit, before finalizing the operation. Currently, this is done by invoking git add --interactive. The --dry-run option can be used to obtain a summary of what is included by any of the above for the next commit by giving the same set of parameters (options and paths). If you make a commit and then find a mistake immediately after that, you can recover from it with git reset.","Process Name":"git-commit","Link":"https:\/\/linux.die.net\/man\/1\/git-commit"}},{"Process":{"Description":null,"Process Name":"git-commit-tree","Link":"https:\/\/linux.die.net\/man\/1\/git-commit-tree"}},{"Process":{"Description":"You can query\/set\/replace\/unset options with this command. The name is actually the section and the key separated by a dot, and the value will be escaped. Multiple lines can be added to an option by using the --add option. If you want to update or unset an option which can occur on multiple lines, a POSIX regexp value_regex needs to be given. Only the existing values that match the regexp are updated or unset. If you want to handle the lines that do not match the regex, just prepend a single exclamation mark in front (see also the section called \"EXAMPLES\"). The type specifier can be either --int or --bool, to make git config ensure that the variable(s) are of the given type and convert the value to the canonical form (simple decimal number for int, a \"true\" or \"false\" string for bool), or --path, which does some path expansion (see --path below). If no type specifier is passed, no checks or transformations are performed on the value. The file-option can be one of --system, --global or --file which specify where the values will be read from or written to. The default is to assume the config file of the current repository, .git\/config unless defined otherwise with GIT_DIR and GIT_CONFIG (see the section called \"FILES\"). This command will fail if: 1. The config file is invalid, 2. Can not write to the config file, 3. no section was provided, 4. the section or key is invalid, 5. you try to unset an option which does not exist, 6. you try to unset\/set an option for which multiple lines match, or 7. you use --global option without $HOME being properly set.","Process Name":"git-config","Link":"https:\/\/linux.die.net\/man\/1\/git-config"}},{"Process":{"Description":"This counts the number of unpacked object files and disk space consumed by them, to help you decide when it is a good time to repack.","Process Name":"git-count-objects","Link":"https:\/\/linux.die.net\/man\/1\/git-count-objects"}},{"Process":{"Description":"Exports a commit from GIT to a CVS checkout, making it easier to merge patches from a git repository into a CVS repository. Specify the name of a CVS checkout using the -w switch or execute it from the root of the CVS working copy. In the latter case GIT_DIR must be defined. See examples below. It does its best to do the safe thing, it will check that the files are unchanged and up to date in the CVS checkout, and it will not autocommit by default. Supports file additions, removals, and commits that affect binary files. If the commit is a merge commit, you must tell git cvsexportcommit what parent the changeset should be done against.","Process Name":"git-cvsexportcommit","Link":"https:\/\/linux.die.net\/man\/1\/git-cvsexportcommit"}},{"Process":{"Description":null,"Process Name":"git-cvsimport","Link":"https:\/\/linux.die.net\/man\/1\/git-cvsimport"}},{"Process":{"Description":"This application is a CVS emulation layer for git. It is highly functional. However, not all methods are implemented, and for those methods that are implemented, not all switches are implemented. Testing has been done using both the CLI CVS client, and the Eclipse CVS plugin. Most functionality works fine with both of these clients.","Process Name":"git-cvsserver","Link":"https:\/\/linux.die.net\/man\/1\/git-cvsserver"}},{"Process":{"Description":"A really simple TCP git daemon that normally listens on port \"DEFAULT_GIT_PORT\" aka 9418. It waits for a connection asking for a service, and will serve that service if it is enabled. It verifies that the directory has the magic file \"git-daemon-export-ok\", and it will refuse to export any git directory that hasn't explicitly been marked for export this way (unless the --export-all parameter is specified). If you pass some directory paths as git daemon arguments, you can further restrict the offers to a whitelist comprising of those. By default, only upload-pack service is enabled, which serves git fetch-pack and git ls-remote clients, which are invoked from git fetch, git pull, and git clone. This is ideally suited for read-only updates, i.e., pulling from git repositories. An upload-archive also exists to serve git archive.","Process Name":"git-daemon","Link":"https:\/\/linux.die.net\/man\/1\/git-daemon"}},{"Process":{"Description":null,"Process Name":"git-describe","Link":"https:\/\/linux.die.net\/man\/1\/git-describe"}},{"Process":{"Description":"Show changes between two trees, a tree and the working tree, a tree and the index file, or the index file and the working tree. git diff [--options] [--] [<path>...] This form is to view the changes you made relative to the index (staging area for the next commit). In other words, the differences are what you could tell git to further add to the index but you still haven't. You can stage these changes by using git-add(1). If exactly two paths are given, and at least one is untracked, compare the two files \/ directories. This behavior can be forced by --no-index. git diff [--options] --cached [<commit>] [--] [<path>...] This form is to view the changes you staged for the next commit relative to the named <commit>. Typically you would want comparison with the latest commit, so if you do not give <commit>, it defaults to HEAD. --staged is a synonym of --cached. git diff [--options] <commit> [--] [<path>...] This form is to view the changes you have in your working tree relative to the named <commit>. You can use HEAD to compare it with the latest commit, or a branch name to compare with the tip of a different branch. git diff [--options] <commit> <commit> [--] [<path>...] This is to view the changes between two arbitrary <commit>. git diff [--options] <commit>..<commit> [--] [<path>...] This is synonymous to the previous form. If <commit> on one side is omitted, it will have the same effect as using HEAD instead. git diff [--options] <commit>...<commit> [--] [<path>...] This form is to view the changes on the branch containing and up to the second <commit>, starting at a common ancestor of both <commit>. \"git diff A...B\" is equivalent to \"git diff $(git-merge-base A B) B\". You can omit any one of <commit>, which has the same effect as using HEAD instead. Just in case if you are doing something exotic, it should be noted that all of the <commit> in the above description, except for the last two forms that use \"..\" notations, can be any <tree-ish>. For a more complete list of ways to spell <commit>, see \"SPECIFYING REVISIONS\" section in git-rev-parse(1). However, \"diff\" is about comparing two endpoints, not ranges, and the range notations (\"<commit>..<commit>\" and \"<commit>...<commit>\") do not mean a range as defined in the \"SPECIFYING RANGES\" section in git-rev-parse(1).","Process Name":"git-diff","Link":"https:\/\/linux.die.net\/man\/1\/git-diff"}},{"Process":{"Description":null,"Process Name":"git-diff-files","Link":"https:\/\/linux.die.net\/man\/1\/git-diff-files"}},{"Process":{"Description":"Compares the content and mode of the blobs found via a tree object with the content of the current index and, optionally ignoring the stat state of the file on disk. When paths are specified, compares only those named paths. Otherwise all entries in the index are compared.","Process Name":"git-diff-index","Link":"https:\/\/linux.die.net\/man\/1\/git-diff-index"}},{"Process":{"Description":"Compares the content and mode of the blobs found via two tree objects. If there is only one <tree-ish> given, the commit is compared with its parents (see --stdin below). Note that git diff-tree can use the tree encapsulated in a commit object.","Process Name":"git-diff-tree","Link":"https:\/\/linux.die.net\/man\/1\/git-diff-tree"}},{"Process":{"Description":"git difftool is a git command that allows you to compare and edit files between revisions using common diff tools. git difftool is a frontend to git diff and accepts the same options and arguments.","Process Name":"git-difftool","Link":"https:\/\/linux.die.net\/man\/1\/git-difftool"}},{"Process":{"Description":null,"Process Name":"git-fast-export","Link":"https:\/\/linux.die.net\/man\/1\/git-fast-export"}},{"Process":{"Description":"This program is usually not what the end user wants to run directly. Most end users want to use one of the existing frontend programs, which parses a specific type of foreign source and feeds the contents stored there to git fast-import. fast-import reads a mixed command\/data stream from standard input and writes one or more packfiles directly into the current repository. When EOF is received on standard input, fast import writes out updated branch and tag refs, fully updating the current repository with the newly imported data. The fast-import backend itself can import into an empty repository (one that has already been initialized by git init) or incrementally update an existing populated repository. Whether or not incremental imports are supported from a particular foreign source depends on the frontend program in use.","Process Name":"git-fast-import","Link":"https:\/\/linux.die.net\/man\/1\/git-fast-import"}},{"Process":{"Description":"Fetches named heads or tags from one or more other repositories, along with the objects necessary to complete them. The ref names and their object names of fetched refs are stored in .git\/FETCH_HEAD. This information is left for a later merge operation done by git merge. When <refspec> stores the fetched result in tracking branches, the tags that point at these branches are automatically followed. This is done by first fetching from the remote using the given <refspec>s, and if the repository has objects that are pointed by remote tags that it does not yet have, then fetch those missing tags. If the other end has tags that point at branches you are not interested in, you will not get them. git fetch can fetch from either a single named repository, or or from several repositories at once if <group> is given and there is a remotes.<group> entry in the configuration file. (See git-config(1)).","Process Name":"git-fetch","Link":"https:\/\/linux.die.net\/man\/1\/git-fetch"}},{"Process":{"Description":"Usually you would want to use git fetch, which is a higher level wrapper of this command, instead. Invokes git-upload-pack on a possibly remote repository and asks it to send objects missing from this repository, to update the named heads. The list of commits available locally is found out by scanning the local refs\/ hierarchy and sent to git-upload-pack running on the other end. This command degenerates to download everything to complete the asked refs from the remote side when the local side does not have a common ancestor commit.","Process Name":"git-fetch-pack","Link":"https:\/\/linux.die.net\/man\/1\/git-fetch-pack"}},{"Process":{"Description":"Lets you rewrite git revision history by rewriting the branches mentioned in the <rev-list options>, applying custom filters on each revision. Those filters can modify each tree (e.g. removing a file or running a perl rewrite on all files) or information about each commit. Otherwise, all information (including original commit times or merge information) will be preserved. The command will only rewrite the positive refs mentioned in the command line (e.g. if you pass a..b, only b will be rewritten). If you specify no filters, the commits will be recommitted without any changes, which would normally have no effect. Nevertheless, this may be useful in the future for compensating for some git bugs or such, therefore such a usage is permitted. NOTE: This command honors .git\/info\/grafts. If you have any grafts defined, running this command will make them permanent. WARNING! The rewritten history will have different object names for all the objects and will not converge with the original branch. You will not be able to easily push and distribute the rewritten branch on top of the original branch. Please do not use this command if you do not know the full implications, and avoid using it anyway, if a simple single commit would suffice to fix your problem. (See the \"RECOVERING FROM UPSTREAM REBASE\" section in git-rebase(1) for further information about rewriting published history.) Always verify that the rewritten version is correct: The original refs, if different from the rewritten ones, will be stored in the namespace refs\/original\/. Note that since this operation is very I\/O expensive, it might be a good idea to redirect the temporary directory off-disk with the -d option, e.g. on tmpfs. Reportedly the speedup is very noticeable. Filters The filters are applied in the order as listed below. The <command> argument is always evaluated in the shell context using the eval command (with the notable exception of the commit filter, for technical reasons). Prior to that, the $GIT_COMMIT environment variable will be set to contain the id of the commit being rewritten. Also, GIT_AUTHOR_NAME, GIT_AUTHOR_EMAIL, GIT_AUTHOR_DATE, GIT_COMMITTER_NAME, GIT_COMMITTER_EMAIL, and GIT_COMMITTER_DATE are set according to the current commit. The values of these variables after the filters have run, are used for the new commit. If any evaluation of <command> returns a non-zero exit status, the whole operation will be aborted. A map function is available that takes an \"original sha1 id\" argument and outputs a \"rewritten sha1 id\" if the commit has been already rewritten, and \"original sha1 id\" otherwise; the map function can return several ids on separate lines if your commit filter emitted multiple commits.","Process Name":"git-filter-branch","Link":"https:\/\/linux.die.net\/man\/1\/git-filter-branch"}},{"Process":{"Description":"Takes the list of merged objects on stdin and produces a suitable commit message to be used for the merge commit, usually to be passed as the <merge-message> argument of git merge. This command is intended mostly for internal use by scripts automatically invoking git merge.","Process Name":"git-fmt-merge-msg","Link":"https:\/\/linux.die.net\/man\/1\/git-fmt-merge-msg"}},{"Process":{"Description":"Iterate over all refs that match <pattern> and show them according to the given <format>, after sorting them according to the given set of <key>. If <count> is given, stop after showing that many refs. The interpolated values in <format> can optionally be quoted as string literals in the specified host language allowing their direct evaluation in that language.","Process Name":"git-for-each-ref","Link":"https:\/\/linux.die.net\/man\/1\/git-for-each-ref"}},{"Process":{"Description":"Prepare each commit with its patch in one file per commit, formatted to resemble UNIX mailbox format. The output of this command is convenient for e-mail submission or for use with git am. There are two ways to specify which commits to operate on. 1. A single commit, <since>, specifies that the commits leading to the tip of the current branch that are not in the history that leads to the <since> to be output. 2. Generic <revision range> expression (see \"SPECIFYING REVISIONS\" section in git-rev-parse(1)) means the commits in the specified range. The first rule takes precedence in the case of a single <commit>. To apply the second rule, i.e., format everything since the beginning of history up until <commit>, use the --root option: git format-patch --root <commit>. If you want to format only <commit> itself, you can do this with git format-patch -1 <commit>. By default, each output file is numbered sequentially from 1, and uses the first line of the commit message (massaged for pathname safety) as the filename. With the --numbered-files option, the output file names will only be numbers, without the first line of the commit appended. The names of the output files are printed to standard output, unless the --stdout option is specified. If -o is specified, output files are created in <dir>. Otherwise they are created in the current working directory. By default, the subject of a single patch is \"[PATCH] First Line\" and the subject when multiple patches are output is \"[PATCH n\/m] First Line\". To force 1\/1 to be added for a single patch, use -n. To omit patch numbers from the subject, use -N. If given --thread, git-format-patch will generate In-Reply-To and References headers to make the second and subsequent patch mails appear as replies to the first mail; this also generates a Message-Id header to reference.","Process Name":"git-format-patch","Link":"https:\/\/linux.die.net\/man\/1\/git-format-patch"}},{"Process":{"Description":"Verifies the connectivity and validity of the objects in the database.","Process Name":"git-fsck","Link":"https:\/\/linux.die.net\/man\/1\/git-fsck"}},{"Process":{"Description":"This is a synonym for git-fsck(1). Please refer to the documentation of that command. Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"git-fsck-objects","Link":"https:\/\/linux.die.net\/man\/1\/git-fsck-objects"}},{"Process":{"Description":"Runs a number of housekeeping tasks within the current repository, such as compressing file revisions (to reduce disk space and increase performance) and removing unreachable objects which may have been created from prior invocations of git add. Users are encouraged to run this task on a regular basis within each repository to maintain good disk space utilization and good operating performance. Some git commands may automatically run git gc; see the --auto flag below for details. If you know what you're doing and all you want is to disable this behavior permanently without further considerations, just do: $ git config --global gc.auto 0","Process Name":"git-gc","Link":"https:\/\/linux.die.net\/man\/1\/git-gc"}},{"Process":{"Description":null,"Process Name":"git-get-tar-commit-id","Link":"https:\/\/linux.die.net\/man\/1\/git-get-tar-commit-id"}},{"Process":{"Description":"Look for specified patterns in the tracked files in the work tree, blobs registered in the index file, or blobs in given tree objects.","Process Name":"git-grep","Link":"https:\/\/linux.die.net\/man\/1\/git-grep"}},{"Process":{"Description":"A Tcl\/Tk based graphical user interface to Git. git gui focuses on allowing users to make changes to their repository by making new commits, amending existing ones, creating branches, performing local merges, and fetching\/pushing to remote repositories. Unlike gitk, git gui focuses on commit generation and single file annotation and does not show project history. It does however supply menu actions to start a gitk session from within git gui. git gui is known to work on all popular UNIX systems, Mac OS X, and Windows (under both Cygwin and MSYS). To the extent possible OS specific user interface guidelines are followed, making git gui a fairly native interface for users.","Process Name":"git-gui","Link":"https:\/\/linux.die.net\/man\/1\/git-gui"}},{"Process":{"Description":null,"Process Name":"git-hash-object","Link":"https:\/\/linux.die.net\/man\/1\/git-hash-object"}},{"Process":{"Description":"With no options and no COMMAND given, the synopsis of the git command and a list of the most commonly used git commands are printed on the standard output. If the option --all or -a is given, then all available commands are printed on the standard output. If a git command is named, a manual page for that command is brought up. The man program is used by default for this purpose, but this can be overridden by other options or configuration variables. Note that git --help ... is identical to git help ... because the former is internally converted into the latter.","Process Name":"git-help","Link":"https:\/\/linux.die.net\/man\/1\/git-help"}},{"Process":{"Description":"A simple CGI program to serve the contents of a Git repository to Git clients accessing the repository over http:\/\/ and https:\/\/ protocols. The program supports clients fetching using both the smart HTTP protocol and the backwards-compatible dumb HTTP protocol, as well as clients pushing using the smart HTTP protocol. It verifies that the directory has the magic file \"git-daemon-export-ok\", and it will refuse to export any git directory that hasn't explicitly been marked for export this way (unless the GIT_HTTP_EXPORT_ALL environmental variable is set). By default, only the upload-pack service is enabled, which serves git fetch-pack and git ls-remote clients, which are invoked from git fetch, git pull, and git clone. If the client is authenticated, the receive-pack service is enabled, which serves git send-pack clients, which is invoked from git push.","Process Name":"git-http-backend","Link":"https:\/\/linux.die.net\/man\/1\/git-http-backend"}},{"Process":{"Description":"Downloads a remote git repository via HTTP.","Process Name":"git-http-fetch","Link":"https:\/\/linux.die.net\/man\/1\/git-http-fetch"}},{"Process":{"Description":null,"Process Name":"git-http-push","Link":"https:\/\/linux.die.net\/man\/1\/git-http-push"}},{"Process":{"Description":"This command uploads a mailbox generated with git format-patch into an IMAP drafts folder. This allows patches to be sent as other email is when using mail clients that cannot read mailbox files directly. The command also works with any general mailbox in which emails have the fields \"From\", \"Date\", and \"Subject\" in that order. Typical usage is something like: git format-patch --signoff --stdout --attach origin | git imap-send","Process Name":"git-imap-send","Link":"https:\/\/linux.die.net\/man\/1\/git-imap-send"}},{"Process":{"Description":"Reads a packed archive (.pack) from the specified file, and builds a pack index file (.idx) for it. The packed archive together with the pack index can then be placed in the objects\/pack\/ directory of a git repository.","Process Name":"git-index-pack","Link":"https:\/\/linux.die.net\/man\/1\/git-index-pack"}},{"Process":{"Description":null,"Process Name":"git-init","Link":"https:\/\/linux.die.net\/man\/1\/git-init"}},{"Process":{"Description":"This is a synonym for git-init(1). Please refer to the documentation of that command. Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"git-init-db","Link":"https:\/\/linux.die.net\/man\/1\/git-init-db"}},{"Process":{"Description":"A simple script to set up gitweb and a web server for browsing the local repository.","Process Name":"git-instaweb","Link":"https:\/\/linux.die.net\/man\/1\/git-instaweb"}},{"Process":{"Description":null,"Process Name":"git-log","Link":"https:\/\/linux.die.net\/man\/1\/git-log"}},{"Process":{"Description":"NOTE: this command is deprecated. Use git-fsck(1) with the option --lost-found instead. Finds dangling commits and tags from the object database, and creates refs to them in the .git\/lost-found\/ directory. Commits and tags that dereference to commits are stored in .git\/lost-found\/commit, and other objects are stored in .git\/lost-found\/other.","Process Name":"git-lost-found","Link":"https:\/\/linux.die.net\/man\/1\/git-lost-found"}},{"Process":{"Description":"This merges the file listing in the directory cache index with the actual working directory list, and shows different combinations of the two. One or more of the options below may be used to determine the files shown:","Process Name":"git-ls-files","Link":"https:\/\/linux.die.net\/man\/1\/git-ls-files"}},{"Process":{"Description":null,"Process Name":"git-ls-remote","Link":"https:\/\/linux.die.net\/man\/1\/git-ls-remote"}},{"Process":{"Description":"Lists the contents of a given tree object, like what \"\/bin\/ls -a\" does in the current working directory. Note that: \u2022 the behaviour is slightly different from that of \"\/bin\/ls\" in that the paths denote just a list of patterns to match, e.g. so specifying directory name (without -r) will behave differently, and order of the arguments does not matter. \u2022 the behaviour is similar to that of \"\/bin\/ls\" in that the paths is taken as relative to the current working directory. E.g. when you are in a directory sub that has a directory dir, you can run git ls-tree -r HEAD dir to list the contents of the tree (that is sub\/dir in HEAD). You don't want to give a tree that is not at the root level (e.g. git ls-tree -r HEAD:sub dir) in this case, as that would result in asking for sub\/sub\/dir in the HEAD commit. However, the current working directory can be ignored by passing --full-tree option.","Process Name":"git-ls-tree","Link":"https:\/\/linux.die.net\/man\/1\/git-ls-tree"}},{"Process":{"Description":"Reads a single e-mail message from the standard input, and writes the commit log message in <msg> file, and the patches in <patch> file. The author name, e-mail and e-mail subject are written out to the standard output to be used by git am to create a commit. It is usually not necessary to use this command directly. See git-am(1) instead.","Process Name":"git-mailinfo","Link":"https:\/\/linux.die.net\/man\/1\/git-mailinfo"}},{"Process":{"Description":null,"Process Name":"git-mailsplit","Link":"https:\/\/linux.die.net\/man\/1\/git-mailsplit"}},{"Process":{"Description":"Incorporates changes from the named commits (since the time their histories diverged from the current branch) into the current branch. This command is used by git pull to incorporate changes from another repository and can be used by hand to merge changes from one branch into another. Assume the following history exists and the current branch is \"master\":       A---B---C topic\n     \/\nD---E---F---G master Then \"git merge topic\" will replay the changes made on the topic branch since it diverged from master (i.e., E) until its current commit (C) on top of master, and record the result in a new commit along with the names of the two parent commits and a log message from the user describing the changes.       A---B---C topic\n     \/         \\\nD---E---F---G---H master The second syntax (<msg> HEAD <commit>...) is supported for historical reasons. Do not use it from the command line or in new scripts. It is the same as git merge -m <msg> <commit>.... Warning: Running git merge with uncommitted changes is discouraged: while possible, it leaves you in a state that is hard to back out of in the case of a conflict.","Process Name":"git-merge","Link":"https:\/\/linux.die.net\/man\/1\/git-merge"}},{"Process":{"Description":"git merge-base finds best common ancestor(s) between two commits to use in a three-way merge. One common ancestor is better than another common ancestor if the latter is an ancestor of the former. A common ancestor that does not have any better common ancestor is a best common ancestor, i.e. a merge base. Note that there can be more than one merge base for a pair of commits. Among the two commits to compute the merge base from, one is specified by the first commit argument on the command line; the other commit is a (possibly hypothetical) commit that is a merge across all the remaining commits on the command line. As the most common special case, specifying only two commits on the command line means computing the merge base between the given two commits. As a consequence, the merge base is not necessarily contained in each of the commit arguments if more than two commits are specified. This is different from git-show-branch(1) when used with the --merge-base option.","Process Name":"git-merge-base","Link":"https:\/\/linux.die.net\/man\/1\/git-merge-base"}},{"Process":{"Description":null,"Process Name":"git-merge-file","Link":"https:\/\/linux.die.net\/man\/1\/git-merge-file"}},{"Process":{"Description":"This looks up the <file>(s) in the index and, if there are any merge entries, passes the SHA1 hash for those files as arguments 1, 2, 3 (empty argument if no file), and <file> as argument 4. File modes for the three files are passed as arguments 5, 6 and 7.","Process Name":"git-merge-index","Link":"https:\/\/linux.die.net\/man\/1\/git-merge-index"}},{"Process":{"Description":null,"Process Name":"git-merge-one-file","Link":"https:\/\/linux.die.net\/man\/1\/git-merge-one-file"}},{"Process":{"Description":"Reads three treeish, and output trivial merge results and conflicting stages to the standard output. This is similar to what three-way git read-tree -m does, but instead of storing the results in the index, the command outputs the entries to the standard output. This is meant to be used by higher level scripts to compute merge results outside of the index, and stuff the results back into the index. For this reason, the output from the command omits entries that match the <branch1> tree.","Process Name":"git-merge-tree","Link":"https:\/\/linux.die.net\/man\/1\/git-merge-tree"}},{"Process":{"Description":"Use git mergetool to run one of several merge utilities to resolve merge conflicts. It is typically run after git merge. If one or more <file> parameters are given, the merge tool program will be run to resolve differences on each file. If no <file> names are specified, git mergetool will run the merge tool program on every file with merge conflicts.","Process Name":"git-mergetool","Link":"https:\/\/linux.die.net\/man\/1\/git-mergetool"}},{"Process":{"Description":null,"Process Name":"git-mergetool--lib","Link":"https:\/\/linux.die.net\/man\/1\/git-mergetool--lib"}},{"Process":{"Description":"Reads a tag contents on standard input and creates a tag object that can also be used to sign other objects. The output is the new tag's <object> identifier.","Process Name":"git-mktag","Link":"https:\/\/linux.die.net\/man\/1\/git-mktag"}},{"Process":{"Description":"Reads standard input in non-recursive ls-tree output format, and creates a tree object. The order of the tree entries is normalised by mktree so pre-sorting the input is not required. The object name of the tree object built is written to the standard output.","Process Name":"git-mktree","Link":"https:\/\/linux.die.net\/man\/1\/git-mktree"}},{"Process":{"Description":null,"Process Name":"git-mv","Link":"https:\/\/linux.die.net\/man\/1\/git-mv"}},{"Process":{"Description":"Finds symbolic names suitable for human digestion for revisions given in any format parsable by git rev-parse.","Process Name":"git-name-rev","Link":"https:\/\/linux.die.net\/man\/1\/git-name-rev"}},{"Process":{"Description":"This command allows you to add\/remove notes to\/from objects, without changing the objects themselves. A typical use of notes is to extend a commit message without having to change the commit itself. Such commit notes can be shown by git log along with the original commit message. To discern these notes from the message stored in the commit object, the notes are indented like the message, after an unindented line saying \"Notes (<refname>):\" (or \"Notes:\" for the default setting). This command always manipulates the notes specified in \"core.notesRef\" (see git-config(1)), which can be overridden by GIT_NOTES_REF. To change which notes are shown by git-log, see the \"notes.displayRef\" configuration. See the description of \"notes.rewrite.<command>\" in git-config(1) for a way of carrying your notes across commands that rewrite commits.","Process Name":"git-notes","Link":"https:\/\/linux.die.net\/man\/1\/git-notes"}},{"Process":{"Description":null,"Process Name":"git-pack-objects","Link":"https:\/\/linux.die.net\/man\/1\/git-pack-objects"}},{"Process":{"Description":"This program computes which packs in your repository are redundant. The output is suitable for piping to xargs rm if you are in the root of the repository. git pack-redundant accepts a list of objects on standard input. Any objects given will be ignored when checking which packs are required. This makes the following command useful when wanting to remove packs which contain unreachable objects. git fsck --full --unreachable | cut -d ' ' -f3 | \\ git pack-redundant --all | xargs rm","Process Name":"git-pack-redundant","Link":"https:\/\/linux.die.net\/man\/1\/git-pack-redundant"}},{"Process":{"Description":"Traditionally, tips of branches and tags (collectively known as refs) were stored one file per ref under $GIT_DIR\/refs directory. While many branch tips tend to be updated often, most tags and some branch tips are never updated. When a repository has hundreds or thousands of tags, this one-file-per-ref format both wastes storage and hurts performance. This command is used to solve the storage and performance problem by stashing the refs in a single file, $GIT_DIR\/packed-refs. When a ref is missing from the traditional $GIT_DIR\/refs hierarchy, it is looked up in this file and used if found. Subsequent updates to branches always create new files under $GIT_DIR\/refs hierarchy. A recommended practice to deal with a repository with too many refs is to pack its refs with --all --prune once, and occasionally run git pack-refs --prune. Tags are by definition stationary and are not expected to change. Branch heads will be packed with the initial pack-refs --all, but only the currently active branch heads will become unpacked, and the next pack-refs (without --all) will leave them unpacked.","Process Name":"git-pack-refs","Link":"https:\/\/linux.die.net\/man\/1\/git-pack-refs"}},{"Process":{"Description":null,"Process Name":"git-parse-remote","Link":"https:\/\/linux.die.net\/man\/1\/git-parse-remote"}},{"Process":{"Description":"A \"patch ID\" is nothing but a SHA1 of the diff associated with a patch, with whitespace and line numbers ignored. As such, it's \"reasonably stable\", but at the same time also reasonably unique, i.e., two patches that have the same \"patch ID\" are almost guaranteed to be the same thing. IOW, you can use this thing to look for likely duplicate commits. When dealing with git diff-tree output, it takes advantage of the fact that the patch is prefixed with the object name of the commit, and outputs two 40-byte hexadecimal strings. The first string is the patch ID, and the second string is the commit ID. This can be used to make a mapping from patch ID to commit ID.","Process Name":"git-patch-id","Link":"https:\/\/linux.die.net\/man\/1\/git-patch-id"}},{"Process":{"Description":"This command is deprecated; use git ls-remote instead.","Process Name":"git-peek-remote","Link":"https:\/\/linux.die.net\/man\/1\/git-peek-remote"}},{"Process":{"Description":null,"Process Name":"git-prune","Link":"https:\/\/linux.die.net\/man\/1\/git-prune"}},{"Process":{"Description":"This program searches the $GIT_OBJECT_DIR for all objects that currently exist in a pack file as well as the independent object directories. All such extra objects are removed. A pack is a collection of objects, individually compressed, with delta compression applied, stored in a single file, with an associated index file. Packs are used to reduce the load on mirror systems, backup engines, disk storage, etc.","Process Name":"git-prune-packed","Link":"https:\/\/linux.die.net\/man\/1\/git-prune-packed"}},{"Process":{"Description":"Runs git fetch with the given parameters, and calls git merge to merge the retrieved head(s) into the current branch. With --rebase, calls git rebase instead of git merge. Note that you can use . (current directory) as the <repository> to pull from the local repository - this is useful when merging local branches into the current branch. Also note that options meant for git pull itself and underlying git merge must be given before the options meant for git fetch. Warning: Running git pull (actually, the underlying git merge) with uncommitted changes is discouraged: while possible, it leaves you in a state that is hard to back out of in the case of a conflict.","Process Name":"git-pull","Link":"https:\/\/linux.die.net\/man\/1\/git-pull"}},{"Process":{"Description":"Updates remote refs using local refs, while sending objects necessary to complete the given refs. You can make interesting things happen to a repository every time you push into it, by setting up hooks there. See documentation for git-receive-pack(1).","Process Name":"git-push","Link":"https:\/\/linux.die.net\/man\/1\/git-push"}},{"Process":{"Description":"Applies a quilt patchset onto the current git branch, preserving the patch boundaries, patch order, and patch descriptions present in the quilt patchset. For each patch the code attempts to extract the author from the patch description. If that fails it falls back to the author specified with --author. If the --author flag was not given the patch description is displayed and the user is asked to interactively enter the author of the patch. If a subject is not found in the patch description the patch name is preserved as the 1 line subject in the git description.","Process Name":"git-quiltimport","Link":"https:\/\/linux.die.net\/man\/1\/git-quiltimport"}},{"Process":{"Description":null,"Process Name":"git-read-tree","Link":"https:\/\/linux.die.net\/man\/1\/git-read-tree"}},{"Process":{"Description":"If <branch> is specified, git rebase will perform an automatic git checkout <branch> before doing anything else. Otherwise it remains on the current branch. All changes made by commits in the current branch but that are not in <upstream> are saved to a temporary area. This is the same set of commits that would be shown by git log <upstream>..HEAD (or git log HEAD, if --root is specified). The current branch is reset to <upstream>, or <newbase> if the --onto option was supplied. This has the exact same effect as git reset --hard <upstream> (or <newbase>). ORIG_HEAD is set to point at the tip of the branch before the reset. The commits that were previously saved into the temporary area are then reapplied to the current branch, one by one, in order. Note that any commits in HEAD which introduce the same textual changes as a commit in HEAD..<upstream> are omitted (i.e., a patch already accepted upstream with a different commit message or timestamp will be skipped). It is possible that a merge failure will prevent this process from being completely automatic. You will have to resolve any such merge failure and run git rebase --continue. Another option is to bypass the commit that caused the merge failure with git rebase --skip. To restore the original <branch> and remove the .git\/rebase-apply working files, use the command git rebase --abort instead. Assume the following history exists and the current branch is \"topic\":       A---B---C topic\n     \/\nD---E---F---G master From this point, the result of either of the following commands: git rebase master\ngit rebase master topic would be:               A'--B'--C' topic\n             \/\nD---E---F---G master The latter form is just a short-hand of git checkout topic followed by git rebase master. If the upstream branch already contains a change you have made (e.g., because you mailed a patch which was applied upstream), then that commit will be skipped. For example, running 'git rebase master' on the following history (in which A' and A introduce the same set of changes, but have different committer information):       A---B---C topic\n     \/\nD---E---A'---F master will result in:                B'---C' topic\n              \/\nD---E---A'---F master Here is how you would transplant a topic branch based on one branch to another, to pretend that you forked the topic branch from the latter branch, using rebase --onto. First let's assume your topic is based on branch next. For example, a feature developed in topic depends on some functionality which is found in next. o---o---o---o---o  master\n     \\\n      o---o---o---o---o  next\n                       \\\n                        o---o---o  topic We want to make topic forked from branch master; for example, because the functionality on which topic depends was merged into the more stable master branch. We want our tree to look like this: o---o---o---o---o  master\n    |            \\\n    |             o'--o'--o'  topic\n     \\\n      o---o---o---o---o  next We can get this using the following command: git rebase --onto master next topic Another example of --onto option is to rebase part of a branch. If we have the following situation:                         H---I---J topicB\n                       \/\n              E---F---G  topicA\n             \/\nA---B---C---D  master then the command git rebase --onto master topicA topicB would result in:              H'--I'--J'  topicB\n            \/\n            | E---F---G  topicA\n            |\/\nA---B---C---D  master This is useful when topicB does not depend on topicA. A range of commits could also be removed with rebase. If we have the following situation: E---F---G---H---I---J  topicA then the command git rebase --onto topicA~5 topicA~3 topicA would result in the removal of commits F and G: E---H'---I'---J'  topicA This is useful if F and G were flawed in some way, or should not be part of topicA. Note that the argument to --onto and the <upstream> parameter can be any valid commit-ish. In case of conflict, git rebase will stop at the first problematic commit and leave conflict markers in the tree. You can use git diff to locate the markers (<<<<<<) and make edits to resolve the conflict. For each file you edit, you need to tell git that the conflict has been resolved, typically this would be done with git add <filename> After resolving the conflict manually and updating the index with the desired resolution, you can continue the rebasing process with git rebase --continue Alternatively, you can undo the git rebase with git rebase --abort","Process Name":"git-rebase","Link":"https:\/\/linux.die.net\/man\/1\/git-rebase"}},{"Process":{"Description":"Invoked by git send-pack and updates the repository with the information fed from the remote end. This command is usually not invoked directly by the end user. The UI for the protocol is on the git send-pack side, and the program pair is meant to be used to push updates to remote repository. For pull operations, see git-fetch-pack(1). The command allows for creation and fast-forwarding of sha1 refs (heads\/tags) on the remote end (strictly speaking, it is the local end git-receive-pack runs, but to the user who is sitting at the send-pack end, it is updating the remote. Confused?) There are other real-world examples of using update and post-update hooks found in the Documentation\/howto directory. git-receive-pack honours the receive.denyNonFastForwards config option, which tells it if updates to a ref should be denied if they are not fast-forwards.","Process Name":"git-receive-pack","Link":"https:\/\/linux.die.net\/man\/1\/git-receive-pack"}},{"Process":{"Description":"The command takes various subcommands, and different options depending on the subcommand: git reflog expire [--dry-run] [--stale-fix] [--verbose]\n        [--expire=<time>] [--expire-unreachable=<time>] [--all] <refs>...\ngit reflog delete ref@{specifier}...\ngit reflog [show] [log-options] [<ref>] Reflog is a mechanism to record when the tip of branches are updated. This command is to manage the information recorded in it. The subcommand \"expire\" is used to prune older reflog entries. Entries older than expire time, or entries older than expire-unreachable time and not reachable from the current tip, are removed from the reflog. This is typically not used directly by the end users - instead, see git-gc(1). The subcommand \"show\" (which is also the default, in the absence of any subcommands) will take all the normal log options, and show the log of the reference provided in the command-line (or HEAD, by default). The reflog will cover all recent actions (HEAD reflog records branch switching as well). It is an alias for git log -g --abbrev-commit --pretty=oneline; see git-log(1). The reflog is useful in various git commands, to specify the old value of a reference. For example, HEAD@{2} means \"where HEAD used to be two moves ago\", master@{one.week.ago} means \"where master used to point to one week ago\", and so on. See git-rev-parse(1) for more details. To delete single entries from the reflog, use the subcommand \"delete\" and specify the exact entry (e.g. \"git reflog delete master@{2}\").","Process Name":"git-reflog","Link":"https:\/\/linux.die.net\/man\/1\/git-reflog"}},{"Process":{"Description":"This will scan 1 or more object repositories and look for objects in common with a master repository. Objects not already hardlinked to the master repository will be replaced with a hardlink to the master repository.","Process Name":"git-relink","Link":"https:\/\/linux.die.net\/man\/1\/git-relink"}},{"Process":{"Description":"Manage the set of repositories (\"remotes\") whose branches you track.","Process Name":"git-remote","Link":"https:\/\/linux.die.net\/man\/1\/git-remote"}},{"Process":{"Description":"This remote helper uses the specified <command> to connect to a remote git server. Data written to stdin of the specified <command> is assumed to be sent to a git:\/\/ server, git-upload-pack, git-receive-pack or git-upload-archive (depending on situation), and data read from stdout of <command> is assumed to be received from the same service. Command and arguments are separated by an unescaped space. The following sequences have a special meaning: '% ' Literal space in command or argument. %% Literal percent sign. %s Replaced with name (receive-pack, upload-pack, or upload-archive) of the service git wants to invoke. %S Replaced with long name (git-receive-pack, git-upload-pack, or git-upload-archive) of the service git wants to invoke. %G (must be the first characters in an argument) This argument will not be passed to <command>. Instead, it will cause the helper to start by sending git:\/\/ service requests to the remote side with the service field set to an appropriate value and the repository field set to rest of the argument. Default is not to send such a request. This is useful if remote side is git:\/\/ server accessed over some tunnel. %V (must be first characters in argument) This argument will not be passed to <command>. Instead it sets the vhost field in the git:\/\/ service request (to rest of the argument). Default is not to send vhost in such request (if sent).","Process Name":"git-remote-ext","Link":"https:\/\/linux.die.net\/man\/1\/git-remote-ext"}},{"Process":{"Description":"This helper uses specified file descriptors to connect to a remote git server. This is not meant for end users but for programs and scripts calling git fetch, push or archive. If only <infd> is given, it is assumed to be a bidirectional socket connected to remote git server (git-upload-pack, git-receive-pack or git-upload-achive). If both <infd> and <outfd> are given, they are assumed to be pipes connected to a remote git server (<infd> being the inbound pipe and <outfd> being the outbound pipe. It is assumed that any handshaking procedures have already been completed (such as sending service request for git:\/\/) before this helper is started. <anything> can be any string. It is ignored. It is meant for providing information to user in the URL in case that URL is displayed in some context.","Process Name":"git-remote-fd","Link":"https:\/\/linux.die.net\/man\/1\/git-remote-fd"}},{"Process":{"Description":null,"Process Name":"git-remote-helpers","Link":"https:\/\/linux.die.net\/man\/1\/git-remote-helpers"}},{"Process":{"Description":"This script is used to combine all objects that do not currently reside in a \"pack\", into a pack. It can also be used to re-organize existing packs into a single, more efficient pack. A pack is a collection of objects, individually compressed, with delta compression applied, stored in a single file, with an associated index file. Packs are used to reduce the load on mirror systems, backup engines, disk storage, etc.","Process Name":"git-repack","Link":"https:\/\/linux.die.net\/man\/1\/git-repack"}},{"Process":{"Description":"Adds a replace reference in .git\/refs\/replace\/ The name of the replace reference is the SHA1 of the object that is replaced. The content of the replace reference is the SHA1 of the replacement object. Unless -f is given, the replace reference must not yet exist in .git\/refs\/replace\/ directory. Replacement references will be used by default by all git commands except those doing reachability traversal (prune, pack transfer and fsck). It is possible to disable use of replacement references for any command using the --no-replace-objects option just after git. For example if commit foo has been replaced by commit bar: $ git --no-replace-objects cat-file commit foo shows information about commit foo, while: $ git cat-file commit foo shows information about commit bar. The GIT_NO_REPLACE_OBJECTS environment variable can be set to achieve the same effect as the --no-replace-objects option.","Process Name":"git-replace","Link":"https:\/\/linux.die.net\/man\/1\/git-replace"}},{"Process":{"Description":null,"Process Name":"git-repo-config","Link":"https:\/\/linux.die.net\/man\/1\/git-repo-config"}},{"Process":{"Description":"Summarizes the changes between two commits to the standard output, and includes the given URL in the generated summary.","Process Name":"git-request-pull","Link":"https:\/\/linux.die.net\/man\/1\/git-request-pull"}},{"Process":{"Description":null,"Process Name":"git-rerere","Link":"https:\/\/linux.die.net\/man\/1\/git-rerere"}},{"Process":{"Description":"Sets the current head to the specified commit and optionally resets the index and working tree to match. This command is useful if you notice some small error in a recent commit (or set of commits) and want to redo that part without showing the undo in the history. If you want to undo a commit other than the latest on a branch, git-revert(1) is your friend. The second and third forms with paths and\/or --patch are used to revert selected paths in the index from a given commit, without moving HEAD.","Process Name":"git-reset","Link":"https:\/\/linux.die.net\/man\/1\/git-reset"}},{"Process":{"Description":"List commits that are reachable by following the parent links from the given commit(s), but exclude commits that are reachable from the one(s) given with a ^ in front of them. The output is given in reverse chronological order by default. You can think of this as a set operation. Commits given on the command line form a set of commits that are reachable from any of them, and then commits reachable from any of the ones given with ^ in front are subtracted from that set. The remaining commits are what comes out in the command's output. Various other options and paths parameters can be used to further limit the result. Thus, the following command: $ git rev-list foo bar ^baz means \"list all the commits which are reachable from foo or bar, but not from baz\". A special notation \"<commit1>..<commit2>\" can be used as a short-hand for \"^<commit1> <commit2>\". For example, either of the following may be used interchangeably: $ git rev-list origin..HEAD\n$ git rev-list HEAD ^origin Another special notation is \" <commit1>... <commit2>\" which is useful for merges. The resulting set of commits is the symmetric difference between the two operands. The following two commands are equivalent: $ git rev-list A B --not $(git merge-base --all A B)\n$ git rev-list A...B rev-list is a very essential git command, since it provides the ability to build and traverse commit ancestry graphs. For this reason, it has a lot of different options that enables it to be used by commands as different as git bisect and git repack.","Process Name":"git-rev-list","Link":"https:\/\/linux.die.net\/man\/1\/git-rev-list"}},{"Process":{"Description":null,"Process Name":"git-rev-parse","Link":"https:\/\/linux.die.net\/man\/1\/git-rev-parse"}},{"Process":{"Description":"Given one existing commit, revert the change the patch introduces, and record a new commit that records it. This requires your working tree to be clean (no modifications from the HEAD commit). Note: git revert is used to record a new commit to reverse the effect of an earlier commit (often a faulty one). If you want to throw away all uncommitted changes in your working directory, you should see git-reset(1), particularly the --hard option. If you want to extract specific files as they were in another commit, you should see git-checkout(1), specifically the git checkout <commit> - <filename> syntax. Take care with these alternatives as both will discard uncommitted changes in your working directory.","Process Name":"git-revert","Link":"https:\/\/linux.die.net\/man\/1\/git-revert"}},{"Process":{"Description":"git-review automates and streamlines some of the tasks involved with submitting local changes to a Gerrit server for review. It is designed to make it easier to apprehend Gerrit, especially for users that have recently switched to Git from another version control system. The following options are available:        -d change, --download=change Download change from Gerrit into a local branch. The branch will be named after the patch author and the name of a topic. If the local branch already exists, it will attempt to update with the latest patchset for this change. -f, --finish Close down the local branch and switch back to the target branch on successful submission. -n, --dry-run Don't actually perform any commands that have direct effects. Print them instead. -r remote, --remote=remote Git remote to use for Gerrit. -s, --setup Just run the repo setup commands but don't submit anything. -t topic, --topic=topic Sets the target topic for this change on the gerrit server. If not specified, a bug number from the commit summary will be used. Alternatively, the local branch name will be used if different from remote branch. -u, --update Skip cached local copies and force updates from network resources. -l, --list List the available reviews on the gerrit server for this project. -y, --yes Indicate that you do, in fact, understand if you are submitting more than one patch. -v --verbose Turns on more verbose output. -D, --draft Submit review as a draft. Requires Gerrit 2.3 or newer. -R, --no-rebase Do not automatically perform a rebase before submitting the change to Gerrit. When submitting a change for review, you will usually want it to be based on the tip of upstream branch in order to avoid possible conflicts. When amending a change and rebasing the new patchset, the Gerrit web interface will show a difference between the two patchsets which contains all commits in between. This may confuse many reviewers that would expect to see a much simpler difference. --version Print the version number and exit.","Process Name":"git-review","Link":"https:\/\/linux.die.net\/man\/1\/git-review"}},{"Process":{"Description":"Remove files from the index, or from the working tree and the index. git rm will not remove a file from just your working directory. (There is no option to remove a file only from the working tree and yet keep it in the index; use \/bin\/rm if you want to do that.) The files being removed have to be identical to the tip of the branch, and no updates to their contents can be staged in the index, though that default behavior can be overridden with the -f option. When --cached is given, the staged content has to match either the tip of the branch or the file on disk, allowing the file to be removed from just the index.","Process Name":"git-rm","Link":"https:\/\/linux.die.net\/man\/1\/git-rm"}},{"Process":{"Description":"Attach each commit between <since> and <until> to the bug <bugid> on GNOME's bugzilla. If ..<until> is not specified, the head of the current working tree is implied. If -n (or bugzilla.numbered in the repository configuration) is specified, instead of \"[PATCH] Subject\", the first line is formatted as \"[n\/m] Subject\".","Process Name":"git-send-bugzilla","Link":"https:\/\/linux.die.net\/man\/1\/git-send-bugzilla"}},{"Process":{"Description":"Takes the patches given on the command line and emails them out. Patches can be specified as files, directories (which will send all files in the directory), or directly as a revision list. In the last case, any format accepted by git-format-patch(1) can be passed to git send-email. The header of the email is configurable by command line options. If not specified on the command line, the user will be prompted with a ReadLine enabled interface to provide the necessary information. There are two formats accepted for patch files: 1. mbox format files This is what git-format-patch(1) generates. Most headers and MIME formatting are ignored. 2. The original format used by Greg Kroah-Hartman's send_lots_of_email.pl script This format expects the first line of the file to contain the \"Cc:\" value and the \"Subject:\" of the message as the second line.","Process Name":"git-send-email","Link":"https:\/\/linux.die.net\/man\/1\/git-send-email"}},{"Process":{"Description":"Usually you would want to use git push, which is a higher-level wrapper of this command, instead. See git-push(1). Invokes git-receive-pack on a possibly remote repository, and updates it from the current repository, sending named refs.","Process Name":"git-send-pack","Link":"https:\/\/linux.die.net\/man\/1\/git-send-pack"}},{"Process":{"Description":"This is not a command the end user would want to run. Ever. This documentation is meant for people who are studying the Porcelain-ish scripts and\/or are writing new ones. The 'git sh-i18n scriptlet is designed to be sourced (using .) by Git's porcelain programs implemented in shell script. It provides wrappers for the GNU gettext and eval_gettext functions accessible through the gettext.sh script, and provides pass-through fallbacks on systems without GNU gettext.","Process Name":"git-sh-i18n","Link":"https:\/\/linux.die.net\/man\/1\/git-sh-i18n"}},{"Process":{"Description":"This is not a command the end user would want to run. Ever. This documentation is meant for people who are studying the plumbing scripts and\/or are writing new ones. git sh-i18n--envsubst is Git's stripped-down copy of the GNU envsubst(1) program that comes with the GNU gettext package. It's used internally by git-sh-i18n(1) to interpolate the variables passed to the the eval_gettext function. No promises are made about the interface, or that this program won't disappear without warning in the next version of Git. Don't use it.","Process Name":"git-sh-i18n--envsubst","Link":"https:\/\/linux.die.net\/man\/1\/git-sh-i18n--envsubst"}},{"Process":{"Description":"This is not a command the end user would want to run. Ever. This documentation is meant for people who are studying the Porcelain-ish scripts and\/or are writing new ones. The git sh-setup scriptlet is designed to be sourced (using .) by other shell scripts to set up some variables pointing at the normal git directories and a few helper shell functions. Before sourcing it, your script should set up a few variables; USAGE (and LONG_USAGE, if any) is used to define message given by usage() shell function. SUBDIRECTORY_OK can be set if the script can run from a subdirectory of the working tree (some commands do not). The scriptlet sets GIT_DIR and GIT_OBJECT_DIRECTORY shell variables, but does not export them to the environment.","Process Name":"git-sh-setup","Link":"https:\/\/linux.die.net\/man\/1\/git-sh-setup"}},{"Process":{"Description":"This is meant to be used as a login shell for SSH accounts you want to restrict to GIT pull\/push access only. It permits execution only of server-side GIT commands implementing the pull\/push functionality. The commands can be executed only by the -c option; the shell is not interactive. Currently, only four commands are permitted to be called, git-receive-pack git-upload-pack and git-upload-archive with a single required argument, or cvs server (to invoke git-cvsserver).","Process Name":"git-shell","Link":"https:\/\/linux.die.net\/man\/1\/git-shell"}},{"Process":{"Description":"Summarizes git log output in a format suitable for inclusion in release announcements. Each commit will be grouped by author and the first line of the commit message will be shown. Additionally, \"[PATCH]\" will be stripped from the commit description.","Process Name":"git-shortlog","Link":"https:\/\/linux.die.net\/man\/1\/git-shortlog"}},{"Process":{"Description":"Shows one or more objects (blobs, trees, tags and commits). For commits it shows the log message and textual diff. It also presents the merge commit in a special format as produced by git diff-tree --cc. For tags, it shows the tag message and the referenced objects. For trees, it shows the names (equivalent to git ls-tree with --name-only). For plain blobs, it shows the plain contents. The command takes options applicable to the git diff-tree command to control how the changes the commit introduces are shown. This manual page describes only the most frequently used options.","Process Name":"git-show","Link":"https:\/\/linux.die.net\/man\/1\/git-show"}},{"Process":{"Description":"Shows the commit ancestry graph starting from the commits named with <rev>s or <globs>s (or all refs under refs\/heads and\/or refs\/tags) semi-visually. It cannot show more than 29 branches and commits at a time. It uses showbranch.default multi-valued configuration items if no <rev> nor <glob> is given on the command line.","Process Name":"git-show-branch","Link":"https:\/\/linux.die.net\/man\/1\/git-show-branch"}},{"Process":{"Description":"Reads given idx file for packed git archive created with git pack-objects command, and dumps its contents. The information it outputs is subset of what you can get from git verify-pack -v; this command only shows the packfile offset and SHA1 of each object.","Process Name":"git-show-index","Link":"https:\/\/linux.die.net\/man\/1\/git-show-index"}},{"Process":{"Description":"Displays references available in a local repository along with the associated commit IDs. Results can be filtered using a pattern and tags can be dereferenced into object IDs. Additionally, it can be used to test whether a particular ref exists. The --exclude-existing form is a filter that does the inverse, it shows the refs from stdin that don't exist in the local repository. Use of this utility is encouraged in favor of directly accessing files under the .git directory.","Process Name":"git-show-ref","Link":"https:\/\/linux.die.net\/man\/1\/git-show-ref"}},{"Process":{"Description":null,"Process Name":"git-stage","Link":"https:\/\/linux.die.net\/man\/1\/git-stage"}},{"Process":{"Description":"Use git stash when you want to record the current state of the working directory and the index, but want to go back to a clean working directory. The command saves your local modifications away and reverts the working directory to match the HEAD commit. The modifications stashed away by this command can be listed with git stash list, inspected with git stash show, and restored (potentially on top of a different commit) with git stash apply. Calling git stash without any arguments is equivalent to git stash save. A stash is by default listed as \"WIP on branchname ...\", but you can give a more descriptive message on the command line when you create one. The latest stash you created is stored in refs\/stash; older stashes are found in the reflog of this reference and can be named using the usual reflog syntax (e.g. stash@{0} is the most recently created stash, stash@{1} is the one before it, stash@{2.hours.ago} is also possible).","Process Name":"git-stash","Link":"https:\/\/linux.die.net\/man\/1\/git-stash"}},{"Process":{"Description":"Displays paths that have differences between the index file and the current HEAD commit, paths that have differences between the working tree and the index file, and paths in the working tree that are not tracked by git (and are not ignored by gitignore(5)). The first are what you would commit by running git commit; the second and third are what you could commit by running git add before running git commit.","Process Name":"git-status","Link":"https:\/\/linux.die.net\/man\/1\/git-status"}},{"Process":{"Description":"Remove multiple empty lines, and empty lines at beginning and end.","Process Name":"git-stripspace","Link":"https:\/\/linux.die.net\/man\/1\/git-stripspace"}},{"Process":{"Description":"Submodules allow foreign repositories to be embedded within a dedicated subdirectory of the source tree, always pointed at a particular commit. They are not to be confused with remotes, which are meant mainly for branches of the same project; submodules are meant for different projects you would like to make part of your source tree, while the history of the two projects still stays completely independent and you cannot modify the contents of the submodule from within the main project. If you want to merge the project histories and want to treat the aggregated whole as a single project from then on, you may want to add a remote for the other project and use the subtree merge strategy, instead of treating the other project as a submodule. Directories that come from both projects can be cloned and checked out as a whole if you choose to go that route. Submodules are composed from a so-called gitlink tree entry in the main repository that refers to a particular commit object within the inner repository that is completely separate. A record in the .gitmodules file at the root of the source tree assigns a logical name to the submodule and describes the default URL the submodule shall be cloned from. The logical name can be used for overriding this URL within your local repository configuration (see submodule init). This command will manage the tree entries and contents of the gitmodules file for you, as well as inspect the status of your submodules and update them. When adding a new submodule to the tree, the add subcommand is to be used. However, when pulling a tree containing submodules, these will not be checked out by default; the init and update subcommands will maintain submodules checked out and at appropriate revision in your working tree. You can briefly inspect the up-to-date status of your submodules using the status subcommand and get a detailed overview of the difference between the index and checkouts using the summary subcommand.","Process Name":"git-submodule","Link":"https:\/\/linux.die.net\/man\/1\/git-submodule"}},{"Process":{"Description":null,"Process Name":"git-svn","Link":"https:\/\/linux.die.net\/man\/1\/git-svn"}},{"Process":{"Description":"Given one argument, reads which branch head the given symbolic ref refers to and outputs its path, relative to the .git\/ directory. Typically you would give HEAD as the <name> argument to see which branch your working tree is on. Given two arguments, creates or updates a symbolic ref <name> to point at the given branch <ref>. A symbolic ref is a regular file that stores a string that begins with ref: refs\/. For example, your .git\/HEAD is a regular file whose contents is ref: refs\/heads\/master.","Process Name":"git-symbolic-ref","Link":"https:\/\/linux.die.net\/man\/1\/git-symbolic-ref"}},{"Process":{"Description":null,"Process Name":"git-tag","Link":"https:\/\/linux.die.net\/man\/1\/git-tag"}},{"Process":{"Description":"THIS COMMAND IS DEPRECATED. Use git archive with --format=tar option instead (and move the <base> argument to --prefix=base\/). Creates a tar archive containing the tree structure for the named tree. When <base> is specified it is added as a leading path to the files in the generated tar archive. git tar-tree behaves differently when given a tree ID versus when given a commit ID or tag ID. In the first case the current time is used as modification time of each file in the archive. In the latter case the commit time as recorded in the referenced commit object is used instead. Additionally the commit ID is stored in a global extended pax header. It can be extracted using git get-tar-commit-id.","Process Name":"git-tar-tree","Link":"https:\/\/linux.die.net\/man\/1\/git-tar-tree"}},{"Process":{"Description":"Creates a file holding the contents of the blob specified by sha1. It returns the name of the temporary file in the following format: .merge_file_XXXXX","Process Name":"git-unpack-file","Link":"https:\/\/linux.die.net\/man\/1\/git-unpack-file"}},{"Process":{"Description":"Read a packed archive (.pack) from the standard input, expanding the objects contained within and writing them into the repository in \"loose\" (one object per file) format. Objects that already exist in the repository will not be unpacked from the pack-file. Therefore, nothing will be unpacked if you use this command on a pack-file that exists within the target repository. See git-repack(1) for options to generate new packs and replace existing ones.","Process Name":"git-unpack-objects","Link":"https:\/\/linux.die.net\/man\/1\/git-unpack-objects"}},{"Process":{"Description":"Modifies the index or directory cache. Each file mentioned is updated into the index and any unmerged or needs updating state is cleared. See also git-add(1) for a more user-friendly way to do some of the most common operations on the index. The way git update-index handles files it is told about can be modified using the various options:","Process Name":"git-update-index","Link":"https:\/\/linux.die.net\/man\/1\/git-update-index"}},{"Process":{"Description":null,"Process Name":"git-update-ref","Link":"https:\/\/linux.die.net\/man\/1\/git-update-ref"}},{"Process":{"Description":"A dumb server that does not do on-the-fly pack generations must have some auxiliary information files in $GIT_DIR\/info and $GIT_OBJECT_DIRECTORY\/info directories to help clients discover what references and packs the server has. This command generates such auxiliary files.","Process Name":"git-update-server-info","Link":"https:\/\/linux.die.net\/man\/1\/git-update-server-info"}},{"Process":{"Description":null,"Process Name":"git-upload-archive","Link":"https:\/\/linux.die.net\/man\/1\/git-upload-archive"}},{"Process":{"Description":"Invoked by git fetch-pack, learns what objects the other side is missing, and sends them after packing. This command is usually not invoked directly by the end user. The UI for the protocol is on the git fetch-pack side, and the program pair is meant to be used to pull updates from a remote repository. For push operations, see git send-pack.","Process Name":"git-upload-pack","Link":"https:\/\/linux.die.net\/man\/1\/git-upload-pack"}},{"Process":{"Description":null,"Process Name":"git-var","Link":"https:\/\/linux.die.net\/man\/1\/git-var"}},{"Process":{"Description":"Reads given idx file for packed git archive created with the git pack-objects command and verifies idx file and the corresponding pack file.","Process Name":"git-verify-pack","Link":"https:\/\/linux.die.net\/man\/1\/git-verify-pack"}},{"Process":{"Description":"Validates the gpg signature created by git tag.","Process Name":"git-verify-tag","Link":"https:\/\/linux.die.net\/man\/1\/git-verify-tag"}},{"Process":{"Description":"This script tries, as much as possible, to display the URLs and FILEs that are passed as arguments, as HTML pages in new tabs on an already opened web browser. The following browsers (or commands) are currently supported: \u2022 firefox (this is the default under X Window when not using KDE) \u2022 iceweasel \u2022 konqueror (this is the default under KDE, see Note about konqueror below) \u2022 w3m (this is the default outside graphical environments) \u2022 links \u2022 lynx \u2022 dillo \u2022 open (this is the default under Mac OS X GUI) \u2022 start (this is the default under MinGW) Custom commands may also be specified.","Process Name":"git-web--browse","Link":"https:\/\/linux.die.net\/man\/1\/git-web--browse"}},{"Process":{"Description":"Shows commit logs and diff output each commit introduces. The command internally invokes git rev-list piped to git diff-tree, and takes command line options for both of these commands. This manual page describes only the most frequently used options.","Process Name":"git-whatchanged","Link":"https:\/\/linux.die.net\/man\/1\/git-whatchanged"}},{"Process":{"Description":"Creates a tree object using the current index. The name of the new tree object is printed to standard output. The index must be in a fully merged state. Conceptually, git write-tree sync()s the current index contents into a set of tree files. In order to have that match what is actually in your directory right now, you need to have done a git update-index phase before you did the git write-tree.","Process Name":"git-write-tree","Link":"https:\/\/linux.die.net\/man\/1\/git-write-tree"}},{"Process":{"Description":"Displays changes in a repository or a selected set of commits. This includes visualizing the commit graph, showing information related to each commit, and the files in the trees of each revision. Historically, gitk was the first repository browser. It's written in tcl\/tk and started off in a separate repository but was later merged into the main git repository.","Process Name":"gitk","Link":"https:\/\/linux.die.net\/man\/1\/gitk"}},{"Process":{"Description":null,"Process Name":"gitso","Link":"https:\/\/linux.die.net\/man\/1\/gitso"}},{"Process":{"Description":null,"Process Name":"gitstats","Link":"https:\/\/linux.die.net\/man\/1\/gitstats"}},{"Process":{"Description":"gjar is an implementation of Sun's jar utility that comes with the JDK . If any file is a directory then it is processed recursively. The manifest file name and the archive file name needs to be specified in the same order the -m and -f flags are specified.","Process Name":"gjar","Link":"https:\/\/linux.die.net\/man\/1\/gjar"}},{"Process":{"Description":null,"Process Name":"gjarsigner","Link":"https:\/\/linux.die.net\/man\/1\/gjarsigner"}},{"Process":{"Description":"The gjavah program is used to generate header files from class files. It can generate both CNI and JNI header files, as well as stub implementation files which can be used as a basis for implementing the required native methods.","Process Name":"gjavah","Link":"https:\/\/linux.die.net\/man\/1\/gjavah"}},{"Process":{"Description":"Gjdoc can be used in two ways: as a stand-alone documentation tool, or as a driver for a user-specified Doclet. In the default mode, Gjdoc will use the Standard Doclet HtmlDoclet to generate a set of HTML pages. The canonical usage is: gjdoc -s src\/java\/ -all -d api-docs\/ Here, src\/java\/ is the root of your source code class hierarchy, -all means that all valid Java files found under this root directory should be processed, and api-docs\/ is the directory where the generated documentation should be placed. To learn more about running Doclets other than the Standard Doclet, refer to the manual.","Process Name":"gjdoc","Link":"https:\/\/linux.die.net\/man\/1\/gjdoc"}},{"Process":{"Description":"The \"gjnih\" program is used to generate JNI header files from class files. Running it is equivalent to running \"gcjh -jni\".","Process Name":"gjnih","Link":"https:\/\/linux.die.net\/man\/1\/gjnih"}},{"Process":{"Description":"G-Kermit is a UNIX program for transferring files using the Kermit protocol. G-Kermit is a product of Kermit Project at Columbia University. It is free software under the GNU Public License. See the COPYING file for details. INVOKING G-KERMIT The G-Kermit binary is called \"gkermit\". It should be stored someplace in your UNIX PATH; normally it is available as \/usr\/local\/bin\/gkermit. To run G-Kermit, just type \"gkermit\" followed by command-line options that tell it what to do. If no options are given, it prints a usage message listing the available options. If an option takes an argument, the argument is required; if an option does not take an argument, no argument may be given (exception: -d). The action options are -r, -s, and -g. Only one action option may be given. If no action options are given, G-Kermit does nothing (except possibly to print its usage message or create a debug.log file). Here are some examples (\"$ \" is the shell prompt): $ gkermit -s hello.c   <-- Send the hello.c file\n$ gkermit -s hello.*   <-- Send all hello.* files\n$ gkermit -r           <-- Wait to receive files\n$ gkermit -g hello.c   <-- Get hello.c file\n$ gkermit -g hello.\\*  <-- Get all hello.* files Options that do not take arguments can be \"bundled\" with other options. An option that takes an argument must always be followed by a space and then its argument(s). Examples: $ gkermit -is hello.o  <-- Send hello.o in binary mode\n$ gkermit -dr          <-- Receive with debugging COMMAND-LINE OPTIONS -r      RECEIVE.  Wait for incoming files.\n-s fn   SEND.  Send file(s) specified by fn.\n-g fn   GET.  Get specified file(s) from server.\n-a fn   AS-NAME.  Alternative name for file.\n-i      IMAGE.  Binary-mode transfer (default).\n-T      TEXT. Text-mode transfer.\n-P      PATH (filename) conversion disabled.\n-w      WRITEOVER when filenames collide.\n-K      KEEP incompletely received files.\n-p x    PARITY. x = e,o,m,s,n; default = n(one).\n-e n    PACKET LENGTH.  n = 40-9000; default=4000.\n-b n    TIMEOUT.  Per-packet timeout, seconds.\n-x      XON\/XOFF.  Set Xon\/Xoff in the tty driver.\n--x     Unset Xon\/Xoff in the tty driver.\n-S      STREAMING disabled.\n-X      EXTERNAL.  G-Kermit is an external protocol.\n-q      QUIET.  Suppress messages.\n-d      DEBUG.  Write debugging info to .\/debug.log.\n-d fn   DEBUG.  Write debugging info to given file.\n-h      HELP.  Display brief usage message. You may supply options to G-Kermit on the command line or through the GKERMIT environment variable, which can contain any valid gkermit command-line options. These are processed before the actual command-line options and so can be overridden by them. Example for bash or ksh, which you can put in your profile if you want to always keep incomplete files, suppress streaming, suppress messages, and use Space parity: export GKERMIT=\"-K -S -q -p s\" MECHANICS OF FILE TRANSFER To transfer files with G-Kermit you must be connected through a terminal emulator to the UNIX system where G-Kermit is running, meaning you are online to UNIX and have access to the shell prompt (or to a menu that has an option to invoke G-Kermit). The connection can be serial (direct or dialed) or network (Telnet, Rlogin, X.25, etc). When you tell G-Kermit to SEND a file (or files), e.g. with: $ gkermit -Ts oofa.txt it pauses for a second and then sends its first packet. What happens next depends on the capabilities of your terminal emulator: \u2022 If your emulator supports Kermit \"autodownloads\" then it receives the file automatically and puts you back in the terminal screen when done. \u2022 Otherwise, you'll need to take whatever action is required by your emulator to get its attention: a mouse action, a keystroke like Alt-x, or a character sequence like Ctrl-\\ or Ctrl-] followed by the letter \"c\" (this is called \"escaping back\") and then tell it to receive the file. When the transfer is complete, you must instruct your emulator to go back to its terminal screen. During file transfer, most terminal emulators put up some kind of running display of the file transfer progress. When you tell G-Kermit to RECEIVE (with \"gkermit -r\"), this requires you to escape back to your terminal emulator and instruct it to send the desired file(s). If your terminal emulator supports Kermit autodownloads AND Kermit server mode, then you can use GET (\"gkermit -g files...\") rather than RECEIVE (\"gkermit -r\"), and the rest happens automatically, as when G-Kermit is sending. INTERRUPTING FILE TRANSFER G-Kermit supports file and group interruption. The method for interrupting a transfer depends on your terminal emulator. For example, while the file-transfer display is active, you might type the letter 'x' to cancel the current file and go on to the next one (if any), and the letter 'z' to cancel the group. Or there might be buttons you can click with your mouse. When G-Kermit is in packet mode and your terminal emulator is in its terminal screen, you can also type three (3) Ctrl-C characters in a row to make G-Kermit exit and restore the normal terminal modes. TEXT AND BINARY TRANSFER MODE When sending files in binary mode, G-Kermit sends every byte exactly as it is stored on the disk. This mode is appropriate for program binaries, graphics files, tar archives, compressed files, etc, and is G-Kermit's default file transfer mode when sending. When receiving files in binary mode, G-Kermit simply copies each byte to disk. (Obviously the bytes are encoded for transmission, but the encoding and decoding procedures give a replica of the original file after transfer.) When sending files in text mode, G-Kermit converts the record format to the common one that is defined for the Kermit protocol, namely lines terminated by carriage return and linefeed (CRLF); the receiver converts the CRLFs to whatever line-end or record-format convention is used on its platform. When receiving files in text mode, G-Kermit simply strips carriage returns, leaving only a linefeed at the end of each line, which is the UNIX convention. When receiving files, the sender's transfer mode (text or binary) predominates if the sender gives this information to G-Kermit in a Kermit File Attribute packet, which of course depends on whether your terminal emulator's Kermit protocol has this feature. Otherwise, if you gave a -i or -T option on the gkermit command line, the corresponding mode is used; otherwise the default mode (binary) is used. Furthermore, when either sending or receiving, G-Kermit and your terminal emulator's Kermit can inform each other of their OS type (UNIX in G-Kermit's case). If your emulator supports this capability, which is called \"automatic peer recognition\", and it tells G-Kermit that its platform is also UNIX, G-Kermit and the emulator's Kermit automatically switch into binary mode, since no record-format conversion is necessary in this case. Automatic peer recognition is disabled automatically if you include the -i (image) or -T (text) option. When sending, G-Kermit sends all files in the same mode, text or binary. There is no automatic per-file mode switching. When receiving, however, per-file switching occurs automatically based on the incoming Attribute packets, if any (explained below), that accompany each file. PATHNAMES When SENDING a file, G-Kermit obtains the filenames from the command line. It depends on the shell to expand metacharacters (wildcards and tilde). G-Kermit uses the full pathname given to find and open the file, but then strips the pathname before sending the name to the receiver. For example: $ gkermit -s \/etc\/hosts results in the receiver getting a file called \"HOSTS\" or \"hosts\" (the directory part, \"\/etc\/\", is stripped). However, if a pathname is included in the -a option, the directory part is not stripped: $ gkermit -s \/etc\/hosts -a \/tmp\/hosts This example sends the \/etc\/hosts file but tells the receiver that its name is \"\/tmp\/hosts\". What the receiver does with the pathname is, of course, up to the receiver, which might have various options for dealing with incoming pathnames. When RECEIVING a file, G-Kermit does NOT strip the pathname. If the incoming filename includes a path, G-Kermit tries to store the file in the specified place. If the path does not exist, the transfer fails. The incoming pathname can, of course, be overridden with the -a option. FILENAME CONVERSION When sending a file, G-Kermit normally converts outbound filenames to common form: uppercase, no more than one period, and no funny characters. So, for example, gkermit.tar.gz would be sent as GKERMIT_TAR.GZ. When receiving a file, if the name is all uppercase, G-Kermit converts it to all lowercase. If the name contains any lowercase letters, G-Kermit leaves the name alone. If the automatic peer recognition feature is available in the terminal emulator, and G-Kermit recognizes the emulator's platform as UNIX, G-Kermit automatically disables filename conversion and sends and accepts filenames literally. You can force literal filenames by including the -P option on the command line. FILENAME COLLISIONS When G-Kermit receives a file whose name is the same as that of an existing file, G-Kermit \"backs up\" the existing file by adding a unique suffix to its name. The suffix is \".~n~\", where n is a number. This kind of backup suffix is compatible with GNU EMACS and various other popular applications. To defeat the backup feature and have incoming files overwrite existing files of the same name, include the -w (writeover) option on the command line.","Process Name":"gkermit","Link":"https:\/\/linux.die.net\/man\/1\/gkermit"}},{"Process":{"Description":"Cryptographic credentials, in a Java environment, are usually stored in a Key Store. The Java SDK specifies a Key Store as a persistent container of two types of objects: Key Entries and Trusted Certificates. The security tool keytool is a Java-based application for managing those types of objects. A Key Entry represents the private key part of a key-pair used in Public-Key Cryptography, and a signed X.509 certificate which authenticates the public key part for a known entity; i.e. the owner of the key-pair. The X.509 certificate itself contains the public key part of the key-pair. A Trusted Certificate is a signed X.509 certificate issued by a trusted entity. The Trust in this context is relative to the User of the keytool. In other words, the existence of a Trusted Certificate in the Key Store processed by a keytool command implies that the User trusts the Issuer of that Trusted Certificate to also sign, and hence authenticates, other Subjects the tool may process. Trusted Certificates are important because they allow the tool to mechanically construct Chains of Trust starting from one of the Trusted Certificates in a Key Store and ending with a certificate whose Issuer is potentially unknown. A valid chain is an ordered list, starting with a Trusted Certificate (also called the anchor), ending with the target certificate, and satisfying the condition that the Subject of certificate \"#i\" is the Issuer of certificate \"#i + 1\". The keytool is invoked from the command line as follows: keytool [COMMAND] ... Multiple COMMAND s may be specified at once, each complete with its own options. keytool will parse all the arguments, before processing, and executing, each \"COMMAND\". If an exception occurs while executing one COMMAND keytool will abort. Note however that because the implementation of the tool uses code to parse command line options that also supports GNU-style options, you have to separate each command group with a double-hyphen; e.g keytool -list -- -printcert -alias mykey","Process Name":"gkeytool","Link":"https:\/\/linux.die.net\/man\/1\/gkeytool"}},{"Process":{"Description":null,"Process Name":"gkrellm","Link":"https:\/\/linux.die.net\/man\/1\/gkrellm"}},{"Process":{"Description":"gkrellmd Listens for connections from gkrellm clients. When a gkrellm client connects to a gkrellmd server all builtin monitors collect their data from the server. However, the gkrellm process is running on the local machine, so plugins enabled in gkrellm will collect data from the local context unless the plugin is client\/server capable and has a gkrellmd plugin counterpart which is installed and enabled on the server. Enabling a gkrellmd plugin installed on a server requires adding a plugin-enable line to a gkrellmd.conf file. Any command launching from a gkrellm client will run commands on the local machine. If you want to execute commands on the server, the client side commands can use ssh. There is no support for file system mounting on the machine where the gkrellmd server is running.","Process Name":"gkrellmd","Link":"https:\/\/linux.die.net\/man\/1\/gkrellmd"}},{"Process":{"Description":null,"Process Name":"gksu","Link":"https:\/\/linux.die.net\/man\/1\/gksu"}},{"Process":{"Description":"This manual page documents briefly the gksu-properties command. gksu-properties allows you to define how gksu(1) grants the privileges and locks your input devices (mouse, keyboard...).","Process Name":"gksu-properties","Link":"https:\/\/linux.die.net\/man\/1\/gksu-properties"}},{"Process":{"Description":"This manual page documents briefly gksu and gksudo gksu is a frontend to su and gksudo is a frontend to sudo. Their primary purpose is to run graphical commands that need root without the need to run an X terminal emulator and using su directly. Notice that all the magic is done by the underlying library, libgksu. Also notice that the library will decide if it should use su or sudo as backend using the \/apps\/gksu\/sudo-mode gconf key, if you call the gksu command. You can force the backend by using the gksudo command, or by using the --sudo-mode and --su-mode options. If no command is given, the gksu program will display a small window that allows you to type in a command to be run, and to select what user the program should be run as. The other options are disregarded, right now, in this mode.","Process Name":"gksudo","Link":"https:\/\/linux.die.net\/man\/1\/gksudo"}},{"Process":{"Description":null,"Process Name":"gksuexec","Link":"https:\/\/linux.die.net\/man\/1\/gksuexec"}},{"Process":{"Description":"glabels is a lightweight program for creating labels and business cards for the GNOME desktop environment. It is designed to work with various laser\/ink-jet peel-off label and business card sheets that you'll find at most office supply stores. glabels is pre-configured with templates for many of these products. It also provides an interactive template designer for creating new templates according to user specifications. For full documentation see the gLabels online help. glabels-batch is a command line utility to print labels previously prepared with glabels.","Process Name":"glabels","Link":"https:\/\/linux.die.net\/man\/1\/glabels"}},{"Process":{"Description":"","Process Name":"glance","Link":"https:\/\/linux.die.net\/man\/1\/glance"}},{"Process":{"Description":null,"Process Name":"glance-api","Link":"https:\/\/linux.die.net\/man\/1\/glance-api"}},{"Process":{"Description":"","Process Name":"glance-cache-cleaner","Link":"https:\/\/linux.die.net\/man\/1\/glance-cache-cleaner"}},{"Process":{"Description":"","Process Name":"glance-cache-manage","Link":"https:\/\/linux.die.net\/man\/1\/glance-cache-manage"}},{"Process":{"Description":null,"Process Name":"glance-cache-prefetcher","Link":"https:\/\/linux.die.net\/man\/1\/glance-cache-prefetcher"}},{"Process":{"Description":"","Process Name":"glance-cache-pruner","Link":"https:\/\/linux.die.net\/man\/1\/glance-cache-pruner"}},{"Process":{"Description":null,"Process Name":"glance-control","Link":"https:\/\/linux.die.net\/man\/1\/glance-control"}},{"Process":{"Description":"","Process Name":"glance-manage","Link":"https:\/\/linux.die.net\/man\/1\/glance-manage"}},{"Process":{"Description":null,"Process Name":"glance-registry","Link":"https:\/\/linux.die.net\/man\/1\/glance-registry"}},{"Process":{"Description":"","Process Name":"glance-scrubber","Link":"https:\/\/linux.die.net\/man\/1\/glance-scrubber"}},{"Process":{"Description":null,"Process Name":"glances","Link":"https:\/\/linux.die.net\/man\/1\/glances"}},{"Process":{"Description":"This program draws a box and a few line segments, and generates a radial blur outward from it. This creates flowing field effects. This is done by rendering the scene into a small texture, then repeatedly rendering increasingly-enlarged and increasingly-transparent versions of that texture onto the frame buffer. As such, it's quite graphics intensive: don't bother trying to run this if you don't have hardware-accelerated texture support. It will hurt your machine bad.","Process Name":"glblur","Link":"https:\/\/linux.die.net\/man\/1\/glblur"}},{"Process":{"Description":null,"Process Name":"glds","Link":"https:\/\/linux.die.net\/man\/1\/glds"}},{"Process":{"Description":"glds creates a 256 by 1 one channel spatial grey level difference matrix (sglds) of the box determined by the parameters (xp, yp; xs, ys) within the vasari image file image. The matrix is written onto the vasari image file matrix. The displacement vector is determined by (dx, dy). The user must ensure that there is enough border pixels around the box within im dictated by the displacement vector (dx,dy) or else the program fails. All entries of the sgld matrix are double normalised to the number of pairs involved. This function is a direct implementation of the paper: Haralick R. M., Shanmugan K. and Dinstein I., 'Textural features for image classification', IEEE Transactions on Systems, Man, and Cybernetics, Vol. SMC-3, No 6, Nov. 1973, pp 610-621. Input im should be one band unsigned char image file. glds_features calculates features on the spatial grey level difference vasari image file matrix. The calculated features are printed in the stderr.","Process Name":"glds_features","Link":"https:\/\/linux.die.net\/man\/1\/glds_features"}},{"Process":{"Description":null,"Process Name":"gle","Link":"https:\/\/linux.die.net\/man\/1\/gle"}},{"Process":{"Description":"A tiled kaleidescope using OpenGL.","Process Name":"gleidescope","Link":"https:\/\/linux.die.net\/man\/1\/gleidescope"}},{"Process":{"Description":null,"Process Name":"glforestfire","Link":"https:\/\/linux.die.net\/man\/1\/glforestfire"}},{"Process":{"Description":"glib-config is a tool that is used to configure to determine the compiler and linker flags that should be used to compile and link programs that use GLib. It is also used internally to the .m4 macros for GNU autoconf that are included with GLib.","Process Name":"glib-config","Link":"https:\/\/linux.die.net\/man\/1\/glib-config"}},{"Process":{"Description":"glib-genmarshal is a small utility that generates C code marshallers for callback functions of the GClosure mechanism in the GObject sublibrary of GLib. The marshaller functions have a standard signature, they get passed in the invoking closure, an array of value structures holding the callback function parameters and a value structure for the return value of the callback. The marshaller is then responsible to call the respective C code function of the closure with all the parameters on the stack and to collect its return value.","Process Name":"glib-genmarshal","Link":"https:\/\/linux.die.net\/man\/1\/glib-genmarshal"}},{"Process":{"Description":"glib-gettextize helps to prepare a source package for being internationalized through gettext. It is a variant of the gettextize that ships with gettext. glib-gettextize differs from gettextize in that it doesn't create an intl\/ subdirectory and doesn't modify po\/ChangeLog (note that newer versions of gettextize behave like this when called with the --no-changelog option). Options --help print help and exit --version print version information and exit -c, --copy copy files instead of making symlinks -f, --force force writing of new files even if old ones exist","Process Name":"glib-gettextize","Link":"https:\/\/linux.die.net\/man\/1\/glib-gettextize"}},{"Process":{"Description":"glib-mkenums is a small perl-script utility that parses C code to extract enum definitions and produces enum descriptions based on text templates specified by the user. Most frequently this script is used to produce C code that contains enum values as strings so programs can provide value name strings for introspection.","Process Name":"glib-mkenums","Link":"https:\/\/linux.die.net\/man\/1\/glib-mkenums"}},{"Process":{"Description":null,"Process Name":"gliv","Link":"https:\/\/linux.die.net\/man\/1\/gliv"}},{"Process":{"Description":"Generates some twisting 3d knot patterns. Spins 'em around.","Process Name":"glknots","Link":"https:\/\/linux.die.net\/man\/1\/glknots"}},{"Process":{"Description":"The glmatrix program draws dropping characters similar to what is seen in the title sequence of the Wachowski brothers' film, \"The Matrix\". Also see xmatrix(1) for a 2D rendering of the similar effect that appeared on the computer monitors actually in the movie.","Process Name":"glmatrix","Link":"https:\/\/linux.die.net\/man\/1\/glmatrix"}},{"Process":{"Description":"globaltime is a fast and easy to use graphical clock for the Xfce Desktop Environment. It can show several clocks in different timezones at the same time. It is also possible to change time, so that it is easier to see when certain time is in other timezones. globaltime is not able to change your operating system time.","Process Name":"globaltime","Link":"https:\/\/linux.die.net\/man\/1\/globaltime"}},{"Process":{"Description":"The globus-domainname program prints the domain name of the host it is running on. It uses the same algorithm as the globus-hostname command. Setting the GLOBUS_HOSTNAME environment variable will override values returned from gethostname() and gethostbyname(). -help Show usage information and exit. -version Show version information and exit.","Process Name":"globus-domainname","Link":"https:\/\/linux.die.net\/man\/1\/globus-domainname"}},{"Process":{"Description":"globus-hostname attempts to determine a fully-qualified name for the host it is executing on. It performs the same name checks that the C globus_common library uses, including checking for the GLOBUS_HOSTNAME environment variable, and optionally performing a name server lookup of the local host name. Options to globus-hostname are: -help Show usage information and exit. -version Show version information and exit.","Process Name":"globus-hostname","Link":"https:\/\/linux.die.net\/man\/1\/globus-hostname"}},{"Process":{"Description":"The globus-job-cancel program cancels the job named by JOBID. Any cached files associated with the job will remain until globus-job-clean is executed for the job. By default, globus-job-cancel prompts the user prior to canceling the job. This behavior can be overridden by specifying the -f or -force command-line options.","Process Name":"globus-job-cancel","Link":"https:\/\/linux.die.net\/man\/1\/globus-job-cancel"}},{"Process":{"Description":"The globus-job-clean program cancels the job named by JOBID if it is still running, and then removes any cached files on the GRAM service node related to that job. In order to do the file clean up, it submits a job which removes the cache files. By default this cleanup job is submitted to the default GRAM resource running on the same host as the job. This behavior can be controlled by specifying a resource manager contact string as the parameter to the -r or -resource option. By default, globus-job-clean prompts the user prior to canceling the job. This behavior can be overridden by specifying the -f or -force command-line options.","Process Name":"globus-job-clean","Link":"https:\/\/linux.die.net\/man\/1\/globus-job-clean"}},{"Process":{"Description":"The globus-job-get-output program retrieves the output and error streams of the job named by JOBID. By default, globus-job-get-output will retrieve all output and error data from the job and display them to its own output and error streams. Other behavior can be controlled by using command-line options. The data retrieval is implemented by submitting another job which simply displays the contents of the first job's output and error streams. By default this retrieval job is submitted to the default GRAM resource running on the same host as the job. This behavior can be controlled by specifying a particular resource manager contact string as the RESOURCE parameter to the -r or -resource option.","Process Name":"globus-job-get-output","Link":"https:\/\/linux.die.net\/man\/1\/globus-job-get-output"}},{"Process":{"Description":"The globus-job-run program constructs a job description from its command-line options and then submits the job to the GRAM service running at SERVICE_CONTACT. The executable and arguments to the executable are provided on the command-line after all other options. Note that the -dumprsl, -dryrun, -verify, and -file command-line options must occur before the first non-option argument, the SERVICE_CONTACT. The globus-job-run provides similar functionality to globusrun in that it allows interactive start-up of GRAM jobs. However, unlike globusrun, it uses command-line parameters to define the job instead of RSL expressions.","Process Name":"globus-job-run","Link":"https:\/\/linux.die.net\/man\/1\/globus-job-run"}},{"Process":{"Description":"The globus-job-status program checks the status of a GRAM job by sending a status request to the job manager contact for that job specifed by the JOBID parameter. If successful, it will print the job status to standard output. The states supported by globus-job-status are: PENDING The job has been submitted to the LRM but has not yet begun execution. ACTIVE The job has begun execution. FAILED The job has failed. SUSPENDED The job is currently suspended by the LRM. DONE The job has completed. UNSUBMITTED The job has been accepted by GRAM, but not yet submitted to the LRM. STAGE_IN The job has been accepted by GRAM and is currently staging files prior to being submitted to the LRM. STAGE_OUT The job has completed execution and is currently staging files from the service node to other http, GASS, or GridFTP servers.","Process Name":"globus-job-status","Link":"https:\/\/linux.die.net\/man\/1\/globus-job-status"}},{"Process":{"Description":null,"Process Name":"globus-job-submit","Link":"https:\/\/linux.die.net\/man\/1\/globus-job-submit"}},{"Process":{"Description":"The globus-personal-gatekeeper command is a utility which manages a gatekeeper and job manager service for a single user. Depending on the command-line arguments it will operate in one of several modes. In the first set of arguments indicated in the synopsis, the program provides information about the globus-personal-gatekeeper command or about instances of the globus-personal-gatekeeper that are running currently. The second set of arguments indicated in the synopsis provide control over starting a new globus-personal-gatekeeper instance. The final set of arguments provide control for terminating one or more globus-personal-gatekeeper instances. The -start mode will create a new subdirectory of $HOME\/.globus and write the configuration files needed to start a globus-gatekeeper daemon which will invoke the globus-job-manager service when new authenticated connections are made to its service port. The globus-personal-gatekeeper then exits, printing the contact string for the new gatekeeper prefixed by GRAM contact: to standard output. In addition to the arguments described above, any arguments described in globus-job-manager(8) can be appended to the command-line and will be added to the job manager configuration for the service started by the globus-gatekeeper. The new globus-gatekeeper will continue to run in the background until killed by invoking globus-personal-gatekeeper with the -kill or -killall argument. When killed, it will kill the globus-gatekeeper and globus-job-manager processes, remove state files and configuration data, and then exit. Jobs which are running when the personal gatekeeper is killed will continue to run, but their job directory will be destroyed so they may fail in the LRM. The full set of command-line options to globus-personal-gatekeeper consists of: -help, -usage Print command-line option summary and exit -version Print software version -versions Print software version including DiRT information -list Print a list of all currently running personal gatekeepers. These entries will be printed one per line. -directory CONTACT Print the configuration directory for the personal gatekeeper with the contact string CONTACT. -debug Print additional debugging information when starting a personal gatekeeper. This option is ignored in other modes. -start Start a new personal gatekeeper process. -jmtype LRM Use LRM as the local resource manager interface. If not provided when starting a personal gatekeeper, the job manager will use the default fork LRM. -auditdir AUDIT_DIRECTORY Write audit report files to AUDIT_DIRECTORY. If not provided, the job manager will not write any audit files. -port PORT Listen for gatekeeper TCP\/IP connections on the port PORT. If not provided, the gatekeeper will let the operating system choose. -log[= DIRECTORY] Write job manager log files to DIRECTORY. If DIRECTORY is omitted, the default of $HOME will be used. If this option is not present, the job manager will not write any log files. -seg Try to use the SEG mechanism to receive job state change information, instead of polling for these. These require either the system administrator or the user to run an instance of the globus-job-manager-event-generator program for the LRM specified by the -jmtype option. -acctfile ACCOUNTING_FILE Write gatekeeper accounting entries to ACCOUNTING_FILE. If not provided, no accounting records are written.","Process Name":"globus-personal-gatekeeper","Link":"https:\/\/linux.die.net\/man\/1\/globus-personal-gatekeeper"}},{"Process":{"Description":"globus-rls-cli provides a command line interface to some of the functions supported by the Globus Replica Location service. It also supports an interactive interface (if command is not specified). In interactive move double quotes may be used to encode an argument that contains white space.","Process Name":"globus-rls-cli","Link":"https:\/\/linux.die.net\/man\/1\/globus-rls-cli"}},{"Process":{"Description":"Defines a set of GLOBUS_SH_ variables, then sources a user script. The user script is a local file, or can be referred to with a GASS URL. Additional arguments \"args ...\" are passed on to the user script. -help Shows help and exists -usage Shows help and exists -version Shows version and exits -list, -l Lists the defined GLOBUS_ variables and exits -exec, -e Commands are read from the argument line","Process Name":"globus-sh-exec","Link":"https:\/\/linux.die.net\/man\/1\/globus-sh-exec"}},{"Process":{"Description":null,"Process Name":"globus-spec-creator","Link":"https:\/\/linux.die.net\/man\/1\/globus-spec-creator"}},{"Process":{"Description":"The globus-url-copy program is a command line tool for multi-protocol data movement. It supports gsiftp:\/\/ (GridFTP), ftp:\/\/, http:\/\/, https:\/\/, sshftp:\/\/ and file:\/\/\/ protocol specifiers in the URL.","Process Name":"globus-url-copy","Link":"https:\/\/linux.die.net\/man\/1\/globus-url-copy"}},{"Process":{"Description":"Displays the installed Globus version. Can also show the dirt timestamp and tag id. -help Shows help and exists -full Show expanded version information -dirt Shows dirt info and exists","Process Name":"globus-version","Link":"https:\/\/linux.die.net\/man\/1\/globus-version"}},{"Process":{"Description":null,"Process Name":"globusrun","Link":"https:\/\/linux.die.net\/man\/1\/globusrun"}},{"Process":{"Description":"lookbib prints a prompt on the standard error (unless the standard input is not a terminal), reads from the standard input a line containing a set of keywords, searches the bibliographic databases filename... for references containing those keywords, prints any references found on the standard output, and repeats this process until the end of input. For each database filename to be searched, if an index filename .i created by indxbib(1) exists, then it will be searched instead; each index can cover multiple databases.","Process Name":"glookbib","Link":"https:\/\/linux.die.net\/man\/1\/glookbib"}},{"Process":{"Description":null,"Process Name":"glplanet","Link":"https:\/\/linux.die.net\/man\/1\/glplanet"}},{"Process":{"Description":"Loads a random sequence of images and smoothly scans and zooms around in each, fading from pan to pan. This program requires a good video card capable of supporting large textures. To specify the directory that images are loaded from, run xscreensaver-demo(1) and click on the \"Advanced\" tab.","Process Name":"glslideshow","Link":"https:\/\/linux.die.net\/man\/1\/glslideshow"}},{"Process":{"Description":"glsnake is an imitation of Rubiks' Snake, using OpenGL.","Process Name":"glsnake","Link":"https:\/\/linux.die.net\/man\/1\/glsnake"}},{"Process":{"Description":null,"Process Name":"glue-validator","Link":"https:\/\/linux.die.net\/man\/1\/glue-validator"}},{"Process":{"Description":"glxgears is a GLX demo that draws three rotating gears, and prints out framerate information to stdout. Command line options include: -info Print out GL implementation information before running the demo. -display displayname Specify the display to query.","Process Name":"glxgears","Link":"https:\/\/linux.die.net\/man\/1\/glxgears"}},{"Process":{"Description":"glxinfo lists information about the GLX extension, OpenGL capable visuals, and the OpenGL renderer on an X server. The GLX and renderer info includes the version and extension attributes. The visual info lists the GLX visual attributes available for each OpenGL capable visual (e.g. whether the visual is double buffered, the component sizes, Z-buffering depth, etc). Command line options include: -t By default the visual info is presented in a concise 80 character wide tabular format. The -t option directs glxinfo to produce a wider, more readable tabular format. -v Directs glxinfo to generate a verbose format output style for the visual list similar to the info of xdpyinfo. -b Print the ID of the \"best\" visual on screen 0. -l Print interesting OpenGL limits. -i Use indirect rendering connection only. -display displayname Specify the display to query.","Process Name":"glxinfo","Link":"https:\/\/linux.die.net\/man\/1\/glxinfo"}},{"Process":{"Description":"GraphicsMagick's gm provides a suite of command-line utilities for creating, converting, editing, and displaying images: Gm display is a machine architecture independent image processing and display facility. It can display an image on any workstation display running an X server. Gm import reads an image from any visible window on an X server and outputs it as an image file. You can capture a single window, the entire screen, or any rectangular portion of the screen. Gm montage creates a composite by combining several separate images. The images are tiled on the composite image with the name of the image optionally appearing just below the individual tile. Gm convert converts an input file using one image format to an output file with the same or differing image format while applying an arbitrary number of image transformations. Gm mogrify transforms an image or a sequence of images. These transforms include image scaling, image rotation, color reduction, and others. The transmogrified image overwrites the original image. Gm identify describes the format and characteristics of one or more image files. It will also report if an image is incomplete or corrupt. Gm composite composites images (blends or merges images together) to create new images. Gm conjure interprets and executes scripts in the Magick Scripting Language (MSL). The GraphicsMagick utilities recognize the following image formats: Name Mode Description o 3FR r-- Hasselblad Photo RAW o 8BIM rw- Photoshop resource format o 8BIMTEXT rw- Photoshop resource text format o 8BIMWTEXT rw- Photoshop resource wide text format o APP1 rw- Raw application information o APP1JPEG rw- Raw JPEG binary data o ART r-- PF1: 1st Publisher o ARW r-- Sony Alpha DSLR RAW o AVI r-- Audio\/Visual Interleaved o AVS rw+ AVS X image o BIE rw- Joint Bi-level Image experts Group interchange format o BMP rw+ Microsoft Windows bitmap image o BMP2 -w- Microsoft Windows bitmap image v2 o BMP3 -w- Microsoft Windows bitmap image v3 o CACHE --- Magick Persistent Cache image format o CALS rw- Continuous Acquisition and Life-cycle Support Type 1 image o CAPTION r-- Caption (requires separate size info) o CIN rw- Kodak Cineon Format o CMYK rw- Raw cyan, magenta, yellow, and black samples (8 or 16 bits, depending on the image depth) o CMYKA rw- Raw cyan, magenta, yellow, black, and matte samples (8 or 16 bits, depending on the image depth) o CR2 r-- Canon Photo RAW o CRW r-- Canon Photo RAW o CUR r-- Microsoft Cursor Icon o CUT r-- DR Halo o DCM r-- Digital Imaging and Communications in Medicine image o DCR r-- Kodak Photo RAW o DCX rw+ ZSoft IBM PC multi-page Paintbrush o DNG r-- Adobe Digital Negative o DPS r-- Display PostScript Interpreter o DPX rw- Digital Moving Picture Exchange o EPDF rw- Encapsulated Portable Document Format o EPI rw- Adobe Encapsulated PostScript Interchange format o EPS rw- Adobe Encapsulated PostScript o EPS2 -w- Adobe Level II Encapsulated PostScript o EPS3 -w- Adobe Level III Encapsulated PostScript o EPSF rw- Adobe Encapsulated PostScript o EPSI rw- Adobe Encapsulated PostScript Interchange format o EPT rw- Adobe Encapsulated PostScript with MS-DOS TIFF preview o EPT2 rw- Adobe Level II Encapsulated PostScript with MS-DOS TIFF preview o EPT3 rw- Adobe Level III Encapsulated PostScript with MS-DOS TIFF preview o EXIF rw- Exif digital camera binary data o FAX rw+ Group 3 FAX (Not TIFF Group3 FAX!) o FITS rw- Flexible Image Transport System o FRACTAL r-- Plasma fractal image o FPX rw- FlashPix Format o GIF rw+ CompuServe graphics interchange format o GIF87 rw- CompuServe graphics interchange format (version 87a) o GRADIENT r-- Gradual passing from one shade to another o GRAY rw+ Raw gray samples (8\/16\/32 bits, depending on the image depth) o HISTOGRAM -w- Histogram of the image o HRZ r-- HRZ: Slow scan TV o HTML -w- Hypertext Markup Language and a client-side image map o ICB rw+ Truevision Targa image o ICC rw- ICC Color Profile o ICM rw- ICC Color Profile o ICO r-- Microsoft icon o ICON r-- Microsoft icon o IDENTITY r-- Hald CLUT identity image o IMAGE r-- GraphicsMagick Embedded Image o INFO -w+ Image descriptive information and statistics o IPTC rw- IPTC Newsphoto o IPTCTEXT rw- IPTC Newsphoto text format o IPTCWTEXT rw- IPTC Newsphoto wide text format o JBG rw+ Joint Bi-level Image experts Group interchange format o JBIG rw+ Joint Bi-level Image experts Group interchange format o JNG rw- JPEG Network Graphics o JP2 rw- JPEG-2000 JP2 File Format Syntax o JPC rw- JPEG-2000 Code Stream Syntax o JPEG rw- Joint Photographic Experts Group JFIF format o JPG rw- Joint Photographic Experts Group JFIF format o K25 r-- Kodak Photo RAW o KDC r-- Kodak Photo RAW o LABEL r-- Text image format o M2V rw+ MPEG-2 Video Stream o MAP rw- Colormap intensities and indices o MAT r-- MATLAB image format o MATTE -w+ MATTE format o MIFF rw+ Magick Image File Format o MNG rw+ Multiple-image Network Graphics o MONO rw- Bi-level bitmap in least-significant- -byte-first order o MPC rw+ Magick Persistent Cache image format o MPEG rw+ MPEG-1 Video Stream o MPG rw+ MPEG-1 Video Stream o MRW r-- Minolta Photo Raw o MSL r-- Magick Scripting Language o MTV rw+ MTV Raytracing image format o MVG rw- Magick Vector Graphics o NEF r-- Nikon Electronic Format o NULL r-- Constant image of uniform color o OTB rw- On-the-air bitmap o P7 rw+ Xv thumbnail format o PAL rw- 16bit\/pixel interleaved YUV o PALM rw- Palm Pixmap o PBM rw+ Portable bitmap format (black and white) o PCD rw- Photo CD o PCDS rw- Photo CD o PCL -w- Page Control Language o PCT rw- Apple Macintosh QuickDraw\/PICT o PCX rw- ZSoft IBM PC Paintbrush o PDB rw+ Palm Database ImageViewer Format o PDF rw+ Portable Document Format o PEF r-- Pentax Electronic File o PFA r-- TrueType font o PFB r-- TrueType font o PGM rw+ Portable graymap format (gray scale) o PGX r-- JPEG-2000 VM Format o PICON rw- Personal Icon o PICT rw- Apple Macintosh QuickDraw\/PICT o PIX r-- Alias\/Wavefront RLE image format o PLASMA r-- Plasma fractal image o PNG rw- Portable Network Graphics o PNG24 rw- Portable Network Graphics, 24 bit RGB opaque only o PNG32 rw- Portable Network Graphics, 32 bit RGBA semitransparency OK o PNG8 rw- Portable Network Graphics, 8-bit indexed, binary transparency only o PNM rw+ Portable anymap o PPM rw+ Portable pixmap format (color) o PREVIEW -w- Show a preview an image enhancement, effect, or f\/x o PS rw+ Adobe PostScript o PS2 -w+ Adobe Level II PostScript o PS3 -w+ Adobe Level III PostScript o PSD rw- Adobe Photoshop bitmap o PTIF rw- Pyramid encoded TIFF o PWP r-- Seattle Film Works o RAF r-- Fuji Photo RAW o RAS rw+ SUN Rasterfile o RGB rw+ Raw red, green, and blue samples o RGBA rw+ Raw red, green, blue, and matte samples o RLA r-- Alias\/Wavefront image o RLE r-- Utah Run length encoded image o SCT r-- Scitex HandShake o SFW r-- Seattle Film Works o SGI rw+ Irix RGB image o SHTML -w- Hypertext Markup Language and a client-side image map o STEGANO r-- Steganographic image o SUN rw+ SUN Rasterfile o SVG rw+ Scalable Vector Gaphics o TEXT rw+ Raw text o TGA rw+ Truevision Targa image o TIFF rw+ Tagged Image File Format o TILE r-- Tile image with a texture o TIM r-- PSX TIM o TOPOL r-- TOPOL X Image o TTF r-- TrueType font o TXT rw+ Raw text o UIL -w- X-Motif UIL table o UYVY rw- 16bit\/pixel interleaved YUV o VDA rw+ Truevision Targa image o VICAR rw- VICAR rasterfile format o VID rw+ Visual Image Directory o VIFF rw+ Khoros Visualization image o VST rw+ Truevision Targa image o WBMP rw- Wireless Bitmap (level 0) image o WMF r-- Windows Metafile o WPG r-- Word Perfect Graphics o X rw- X Image o X3F r-- Foveon X3 (Sigma\/Polaroid) RAW o XBM rw- X Windows system bitmap (black and white) o XC r-- Constant image uniform color o XCF r-- GIMP image o XMP rw- Adobe XML metadata o XPM rw- X Windows system pixmap (color) o XV rw+ Khoros Visualization image o XWD rw- X Windows system window dump (color) o YUV rw- CCIR 601 4:1:1 or 4:2:2 (8-bit only) Modes: r Read w Write + Multi-image Support for some of these formats require additional programs or libraries. README tells where to find this software. Note, a format delineated with + means that if more than one image is specified, it is composited into a single multi-image file. Use +adjoin if you want a single image produced for each frame. Your installation might not support all of the formats in the list. To get an up-to-date listing of the formats supported by your particular configuration, run \"convert -list format\". Raw images are expected to have one byte per pixel unless gm is compiled in 16-bit mode or in 32-bit mode. Here, the raw data is expected to be stored two or four bytes per pixel, respectively, in most-significant-byte-first order. You can tell if gm was compiled in 16-bit mode by typing \"gm version\" without any options, and looking for \"Q:16\" in the first line of output.","Process Name":"gm","Link":"https:\/\/linux.die.net\/man\/1\/gm"}},{"Process":{"Description":"gmac is a microcode and macrocode compiler for tkgate. More complete documentation can be found at: http:\/\/www.cs.cmu.edu\/~hansen\/tkgate","Process Name":"gmac","Link":"https:\/\/linux.die.net\/man\/1\/gmac"}},{"Process":{"Description":"The purpose of the make utility is to determine automatically which pieces of a large program need to be recompiled, and issue the commands to recompile them. The manual describes the GNU implementation of make, which was written by Richard Stallman and Roland McGrath, and is currently maintained by Paul Smith. Our examples show C programs, since they are most common, but you can use make with any programming language whose compiler can be run with a shell command. In fact, make is not limited to programs. You can use it to describe any task where some files must be updated automatically from others whenever the others change. To prepare to use make, you must write a file called the makefile that describes the relationships among files in your program, and the states the commands for updating each file. In a program, typically the executable file is updated from object files, which are in turn made by compiling source files. Once a suitable makefile exists, each time you change some source files, this simple shell command: make suffices to perform all necessary recompilations. The make program uses the makefile data base and the last-modification times of the files to decide which of the files need to be updated. For each of those files, it issues the commands recorded in the data base. make executes commands in the makefile to update one or more target names, where name is typically a program. If no -f option is present, make will look for the makefiles GNUmakefile, makefile, and Makefile, in that order. Normally you should call your makefile either makefile or Makefile. (We recommend Makefile because it appears prominently near the beginning of a directory listing, right near other important files such as README.) The first name checked, GNUmakefile, is not recommended for most makefiles. You should use this name if you have a makefile that is specific to GNU make, and will not be understood by other versions of make. If makefile is '-', the standard input is read. make updates a target if it depends on prerequisite files that have been modified since the target was last modified, or if the target does not exist.","Process Name":"gmake","Link":"https:\/\/linux.die.net\/man\/1\/gmake"}},{"Process":{"Description":"The Ganglia Meta Daemon (gmetad) collects information from multiple gmond or gmetad data sources, saves the information to local round-robin databases, and exports XML which is the concatentation of all data sources -h, --help Print help and exit -V, --version Print version and exit -c, --conf= STRING Location of gmetad configuration file (default='\/etc\/ganglia\/gmetad.conf') -d, --debug= INT Debug level. If greater than zero, daemon will stay in foreground. (default='0') -p, --pid-file= STRING Write process-id to file","Process Name":"gmetad","Link":"https:\/\/linux.die.net\/man\/1\/gmetad"}},{"Process":{"Description":"The Ganglia Metric Client (gmetric) announces a metric on the list of defined send channels defined in a configuration file -h, --help Print help and exit -V, --version Print version and exit -c, --conf= STRING The configuration file to use for finding send channels (default='\/etc\/ganglia\/gmond.conf') -n, --name= STRING Name of the metric -v, --value= STRING Value of the metric -t, --type= STRING Either string|int8|uint8|int16|uint16|int32|uint32|float|double -u, --units= STRING Unit of measure for the value e.g. Kilobytes, Celcius (default='') -s, --slope= STRING Either zero|positive|negative|both (default='both') -x, --tmax= INT The maximum time in seconds between gmetric calls (default='60') -d, --dmax= INT The lifetime in seconds of this metric (default='0') -S, --spoof= STRING IP address and name of host\/device (colon separated) we are spoofing (default='') -H, --heartbeat spoof a heartbeat message (use with spoof option)","Process Name":"gmetric","Link":"https:\/\/linux.die.net\/man\/1\/gmetric"}},{"Process":{"Description":"gml2gv converts a graph specified in the GML format to a graph in the GV (formerly DOT) format.","Process Name":"gml2gv","Link":"https:\/\/linux.die.net\/man\/1\/gml2gv"}},{"Process":{"Description":"The Ganglia Monitoring Daemon (gmond) listens to the cluster message channel, stores the data in-memory and when requested will output an XML description of the state of the cluster -h, --help Print help and exit -V, --version Print version and exit -c, --conf= STRING Location of gmond configuration file (default= '\/etc\/ganglia\/gmond.conf') -l, --location= STRING Location of this host in the cluster 'rack,rank,plane'. (default='0,0,0') -d, --debug= INT Debug level. If greater than zero, daemon will stay in foreground. (default='0') -f, --foreground Run in foreground (don't daemonize) (default=off) -t, --default_config Print the default configuration to stdout and exit (default=off) -m, --metrics Print the list of metrics this gmond supports (default=off) -b, --bandwidth Calculate minimum bandwidth use for configuration (default=off) -r, --convert= STRING Convert a 2.5.x configuration file to the new 3.x format -p, --pid-file= STRING Write process-id to file","Process Name":"gmond","Link":"https:\/\/linux.die.net\/man\/1\/gmond"}},{"Process":{"Description":"","Process Name":"gmt","Link":"https:\/\/linux.die.net\/man\/1\/gmt"}},{"Process":{"Description":"gmt2bin reads a file with a list of leg ids, opens each gmt file, and creates a bin-index file for each leg called <legid>.bix. These are used by binlegs to update the system files read by gmtlegs. Normally, only the data archivist will need to be concerned with these operations. listfile This is a one column ASCII file with one leg id pr line.","Process Name":"gmt2bin","Link":"https:\/\/linux.die.net\/man\/1\/gmt2bin"}},{"Process":{"Description":"gmt2dat reads a <legid>.gmt file and converts it to a regular ASCII text file that can be edited using the text editor. The ASCII text is sent to standard output and can be redirected to a file\/program\/printer, etc.","Process Name":"gmt2dat","Link":"https:\/\/linux.die.net\/man\/1\/gmt2dat"}},{"Process":{"Description":"gmt2kml reads one or more GMT table file and converts them to a single output file using Google Earth's KML format. Data may represent points, lines, or polygons, and you may specify additional attributes such as title, altitude mode, colors, pen widths, transparency, regions, and data descriptions. You may also extend the feature down to ground level (assuming it is above it) and use custom icons for point symbols. The input file should contain the following columns: lon lat [ alt ] [ timestart [ timestop ] ] where lon and lat are required for all features, alt is optional for all features (see also -A and -C), and timestart and timestop apply to events and timespan features. infile(s) ASCII (or binary, see -bi) data file(s) to be operated on. If not given, standard input will be read.","Process Name":"gmt2kml","Link":"https:\/\/linux.die.net\/man\/1\/gmt2kml"}},{"Process":{"Description":"gmt2rgb reads one of three types of input files: (1) A Sun 8-, 24-, or 32-bit raster file; we the write out the red, green, and blue components (0-255 range) to separate grid files. Since the raster file header is limited you may use the -R, -F, -I options to set a complete header record [Default is simply based on the number of rows and columns]. (2) A binary 2-D grid file; we then convert the z-values to red, green, blue via the provided cpt file. Optionally, only write out one of the r, g, b, layers. (3) A RGB or RGBA raw raster file. Since raw rasterfiles have no header, you have to give the image dimensions via the -W option. infile The (1) Sun raster file, (2) 2-D binary grid file, or (3) raw raster file to be converted. -G Provide an output name template for the three output grids. The template should be a regular grid file name except it must contain the string %c which on output will be replaced by r, g, or b.","Process Name":"gmt2rgb","Link":"https:\/\/linux.die.net\/man\/1\/gmt2rgb"}},{"Process":{"Description":"gmt_shell_functions.sh provides a set of functions to Bourne (again) shell scripts in support of GMT. The calling shell script should include the following line, before the functions can be used: . gmt_shell_functions.sh Once included in a shell script, gmt_shell_functions.sh allows GMT users to do some scripting more easily than otherwise. The functions made available are: gmt_init_tmpdir Creates a temporary directory in \/tmp or (when defined) in the directory specified by the environment variable TMPDIR. The name of the temporary directory is returned as environment variable GMT_TMPDIR. This function also causes GMT to run in 'isolation mode', i.e., all temporary files will be created in GMT_TMPDIR and the .gmtdefaults file will not be adjusted. gmt_remove_tmpdir Removes the temporary directory and unsets the GMT_TMPDIR environment variable. gmt_cleanup Remove all files and directories in which the current process number is part of the file name. If the optional prefix is given then we also delete all files and directories that begins with the given prefix. gmt_message Send a message to standard error. gmt_abort Send a message to standard error and exit the shell. gmt_nrecords Returns the total number of lines in file(s) gmt_nfields Returns the number of fields or words in string gmt_get_field Returns the given field in a string. Must pass string between double quotes to preserve it as one item. gmt_get_region Returns the region in the form w\/e\/s\/n based on the data in table file(s). Optionally add -I dx\/ dy to round off the answer. gmt_get_gridregion Returns the region in the form w\/e\/s\/n based on the header of a grid file. Optionally add -I dx\/ dy to round off the answer. gmt_map_width Expects the user to give the desired -R -J settings and returns the map width in the current measurement unit. gmt_map_height Expects the user to give the desired -R -J settings and returns the map height in the current measurement unit. gmt_set_psfile Create the output PostScript file name based on the base name of a given file (usually the script name $0). gmt_set_framename Returns a lexically ordered filename stem (i.e., no extension) given the file prefix and the current frame number, using a width of 6 for the integer including leading zeros. Useful when creating animations and lexically sorted filenames are required. gmt_set_framenext Accepts the current frame integer counter and returns the next integer counter.","Process Name":"gmt_shell_functions","Link":"https:\/\/linux.die.net\/man\/1\/gmt_shell_functions"}},{"Process":{"Description":"gmtconvert reads its standard input [or inputfiles] and writes out the desired information to standard output. It can do a combination of three things: (1) convert between binary and ASCII data tables, (2) paste corresponding records from multiple files into a single file, (3) extract a subset of the columns, (4) only extract segments whose header matches a text pattern search, (5) just list all multisegment headers and no data records, and (6) extract first and last data record for each segment. Input (and hence output) may have multiple subheaders if -m is selected, and ASCII tables may have regular headers as well. datafile(s) ASCII (or binary, see -bi) file(s) holding a number of data columns.","Process Name":"gmtconvert","Link":"https:\/\/linux.die.net\/man\/1\/gmtconvert"}},{"Process":{"Description":"gmtdefaults lists the GMT parameter defaults if the option -D is used. There are three ways to change some of the settings: (1) Use the command gmtset, (2) use any texteditor to edit the file .gmtdefaults4 in your home, ~\/.gmt or current directory (if you do not have this file, run gmtdefaults -D > ~\/.gmtdefaults4 to get one with the system default settings), or (3) override any parameter by specifying one or more --PARAMETER=value statements on the commandline of any GMT command (PARAMETER and VALUE are any combination listed below). The first two options are permanent changes until explicitly changed back, while the last option is ephemeral and only applies to the single GMT command that received the override. GMT can provide default values in US or SI units. This choice is determined by the contents of the gmt.conf file in GMT's share directory. -D Print the system GMT defaults to standard output. Append u for US defaults or s for SI defaults. [-D alone gives current choice in gmt.conf]. -L Print the user's currently active defaults to standard output. Your currently active defaults come from the .gmtdefaults4 file in the current working directory, if present; else from the .gmtdefaults4 file in your home directory, if present; else from the file ~\/.gmt\/.gmtdefaults4, if present; else from the system defaults set at the time GMT was compiled.","Process Name":"gmtdefaults","Link":"https:\/\/linux.die.net\/man\/1\/gmtdefaults"}},{"Process":{"Description":"gmtdigitize digitizes points from a digitizer via a serial line connection and computes map coordinates using the specified map projection. The program is interactive and will take you through the setup procedure and how you will digitize points. The program will determine the actual map scale as well as rotation of the paper that is taped to the digitizer table. By default the output will go to stdout. No space between the option flag and the associated arguments. Use upper case for the option flags and lower case for modifiers. -J Selects the map projection. Scale is UNIT\/degree, 1:xxxxx, or width in UNIT (upper case modifier). UNIT is cm, inch, or m, depending on the MEASURE_UNIT setting in .gmtdefaults4, but this can be overridden on the command line by appending c, i, or m to the scale\/width value. When central meridian is optional, default is center of longitude range on -R option. Default standard parallel is the equator. For map height, max dimension, or min dimension, append h, +, or - to the width, respectively. More details can be found in the psbasemap man pages. CYLINDRICAL PROJECTIONS: -Jclon0\/lat0\/scale (Cassini) -Jcyl_stere\/[lon0\/[lat0\/]]scale (Cylindrical Stereographic) -Jj[lon0\/]scale (Miller) -Jm[lon0\/[lat0\/]]scale (Mercator) -Jmlon0\/lat0\/scale (Mercator - Give meridian and standard parallel) -Jo[a]lon0\/lat0\/azimuth\/scale (Oblique Mercator - point and azimuth) -Jo[b]lon0\/lat0\/lon1\/lat1\/scale (Oblique Mercator - two points) -Joclon0\/lat0\/lonp\/latp\/scale (Oblique Mercator - point and pole) -Jq[lon0\/[lat0\/]]scale (Cylindrical Equidistant) -Jtlon0\/[lat0\/]scale (TM - Transverse Mercator) -Juzone\/scale (UTM - Universal Transverse Mercator) -Jy[lon0\/[lat0\/]]scale (Cylindrical Equal-Area) CONIC PROJECTIONS: -Jblon0\/lat0\/lat1\/lat2\/scale (Albers) -Jdlon0\/lat0\/lat1\/lat2\/scale (Conic Equidistant) -Jllon0\/lat0\/lat1\/lat2\/scale (Lambert Conic Conformal) -Jpoly\/[lon0\/[lat0\/]]scale ((American) Polyconic) AZIMUTHAL PROJECTIONS: -Jalon0\/lat0[\/horizon]\/scale (Lambert Azimuthal Equal-Area) -Jelon0\/lat0[\/horizon]\/scale (Azimuthal Equidistant) -Jflon0\/lat0[\/horizon]\/scale (Gnomonic) -Jglon0\/lat0[\/horizon]\/scale (Orthographic) -Jglon0\/lat0\/altitude\/azimuth\/tilt\/twist\/Width\/Height\/scale (General Perspective). -Jslon0\/lat0[\/horizon]\/scale (General Stereographic) MISCELLANEOUS PROJECTIONS: -Jh[lon0\/]scale (Hammer) -Ji[lon0\/]scale (Sinusoidal) -Jkf[lon0\/]scale (Eckert IV) -Jk[s][lon0\/]scale (Eckert VI) -Jn[lon0\/]scale (Robinson) -Jr[lon0\/]scale (Winkel Tripel) -Jv[lon0\/]scale (Van der Grinten) -Jw[lon0\/]scale (Mollweide) NON-GEOGRAPHICAL PROJECTIONS: -Jp[a]scale[\/origin][r|z] (Polar coordinates (theta,r)) -Jxx-scale[d|l|ppow|t|T][\/y-scale[d|l|ppow|t|T]] (Linear, log, and power scaling) For geographic projections you can give 1 as the scale will be solved for anyway. -R xmin, xmax, ymin, and ymax specify the Region of interest. For geographic regions, these limits correspond to west, east, south, and north and you may specify them in decimal degrees or in [+-]dd:mm[:ss.xxx][W|E|S|N] format. Append r if lower left and upper right map coordinates are given instead of w\/e\/s\/n. The two shorthands -Rg and -Rd stand for global domain (0\/360 and -180\/+180 in longitude respectively, with -90\/+90 in latitude). Alternatively, specify the name of an existing grid file and the -R settings (and grid spacing, if applicable) are copied from the grid. For calendar time coordinates you may either give (a) relative time (relative to the selected TIME_EPOCH and in the selected TIME_UNIT; append t to -JX| x), or (b) absolute time of the form [ date] T[ clock] (append T to -JX| x). At least one of date and clock must be present; the T is always required. The date string must be of the form [-]yyyy[-mm[-dd]] (Gregorian calendar) or yyyy[-Www[-d]] (ISO week calendar), while the clock string must be of the form hh:mm:ss[.xxx]. The use of delimiters and their type and positions must be exactly as indicated (however, input, output and plot formats are customizable; see gmtdefaults).","Process Name":"gmtdigitize","Link":"https:\/\/linux.die.net\/man\/1\/gmtdigitize"}},{"Process":{"Description":"gmtdp reads one or more data files (which may be multisegment files; see -m) and apply the Douglas-Peucker line simplification algorithm. The method recursively subdivides a polygon until a run of points can be replaced by a straight line segment, with no point in that run deviating from the straight line by more than the tolerance. Have a look at this site to get a visual insight on how the algorithm works http:\/\/geometryalgorithms.com\/Archive\/algorithm_0205\/algorithm_0205.htm WARNING: currently this program should be used only with geographical coordinates. file(s) One of more data files. If none are supplied then we read standard input.","Process Name":"gmtdp","Link":"https:\/\/linux.die.net\/man\/1\/gmtdp"}},{"Process":{"Description":null,"Process Name":"gmtget","Link":"https:\/\/linux.die.net\/man\/1\/gmtget"}},{"Process":{"Description":"gmtinfo reports the minimum\/maximum coordinates, year of the cruise, leg id, number of records, the number of gravity, magnetics, and bathymetry points found, and the start\/stop dates for the leg. All the information is written to standard output on one line so that it is easy to use awk to add up numbers etc. leg-ids Can be one or more gmtleg-names, like c2610. Use 'cat list_of_legs' to pass all the legnames in the file list_of_legs.","Process Name":"gmtinfo","Link":"https:\/\/linux.die.net\/man\/1\/gmtinfo"}},{"Process":{"Description":null,"Process Name":"gmtlegs","Link":"https:\/\/linux.die.net\/man\/1\/gmtlegs"}},{"Process":{"Description":"gmtlist reads <legid>.gmt files and produces an ASCII [or binary] table. The <legid>.gmt files contain time( s), latitude( y), longitude( x), gravity( g\/G), magnetics( m\/M), and bathymetry( t\/T), and the user may extract any combination of these 6 parameters + distance( d), heading ( h), velocity ( v), and weight ( w, see -W). A sub-section can be specified by passing time- or distance-intervals along track or by selecting a region. leg-ids Can be one or more cruisenames. To give a list of names, use 'cat list_of_legs'.","Process Name":"gmtlist","Link":"https:\/\/linux.die.net\/man\/1\/gmtlist"}},{"Process":{"Description":null,"Process Name":"gmtlogo","Link":"https:\/\/linux.die.net\/man\/1\/gmtlogo"}},{"Process":{"Description":"gmtmath will perform operations like add, subtract, multiply, and divide on one or more table data files or constants using Reverse Polish Notation (RPN) syntax (e.g., Hewlett-Packard calculator-style). Arbitrarily complicated expressions may therefore be evaluated; the final result is written to an output file [or standard output]. When two data tables are on the stack, each element in file A is modified by the corresponding element in file B. However, some operators only require one operand (see below). If no data tables are used in the expression then options -T, -N can be set (and optionally -b to indicate the data domain). If STDIN is given, <stdin> will be read and placed on the stack as if a file with that content had been given on the command line. By default, all columns except the \"time\" column are operated on, but this can be changed (see -C). operand If operand can be opened as a file it will be read as an ASCII (or binary, see -bi) table data file. If not a file, it is interpreted as a numerical constant or a special symbol (see below). The special argument STDIN means that stdin will be read and placed on the stack; STDIN can appear more than once if necessary. outfile The name of a table data file that will hold the final result. If not given then the output is sent to stdout. OPERATORS Choose among the following 131 operators. \"args\" are the number of input and output arguments. Operator args Returns ABS 1 1 abs (A). ACOS 1 1 acos (A). ACOSH 1 1 acosh (A). ACOT 1 1 acot (A). ACSC 1 1 acsc (A). ADD 2 1 A + B. AND 2 1 NaN if A and B == NaN, B if A == NaN, else A. ASEC 1 1 asec (A). ASIN 1 1 asin (A). ASINH 1 1 asinh (A). ATAN 1 1 atan (A). ATAN2 2 1 atan2 (A, B). ATANH 1 1 atanh (A). BEI 1 1 bei (A). BER 1 1 ber (A). CEIL 1 1 ceil (A) (smallest integer >= A). CHICRIT 2 1 Critical value for chi-squared-distribution, with alpha = A and n = B. CHIDIST 2 1 chi-squared-distribution P(chi2,n), with chi2 = A and n = B. COL 1 1 Places column A on the stack. CORRCOEFF 2 1 Correlation coefficient r(A, B). COS 1 1 cos (A) (A in radians). COSD 1 1 cos (A) (A in degrees). COSH 1 1 cosh (A). COT 1 1 cot (A) (A in radians). COTD 1 1 cot (A) (A in degrees). CPOISS 2 1 Cumulative Poisson distribution F(x,lambda), with x = A and lambda = B. CSC 1 1 csc (A) (A in radians). CSCD 1 1 csc (A) (A in degrees). D2DT2 1 1 d^2(A)\/dt^2 2nd derivative. D2R 1 1 Converts Degrees to Radians. DDT 1 1 d(A)\/dt Central 1st derivative. DILOG 1 1 dilog (A). DIV 2 1 A \/ B. DUP 1 2 Places duplicate of A on the stack. EQ 2 1 1 if A == B, else 0. ERF 1 1 Error function erf (A). ERFC 1 1 Complementary Error function erfc (A). ERFINV 1 1 Inverse error function of A. EXCH 2 2 Exchanges A and B on the stack. EXP 1 1 exp (A). FACT 1 1 A! (A factorial). FCRIT 3 1 Critical value for F-distribution, with alpha = A, n1 = B, and n2 = C. FDIST 3 1 F-distribution Q(F,n1,n2), with F = A, n1 = B, and n2 = C. FLIPUD 1 1 Reverse order of each column. FLOOR 1 1 floor (A) (greatest integer <= A). FMOD 2 1 A % B (remainder after truncated division). GE 2 1 1 if A >= B, else 0. GT 2 1 1 if A > B, else 0. HYPOT 2 1 hypot (A, B) = sqrt (A*A + B*B). I0 1 1 Modified Bessel function of A (1st kind, order 0). I1 1 1 Modified Bessel function of A (1st kind, order 1). IN 2 1 Modified Bessel function of A (1st kind, order B). INRANGE 3 1 1 if B <= A <= C, else 0. INT 1 1 Numerically integrate A. INV 1 1 1 \/ A. ISNAN 1 1 1 if A == NaN, else 0. J0 1 1 Bessel function of A (1st kind, order 0). J1 1 1 Bessel function of A (1st kind, order 1). JN 2 1 Bessel function of A (1st kind, order B). K0 1 1 Modified Kelvin function of A (2nd kind, order 0). K1 1 1 Modified Bessel function of A (2nd kind, order 1). KEI 1 1 kei (A). KER 1 1 ker (A). KN 2 1 Modified Bessel function of A (2nd kind, order B). KURT 1 1 Kurtosis of A. LE 2 1 1 if A <= B, else 0. LMSSCL 1 1 LMS scale estimate (LMS STD) of A. LOG 1 1 log (A) (natural log). LOG10 1 1 log10 (A) (base 10). LOG1P 1 1 log (1+A) (accurate for small A). LOG2 1 1 log2 (A) (base 2). LOWER 1 1 The lowest (minimum) value of A. LRAND 2 1 Laplace random noise with mean A and std. deviation B. LSQFIT 1 0 Let current table be [A | b]; return least squares solution x = A \\ b. LT 2 1 1 if A < B, else 0. MAD 1 1 Median Absolute Deviation (L1 STD) of A. MAX 2 1 Maximum of A and B. MEAN 1 1 Mean value of A. MED 1 1 Median value of A. MIN 2 1 Minimum of A and B. MOD 2 1 A mod B (remainder after floored division). MODE 1 1 Mode value (Least Median of Squares) of A. MUL 2 1 A * B. NAN 2 1 NaN if A == B, else A. NEG 1 1 -A. NEQ 2 1 1 if A != B, else 0. NOT 1 1 NaN if A == NaN, 1 if A == 0, else 0. NRAND 2 1 Normal, random values with mean A and std. deviation B. OR 2 1 NaN if A or B == NaN, else A. PLM 3 1 Associated Legendre polynomial P(A) degree B order C. PLMg 3 1 Normalized associated Legendre polynomial P(A) degree B order C (geophysical convention). POP 1 0 Delete top element from the stack. POW 2 1 A ^ B. PQUANT 2 1 The B'th Quantile (0-100%) of A. PSI 1 1 Psi (or Digamma) of A. PV 3 1 Legendre function Pv(A) of degree v = real(B) + imag(C). QV 3 1 Legendre function Qv(A) of degree v = real(B) + imag(C). R2 2 1 R2 = A^2 + B^2. R2D 1 1 Convert Radians to Degrees. RAND 2 1 Uniform random values between A and B. RINT 1 1 rint (A) (nearest integer). ROOTS 2 1 Treats col A as f(t) = 0 and returns its roots. ROTT 2 1 Rotate A by the (constant) shift B in the t-direction. SEC 1 1 sec (A) (A in radians). SECD 1 1 sec (A) (A in degrees). SIGN 1 1 sign (+1 or -1) of A. SIN 1 1 sin (A) (A in radians). SINC 1 1 sinc (A) (sin (pi*A)\/(pi*A)). SIND 1 1 sin (A) (A in degrees). SINH 1 1 sinh (A). SKEW 1 1 Skewness of A. SQR 1 1 A^2. SQRT 1 1 sqrt (A). STD 1 1 Standard deviation of A. STEP 1 1 Heaviside step function H(A). STEPT 1 1 Heaviside step function H(t-A). SUB 2 1 A - B. SUM 1 1 Cumulative sum of A. TAN 1 1 tan (A) (A in radians). TAND 1 1 tan (A) (A in degrees). TANH 1 1 tanh (A). TCRIT 2 1 Critical value for Student's t-distribution, with alpha = A and n = B. TDIST 2 1 Student's t-distribution A(t,n), with t = A, and n = B. TN 2 1 Chebyshev polynomial Tn(-1<A<+1) of degree B. UPPER 1 1 The highest (maximum) value of A. XOR 2 1 B if A == NaN, else A. Y0 1 1 Bessel function of A (2nd kind, order 0). Y1 1 1 Bessel function of A (2nd kind, order 1). YN 2 1 Bessel function of A (2nd kind, order B). ZCRIT 1 1 Critical value for the normal-distribution, with alpha = A. ZDIST 1 1 Cumulative normal-distribution C(x), with x = A. SYMBOLS The following symbols have special meaning: PI 3.1415926... E 2.7182818... EULER 0.5772156... TMIN Minimum t value TMAX Maximum t value TINC t increment N The number of records T Table with t-coordinates","Process Name":"gmtmath","Link":"https:\/\/linux.die.net\/man\/1\/gmtmath"}},{"Process":{"Description":"gmtpath returns the full pathname to one or more gmt-files. The pathname returned for a given cruise may change with time due to reshuffling of disks\/subdirectories. leg-ids Can be one or more gmtleg-names, like c2610. Use 'cat list_of_legs' to pass all the legnames in the file list_of_legs.","Process Name":"gmtpath","Link":"https:\/\/linux.die.net\/man\/1\/gmtpath"}},{"Process":{"Description":null,"Process Name":"gmtselect","Link":"https:\/\/linux.die.net\/man\/1\/gmtselect"}},{"Process":{"Description":"gmtset will adjust individual GMT defaults settings in the current directory's .gmtdefaults4 file. If no such file exists one will be created. The main purpose of gmtset is temporarily to change certain parameters inside a shell script, e.g., set the dots-per-inch to 600, run the script, and reset to 300 dpi. Optionally, you can specify one or more temporary changes directly on any GMT command line with the syntax --PARAMETER= value; such changes are only in effect for that command and do not permanently change the default settings on disk. PARAMETER value Provide one or several pairs of parameter\/value combinations that you want to modify. For a complete listing of available parameters and their meaning, see the gmtdefaults man page.","Process Name":"gmtset","Link":"https:\/\/linux.die.net\/man\/1\/gmtset"}},{"Process":{"Description":"gmtstitch reads one or more data files (which may be multisegment files; see -m) and examines the coordinates of the end points of all line segments. If a pair of end points are identical or closer to each other than the specified separation tolerance then the two line segments are joined into a single segment. The process repeats until all the remaining endpoints no longer pass the tolerance test; the resulting segments are then written out to standard output. It it is not clear what the separation tolerance should be then use -L to get a list of all separation distances and analyze them to determine a suitable cutoff. file(s) One of more data files. If none are supplied then we read standard input.","Process Name":"gmtstitch","Link":"https:\/\/linux.die.net\/man\/1\/gmtstitch"}},{"Process":{"Description":"gmttrack reads gmt cruises and creates PostScript code that will plot one or more ship tracks on a map using the specified projection. The PostScript code is written to standard output. leg-ids Can be one or more gmtleg-names, like c2104 v3206 etc. -J Selects the map projection. Scale is UNIT\/degree, 1:xxxxx, or width in UNIT (upper case modifier). UNIT is cm, inch, or m, depending on the MEASURE_UNIT setting in .gmtdefaults4, but this can be overridden on the command line by appending c, i, or m to the scale\/width value. When central meridian is optional, default is center of longitude range on -R option. Default standard parallel is the equator. For map height, max dimension, or min dimension, append h, +, or - to the width, respectively. More details can be found in the psbasemap man pages. CYLINDRICAL PROJECTIONS: -Jclon0\/lat0\/scale (Cassini) -Jcyl_stere\/[lon0\/[lat0\/]]scale (Cylindrical Stereographic) -Jj[lon0\/]scale (Miller) -Jm[lon0\/[lat0\/]]scale (Mercator) -Jmlon0\/lat0\/scale (Mercator - Give meridian and standard parallel) -Jo[a]lon0\/lat0\/azimuth\/scale (Oblique Mercator - point and azimuth) -Jo[b]lon0\/lat0\/lon1\/lat1\/scale (Oblique Mercator - two points) -Joclon0\/lat0\/lonp\/latp\/scale (Oblique Mercator - point and pole) -Jq[lon0\/[lat0\/]]scale (Cylindrical Equidistant) -Jtlon0\/[lat0\/]scale (TM - Transverse Mercator) -Juzone\/scale (UTM - Universal Transverse Mercator) -Jy[lon0\/[lat0\/]]scale (Cylindrical Equal-Area) CONIC PROJECTIONS: -Jblon0\/lat0\/lat1\/lat2\/scale (Albers) -Jdlon0\/lat0\/lat1\/lat2\/scale (Conic Equidistant) -Jllon0\/lat0\/lat1\/lat2\/scale (Lambert Conic Conformal) -Jpoly\/[lon0\/[lat0\/]]scale ((American) Polyconic) AZIMUTHAL PROJECTIONS: -Jalon0\/lat0[\/horizon]\/scale (Lambert Azimuthal Equal-Area) -Jelon0\/lat0[\/horizon]\/scale (Azimuthal Equidistant) -Jflon0\/lat0[\/horizon]\/scale (Gnomonic) -Jglon0\/lat0[\/horizon]\/scale (Orthographic) -Jglon0\/lat0\/altitude\/azimuth\/tilt\/twist\/Width\/Height\/scale (General Perspective). -Jslon0\/lat0[\/horizon]\/scale (General Stereographic) MISCELLANEOUS PROJECTIONS: -Jh[lon0\/]scale (Hammer) -Ji[lon0\/]scale (Sinusoidal) -Jkf[lon0\/]scale (Eckert IV) -Jk[s][lon0\/]scale (Eckert VI) -Jn[lon0\/]scale (Robinson) -Jr[lon0\/]scale (Winkel Tripel) -Jv[lon0\/]scale (Van der Grinten) -Jw[lon0\/]scale (Mollweide) NON-GEOGRAPHICAL PROJECTIONS: -Jp[a]scale[\/origin][r|z] (Polar coordinates (theta,r)) -Jxx-scale[d|l|ppow|t|T][\/y-scale[d|l|ppow|t|T]] (Linear, log, and power scaling) -R west, east, south, and north specify the Region of interest, and you may specify them in decimal degrees or in [+-]dd:mm[:ss.xxx][W|E|S|N] format. Append r if lower left and upper right map coordinates are given instead of w\/e\/s\/n. The two shorthands -Rg and -Rd stand for global domain (0\/360 and -180\/+180 in longitude respectively, with -90\/+90 in latitude). Alternatively, specify the name of an existing grid file and the -R settings (and grid spacing, if applicable) are copied from the grid.","Process Name":"gmttrack","Link":"https:\/\/linux.die.net\/man\/1\/gmttrack"}},{"Process":{"Description":null,"Process Name":"gnee","Link":"https:\/\/linux.die.net\/man\/1\/gnee"}},{"Process":{"Description":"The neqn program is actually just a shell script which invokes the eqn(1) command with the ascii output device. Note that eqn does not support low-resolution, typewriter-like devices (although it may work adequately for very simple input).","Process Name":"gneqn","Link":"https:\/\/linux.die.net\/man\/1\/gneqn"}},{"Process":{"Description":"gnetlist is the netlist extraction\/generation program which is part gEDA (GPL Electronic Design Automation) toolset. This program takes a schematic for its input and outputs a netlist. gnetlist depends heavily on guile (a scheme based scripting language). It uses guile to define the output format. Basically gnetlist reads a schematic, creates an internal representation of the various connections, and then a guile script extracts the connections into some netlist format. gnetlist is very much so a work in progress. Currently it supports the following backends: \u2022 Allegro netlist format (-g allegro) \u2022 BOM \/ BOM2 - Bill of Materials (-g bom and -g bom2) \u2022 Partslist 1,2,3 - More Bill of Materials (-g partslist[1-3]) \u2022 DRC - Start of a design rule checker (-g drc) \u2022 DRC2 - A second design rule checker (-g drc2) \u2022 gEDA - native format, mainly used for testing (-g geda) \u2022 Gossip netlist format (-g gossip) \u2022 PADS netlist format (-g pads) \u2022 PCB \/ PCBboard (-g PCB and -g PCBboard) \u2022 PCB actions file for forward annotating pin\/pad names from schematic to layout (-g pcbpins) \u2022 gsch2pcb backend (-g gsch2pcb) \u2022 ProtelII netlist format (-g protelII) \u2022 Spice compatible netlist format (-g spice) \u2022 Enhanced spice compatible netlist format (-g spice-sdb) \u2022 Tango netlist format (-g tango) \u2022 Verilog code (-g verilog) \u2022 VHDL code (-g vhdl) \u2022 VIPEC netlist format (-g vipec) \u2022 Bartels Autoengineer netlist format (-g bae) \u2022 GOSSIP system simulation system netlist format (-g gossip) \u2022 MAXASCII netlist format (-g maxascii) \u2022 VHDL-AMS netlist format (-g vams) \u2022 Futurenet2 netlist format (-g futurenet2) \u2022 SWITCAP switched capacitor simulator netlist format (-g switcap) \u2022 RF Cascade netlist format (-g cascade) \u2022 RACAL-REDAC netlist format (-g redac) \u2022 SystemC netlist backend (-g systemc) \u2022 Calay format netlist backend (-g calay) \u2022 Osmond format netlist backend (-g osmond) \u2022 Eagle netlist format (-g eagle) \u2022 Netlister for symbolic circuit analysis using Mathematica (-g mathematica) \u2022 LiquidPCB format netlist backend (-g liquidpcb) For more info on these formats please look at the README.* Please read the official documentation on how to use gnetlist, since this man page just describes the command line arguments and a few examples on how to run gnetlist.","Process Name":"gnetlist","Link":"https:\/\/linux.die.net\/man\/1\/gnetlist"}},{"Process":{"Description":null,"Process Name":"gneutronica","Link":"https:\/\/linux.die.net\/man\/1\/gneutronica"}},{"Process":{"Description":"gnokii is a multiple systems tool suite and (eventually) modem\/fax driver for the mobile phones. gnokii at the beginning was designed to support the proprietary protocols of Nokia phones and at the moment it also supports phones and GSM modems that understand AT commands, both from Nokia and from other vendors, and SIM cards in PC\/SC compatible Smart Card readers. Limited support for the older and slow Nokia MBUS protocol is also available. You can assume that your phone is supported, however there are rare cases that you will get very limited functionality with gnokii. If you have a fairly modern phone you should use the following model setting in your config file: - model = series40, if you have Nokia non-Symbian phone - model = gnapplet, if you have Nokia Symbian Series60 prior to 3rd Edition phone - model = AT, for all other - if you have some older Nokia phone you may try using its brand name, eg. for Nokia 6210 use model = 6210. See also our <http:\/\/wiki.gnokii.org\/index.php\/Config> for configurations known to work. Symbian series60 3rd edition (most Nokia n and e series) are not supported by gnapplet driver due to changes in Symbian API. For now you can get some functionality using AT driver. Please note that currently there are Nokia models with almost the same names as the old ones, like 6110c vs 6110 or 3110c vs 3110. They are completly incompatible. DO NOT use model = 6110 or model = 3110 setting for them.","Process Name":"gnokii","Link":"https:\/\/linux.die.net\/man\/1\/gnokii"}},{"Process":{"Description":"gnome-about Informative little about thing that lets us brag to our friends as our name scrolls by, and lets users click to load the GNOME home pages.","Process Name":"gnome-about","Link":"https:\/\/linux.die.net\/man\/1\/gnome-about"}},{"Process":{"Description":"GNOME Commander is a fast and powerful graphical filemanager for the GNOME desktop environment, it has a \"two-pane\" interface in the tradition of Norton and Midnight Commander.","Process Name":"gnome-commander","Link":"https:\/\/linux.die.net\/man\/1\/gnome-commander"}},{"Process":{"Description":"GNOME Dictionary provides dictionary definitions of words, using a dictionary source. For full documentation see the GNOME Dictionary online help.","Process Name":"gnome-dictionary","Link":"https:\/\/linux.die.net\/man\/1\/gnome-dictionary"}},{"Process":{"Description":null,"Process Name":"gnome-gmail","Link":"https:\/\/linux.die.net\/man\/1\/gnome-gmail"}},{"Process":{"Description":null,"Process Name":"gnome-keyring-manager","Link":"https:\/\/linux.die.net\/man\/1\/gnome-keyring-manager"}},{"Process":{"Description":"Gnome-luks-format is a GUI tool that will format a disk to contain a dm-crypt encrypted filesystem with a LUKS header.","Process Name":"gnome-luks-format","Link":"https:\/\/linux.die.net\/man\/1\/gnome-luks-format"}},{"Process":{"Description":null,"Process Name":"gnome-mount","Link":"https:\/\/linux.die.net\/man\/1\/gnome-mount"}},{"Process":{"Description":"The GNOME panel displays an area on your screen, which acts as a repository for the main menu, application launchers, and applets.","Process Name":"gnome-panel","Link":"https:\/\/linux.die.net\/man\/1\/gnome-panel"}},{"Process":{"Description":null,"Process Name":"gnome-power-manager","Link":"https:\/\/linux.die.net\/man\/1\/gnome-power-manager"}},{"Process":{"Description":"This manual page documents briefly the gnome-power-preferences command. gnome-power-preferences is the gui program for the gnome power management infrastructure","Process Name":"gnome-power-preferences","Link":"https:\/\/linux.die.net\/man\/1\/gnome-power-preferences"}},{"Process":{"Description":null,"Process Name":"gnome-power-statistics","Link":"https:\/\/linux.die.net\/man\/1\/gnome-power-statistics"}},{"Process":{"Description":null,"Process Name":"gnome-screensaver","Link":"https:\/\/linux.die.net\/man\/1\/gnome-screensaver"}},{"Process":{"Description":"gnome-screensaver-command is a tool for controlling an already running instance of gnome-screensaver.","Process Name":"gnome-screensaver-command","Link":"https:\/\/linux.die.net\/man\/1\/gnome-screensaver-command"}},{"Process":{"Description":null,"Process Name":"gnome-screensaver-preferences","Link":"https:\/\/linux.die.net\/man\/1\/gnome-screensaver-preferences"}},{"Process":{"Description":"gnome-screenshot is a GNOME utility for taking screenshots of the entire screen, a window or an user-defined area of the screen, with optional beutifying border effects.","Process Name":"gnome-screenshot","Link":"https:\/\/linux.die.net\/man\/1\/gnome-screenshot"}},{"Process":{"Description":null,"Process Name":"gnome-search-tool","Link":"https:\/\/linux.die.net\/man\/1\/gnome-search-tool"}},{"Process":{"Description":"The gnome-session program starts up the GNOME desktop environment. This command is typically executed by your login manager (either gdm, xdm, or from your X startup scripts). It will load either your saved session, or it will provide a default session for the user as defined by the system administrator (or the default GNOME installation on your system). The default session is defined in the GConf keys under \/desktop\/gnome\/session. When saving a session, gnome-session saves the currently running applications in the $XDG_CONFIG_HOME\/gnome-session\/saved-session directory. gnome-session is an X11R6 session manager. It can manage GNOME applications as well as any X11R6 SM compliant.","Process Name":"gnome-session","Link":"https:\/\/linux.die.net\/man\/1\/gnome-session"}},{"Process":{"Description":null,"Process Name":"gnome-session-properties","Link":"https:\/\/linux.die.net\/man\/1\/gnome-session-properties"}},{"Process":{"Description":"gnome-session-save can be used to log out of a GNOME session. It's called gnome-session-save instead of gnome-session-logout for historical reasons. The --gui option will report errors in dialog boxes instead of printing to stderr. If called with the --logout option, the current GNOME session will be ended, unless logging out has been inhibited by an application. The --force-logout option can be used to end the session regardless of the inhibition state. When the --logout-dialog option is given, the standard dialog displaying logout options is displayed. When --shutdown-dialog option is given, the standard dialog displaying shutdown options is displayed. The --kill and --silent options are deprecated. The --kill option is equivalent to the --logout-dialog option. If --silent is used with --kill, then it will behave as if --logout was used.","Process Name":"gnome-session-save","Link":"https:\/\/linux.die.net\/man\/1\/gnome-session-save"}},{"Process":{"Description":"gnome-session-selector can be used from a xsession desktop file to select a session before gnome-session is run. gnome-session reads and stores its session in the \"${XDG_DATA_HOME}\/gnome-session\/saved-session\" directory. gnome-session-selector works by replacing the saved-session directory by a symlink to another directory. Since the session name is used as the directory name, it may not contain '\/' characters or begin with a '.'. When a session name is specified, gnome-session-selector will create a symlink to select this session. When started without arguments or with --action load, gnome-session-selector will present a dialog that allows to choose one of the existing sessions or create a new one. When started with --action save, gnome-session-selector will present a dialog that allows to choose a session, or create a new one, then performs a session save operation.","Process Name":"gnome-session-selector","Link":"https:\/\/linux.die.net\/man\/1\/gnome-session-selector"}},{"Process":{"Description":null,"Process Name":"gnome-smproxy","Link":"https:\/\/linux.die.net\/man\/1\/gnome-smproxy"}},{"Process":{"Description":"GNOME System Log Viewer is a simple utility to display system log files.","Process Name":"gnome-system-log","Link":"https:\/\/linux.die.net\/man\/1\/gnome-system-log"}},{"Process":{"Description":"The gnome-wm script invokes the user selected window manager. If the user has not chosen a window manager it will launch a GNOME compliant window manager. The user can overwrite the selection of a window manager by setting the WINDOW_MANAGER environment variable. If a --default-wm option is given the script uses that window manager in case WINDOW_MANAGER is not set. The script takes the --sm-client-id option and translates the id to an appropriate option depending on which window manager was selected.","Process Name":"gnome-wm","Link":"https:\/\/linux.die.net\/man\/1\/gnome-wm"}},{"Process":{"Description":"GnomeMeeting is a H.323 Voip, Telephony and Video Conferencing application which uses the H.323 protocol (provided by the OpenH323 library). It can connect to a variety of other H323 applications including Microsoft NetMeeting. It also supports ILS (the Microsoft version of LDAP) servers. GnomeMeeting can work with or without a webcam, and is able to create pure audio communications or traditionnal audio+video communications. GnomeMeeting has been designed for the GNOME desktop and therefore uses gconfd-2(1) for storing its userdata. It offers to configure almost every option from within the GUI. Command-line options include -d to turn on debugging during calls. -c to call another URL.","Process Name":"gnomemeeting","Link":"https:\/\/linux.die.net\/man\/1\/gnomemeeting"}},{"Process":{"Description":"This manual page documents briefly the gnote command. Gnote is a desktop note-taking application for Linux and Unix. Simple and easy to use, but with potential to help you organize the ideas and information you deal with every day. Gnote's usefulness lies in the ability to relate notes and ideas together, using a WikiWiki-like linking system. When run without any options, the Gnote note menu appears in the Gnome Panel's notification tray area. Selecting a note from the menu displays the note's spatial window. Changes to notes are saved automatically. Links to URLs, files, and other notes are detected as you type. Rich editing such as highlighting, bold, italics, undo\/redo, and variable font sizes are supported. Global keybindings exist to allow fast access to Gnote notes and dialogs. These keybindings are set in GConf (see below) and may be activated from any application.","Process Name":"gnote","Link":"https:\/\/linux.die.net\/man\/1\/gnote"}},{"Process":{"Description":"The nroff script emulates the nroff command using groff. Only ascii, ascii8, latin1, utf8, nippon, and cp1047 are valid arguments for the -T option. If an invalid or no -T option is given, nroff checks the current locale to select a default output device. It first tries the locale program, then the environment variables LC_ALL, LC_CTYPE, and LANG, and finally the LESSCHARSET environment variable. The -h and -c options are equivalent to grotty's options -h (using tabs in the output) and -c (using the old output scheme instead of SGR escape sequences). The -C, -i, -n, -m, -o, and -r options have the effect described in troff(1). In addition, nroff silently ignores the options -e, -q, and -s (which are not implemented in troff). Options -p (pic), -t (tbl), -S (safer), and -U (unsafe) are passed to groff. -v shows the version number.","Process Name":"gnroff","Link":"https:\/\/linux.die.net\/man\/1\/gnroff"}},{"Process":{"Description":"gnuserv is a server program run as a subprocess of XEmacs to handle all incoming and outgoing requests from gnuclient. It is not usually invoked directly, but is started from XEmacs by loading the gnuserv package and evaluating the Lisp form (gnuserv-start). gnuclient allows the user to request a running XEmacs process to edit the named files or directories and\/or evaluate lisp forms. Depending on your environment, TTY, X, GTK, or MS Windows frames, as well as batch (frameless) execution of Lisp may be available. One typical use for this is with a dialup connection to a machine on which an XEmacs process is currently running. gnudoit is a shell script frontend to ''gnuclient -batch -eval form''. Its use is deprecated. Try to get used to calling gnuclient directly. gnuattach no longer exists. Its functionality has been replaced by gnuclient -nw.","Process Name":"gnuattach","Link":"https:\/\/linux.die.net\/man\/1\/gnuattach"}},{"Process":{"Description":null,"Process Name":"gnubarcode","Link":"https:\/\/linux.die.net\/man\/1\/gnubarcode"}},{"Process":{"Description":"gnubiff is a mail notification program that checks for mail, displays headers when new mail has arrived and allow to read first lines of new messages. It relies on the GNOME and GTK libraries but can be compiled and used with or without GNOME support. Supported protocols are pop3, apop, imap4, mh, qmail and mailfile. Furthermore, gnubiff is fully configurable with a lot of options like polltime, poptime, sounds, mail reader, mailbox names, etc. and can also filter spam. Note that gnubiff is not a mail reader even if it offers the possibility of reading very first lines of new messages. If you send some requests for specific features to be implemented, keep that in mind!","Process Name":"gnubiff","Link":"https:\/\/linux.die.net\/man\/1\/gnubiff"}},{"Process":{"Description":"GNUCAP is a general purpose circuit simulator. It performs nonlinear dc and transient analyses, fourier analysis, and ac analysis linearized at an operating point. It is fully interactive and command driven. It can also be run in batch mode or as a server. The output is produced as it simulates. Spice compatible models for the MOSFET (level 1,2,3) and diode are included in this release. Since it is fully interactive, it is possible to make changes and re-simulate quickly. The interactive design makes it well suited to the typical iterative design process used it optimizing a circuit design. It is also well suited to undergraduate teaching where Spice in batch mode can be quite intimidating. This version, while still officially in beta test, should be stable enough for basic undergraduate teaching and courses in MOS design, but not for bipolar design. In batch mode it is mostly Spice compatible, so it is often possible to use the same file for both GNUCAP and Spice. The analog simulation is based on traditional nodal analysis with iteration by Newton's method and LU decomposition. An event queue and incremental matrix update speed up the solution for large circuits (at some expense for small circuits). It also has digital devices for mixed signal simulation. The digital devices may be implemented as either analog subcircuits or as true digital models. The simulator will automatically determine which to use. Networks of digital devices are simulated as digital, with no conversions to analog between gates. This results in digital circuits being simulated faster than on a typical analog simulator, even with behavioral models. The digital mode is experimental and needs work. There will be substantial improvements in future releases.","Process Name":"gnucap","Link":"https:\/\/linux.die.net\/man\/1\/gnucap"}},{"Process":{"Description":null,"Process Name":"gnucash","Link":"https:\/\/linux.die.net\/man\/1\/gnucash"}},{"Process":{"Description":"gnuserv is a server program run as a subprocess of XEmacs to handle all incoming and outgoing requests from gnuclient. It is not usually invoked directly, but is started from XEmacs by loading the gnuserv package and evaluating the Lisp form (gnuserv-start). gnuclient allows the user to request a running XEmacs process to edit the named files or directories and\/or evaluate lisp forms. Depending on your environment, TTY, X, GTK, or MS Windows frames, as well as batch (frameless) execution of Lisp may be available. One typical use for this is with a dialup connection to a machine on which an XEmacs process is currently running. gnudoit is a shell script frontend to ''gnuclient -batch -eval form''. Its use is deprecated. Try to get used to calling gnuclient directly. gnuattach no longer exists. Its functionality has been replaced by gnuclient -nw.","Process Name":"gnuclient","Link":"https:\/\/linux.die.net\/man\/1\/gnuclient"}},{"Process":{"Description":"gnuserv is a server program run as a subprocess of XEmacs to handle all incoming and outgoing requests from gnuclient. It is not usually invoked directly, but is started from XEmacs by loading the gnuserv package and evaluating the Lisp form (gnuserv-start). gnuclient allows the user to request a running XEmacs process to edit the named files or directories and\/or evaluate lisp forms. Depending on your environment, TTY, X, GTK, or MS Windows frames, as well as batch (frameless) execution of Lisp may be available. One typical use for this is with a dialup connection to a machine on which an XEmacs process is currently running. gnudoit is a shell script frontend to ''gnuclient -batch -eval form''. Its use is deprecated. Try to get used to calling gnuclient directly. gnuattach no longer exists. Its functionality has been replaced by gnuclient -nw.","Process Name":"gnudoit","Link":"https:\/\/linux.die.net\/man\/1\/gnudoit"}},{"Process":{"Description":"Gnumeric is a powerful spreadsheet program created by the GNOME project. Gnumeric intends to compete with commercial spreadsheets by becoming fully compatible with Microsoft Excel(TM) and through the support of most popular spreadsheet file formats such as Excel(TM), Lotus 1-2-3(TM), Applix(TM), Sylk(TM), XBase(TM), Oleo(TM) and OpenOffice.org formats. Gnumeric supports advanced calculations using statistical, financial and database access functions. Plotting data is supported through an incomplete but powerful plotting system. A plugin system extends gnumeric , for instance enabling the export of data to the LaTeX \\longtable format. Plugins can be used to define custom functionality. A rudimentary scripting API for the Python language exists and will be extended in the near future. Since gnumeric is free software, gnumeric can also be extended directly at the source code level by any competent programmer. The program can be started from the command line as gnumeric or from one of the menus provided by the underlying platform. When started on the command line, gnumeric may be followed by the options listed below and possibly the names of files in various spreadsheet formats which will then be opened immediately. For instance, the command: gnumeric myfile.gnumeric will launch gnumeric and open the file called \"myfile.gnumeric\". The default gnumeric file format is in extensible markup language (XML) which subsequently has been compressed with gzip.","Process Name":"gnumeric","Link":"https:\/\/linux.die.net\/man\/1\/gnumeric"}},{"Process":{"Description":null,"Process Name":"gnunet-auto-share","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-auto-share"}},{"Process":{"Description":"Chat via GNUnet -h, --help print help page -c FILENAME, --config=FILENAME load config file (defaults: ~\/.gnunet\/gnunet.conf) -v, --version print the version number -n NICKNAME use NICKNAME for the nick -L LOGLEVEL, --loglevel=LOGLEVEL change the loglevel. Possible values for LOGLEVEL are NOTHING, FATAL, ERROR, FAILURE, WARNING, MESSAGE, INFO, DEBUG, CRON and EVERYTHING.","Process Name":"gnunet-chat","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-chat"}},{"Process":{"Description":"gnunet-directory lists the contents of a GNUnet directory. It also can be used to manipulate the file identifier database which is used by GNUnet for building directories. gnunet-directory will always list the contents of the GNUnet directories that are passed as filenames. Manipulating the file identifier database is done by passing additional options to gnunet-directory. Note that by default GNUnet does not build the file identifier database and the database will thus always be empty. You need to run gnunet-directory with the -t option to enable tracking of file identifiers. The reason is that storing file identifiers in plaintext in the database can compromise your privacy if your machine should fall under the control of an adversary. -c FILENAME, --config=FILENAME use config file (defaults: ~\/.gnunet\/gnunet.conf) -h, --help print help page -k, --kill delete all entries from the file identifier database and stop tracking file identifiers -l, --list display entries from the file identifier database -L LOGLEVEL, --loglevel=LOGLEVEL Change the loglevel. Possible values for LOGLEVEL are NOTHING, FATAL, ERROR, WARNING, INFO, STATUS and DEBUG. Note that options in the configuration file take precedence over this option (the argument will be ignored in that case). -t, --track start tracking file identifiers -v, --version print the version number -V, --verbose be verbose; prints progress information","Process Name":"gnunet-directory","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-directory"}},{"Process":{"Description":"Download files from GNUnet. -a LEVEL, --anonymity=LEVEL set desired level of receiver anonymity. Default is 1. -c FILENAME, --config=FILENAME use config file (defaults: ~\/.gnunet\/gnunet.conf) -d, --directory download a GNUnet directory that has already been downloaded. Requires that a filename of an existing file is specified instead of the URI. The download will only download the top-level files in the directory unless the '-R' option is also specified. -D, --delete-incomplete causes gnunet-download to delete incomplete downloads when aborted with CTRL-C. Note that complete files that are part of an incomplete recursive download will not be deleted even with this option. Without this option, terminating gnunet-download with a signal will cause incomplete downloads to stay on disk. If gnunet-download runs to (normal) completion finishing the download, this option has no effect. -h, --help print help page -H HOSTNAME, --host=HOSTNAME on which host is gnunetd running (default: localhost). You can also specify a port using the syntax HOSTNAME:PORT. The default port is 2087. -L LOGLEVEL, --loglevel=LOGLEVEL Change the loglevel. Possible values for LOGLEVEL are NOTHING, FATAL, ERROR, WARNING, INFO, STATUS and DEBUG. Note that options in the configuration file take precedence over this option (the argument will be ignored in that case). -o FILENAME, --output=FILENAME write the file to FILENAME. Hint: when recursively downloading a directory, append a '\/' to the end of the FILENAME to create a directory of that name. If no FILENAME is specified, gnunet-download constructs a temporary ID from the URI of the file. The final filename is constructed based on meta-data extracted using libextractor (if available). -p DOWNLOADS, --parallelism=DOWNLOADS set the maximum number of parallel downloads that is allowed. More parallel downloads can, to some extent, improve the overall time to download content. However, parallel downloads also take more memory. The specified number is the number of files that are downloaded in parallel, not the number of blocks that are concurrently requested. As a result, the number only matters for recursive downloads. The default value is 32. -R, --recursive download directories recursively (and in parallel); note that the URI must belong to a GNUnet directory and that the filename given must end with a '\/' -- otherwise, only the file corresponding to the URI will be downloaded. -v, --version print the version number -V, --verbose print progress information","Process Name":"gnunet-download","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-download"}},{"Process":{"Description":"gnunet-download-manager is a script that can be used to track download sessions. It makes the process of resuming downloads after a system reboot easier. A typical use is to define an alias (depending on your shell) of the form $ alias gnunet-download='gnunet-download-manager.scm download' Other commands for the download manager include resume (resumes all downloads), status (show status of pending downloads), killall (abort all downloads), settings (for configuration) and help (print help text). gnunet-download-manager is a scheme script and is only installed if guile was found by configure.","Process Name":"gnunet-download-manager","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-download-manager"}},{"Process":{"Description":null,"Process Name":"gnunet-insert","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-insert"}},{"Process":{"Description":"gnunet-peer-info display the last known IP and GNUnet host address of known nodes.","Process Name":"gnunet-peer-info","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-peer-info"}},{"Process":{"Description":"gnunet-pseudonym is a tool for managing pseudonyms and namespaces. A pseudonym is the persona that controls a namespace. As such, it is identical to a public-private RSA key pair. A namespace is a collection of files that have been signed by the corresponding private RSA key. A namespace is typically associated with a nickname and other metadata which is kept in a specially named file in the namespace. Namespaces are an important tool for providing assurances about content integrity and authenticity in GNUnet. Since all of the content in the namespace must have been provided by the same entity, users can form an opinion about that entity and learn to search (or avoid) certain namespaces. gnunet-pseudonym can be used to list all of the pseudonyms that were created locally, to create new pseudonyms, to delete existing pseudonyms (the namespace will continue to exist, but it will be impossible to add additional data to it) and to list all of the namespaces (with their meta-data) known to the local user. By default, gnunet-pseudonym lists all pseudonyms created locally and all of the namespaces that were discovered so far. Creating a new pseudonym requires using the -C option together with a nickname that is to be used for the namespace. Nicknames must be unique for each user, global uniqueness is desireable but not necessary. If two namespaces in GNUnet use the same nickname all GNUnet tools will display the nickname together with the unique namespace identifier (which is derived from the public key and hence guaranteed to be unique) to avoid ambiguity. Additional options can be passed together with the -C option to provide additional meta-data that describes the namespace. Possible meta-data includes the 'realname' of the person controlling the namespace, a description, the mime-type for content in the namespace (useful if the namespace is dedicated to some specific type of content) and contact information. One important piece of meta-data that can be specified is the identifier of a document root, that is the name of a file in the namespace that is a portal to the rest of the content. This is useful to help users find this root in the absence of conventions. Note that all of this meta-data is optional and should never be trusted blindly. gnunet-pseudonym also lists the meta-data available for other namespaces. Namespaces can be discovered whenever the peer obtains the namespace advertisement that is created at the time where the pseudonym is created. Namespace advertisements can be found in directories (not implemented), ordinary keyword-based searches (by default gnunet-pseudonym publishes the namespace advertisement under the keyword 'namespace', but the -k option can be used to specify other keywords) and under the 'all-zeros' identifier of the respective namespace (using a namespace-search if the namespace ID is already known). For more details about GNUnet namespaces and content encoding please read the 'Encoding for Censorship-resistant Sharing' (ECRS) paper which can be found on the GNUnet webpage. -a LEVEL, --anonymity=LEVEL set desired level of sender anonymity. Default is 1. -A, --automate Start a (new) collection. Works only in conjunction with the -C option. A collection is an automatically managed directory in a namespace. In essence, after starting the collection every file that you insert into GNUnet will automatically be placed into the collection. Other users can browse your collection and be certain (thanks to cryptography) that all of these files were inserted into GNUnet by the same user (they do not necessarily know who it is, but if you specify your realname (-r) they will be able to see that handle). Collections are useful for establishing a reputation for your GNUnet content, such that readers can form an opinion about quality and availability. Namespaces can be used to achieve the same thing, but collections are automatic and thus less work for you. Using collections has some security implications since it is possible for an adversary to see that all of these files originate from the same user. This may help a correlation attack to break anonymity. Nevertheless we encourage using collections, they are likely to be the right choice for most users. -C NAME, --create=NAME Creates a new pseudonym with the given NAME. -D NAME, --delete=NAME Delete the pseudonym with the given NAME. -e EMAIL, --email=EMAIL Include EMAIL a contact address to contact the author of the namespace (use with -C). -E, --end End a collection. This option is the opposite of the -a option in that it stops the collection. Note that currently, once you stop a collection you can never restart it. However, you can start a new collection. There can only be one collection at any given point in time for a particular user. -h, --help print help page -k KEYWORD, --keyword=KEYWORD Publish the namespace advertisement under the keyword 'KEYWORD'. Default is 'namespace' (use with -C). You can specify -k multiple times. In that case, the namespace will be published under each of those keywords. -m MIMETYPE, --mimetype=MIMETYPE Advertise that the namespace contains files of the given MIMETYPE (use with -C). -n, --no-advertisement Do not generate an advertisement for the namespace (use with -C). -q, --quiet Do not print the list of pseudonyms (only perform create or delete operation). -r NAME, --realname=NAME Claim that the name of the author of the content in the namespace in 'real' life is NAME (use with -C). -R IDENTIFIER, --root=IDENTIFIER Specify the identifier for the root of the namespace. Used in the namespace advertisement to tell users that find the namespace advertisement about an entry-point into the namespace (use with -C). -s ID:VALUE, --set-rating=ID:VALUE Change the rating for the namespace identified by ID by VALUE. For example, \"-s test:-3\" decrements the rating of the pseudonym \"test\" by 3. Note that ratings are purely local. Each user has his own independent rating of namespaces. The rating is merely a way for each user to keep track of his own experience with a given namespace. -t TEXT, --text=TEXT Use TEXT as the description for the namespace (use with -C). -u URI, --uri=URI Include URI as an address where additional information about the namespace can be found (use with -C).","Process Name":"gnunet-pseudonym","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-pseudonym"}},{"Process":{"Description":"Search for content on GNUnet. The keywords are case-sensitive. gnunet-search can be used both for a search in the global namespace as well as for searching a private subspace. -a LEVEL, --anonymity=LEVEL The -a option can be used to specify additional anonymity constraints. If set to 0, GNUnet will try to download the file as fast as possible without any additional slowdown for anonymous routing. Note that you may still have some amount of anonymity depending on the current network load and the power of the adversary. Use at least 1 to force GNUnet to use anonymous routing. This option can be used to limit requests further than that. In particular, you can require GNUnet to have a certain amount of cover traffic from other peers before sending your queries. This way, you can gain very high levels of anonymity - at the expense of much more traffic and much higher latency. So set this option to values beyond 1 only if you really believe you need it. The definition of ANONYMITY-RECEIVE is the following: If the value v is 0, anonymous routing is not required. For 1, anonymous routing is required, but there is no lower bound on how much cover traffic must be present. For values > 1 and < 1000, it means that if GNUnet routes n bytes of messages from foreign peers, it may originate n\/v bytes of queries in the same time-period. The time-period is twice the average delay that GNUnet deferrs forwarded queries. If the value v is >= 1000, it means that if GNUnet routes n bytes of QUERIES from at least (v % 1000) peers, it may originate n\/v\/1000 bytes of queries in the same time-period. The default is 1 and this should be fine for most users. Also notice that if you choose values above 1000, you may end up having no throughput at all, especially if many of your fellow GNUnet-peers do the same. -c FILENAME, --config=FILENAME use config file (defaults: ~\/.gnunet\/gnunet.conf) -h, --help print help page -H HOSTNAME, --host=HOSTNAME on which host is gnunetd running (default: localhost). You can also specify a port using the syntax HOSTNAME:PORT. The default port is 2087. -L LOGLEVEL, --loglevel=LOGLEVEL Change the loglevel. Possible values for LOGLEVEL are NOTHING, FATAL, ERROR, WARNING, INFO, STATUS and DEBUG. Note that options in the configuration file take precedence over this option (the argument will be ignored in that case). -o PREFIX, --output=PREFIX Writes the encountered (unencrypted) RBlocks or SBlocks to files with name PREFIX.XXX, where XXX is a number. This is useful to keep search results around. -v, --version print the version number","Process Name":"gnunet-search","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-search"}},{"Process":{"Description":null,"Process Name":"gnunet-setup","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-setup"}},{"Process":{"Description":null,"Process Name":"gnunet-stats","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-stats"}},{"Process":{"Description":"gnunet-tbench can be used to test the performance of the GNUnet core (link-to-link encryption and the available transport services). gnunet-tbench is useless to most ordinary users since its primary function is to test the performance and correctness of GNUnet transport service implementations. gnunet-tbench sends a sequence of messages to another peer that has the tbench module loaded. The service then measures the throughput, latency and loss of the messages round-trip. gnunet-tbench can only be used to test a direct peer-to-peer connection. You must load the tbench module (via the configuration gnunetd.conf, section GNUNETD under APPLICATIONS) in each of the two peers before gnunet-tbench can be used. The two peers must know of each other and be connected (use gnunet-stats to test for connections). Typically, gnunet-tbench reports the time it took to sent all specified messages and the percentage of messages lost. -c FILENAME, --config=FILENAME load config file (defaults: ~\/.gnunet\/gnunet.conf) -g --gnuplot create output in two colums suitable for gnuplot. When using this option, concatenate the output of multiple runs with various options into a file 'tbench' and run the following gnuplot script to visualize the time\/loss ratio: set xlabel \"time\" set ylabel \"percent transmitted\" plot \"tbench\" title 'Transport benchmarking' with points -h, --help print help page -L LOGLEVEL, --loglevel=LOGLEVEL Change the loglevel. Possible values for LOGLEVEL are NOTHING, FATAL, ERROR, WARNING, INFO, STATUS and DEBUG. Note that options in the configuration file take precedence over this option (the argument will be ignored in that case). -i ITER --iterations=ITER perform ITER iterations of the benchmark -n MESSAGES, --msg=MESSAGES how many messages should be used in each iteration (used to compute average, min, max, etc.) -r RECEIVER, --rec=RECEIVER use this option to specify the identity of the RECEIVER peer that is used for the benchmark. This option is required. -s SIZE --size=SIZE test using the specified message size -S SPACE --space=PACE use SPACE milli-seconds of delays between trains of messages -t TIMEOUT --timeout=IMEOUT wait TIMEOUT milli-seconds for replies to arrive before aborting -v, --version print the version number -X COUNT --xspace=OUNT use trains of COUNT messages","Process Name":"gnunet-tbench","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-tbench"}},{"Process":{"Description":"gnunet-testbed can be used to control GNUnet peers to perform testing and benchmarking of the system. Note that gnunet-testbed is at this point still a very new tool and not functional. gnunet-testbed is fully scriptable and uses bash as a scripting language. All features of bash are supported, including the execution of arbitrary shell commands. gnunet-testbed extends bash with additional, testbed specific commands. -h, --help print help page -c FILENAME, --config=FILENAME load config file (defaults: ~\/.gnunet\/gnunet.conf) -v, --version print the version number -L LOGLEVEL, --loglevel=LOGLEVEL change the loglevel. Possible values for LOGLEVEL are NOTHING, FATAL, ERROR, FAILURE, WARNING, MESSAGE, INFO, DEBUG, CRON and EVERYTHING.","Process Name":"gnunet-testbed","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-testbed"}},{"Process":{"Description":null,"Process Name":"gnunet-tracekit","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-tracekit"}},{"Process":{"Description":"gnunet-transport-check can be used to test or profile a GNUnet transport service. The tool can be used to test both the correctness of the software as well as the correctness of the configuration. gnunet-transport-check features two modes, called loopback mode and ping mode. In loopback mode the test is limited to testing if the transport can be used to communicate with itself (loopback). This mode does not include communication with other peers which may be blocked by firewalls and other general Internet connectivity problems. The loopback mode is particularly useful to test the SMTP transport service since this service is fairly hard to configure correctly and most problems can be reveiled by just testing the loopback. In ping mode the tool will attempt to download peer advertisements from the URL specified in the configuration file and then try to contact each of the peers. Note that it is perfectly normal that some peers do not respond, but if no peer responds something is likely to be wrong. The configuration is always taken from the configuration file. Do not run gnunetd while running gnunet-transport-check since the transport services cannot be used by two processes at the same time. gnunet-transport-check will always produce an error-message for the NAT transport in loopback mode. If NAT is configured in accept-mode (as in, accept connections from peers using network address translation), the check will fail with the message \"could not create HELO\", which is correct since the peer itself is clearly not going to advertise itself as a NAT. If the peer is configured in NAT-mode, that is, the peer is behind a NAT box, the message will be but exactly what is supposed to happen. Similarly, a NAT-ed peer should typically configure the TCP transport to use port 0 (not listen on any port). In this case, gnunet-transport-check will print 'could not create HELO' for the TCP transport. This is also ok. In fact, a correctly configured peer using NAT should give just two errors (could not connect for tcp and could not create HELO for NAT) when tested using gnunet-transport-check. The reason is, that gnunet-transport-check only tests loopback connectivity, and for a NAT-ed peer, that just does not apply. Note that in ping mode the HTTP download times out after 5 minutes, so if the list of peers is very large and not all peers can be queried within the 5 minutes the tool may abort before trying all peers. -c FILENAME, --config= FILENAME use config file (default: \/etc\/gnunetd.conf) -h, --help print help page -L LOGLEVEL, --loglevel= LOGLEVEL change the loglevel. Possible values for LOGLEVEL are NOTHING, FATAL, ERROR, FAILURE, WARNING, MESSAGE, INFO, DEBUG, CRON and EVERYTHING. -p, --ping use ping mode (loopback mode is default) -r COUNT --repeat= COUNT send COUNT messages in a sequence over the same connection -s SIZE --size= SIZE test using the specified message size, default is 11 -t TRANSPORT, --transport= TRANSPORT run using the specified transport, if not given the transports configured in the configuration file are used. -u USER, --user=USER run as user USER (and if available as group USER). Note that to use this option, you will probably have to start gnunet-transport-check as root. It is typically better to directly start gnunet-transport-check as that user instead. -v, --version print the version number -V, --verbose be verbose","Process Name":"gnunet-transport-check","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-transport-check"}},{"Process":{"Description":null,"Process Name":"gnunet-unindex","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-unindex"}},{"Process":{"Description":"gnunet-update updates gnunets databases after software updates that change the database format. -c FILENAME, --config=FILENAME load config file (default: \/etc\/gnunet.conf) -g SECTION:ENTRY, --get=SECTION:ENTRY print the value from the configuration in section SECTION under entry ENTRY. Use :ENTRY to access the global section. -h, --help print help page -L LOGLEVEL, --loglevel=LOGLEVEL Change the loglevel. Possible values for LOGLEVEL are NOTHING, FATAL, ERROR, WARNING, INFO, STATUS and DEBUG. Note that options in the configuration file take precedence over this option (the argument will be ignored in that case). -U, --client run in client mode (required to get values from the client configuration). This option must be used only together with the -g option. -u USER, --user=USER run as user USER (and if available as group USER). Note that to use this option, you will probably have to start gnunet-update as root. It is typically better to directly start gnunet-update as that user instead. -v, --version print the version number -V --verbose Be verbose.","Process Name":"gnunet-update","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-update"}},{"Process":{"Description":"gnunet-vpn is a tool to be used together with the GNUnet VPN module. See http:\/\/uk.gnunet.org\/vpn\/ for more information about the VPN module.","Process Name":"gnunet-vpn","Link":"https:\/\/linux.die.net\/man\/1\/gnunet-vpn"}},{"Process":{"Description":null,"Process Name":"gnunetd","Link":"https:\/\/linux.die.net\/man\/1\/gnunetd"}},{"Process":{"Description":"Gnuplot is a command-driven interactive function plotting program. If files are given, gnuplot loads each file with the load command, in the order specified. Gnuplot exits after the last file is processed. Here are some of its features: Plots any number of functions, built up of C operators, C library functions, and some things C doesn't have like **, sgn(), etc. Also support for plotting data files, to compare actual data to theoretical curves. User-defined X and Y ranges (optional auto-ranging), smart axes scaling, smart tic marks. Labelling of X and Y axes. User-defined constants and functions. Support for many output devices and file formats Shell escapes and command line substitution. Load and save capability. Output redirection. All computations performed in the complex domain. Just the real part is plotted by default, but functions like imag() and abs() and arg() are available to override this.","Process Name":"gnuplot","Link":"https:\/\/linux.die.net\/man\/1\/gnuplot"}},{"Process":{"Description":null,"Process Name":"gnuplot44","Link":"https:\/\/linux.die.net\/man\/1\/gnuplot44"}},{"Process":{"Description":"gnuserv is a server program run as a subprocess of XEmacs to handle all incoming and outgoing requests from gnuclient. It is not usually invoked directly, but is started from XEmacs by loading the gnuserv package and evaluating the Lisp form (gnuserv-start). gnuclient allows the user to request a running XEmacs process to edit the named files or directories and\/or evaluate lisp forms. Depending on your environment, TTY, X, GTK, or MS Windows frames, as well as batch (frameless) execution of Lisp may be available. One typical use for this is with a dialup connection to a machine on which an XEmacs process is currently running. gnudoit is a shell script frontend to ''gnuclient -batch -eval form''. Its use is deprecated. Try to get used to calling gnuclient directly. gnuattach no longer exists. Its functionality has been replaced by gnuclient -nw.","Process Name":"gnuserv","Link":"https:\/\/linux.die.net\/man\/1\/gnuserv"}},{"Process":{"Description":"--load-modules= MODULE1,MODULE2,... Dynamic modules to load Help options -?, --help Show this help message --usage Display brief usage message Gtk+ --gdk-debug= FLAGS Gdk debugging flags to set --gdk-no-debug= FLAGS Gdk debugging flags to unset --display= DISPLAY X display to use --screen= SCREEN X screen to use --sync Make X calls synchronous --name= NAME Program name as used by the window manager --class= CLASS Program class as used by the window manager --gxid-host= HOST --gxid-port= PORT --gtk-debug= FLAGS Gtk+ debugging flags to set --gtk-no-debug= FLAGS Gtk+ debugging flags to unset --g-fatal-warnings Make all warnings fatal --gtk-module= MODULE Load an additional Gtk module Bonobo activation Support --oaf-ior-fd= FD File descriptor to print IOR on --oaf-activate-iid= IID IID to activate --oaf-private Prevent registering of server with OAF GNOME Library --disable-sound Disable sound server usage --enable-sound Enable sound server usage --espeaker= HOSTNAME:PORT Host:port on which the sound server to use is running --version Session management --sm-client-id= ID Specify session management ID --sm-config-prefix= PREFIX Specify prefix of saved configuration --sm-disable Disable connection to session manager GNOME GUI Library --disable-crash-dialog","Process Name":"gnusim8085","Link":"https:\/\/linux.die.net\/man\/1\/gnusim8085"}},{"Process":{"Description":"gnustep-config can print information about the currently installed GNUstep system. It can output dependend on the options used the different flags used for compilation, but also the different internal variables used by the make system.","Process Name":"gnustep-config","Link":"https:\/\/linux.die.net\/man\/1\/gnustep-config"}},{"Process":{"Description":"Simple client program to set up a TLS connection to some other computer. It sets up a TLS connection and forwards data from the standard input to the secured socket and vice versa.","Process Name":"gnutls-cli","Link":"https:\/\/linux.die.net\/man\/1\/gnutls-cli"}},{"Process":{"Description":"Simple client program to set up a TLS connection to some other computer. Like gnutls-cli(1), it sets up a TLS connection and forwards data from the standard input to the socket. Any information about the TLS control connection is printed to standard error.","Process Name":"gnutls-cli-debug","Link":"https:\/\/linux.die.net\/man\/1\/gnutls-cli-debug"}},{"Process":{"Description":"Simple server program that listens to incoming TLS connections.","Process Name":"gnutls-serv","Link":"https:\/\/linux.die.net\/man\/1\/gnutls-serv"}},{"Process":{"Description":"gnuvd is a commandline frontend for the online Van Dale dictionary. It will search any word in the dictionary and return it's description in Dutch.","Process Name":"gnuvd","Link":"https:\/\/linux.die.net\/man\/1\/gnuvd"}},{"Process":{"Description":null,"Process Name":"gob","Link":"https:\/\/linux.die.net\/man\/1\/gob"}},{"Process":{"Description":"GObject Builder is a simple preprocessor for easily creating GObject objects. It does not parse any C code and ignores any C errors. It is in spirit similar to things like lex or yacc. In some ways it also resembles java. But it is really just a simple preprocessor for creating GObjects for use in C or C++ and it is not a programming language.","Process Name":"gob2","Link":"https:\/\/linux.die.net\/man\/1\/gob2"}},{"Process":{"Description":null,"Process Name":"gobby","Link":"https:\/\/linux.die.net\/man\/1\/gobby"}},{"Process":{"Description":"gobject-query is a small utility that draws a tree of types.","Process Name":"gobject-query","Link":"https:\/\/linux.die.net\/man\/1\/gobject-query"}},{"Process":{"Description":"gocr is an optical character recognition program that can be used from the command line. It takes input in PNM, PGM, PBM, PPM, or PCX format, and writes recognized text to stdout. If the pnm file is a single dash, PNM data is read from stdin. If gzip, bzip2 and netpbm-progs are installed and your system supports popen(3) also pnm.gz, pnm.bz2, png, jpg, jpeg, tiff, gif, bmp, ps (only single pages) and eps are supported as input files (not as input stream), where pnm can be replaced by one of ppm, pgm and pbm.","Process Name":"gocr","Link":"https:\/\/linux.die.net\/man\/1\/gocr"}},{"Process":{"Description":null,"Process Name":"goferd","Link":"https:\/\/linux.die.net\/man\/1\/goferd"}},{"Process":{"Description":"This program provides command-line access to (some) google services via their gdata APIs. Called without a service name, it starts an interactive session. Available tasks for service picasa: 'get', 'create', 'list', 'list-albums', 'tag', 'post', 'delete' get: Download albums Requires: none Optional: title, owner, format Arguments: LOCATION create: Create an album Requires: title Optional: date, summary, tags Arguments: PATH_TO_PHOTOS list: List photos Requires: delimiter Optional: title, query, owner list-albums: List albums Requires: delimiter Optional: title, owner tag: Tag photos Requires: tags AND (title OR query) Optional: owner post: Post photos to an album Requires: title Optional: tags, owner Arguments: PATH_TO_PHOTOS delete: Delete photos or albums Requires: (title OR query) Available tasks for service blogger: 'post', 'tag', 'list', 'delete' post: Post content. Requires: none Optional: blog, title, tags Arguments: PATH_TO_CONTENT or CONTENT tag: Label posts Requires: tags AND title Optional: blog list: List posts in a blog Requires: delimiter Optional: blog, title, owner delete: Delete a post. Requires: title Optional: blog Available tasks for service youtube: 'post', 'tag', 'list', 'delete' post: Post a video. Requires: category AND devkey Optional: title, summary, tags Arguments: PATH_TO_VIDEO tag: Add tags to a video and\/or change its category. Requires: devkey AND title AND (category OR tags) list: List videos by user. Requires: delimiter Optional: title, owner delete: Delete videos. Requires: devkey Optional: title Available tasks for service docs: 'edit', 'delete', 'list', 'upload', 'get' edit: Edit a document Requires: title Optional: format, editor, folder delete: Delete documents Requires: none Optional: title list: List documents Requires: delimiter Optional: title, folder upload: Upload a document Requires: none Optional: title, folder, format Arguments: PATH_TO_FILE get: Download a document Requires: (title OR folder) Optional: format Arguments: LOCATION Available tasks for service contacts: 'list', 'list-groups', 'add', 'add-groups', 'delete-groups', 'delete' list: List contacts Requires: none Arguments: Fields to show (example: name,email) list-groups: List contact groups Requires: none Arguments: Specific groups to list (if any) add: Add contacts Requires: none Arguments: \"name,email\" pair or CSV filename add-groups: Add contact group(s) Requires: none Arguments: Group name(s) delete-groups: Delete contact group(s) Requires: none Arguments: Group name(s) delete: Delete contacts Requires: none Arguments: names of contact(s) to delete (e.g. \"John Doe\" \"Jane Doe\") Available tasks for service calendar: 'add', 'list', 'today', 'delete' add: Add event to a calendar Requires: none Optional: cal Arguments: QUICK_ADD_TEXT list: List events on a calendar Requires: delimiter Optional: title, query, date, cal today: List events for the next 24 hours Requires: delimiter Optional: title, query, cal delete: Delete event from a calendar Requires: (title OR query) Optional: date, cal","Process Name":"google","Link":"https:\/\/linux.die.net\/man\/1\/google"}},{"Process":{"Description":"The goop program draws a simulation of bubbles in layers of overlapping multicolored translucent fluid.","Process Name":"goop","Link":"https:\/\/linux.die.net\/man\/1\/goop"}},{"Process":{"Description":null,"Process Name":"gopen","Link":"https:\/\/linux.die.net\/man\/1\/gopen"}},{"Process":{"Description":null,"Process Name":"gorbd","Link":"https:\/\/linux.die.net\/man\/1\/gorbd"}},{"Process":{"Description":"An I\/O performance measurement and load generation tool. Writes and\/or reads generated data to or from a character device, block device, or regular file. -B, --max-buffer-size= BUFFER_SIZE Each read(2)\/ write(2) call uses a maximum buffer of size BUFFER_SIZE. -b, --min-buffer-size= BUFFER_SIZE Each read(2)\/ write(2) call uses a minimum buffer of size BUFFER_SIZE. -c, --continue-after-error Continue after data integrity errors. -d, --direct Use direct I\/O. Should only be used on block device files. Not all operating systems support direct I\/O -g, --generate-load Equivalent to: -v -t -P -p random -i 0. -i, --iterations= COUNT Write\/read data COUNT times. If count is 0, repeats forever. -l, --logfile= LOGFILE Send log messages to LOGFILE. --no-progress Don't show progress (default). --no-rcfiles Don't use standard rcfiles. -q, --no-statistics Don't output statistics. --no-tui Don't use TUI interface. -o, --offset= OFFSET Seek to OFFSET before starting I\/O. -P, --progress Show progress. -p, --pattern= PATTERN Use data pattern PATTERN when reading or writing data. -r, --random Read\/Write buffers to random offsets. --raw An alias for --read-after-write. --rcfile= RCFILE Read command-line options from RCFILE. --read Read date from FILE. --read-after-write Read back data after writing to FILE. -S, --seed= SEED Use SEED for random number seed. -s, --sync Use synchronous I\/O. --statistics Output statistics (default). -t, --tui Use curses-based, terminal user interface. -u, --units= UNITS Show transfer rate in UNITS units. --usage Show brief usage message and exit. -V, --version Output version information and exit. -v, --detailed-statistics Output detailed statistics. --write Write data to FILE. -?, --help Show this help and exit. FILE Regular or device file to write data to. LOGFILE Path to a file used for logging. MAX_BUFFER_SIZE Minimum buffer size used in each read(2)\/ write(2) call (default is MIN_BUFFER_SIZE bytes). MAX_BUFFER_SIZE. Must be an even multiple of 512 bytes and can be specified in bytes, kibibytes(k), kilobytes(K), mebibytes(m), megabytes(M), gibibytes(g), gigabytes(G). tebibytes(t), or terabytes(T). MAX_BUFFER_SIZE defaults to MIN_BUFFER_SIZE. If MAX_BUFFER_SIZE > MIN_BUFFER_SIZE, random buffers sizes between the two limits are used. MAX_BUFFER_SIZE must be an even multiple of MIN_BUFFER_SIZE. MIN_BUFFER_SIZE Minimum buffer size used in each read(2)\/ write(2) call (default is 512 bytes). MIN_BUFFER_SIZE. Must be an even multiple of 512 bytes and can be specified in bytes, kibibytes(k), kilobytes(K), mebibytes(m), megabytes(M), gibibytes(g), gigabytes(G). tebibytes(t), or terabytes(T). OFFSET Position to seek to in the file before starting I\/O (default is 0). OFFSET must be an even multiple of 512 bytes and can be specified in bytes, kibibytes(k), kilobytes(K), mebibytes(m), megabytes(M), gibibytes(g), gigabytes(G). tebibytes(t), or terabytes(T). PATTERN Data pattern used when writing\/reading data. Available patterns are: none, zeros, ones, alt, random, numbers, and \"#\" (where \"#\" is a number between 0-255). The default pattern is \"none\". RCFILE Read additional command-line options from RCFILE. Other options on the command-line will override options in RCFILE. SEED Used to seed the random number generator Must be >= 1 and <= 2^32. TRANSFER_SIZE Total number of bytes to transfer (must be an even multiple of both MIN_BUFFER_SIZE and MAX_BUFFER)SIZE). TRANSFER_SIZE can be specified in bytes, kilobytes, megabytes, or gigabytes. UNITS Kibibytes(k), kilobytes(K), mebibytes(m), megabytes(M), gibibytes(g), gigabytes(G). tebibytes(t), or terabytes(T).","Process Name":"gorge","Link":"https:\/\/linux.die.net\/man\/1\/gorge"}},{"Process":{"Description":null,"Process Name":"gouldtoppm","Link":"https:\/\/linux.die.net\/man\/1\/gouldtoppm"}},{"Process":{"Description":"Invokes the PARI-GP calculator, loading the file1, file2, ... (written in the GP language) on startup. gp is an advanced programmable calculator, which computes symbolically as long as possible, numerically where needed, and contains a wealth of number-theoretic functions (elliptic curves, class field theory...). It can be programmed with the GP scripting language. Its basic data types are numbers integers, real numbers, exact rational numbers, algebraic numbers, p-adic numbers, modular integers (integers modulo n), complex numbers, polynomials, rational functions, and power series, integral binary quadratic forms, matrices, vectors, and lists, character strings, and recursive combinations of these.","Process Name":"gp","Link":"https:\/\/linux.die.net\/man\/1\/gp"}},{"Process":{"Description":null,"Process Name":"gp-2.3","Link":"https:\/\/linux.die.net\/man\/1\/gp-2.3"}},{"Process":{"Description":"gpal is a compiler for Microchip (TM) PIC (TM) micro-controllers. gpal is part of gputils. Check the gputils(1) manpage for details on other GNU PIC utilities.","Process Name":"gpal","Link":"https:\/\/linux.die.net\/man\/1\/gpal"}},{"Process":{"Description":null,"Process Name":"gpasm","Link":"https:\/\/linux.die.net\/man\/1\/gpasm"}},{"Process":{"Description":"The gpasswd command is used to administer \/etc\/group, and \/etc\/gshadow. Every group can have administrators, members and a password. System administrators can use the -A option to define group administrator(s) and the -M option to define members. They have all rights of group administrators and members. gpasswd called by a group administrator with a group name only prompts for the new password of the group. If a password is set the members can still use newgrp(1) without a password, and non-members must supply the password. Notes about group passwords Group passwords are an inherent security problem since more than one person is permitted to know the password. However, groups are a useful tool for permitting co-operation between different users.","Process Name":"gpasswd","Link":"https:\/\/linux.die.net\/man\/1\/gpasswd"}},{"Process":{"Description":null,"Process Name":"gpbs","Link":"https:\/\/linux.die.net\/man\/1\/gpbs"}},{"Process":{"Description":"gpdasm is an disassembler for Microchip (TM) PIC (TM) micro-controllers. gpdasm is part of gputils. Check the gputils(1) manpage for details on other GNU PIC utilities.","Process Name":"gpdasm","Link":"https:\/\/linux.die.net\/man\/1\/gpdasm"}},{"Process":{"Description":null,"Process Name":"gperf","Link":"https:\/\/linux.die.net\/man\/1\/gperf"}},{"Process":{"Description":"gpg2 is the OpenPGP part of the GNU Privacy Guard (GnuPG). It is a tool to provide digital encryption and signing services using the OpenPGP standard. gpg2 features complete key management and all bells and whistles you can expect from a decent OpenPGP implementation. In contrast to the standalone version gpg, which is more suited for server and embedded platforms, this version is installed under the name gpg2 and more targeted to the desktop as it requires several other modules to be installed. The standalone version will be kept maintained and it is possible to install both versions on the same system. If you need to use different configuration files, you should make use of something like oqgpg.conf-2cq instead of just oqgpg.confcq.","Process Name":"gpg","Link":"https:\/\/linux.die.net\/man\/1\/gpg"}},{"Process":{"Description":"gpg-agent is a daemon to manage secret (private) keys independently from any protocol. It is used as a backend for gpg and gpgsm as well as for a couple of other utilities. The usual way to run the agent is from the ~\/.xsession file: eval $(gpg-agent --daemon) If you don't use an X server, you can also put this into your regular startup file ~\/.profile or .bash_profile. It is best not to run multiple instance of the gpg-agent, so you should make sure that only one is running: gpg-agent uses an environment variable to inform clients about the communication parameters. You can write the content of this environment variable to a file so that you can test for a running agent. Here is an example using Bourne shell syntax: gpg-agent --daemon --enable-ssh-support \\\n          --write-env-file \"${HOME}\/.gpg-agent-info\" This code should only be run once per user session to initially fire up the agent. In the example the optional support for the included Secure Shell agent is enabled and the information about the agent is written to a file in the HOME directory. Note that by running gpg-agent without arguments you may test whether an agent is already running; however such a test may lead to a race condition, thus it is not suggested. The second script needs to be run for each interactive session: if [ -f \"${HOME}\/.gpg-agent-info\" ]; then\n  . \"${HOME}\/.gpg-agent-info\"\n  export GPG_AGENT_INFO\n  export SSH_AUTH_SOCK\n  export SSH_AGENT_PID\nfi It reads the data out of the file and exports the variables. If you don't use Secure Shell, you don't need the last two export statements. You should always add the following lines to your .bashrc or whatever initialization file is used for all shell invocations: GPG_TTY=$(tty)\nexport GPG_TTY It is important that this environment variable always reflects the output of the tty command. For W32 systems this option is not required. Please make sure that a proper pinentry program has been installed under the default filename (which is system dependant) or use the option pinentry-program to specify the full name of that program. It is often useful to install a symbolic link from the actual used pinentry (e.g. oq\/usr\/bin\/pinentry-gtkcq) to the expected one (e.g. oq\/usr\/bin\/pinentrycq).","Process Name":"gpg-agent","Link":"https:\/\/linux.die.net\/man\/1\/gpg-agent"}},{"Process":{"Description":"The gpg-connect-agent is a utility to communicate with a running gpg-agent. It is useful to check out the commands gpg-agent provides using the Assuan interface. It might also be useful for scripting simple applications. Input is expected at stdin and out put gets printed to stdout. It is very similar to running gpg-agent in server mode; but here we connect to a running instance. The following options may be used: -v --verbose Output additional information while running. -q --quiet Try to be as quiet as possible. --homedir dir Set the name of the home directory to dir. If this option is not used, the home directory defaults to oq ~\/.gnupgcq. It is only recognized when given on the command line. It also overrides any home directory stated through the environment variable oq GNUPGHOMEcq or (on W32 systems) by means of the Registry entry HKCU\\Software\\GNU\\GnuPG:HomeDir. -S --raw-socket name Connect to socket name assuming this is an Assuan style server. Do not run any special initializations or environment checks. This may be used to directly connect to any Assuan style socket server. -E --exec Take the rest of the command line as a program and it's arguments and execute it as an assuan server. Here is how you would run gpgsm: gpg-connect-agent --exec gpgsm --server Note that you may not use options on the command line in this case. --no-ext-connect When using -S or --exec, gpg-connect-agent connects to the assuan server in extended mode to allow descriptor passing. This option makes it use the old mode. --run file Run the commands from file at startup and then continue with the regular input method. Note, that commands given on the command line are executed after this file. -s --subst Run the command \/subst at startup. --hex Print data lines in a hex format and the ASCII representation of non-control characters. --decode Decode data lines. That is to remove percent escapes but make sure that a new line always starts with a D and a space.","Process Name":"gpg-connect-agent","Link":"https:\/\/linux.die.net\/man\/1\/gpg-connect-agent"}},{"Process":{"Description":"The gpg-preset-passphrase is a utility to seed the internal cache of a running gpg-agent with passphrases. It is mainly useful for unattended machines, where the usual pinentry tool may not be used and the passphrases for the to be used keys are given at machine startup. Passphrases set with this utility don't expire unless the --forget option is used to explicitly clear them from the cache --- or gpg-agent is either restarted or reloaded (by sending a SIGHUP to it). It is necessary to allow this passphrase presetting by starting gpg-agent with the --allow-preset-passphrase. gpg-preset-passphrase is invoked this way: gpg-preset-passphrase [options] [command] cacheid cacheid is either a 40 character keygrip of hexadecimal characters identifying the key for which the passphrase should be set or cleared. The keygrip is listed along with the key when running the command: gpgsm --dump-secret-keys. Alternatively an arbitrary string may be used to identify a passphrase; it is suggested that such a string is prefixed with the name of the application (e.g foo:12346). One of the following command options must be given: --preset Preset a passphrase. This is what you usually will use. gpg-preset-passphrase will then read the passphrase from stdin. --forget Flush the passphrase for the given cache ID from the cache. The following additional options may be used: -v --verbose Output additional information while running. -P string --passphrase string Instead of reading the passphrase from stdin, use the supplied string as passphrase. Note that this makes the passphrase visible for other users.","Process Name":"gpg-preset-passphrase","Link":"https:\/\/linux.die.net\/man\/1\/gpg-preset-passphrase"}},{"Process":{"Description":"gpg-zip encrypts or signs files into an archive. It is an gpg-ized tar using the same format as used by PGP's PGP Zip.","Process Name":"gpg-zip","Link":"https:\/\/linux.die.net\/man\/1\/gpg-zip"}},{"Process":{"Description":null,"Process Name":"gpg.ru","Link":"https:\/\/linux.die.net\/man\/1\/gpg.ru"}},{"Process":{"Description":"gpg2 is the OpenPGP part of the GNU Privacy Guard (GnuPG). It is a tool to provide digital encryption and signing services using the OpenPGP standard. gpg2 features complete key management and all bells and whistles you can expect from a decent OpenPGP implementation. In contrast to the standalone version gpg, which is more suited for server and embedded platforms, this version is installed under the name gpg2 and more targeted to the desktop as it requires several other modules to be installed. The standalone version will be kept maintained and it is possible to install both versions on the same system. If you need to use different configuration files, you should make use of something like oqgpg.conf-2cq instead of just oqgpg.confcq.","Process Name":"gpg2","Link":"https:\/\/linux.die.net\/man\/1\/gpg2"}},{"Process":{"Description":"The gpgconf is a utility to automatically and reasonable safely query and modify configuration files in the oq .gnupgcq home directory. It is designed not to be invoked manually by the user, but automatically by graphical user interfaces (GUI). ([Please note that currently no locking is done, so concurrent access should be avoided. There are some precautions to avoid corruption with concurrent usage, but results may be inconsistent and some changes may get lost. The stateless design makes it difficult to provide more guarantees.]) gpgconf provides access to the configuration of one or more components of the GnuPG system. These components correspond more or less to the programs that exist in the GnuPG framework, like GnuPG, GPGSM, DirMngr, etc. But this is not a strict one-to-one relationship. Not all configuration options are available through gpgconf. gpgconf provides a generic and abstract method to access the most important configuration options that can feasibly be controlled via such a mechanism. gpgconf can be used to gather and change the options available in each component, and can also provide their default values. gpgconf will give detailed type information that can be used to restrict the user's input without making an attempt to commit the changes. gpgconf provides the backend of a configuration editor. The configuration editor would usually be a graphical user interface program, that allows to display the current options, their default values, and allows the user to make changes to the options. These changes can then be made active with gpgconf again. Such a program that uses gpgconf in this way will be called GUI throughout this section.","Process Name":"gpgconf","Link":"https:\/\/linux.die.net\/man\/1\/gpgconf"}},{"Process":{"Description":null,"Process Name":"gpgmailtunl","Link":"https:\/\/linux.die.net\/man\/1\/gpgmailtunl"}},{"Process":{"Description":"The gpgparsemail is a utility currently only useful for debugging. Run it with --help for usage information. Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"gpgparsemail","Link":"https:\/\/linux.die.net\/man\/1\/gpgparsemail"}},{"Process":{"Description":"gpgsm is a tool similar to gpg to provide digital encryption and signing services on X.509 certificates and the CMS protocol. It is mainly used as a backend for S\/MIME mail processing. gpgsm includes a full features certificate management and complies with all rules defined for the German Sphinx project.","Process Name":"gpgsm","Link":"https:\/\/linux.die.net\/man\/1\/gpgsm"}},{"Process":{"Description":null,"Process Name":"gpgsm-gencert.sh","Link":"https:\/\/linux.die.net\/man\/1\/gpgsm-gencert.sh"}},{"Process":{"Description":"gpgv2 is an OpenPGP signature verification tool. This program is actually a stripped-down version of gpg which is only able to check signatures. It is somewhat smaller than the fully-blown gpg and uses a different (and simpler) way to check that the public keys used to make the signature are valid. There are no configuration files and only a few options are implemented. gpgv2 assumes that all keys in the keyring are trustworthy. By default it uses a keyring named oqtrustedkeys.gpgcq which is assumed to be in the home directory as defined by GnuPG or set by an option or an environment variable. An option may be used to specify another keyring or even multiple keyrings.","Process Name":"gpgv","Link":"https:\/\/linux.die.net\/man\/1\/gpgv"}},{"Process":{"Description":"gpgv2 is an OpenPGP signature verification tool. This program is actually a stripped-down version of gpg which is only able to check signatures. It is somewhat smaller than the fully-blown gpg and uses a different (and simpler) way to check that the public keys used to make the signature are valid. There are no configuration files and only a few options are implemented. gpgv2 assumes that all keys in the keyring are trustworthy. By default it uses a keyring named oqtrustedkeys.gpgcq which is assumed to be in the home directory as defined by GnuPG or set by an option or an environment variable. An option may be used to specify another keyring or even multiple keyrings.","Process Name":"gpgv2","Link":"https:\/\/linux.die.net\/man\/1\/gpgv2"}},{"Process":{"Description":null,"Process Name":"gphelp","Link":"https:\/\/linux.die.net\/man\/1\/gphelp"}},{"Process":{"Description":"libgphoto2(3) is a cross-platform digital camera library, and gphoto2(1) is a command-line client for it. Where an option takes a range of files, thumbnails, or other data, they are numbered beginning at 1. A range is a comma-separated list of numbers or spans (''first-last''). Ranges are XOR (exclusive or), so that ''1-5,3,7'' is equivalent to ''1,2,4,5,7''. --debug Turn on debugging. -q, --quiet Quiet output (default=verbose). -v, --version Display version and exit. -h, --help Display a short usage message. --list-cameras List supported camera models. --list-ports List supported port devices. --stdout Send file to stdout. --stdout-size Print filesize before data. --auto-detect List auto-detected cameras and the ports to which they are connected. --port PATH Specify port device. The --list-ports prints a list of valid, usable ports. In case of multiple USB cameras, the --auto-detect shows you the specific port each camera is connected to. --speed SPEED Specify serial transfer speed. --camera MODEL Specify camera model. The --list-cameras option prints a list of all explicitly supported cameras. Most model names contain spaces: remember to enclose the name in quotes so that the shell knows it is one parameter. For example: --camera \"Kodak DC240\". Note that if you specify --camera, you must also specify --port. Otherwise the --camera option will be silently ignored. --filename FILENAME Specify the filename to use when saving downloaded files. The --filename option accepts %a, %A, %b, %B, %d, %H, %k, %I, %l, %j, %m, %M, %S, %y, %%, (see date(1)) and, in addition, %n for the number, %C for the filename suffix, and %f for the filename without suffix. %n is the only conversion specifier to accept a padding character and width: %03n will pad with zeros to width 3 (e.g. print the number 7 as''007''). Leaving out the padding character (e.g. %3n) will use an implementation specific default padding character. --usbid USBIDS (Expert only) Override USB IDs. USBIDSmust be of the form DetectedVendorID: DetectedProductID= TreatAsVendorID: TreatAsProductID to treat any USB device detected as DetectedVendorID: DetectedProductID as TreatAsVendorID: TreatAsProductID instead. All the VendorIDs and ProductIDs should be hexadecimal numbers beginning in C notation, i.e. beginning with '0x'. Example: --usbid 0x4a9:0x306b=0x4a9:0x306c -a, --abilities Display camera abilities. -f, --folder FOLDER Specify camera folder (default=\"\/\"). --filename FILENAME Specify the filename to use when saving downloaded files. The --filename option accepts %a, %A, %b, %B, %d, %H, %k, %I, %l, %j, %m, %M, %S, %y, %%, (see date(1)) and, in addition, %n for the number, %C for the filename suffix, and %f for the filename without suffix. -R, --recurse Recursion (default for download). --no-recurse No recursion (default for deletion). -l, --list-folders List folders in folder. -L, --list-files List files in folder. -m, --mkdir NAME Create a directory. -r, --rmdir NAME Remove a directory. -n, --num-files Display number of files. -p, --get-file RANGE Get files given in range. -P, --get-all-files Get all files from folder. -t, --get-thumbnail RANGE Get thumbnails given in range. -T, --get-all-thumbnails Get all thumbnails from folder. --get-raw-data RANGE Get raw data given in range. --get-all-raw-data Get all raw data from folder. --get-audio-data RANGE Get audio data given in range. --get-all-audio-data Get all audio data from folder. --force-overwrite Overwrite files without asking. --new Only get not already downloaded files. This option depends on camera support of flagging already downloaded images and is not available for all drivers. -d, --delete-files RANGE Delete files given in range. -D, --delete-all-files Delete all files in folder. -u, --upload-file FILENAME Upload a file to camera. --capture-preview Capture a quick preview. -F COUNT, --frames COUNT Number of frames to capture in one run. Default is infinite number of frames. -I SECONDS, --interval SECONDS Time between capture of multiple frames. --capture-image Capture an image. --capture-movie Capture a movie. --capture-sound Capture an audio clip. --show-info RANGE Show info. --list-config List all configuration entries. --get-config CONFIGENTRY Get the specified configuration entry. --set-config CONFIGENTRY=CONFIGVALUE Set the specified configuration entry. --summary Summary of camera status. --manual Camera driver manual. --about About the camera driver. --shell Start the gphoto2 shell, an interactive environment. See SHELL MODE for a detailed description.","Process Name":"gphoto2","Link":"https:\/\/linux.die.net\/man\/1\/gphoto2"}},{"Process":{"Description":"This manual page describes the GNU version of pic, which is part of the groff document formatting system. pic compiles descriptions of pictures embedded within troff or TeX input files into commands that are understood by TeX or troff. Each picture starts with a line beginning with .PS and ends with a line beginning with .PE. Anything outside of .PS and .PE is passed through without change. It is the user's responsibility to provide appropriate definitions of the PS and PE macros. When the macro package being used does not supply such definitions (for example, old versions of -ms), appropriate definitions can be obtained with -mpic: these will center each picture.","Process Name":"gpic","Link":"https:\/\/linux.die.net\/man\/1\/gpic"}},{"Process":{"Description":null,"Process Name":"gpilot-install-file","Link":"https:\/\/linux.die.net\/man\/1\/gpilot-install-file"}},{"Process":{"Description":"This manual page documents briefly the gpk-application command. gpk-application allows you to install a package that provides a filename on the system.","Process Name":"gpk-application","Link":"https:\/\/linux.die.net\/man\/1\/gpk-application"}},{"Process":{"Description":"This manual page documents briefly the gpk-backend-status command. gpk-backend-status allows you to install a package that provides a filename on the system.","Process Name":"gpk-backend-status","Link":"https:\/\/linux.die.net\/man\/1\/gpk-backend-status"}},{"Process":{"Description":"This manual page documents briefly the gpk-install-local-file command. gpk-install-local-file allows you to install a local package file such as an rpm or deb file.","Process Name":"gpk-install-local-file","Link":"https:\/\/linux.die.net\/man\/1\/gpk-install-local-file"}},{"Process":{"Description":"This manual page documents briefly the gpk-install-mime-type command. gpk-install-mime-type allows you to install a package that provides a MimeType.","Process Name":"gpk-install-mime-type","Link":"https:\/\/linux.die.net\/man\/1\/gpk-install-mime-type"}},{"Process":{"Description":"This manual page documents briefly the gpk-install-package-name command. gpk-install-package-name allows you to install a a package from the package name.","Process Name":"gpk-install-package-name","Link":"https:\/\/linux.die.net\/man\/1\/gpk-install-package-name"}},{"Process":{"Description":"This manual page documents briefly the gpk-install-provide-file command. gpk-install-provide-file allows you to install a package that provides a filename on the system.","Process Name":"gpk-install-provide-file","Link":"https:\/\/linux.die.net\/man\/1\/gpk-install-provide-file"}},{"Process":{"Description":null,"Process Name":"gpk-prefs","Link":"https:\/\/linux.die.net\/man\/1\/gpk-prefs"}},{"Process":{"Description":"This manual page documents briefly the gpk-repo command. gpk-repo allows you to enable and disable software sources.","Process Name":"gpk-repo","Link":"https:\/\/linux.die.net\/man\/1\/gpk-repo"}},{"Process":{"Description":null,"Process Name":"gpk-update-icon","Link":"https:\/\/linux.die.net\/man\/1\/gpk-update-icon"}},{"Process":{"Description":"This manual page documents briefly the gpk-update-viewer command. gpk-update-viewer allows you to view and select updates to install. the system.","Process Name":"gpk-update-viewer","Link":"https:\/\/linux.die.net\/man\/1\/gpk-update-viewer"}},{"Process":{"Description":"gplib creates, modifies, and extracts from COFF archives for Microchip (TM) PIC (TM) micro-controllers. gplib is part of gputils. Check the gputils(1) manpage for details on other GNU PIC utilities.","Process Name":"gplib","Link":"https:\/\/linux.die.net\/man\/1\/gplib"}},{"Process":{"Description":"gplink combines a number of object and archive files, relocates their data, and ties up their symbol references. It outputs an executable for Microchip (TM) PIC (TM) micro-controllers. gplink is part of gputils. Check the gputils(1) manpage for details on other GNU PIC utilities.","Process Name":"gplink","Link":"https:\/\/linux.die.net\/man\/1\/gplink"}},{"Process":{"Description":null,"Process Name":"gpm-root","Link":"https:\/\/linux.die.net\/man\/1\/gpm-root"}},{"Process":{"Description":"This is the command-line frontend to gPodder. Run the command without parameters to show an overview of possible options and sub-commands.","Process Name":"gpo","Link":"https:\/\/linux.die.net\/man\/1\/gpo"}},{"Process":{"Description":null,"Process Name":"gpodder","Link":"https:\/\/linux.die.net\/man\/1\/gpodder"}},{"Process":{"Description":"This utility can be used to create a dump of the current gPodder data (configuration files + downloads), optionally replacing the real contents of the download folder with zero-byte files (for submitting your data to a bug report without having to transfer lots of data).","Process Name":"gpodder-backup","Link":"https:\/\/linux.die.net\/man\/1\/gpodder-backup"}},{"Process":{"Description":null,"Process Name":"gpredict","Link":"https:\/\/linux.die.net\/man\/1\/gpredict"}},{"Process":{"Description":"\"gprof\" produces an execution profile of C, Pascal, or Fortran77 programs. The effect of called routines is incorporated in the profile of each caller. The profile data is taken from the call graph profile file (gmon.out default) which is created by programs that are compiled with the -pg option of \"cc\", \"pc\", and \"f77\". The -pg option also links in versions of the library routines that are compiled for profiling. \"Gprof\" reads the given object file (the default is \"a.out\") and establishes the relation between its symbol table and the call graph profile from gmon.out. If more than one profile file is specified, the \"gprof\" output shows the sum of the profile information in the given profile files. \"Gprof\" calculates the amount of time spent in each routine. Next, these times are propagated along the edges of the call graph. Cycles are discovered, and calls into a cycle are made to share the time of the cycle. Several forms of output are available from the analysis. The flat profile shows how much time your program spent in each function, and how many times that function was called. If you simply want to know which functions burn most of the cycles, it is stated concisely here. The call graph shows, for each function, which functions called it, which other functions it called, and how many times. There is also an estimate of how much time was spent in the subroutines of each function. This can suggest places where you might try to eliminate function calls that use a lot of time. The annotated source listing is a copy of the program's source code, labeled with the number of times each line of the program was executed.","Process Name":"gprof","Link":"https:\/\/linux.die.net\/man\/1\/gprof"}},{"Process":{"Description":"gpsmap reads GPS and Network XML datafiles and plots networks on downloaded vector maps or satellite photos or user-supplied images.","Process Name":"gpsmap","Link":"https:\/\/linux.die.net\/man\/1\/gpsmap"}},{"Process":{"Description":null,"Process Name":"gpstrip","Link":"https:\/\/linux.die.net\/man\/1\/gpstrip"}},{"Process":{"Description":"gputils is a collection of development tools for Microchip (TM) PIC (TM) microcontrollers. It's intended to be compatible with the manufacturer's tools, MPASM, MPLINK and MPLIB.","Process Name":"gputils","Link":"https:\/\/linux.die.net\/man\/1\/gputils"}},{"Process":{"Description":"gpvc is an COD file viewer for Microchip (TM) PIC (TM) micro-controllers. COD files are and output from gpasm. They contain information used for simulation. gpvc is part of gputils. Check the gputils(1) manpage for details on other GNU PIC utilities.","Process Name":"gpvc","Link":"https:\/\/linux.die.net\/man\/1\/gpvc"}},{"Process":{"Description":"gpvo is an COFF object file viewer for Microchip (TM) PIC (TM) micro-controllers. COFF files are relocatable objects output from gpasm. gpvo is part of gputils. Check the gputils(1) manpage for details on other GNU PIC utilities.","Process Name":"gpvo","Link":"https:\/\/linux.die.net\/man\/1\/gpvo"}},{"Process":{"Description":"GQview is an interactive GTK based image viewer that supports multiple image formats, zooming, panning, thumbnails and sorting images into collections.","Process Name":"gqview","Link":"https:\/\/linux.die.net\/man\/1\/gqview"}},{"Process":{"Description":"Graal is a hierarchical symbolic layout editor. All functionnalities can be accessed through different menus. Among them exists a design rule checker performing verifications inside a user defined window. Graal works under Motif and X11r6. When entering Graal, the main window appears and shows 7 different menus on the top bar.These menus can be entered by simply clicking on the mouse left button. The main window is sensitive to the mouse, and the mouse wheel can be used for scrolling up and down, left and right when <Shift> is pressed during wheel movement, and zooming in and out, when <Ctrl> is pressed while turning the wheel. Below is given the description of graal's menus. File New : clean the entire window and save the current cell. Open : load an already existing cell. Save : save the current cell. Save as : rename and save the current cell. Quit : quit graal. Edit Undo : undo the previous action. Copy : copy a selected object. Move : move a selected object. Delete : delete a selected object. Stretch : strech a selected object (transistor or segment length). Modify : modify the caracteristics of an object (name, width, etc...). Identify : identify a selected object. Search : search an object according to its type. Window This menu allows user to perform the same actions than the previous menu Edit, not on an object but on a group of objects belonging to a defined window.The two opposite corner of the window are set by clicking on the mouse left button. Create Instance : add a new instance in the current cell. Abutmentbox : define the abutment box of the current cell. Segment : add segments. Via : add vias or contacts. Connector : add connectors. Transistor : add transistors. Reference : add references. View Zoom : perform zoom in, zoom out, center, fit, refresh on figure. Layer : select types of layers displayed. Map : show cursor position in the entire figure. Arrows : show arrows for moving at the grid step. Grid : Set the X,Y step of the grid if displayed. Tools Equi : highlight all objects electrically connected to a given object. Flat : virtual hierarchy flattening of an instance (display all visible objects inside a given instance). UnFlat : undo the virtual flattening of an instance (done before with the Flat command) Peek : virtual hierarchy flattening (display all visible objects inside a given window). UnPeek : undo the virtual flattening of a window (done before with the Peek command) Druc : call the design rule checker in a given window. RealFlat : real hierarchy flattening of instances included in a window (this command can not be undone, and should be used very carefully) Hierarchy : navigate into the current figure hierarchy Load Error : load a real layout file and superpose it to the current figure. Message : display the last error messages Setup Save or load a user defined configuration of default displayed menus.","Process Name":"graal","Link":"https:\/\/linux.die.net\/man\/1\/graal"}},{"Process":{"Description":null,"Process Name":"grab_vcsa","Link":"https:\/\/linux.die.net\/man\/1\/grab_vcsa"}},{"Process":{"Description":"Grace is a WYSIWYG tool to make two-dimensional plots of scientific data.","Process Name":"grace","Link":"https:\/\/linux.die.net\/man\/1\/grace"}},{"Process":{"Description":"Grace is a WYSIWYG tool to make two-dimensional plots of scientific data.","Process Name":"gracebat","Link":"https:\/\/linux.die.net\/man\/1\/gracebat"}},{"Process":{"Description":"Gramps is a Free\/OpenSource genealogy program. It is written in Python, using the GTK+\/GNOME interface. Gramps should seem familiar to anyone who has used other genealogy programs before such as Family Tree Maker (TM), Personal Ancestral Files (TM), or the GNU Geneweb. It supports importing of the ever popular GEDCOM format which is used world wide by almost all other genealogy software.","Process Name":"gramps","Link":"https:\/\/linux.die.net\/man\/1\/gramps"}},{"Process":{"Description":"THIS NEEDS MAJOR EDITTING !!! Warning: This is an early prototype. Consider it to be beta quality, if not alpha. grandvizier is a GUI tool for use with monitoring and controlling the DNSSEC-Tools dtrealms program. It displays information on the current state of the realms dtrealms is managing. The user may control some aspects of dtrealms's execution using grandvizier menu commands. grandvizier creates a window in which to display information about each realms. (These realms are those in dtrealms's current realms file.) For each realm, it displays the realm name and the count of zones in each of these four states: normal, ZSK rollover, KSK rollover, and KSK phase 6 wait state. As the rollover status of the zones in each realm changes, grandvizier will update its display for that realm. Inactive realms, realms listed in the realms file but which are not in currently being run, are displayed but have no useful information to display. The user may also hide realms from the display. These realms, if in the active state, will continue to execute; however, their information will not be displayed. Display state for each realm will persist across grandvizier executions. Menu commands are available for some control over dtrealms. Display and execution options for grandvizier are also available through menu commands. More information about the menu commands is available in the MENU COMMANDS section. grandvizier is only intended to be started by dtrealms, not directly by a user. There are two ways to have dtrealms start grandvizier. First, realmctl may be given the -display option. Second, the -display option may be given on dtrealms' command line.","Process Name":"grandvizier","Link":"https:\/\/linux.die.net\/man\/1\/grandvizier"}},{"Process":{"Description":"\"graph-easy\" reads a description of a graph (a connected network of nodes and edges, not a pie chart :-) and then converts this to the desired output format. By default, the input will be read from STDIN , and the output will go to STDOUT . The input is expected to be encoded in UTF-8 , the output will also be UTF-8 . It understands the following formats as input: Graph::Easy  http:\/\/bloodgate.com\/perl\/graph\/manual\/\nDOT          http:\/\/www.graphviz.org\/\nVCG          http:\/\/rw4.cs.uni-sb.de\/~sander\/html\/gsvcg1.html\nGDL          http:\/\/www.aisee.com\/ The formats are automatically detected, regardless of the input file name, but you can also explicitely declare your input to be in one specific format. The output can be a dump of the graph in one of the following formats: Graph::Easy  http:\/\/bloodgate.com\/perl\/graph\/manual\/\nDOT          http:\/\/www.graphviz.org\/\nVCG          http:\/\/rw4.cs.uni-sb.de\/~sander\/html\/gsvcg1.html\nGDL          http:\/\/www.aisee.com\/\nGraphML      http:\/\/graphml.graphdrawing.org\/ In addition, \"Graph::Easy\" can also create layouts of graphs in one of the following output formats: HTML   SVG   ASCII   BOXART Note that for SVG output, you need to install the module Graph::Easy::As_svg first. As a shortcut, you can also specify the output format as 'png', this will cause \"graph-easy\" to pipe the input in graphviz format to the \"dot\" program to create a PNG file in one step. The following two examples are equivalent: graph-easy graph.txt --dot | dot -Tpng -o graph.png\ngraph-easy graph.txt --png","Process Name":"graph-easy","Link":"https:\/\/linux.die.net\/man\/1\/graph-easy"}},{"Process":{"Description":null,"Process Name":"graphc","Link":"https:\/\/linux.die.net\/man\/1\/graphc"}},{"Process":{"Description":null,"Process Name":"graphicsmagick++-config","Link":"https:\/\/linux.die.net\/man\/1\/graphicsmagick++-config"}},{"Process":{"Description":"GraphicsMagick-config prints the compiler and linker flags required to compile and link programs that use the GraphicsMagick Application Programmer Interface.","Process Name":"graphicsmagick-config","Link":"https:\/\/linux.die.net\/man\/1\/graphicsmagick-config"}},{"Process":{"Description":null,"Process Name":"graphicsmagickwand-config","Link":"https:\/\/linux.die.net\/man\/1\/graphicsmagickwand-config"}},{"Process":{"Description":"The grav program draws a simple orbital simulation","Process Name":"grav","Link":"https:\/\/linux.die.net\/man\/1\/grav"}},{"Process":{"Description":"graveman! is a GUI frontend for cdrtools (cdrecord, readcd, mkisofs), dvd+rw-tools, sox and flac. It allows you to burn audio CDs (from WAV, Ogg, MP3 or Flac files) and data CDs \/ DVDs, and allows you to duplicate CDs. You can also import M3U and PTS playlists.","Process Name":"graveman","Link":"https:\/\/linux.die.net\/man\/1\/graveman"}},{"Process":{"Description":null,"Process Name":"grconvert","Link":"https:\/\/linux.die.net\/man\/1\/grconvert"}},{"Process":{"Description":"grd2cpt reads one or more grid files and writes a color palette (cpt) file to standard output. The cpt file is based on an existing master cpt file of your choice, and the mapping from data value to colors is through the data's cumulative distribution function (CDF), so that the colors are histogram equalized. Thus if the grid(s) and the resulting cpt file are used in grdimage with a linear projection, the colors will be uniformly distributed in area on the plot. Let z be the data values in the grid. Define CDF(Z) = (# of z < Z) \/ (# of z in grid). (NaNs are ignored). These z-values are then normalized to the master cpt file and colors are sampled at the desired intervals. The color palette includes three additional colors beyond the range of z-values. These are the background color (B) assigned to values lower than the lowest z-value, the foreground color (F) assigned to values higher than the highest z-value, and the NaN color (N) painted whereever values are undefined. If the master cpt file includes B, F, and N entries, these will be copied into the new master file. If not, the parameters COLOR_BACKGROUND, COLOR_FOREGROUND, and COLOR_NAN from the .gmtdefaults4 file or the command line will be used. This default behavior can be overruled using the options -D, -M or -N. The color model (RGB, HSV or CMYK) of the palette created by makecpt will be the same as specified in the header of the master cpt file. When there is no COLOR_MODEL entry in the master cpt file, the COLOR_MODEL specified in the .gmtdefaults4 file or on the command line will be used. grdfiles Names of one or more 2-D binary grid files used to derive the color palette table. All grids need to have the same size and dimensions. (See GRID FILE FORMATS below).","Process Name":"grd2cpt","Link":"https:\/\/linux.die.net\/man\/1\/grd2cpt"}},{"Process":{"Description":"grd2xyz reads one or more binary 2-D grid files and writes out xyz-triplets in ASCII [or binary] format to standard output. Modify the precision of the ASCII output format by editing the D_FORMAT parameter in your .gmtdefaults4 file or use --D_FORMAT= value on the command line, or choose binary output using single or double precision storage. As an option you may output z-values without the (x,y) in a number of formats, see -E or -Z below. grdfiles Names of 2-D binary grid files to be converted. (See GRID FILE FORMATS below.)","Process Name":"grd2xyz","Link":"https:\/\/linux.die.net\/man\/1\/grd2xyz"}},{"Process":{"Description":null,"Process Name":"grdblend","Link":"https:\/\/linux.die.net\/man\/1\/grdblend"}},{"Process":{"Description":"grdclip will set values < low to below and\/or values > high to above. Useful when you want all of a continent or an ocean to fall into one color or grayshade in image processing, or clipping of the range of data values is required. above\/below can be any number or NaN (Not a Number). You must choose at least one of -Sa or -Sb. input_file.grd The input 2-D binary grid file. -G output_file.grd is the modified output grid file.","Process Name":"grdclip","Link":"https:\/\/linux.die.net\/man\/1\/grdclip"}},{"Process":{"Description":"grdcontour reads a 2-D grid file and produces a contour map by tracing each contour through the grid. As an option, the x\/y\/z positions of the contour lines may be dumped to a single multisegment file or many separate files. PostScript code is generated and sent to standard output. Various options that affect the plotting are available. grdfile 2-D gridded data set to be contoured. (See GRID FILE FORMATS below). -C The contours to be drawn may be specified in one of three possible ways: (1) If cont_int has the suffix \".cpt\" and can be opened as a file, it is assumed to be a color palette table. The color boundaries are then used as contour levels. If the cpt-file has annotation flags in the last column then those contours will be annotated. By default all contours are labeled; use -A- to disable all annotations. (2) If cont_int is a file but not a cpt-file, it is expected to contain contour levels in column 1 and a C(ontour) OR A(nnotate) in col 2. The levels marked C (or c) are contoured, the levels marked A (or a) are contoured and annotated. Optionally, a third column may be present and contain the fixed annotation angle for this contour level. (3) If no file is found, then cont_int is interpreted as a constant contour interval. If -A is set and -C is not, then the contour interval is set equal to the specified annotation interval. If a file is given and -T is set, then only contours marked with upper case C or A will have tickmarks. In all cases the contour values have the same units as the grid. -J Selects the map projection. Scale is UNIT\/degree, 1:xxxxx, or width in UNIT (upper case modifier). UNIT is cm, inch, or m, depending on the MEASURE_UNIT setting in .gmtdefaults4, but this can be overridden on the command line by appending c, i, or m to the scale\/width value. When central meridian is optional, default is center of longitude range on -R option. Default standard parallel is the equator. For map height, max dimension, or min dimension, append h, +, or - to the width, respectively. More details can be found in the psbasemap man pages. CYLINDRICAL PROJECTIONS: -Jclon0\/lat0\/scale (Cassini) -Jcyl_stere\/[lon0\/[lat0\/]]scale (Cylindrical Stereographic) -Jj[lon0\/]scale (Miller) -Jm[lon0\/[lat0\/]]scale (Mercator) -Jmlon0\/lat0\/scale (Mercator - Give meridian and standard parallel) -Jo[a]lon0\/lat0\/azimuth\/scale (Oblique Mercator - point and azimuth) -Jo[b]lon0\/lat0\/lon1\/lat1\/scale (Oblique Mercator - two points) -Joclon0\/lat0\/lonp\/latp\/scale (Oblique Mercator - point and pole) -Jq[lon0\/[lat0\/]]scale (Cylindrical Equidistant) -Jtlon0\/[lat0\/]scale (TM - Transverse Mercator) -Juzone\/scale (UTM - Universal Transverse Mercator) -Jy[lon0\/[lat0\/]]scale (Cylindrical Equal-Area) CONIC PROJECTIONS: -Jblon0\/lat0\/lat1\/lat2\/scale (Albers) -Jdlon0\/lat0\/lat1\/lat2\/scale (Conic Equidistant) -Jllon0\/lat0\/lat1\/lat2\/scale (Lambert Conic Conformal) -Jpoly\/[lon0\/[lat0\/]]scale ((American) Polyconic) AZIMUTHAL PROJECTIONS: -Jalon0\/lat0[\/horizon]\/scale (Lambert Azimuthal Equal-Area) -Jelon0\/lat0[\/horizon]\/scale (Azimuthal Equidistant) -Jflon0\/lat0[\/horizon]\/scale (Gnomonic) -Jglon0\/lat0[\/horizon]\/scale (Orthographic) -Jglon0\/lat0\/altitude\/azimuth\/tilt\/twist\/Width\/Height\/scale (General Perspective). -Jslon0\/lat0[\/horizon]\/scale (General Stereographic) MISCELLANEOUS PROJECTIONS: -Jh[lon0\/]scale (Hammer) -Ji[lon0\/]scale (Sinusoidal) -Jkf[lon0\/]scale (Eckert IV) -Jk[s][lon0\/]scale (Eckert VI) -Jn[lon0\/]scale (Robinson) -Jr[lon0\/]scale (Winkel Tripel) -Jv[lon0\/]scale (Van der Grinten) -Jw[lon0\/]scale (Mollweide) NON-GEOGRAPHICAL PROJECTIONS: -Jp[a]scale[\/origin][r|z] (Polar coordinates (theta,r)) -Jxx-scale[d|l|ppow|t|T][\/y-scale[d|l|ppow|t|T]] (Linear, log, and power scaling)","Process Name":"grdcontour","Link":"https:\/\/linux.die.net\/man\/1\/grdcontour"}},{"Process":{"Description":"grdcut will produce a new output_file.grd file which is a subregion of input_file.grd. The subregion is specified with -R as in other programs; the specified range must not exceed the range of input_file.grd. If in doubt, run grdinfo to check range. Alternatively, define the subregion indirectly via a range check on the node values. Complementary to grdcut there is grdpaste, which will join together two grid files along a common edge. input_file.grd this is the input .grd format file. -G output_file.grd this is the output .grd format file. -R xmin, xmax, ymin, and ymax specify the Region of interest. For geographic regions, these limits correspond to west, east, south, and north and you may specify them in decimal degrees or in [+-]dd:mm[:ss.xxx][W|E|S|N] format. Append r if lower left and upper right map coordinates are given instead of w\/e\/s\/n. The two shorthands -Rg and -Rd stand for global domain (0\/360 and -180\/+180 in longitude respectively, with -90\/+90 in latitude). Alternatively, specify the name of an existing grid file and the -R settings (and grid spacing, if applicable) are copied from the grid. For calendar time coordinates you may either give (a) relative time (relative to the selected TIME_EPOCH and in the selected TIME_UNIT; append t to -JX| x), or (b) absolute time of the form [ date] T[ clock] (append T to -JX| x). At least one of date and clock must be present; the T is always required. The date string must be of the form [-]yyyy[-mm[-dd]] (Gregorian calendar) or yyyy[-Www[-d]] (ISO week calendar), while the clock string must be of the form hh:mm:ss[.xxx]. The use of delimiters and their type and positions must be exactly as indicated (however, input, output and plot formats are customizable; see gmtdefaults). This defines the subregion to be cut out.","Process Name":"grdcut","Link":"https:\/\/linux.die.net\/man\/1\/grdcut"}},{"Process":{"Description":"grdedit reads the header information in a binary 2-D grid file and replaces the information with values provided on the command line [if any]. As an option, global, geographical grids (with 360 degrees longitude range) can be rotated in the east-west direction, and individual nodal values can be replaced from a table of x, y, z values. grdedit only operates on files containing a grdheader. grdfile Name of the 2-D grid file to modify. (See GRID FILE FORMATS below).","Process Name":"grdedit","Link":"https:\/\/linux.die.net\/man\/1\/grdedit"}},{"Process":{"Description":"grdfft will take the 2-D forward Fast Fourier Transform and perform one or more mathematical operations in the frequency domain before transforming back to the space domain. An option is provided to scale the data before writing the new values to an output file. The horizontal dimensions of the grid are assumed to be in meters. Geographical grids may be used by specifying the -M option that scales degrees to meters. If you have grids with dimensions in km, you could change this to meters using grdedit or scale the output with grdmath. in_grdfile 2-D binary grid file to be operated on. (See GRID FILE FORMATS below). -G Specify the name of the output grid file. (See GRID FILE FORMATS below).","Process Name":"grdfft","Link":"https:\/\/linux.die.net\/man\/1\/grdfft"}},{"Process":{"Description":"grdfilter will filter a .grd file in the time domain using one of the selected convolution or non-convolution isotropic filters and compute distances using Cartesian or Spherical geometries. The output .grd file can optionally be generated as a subOPT(R)egion of the input and\/or with a new -Increment. In this way, one may have \"extra space\" in the input data so that the edges will not be used and the output can be within one-half- width of the input edges. If the filter is low-pass, then the output may be less frequently sampled than the input. input_file The grid file of points to be filtered. (See GRID FILE FORMATS below). -D Distance flag tells how grid (x,y) relates to filter width as follows: flag = 0: grid (x,y) same units as width, Cartesian distances. flag = 1: grid (x,y) in degrees, width in kilometers, Cartesian distances. flag = 2: grid (x,y) in degrees, width in km, dx scaled by cos(middle y), Cartesian distances. The above options are fastest because they allow weight matrix to be computed only once. The next three options are slower because they recompute weights for each latitude. flag = 3: grid (x,y) in degrees, width in km, dx scaled by cosine(y), Cartesian distance calculation. flag = 4: grid (x,y) in degrees, width in km, Spherical distance calculation. flag = 5: grid (x,y) in Mercator -Jm1 img units, width in km, Spherical distance calculation. -F Sets the filter type. Choose among convolution and non-convolution filters. Append the filter code followed by the full diameter width. Available convolution filters are: ( b) Boxcar: All weights are equal. ( c) Cosine Arch: Weights follow a cosine arch curve. ( g) Gaussian: Weights are given by the Gaussian function, where width is 6 times the conventional Gaussian sigma. Non-convolution filters are: ( m) Median: Returns median value. ( p) Maximum likelihood probability (a mode estimator): Return modal value. If more than one mode is found we return their average value. Append - or + to the filter width if you rather want to return the smallest or largest of the modal values. ( l) Lower: Return the minimum of all values. ( L) Lower: Return minimum of all positive values only. ( u) Upper: Return maximum of all values. ( U) Upper: Return maximum or all negative values only. In the case of L| U it is possible that no data passes the initial sign test; in that case the filter will return 0.0. -G output_file is the output grid file of the filter. (See GRID FILE FORMATS below).","Process Name":"grdfilter","Link":"https:\/\/linux.die.net\/man\/1\/grdfilter"}},{"Process":{"Description":"grdgradient may be used to compute the directional derivative in a given direction ( -A), or the direction ( -S) [and the magnitude ( -D)] of the vector gradient of the data. Estimated values in the first\/last row\/column of output depend on boundary conditions (see -L). in_grdfile 2-D grid file from which to compute directional derivative. (See GRID FILE FORMATS below). -G Name of the output grid file for the directional derivative. (See GRID FILE FORMATS below).","Process Name":"grdgradient","Link":"https:\/\/linux.die.net\/man\/1\/grdgradient"}},{"Process":{"Description":null,"Process Name":"grdhisteq","Link":"https:\/\/linux.die.net\/man\/1\/grdhisteq"}},{"Process":{"Description":"grdimage reads one 2-D grid file and produces a gray-shaded (or colored) map by plotting rectangles centered on each grid node and assigning them a gray-shade (or color) based on the z-value. Alternatively, grdimage reads three 2-D grid files with the red, green, and blue components directly (all must be in the 0-255 range). Optionally, illumination may be added by providing a file with intensities in the (-1,+1) range. Values outside this range will be clipped. Such intensity files can be created from the grid using grdgradient and, optionally, modified by grdmath or grdhisteq. When using map projections, the grid is first resampled on a new rectangular grid with the same dimensions. Higher resolution images can be obtained by using the -E option. To obtain the resampled value (and hence shade or color) of each map pixel, its location is inversely projected back onto the input grid after which a value is interpolated between the surrounding input grid values. By default bi-cubic interpolation is used. Aliasing is avoided by also forward projecting the input grid nodes. If two or more nodes are projected onto the same pixel, their average will dominate in the calculation of the pixel value. Interpolation and aliasing is controlled with the -S option. The -R option can be used to select a map region larger or smaller than that implied by the extent of the grid. A (color) PostScript file is output. grd_z | grd_r grd_g grd_b 2-D gridded data set (or red, green, blue grids) to be imaged (See GRID FILE FORMATS below.) -C name of the color palette table (for grd_z only). -J Selects the map projection. Scale is UNIT\/degree, 1:xxxxx, or width in UNIT (upper case modifier). UNIT is cm, inch, or m, depending on the MEASURE_UNIT setting in .gmtdefaults4, but this can be overridden on the command line by appending c, i, or m to the scale\/width value. When central meridian is optional, default is center of longitude range on -R option. Default standard parallel is the equator. For map height, max dimension, or min dimension, append h, +, or - to the width, respectively. More details can be found in the psbasemap man pages. CYLINDRICAL PROJECTIONS: -Jclon0\/lat0\/scale (Cassini) -Jcyl_stere\/[lon0\/[lat0\/]]scale (Cylindrical Stereographic) -Jj[lon0\/]scale (Miller) -Jm[lon0\/[lat0\/]]scale (Mercator) -Jmlon0\/lat0\/scale (Mercator - Give meridian and standard parallel) -Jo[a]lon0\/lat0\/azimuth\/scale (Oblique Mercator - point and azimuth) -Jo[b]lon0\/lat0\/lon1\/lat1\/scale (Oblique Mercator - two points) -Joclon0\/lat0\/lonp\/latp\/scale (Oblique Mercator - point and pole) -Jq[lon0\/[lat0\/]]scale (Cylindrical Equidistant) -Jtlon0\/[lat0\/]scale (TM - Transverse Mercator) -Juzone\/scale (UTM - Universal Transverse Mercator) -Jy[lon0\/[lat0\/]]scale (Cylindrical Equal-Area) CONIC PROJECTIONS: -Jblon0\/lat0\/lat1\/lat2\/scale (Albers) -Jdlon0\/lat0\/lat1\/lat2\/scale (Conic Equidistant) -Jllon0\/lat0\/lat1\/lat2\/scale (Lambert Conic Conformal) -Jpoly\/[lon0\/[lat0\/]]scale ((American) Polyconic) AZIMUTHAL PROJECTIONS: -Jalon0\/lat0[\/horizon]\/scale (Lambert Azimuthal Equal-Area) -Jelon0\/lat0[\/horizon]\/scale (Azimuthal Equidistant) -Jflon0\/lat0[\/horizon]\/scale (Gnomonic) -Jglon0\/lat0[\/horizon]\/scale (Orthographic) -Jglon0\/lat0\/altitude\/azimuth\/tilt\/twist\/Width\/Height\/scale (General Perspective). -Jslon0\/lat0[\/horizon]\/scale (General Stereographic) MISCELLANEOUS PROJECTIONS: -Jh[lon0\/]scale (Hammer) -Ji[lon0\/]scale (Sinusoidal) -Jkf[lon0\/]scale (Eckert IV) -Jk[s][lon0\/]scale (Eckert VI) -Jn[lon0\/]scale (Robinson) -Jr[lon0\/]scale (Winkel Tripel) -Jv[lon0\/]scale (Van der Grinten) -Jw[lon0\/]scale (Mollweide) NON-GEOGRAPHICAL PROJECTIONS: -Jp[a]scale[\/origin][r|z] (Polar coordinates (theta,r)) -Jxx-scale[d|l|ppow|t|T][\/y-scale[d|l|ppow|t|T]] (Linear, log, and power scaling)","Process Name":"grdimage","Link":"https:\/\/linux.die.net\/man\/1\/grdimage"}},{"Process":{"Description":"grdinfo reads a 2-D binary grid file and reports various statistics for the ( x,y,z) data in the grid file(s). The output information contains the minimum\/maximum values for x, y, and z, where the min\/max of z occur, the x- and y-increments, and the number of x and y nodes, and [optionally] the mean, standard deviation, and\/or the median, L1 scale of z, and number of nodes set to NaN. grdfile The name of one or several 2-D grid files. (See GRID FILE FORMATS below.)","Process Name":"grdinfo","Link":"https:\/\/linux.die.net\/man\/1\/grdinfo"}},{"Process":{"Description":"grdlandmask reads the selected shoreline database and uses that information to decide which nodes in the specified grid are over land or over water. The nodes defined by the selected region and lattice spacing will be set according to one of two criteria: (1) land vs water, or (2) the more detailed (hierarchical) ocean vs land vs lake vs island vs pond. The resulting mask may be used in subsequent operations involving grdmath to mask out data from land [or water] areas. -G Name of resulting output mask grid file. (See GRID FILE FORMATS below). -I x_inc [and optionally y_inc] is the grid spacing. Optionally, append a suffix modifier. Geographical (degrees) coordinates: Append m to indicate arc minutes or c to indicate arc seconds. If one of the units e, k, i, or n is appended instead, the increment is assumed to be given in meter, km, miles, or nautical miles, respectively, and will be converted to the equivalent degrees longitude at the middle latitude of the region (the conversion depends on ELLIPSOID). If \/ y_inc is given but set to 0 it will be reset equal to x_inc; otherwise it will be converted to degrees latitude. All coordinates: If = is appended then the corresponding max x ( east) or y ( north) may be slightly adjusted to fit exactly the given increment [by default the increment may be adjusted slightly to fit the given domain]. Finally, instead of giving an increment you may specify the number of nodes desired by appending + to the supplied integer argument; the increment is then recalculated from the number of nodes and the domain. The resulting increment value depends on whether you have selected a gridline-registered or pixel-registered grid; see Appendix B for details. Note: if -R grdfile is used then grid spacing has already been initialized; use -I to override the values. -R west, east, south, and north specify the Region of interest, and you may specify them in decimal degrees or in [+-]dd:mm[:ss.xxx][W|E|S|N] format. Append r if lower left and upper right map coordinates are given instead of w\/e\/s\/n. The two shorthands -Rg and -Rd stand for global domain (0\/360 and -180\/+180 in longitude respectively, with -90\/+90 in latitude). Alternatively, specify the name of an existing grid file and the -R settings (and grid spacing, if applicable) are copied from the grid.","Process Name":"grdlandmask","Link":"https:\/\/linux.die.net\/man\/1\/grdlandmask"}},{"Process":{"Description":null,"Process Name":"grdmask","Link":"https:\/\/linux.die.net\/man\/1\/grdmask"}},{"Process":{"Description":"grdmath will perform operations like add, subtract, multiply, and divide on one or more grid files or constants using Reverse Polish Notation (RPN) syntax (e.g., Hewlett-Packard calculator-style). Arbitrarily complicated expressions may therefore be evaluated; the final result is written to an output grid file. When two grids are on the stack, each element in file A is modified by the corresponding element in file B. However, some operators only require one operand (see below). If no grid files are used in the expression then options -R, -I must be set (and optionally -F). The expression = outgrdfile can occur as many times as the depth of the stack allows. operand If operand can be opened as a file it will be read as a grid file. If not a file, it is interpreted as a numerical constant or a special symbol (see below). outgrdfile The name of a 2-D grid file that will hold the final result. (See GRID FILE FORMATS below). OPERATORS Choose among the following 145 operators. \"args\" are the number of input and output arguments. Operator args Returns ABS 1 1 abs (A). ACOS 1 1 acos (A). ACOSH 1 1 acosh (A). ACOT 1 1 acot (A). ACSC 1 1 acsc (A). ADD 2 1 A + B. AND 2 1 NaN if A and B == NaN, B if A == NaN, else A. ASEC 1 1 asec (A). ASIN 1 1 asin (A). ASINH 1 1 asinh (A). ATAN 1 1 atan (A). ATAN2 2 1 atan2 (A, B). ATANH 1 1 atanh (A). BEI 1 1 bei (A). BER 1 1 ber (A). CAZ 2 1 Cartesian azimuth from grid nodes to stack x,y. CBAZ 2 1 Cartesian backazimuth from grid nodes to stack x,y. CDIST 2 1 Cartesian distance between grid nodes and stack x,y. CEIL 1 1 ceil (A) (smallest integer >= A). CHICRIT 2 1 Critical value for chi-squared-distribution, with alpha = A and n = B. CHIDIST 2 1 chi-squared-distribution P(chi2,n), with chi2 = A and n = B. CORRCOEFF 2 1 Correlation coefficient r(A, B). COS 1 1 cos (A) (A in radians). COSD 1 1 cos (A) (A in degrees). COSH 1 1 cosh (A). COT 1 1 cot (A) (A in radians). COTD 1 1 cot (A) (A in degrees). CPOISS 2 1 Cumulative Poisson distribution F(x,lambda), with x = A and lambda = B. CSC 1 1 csc (A) (A in radians). CSCD 1 1 csc (A) (A in degrees). CURV 1 1 Curvature of A (Laplacian). D2DX2 1 1 d^2(A)\/dx^2 2nd derivative. D2DXY 1 1 d^2(A)\/dxdy 2nd derivative. D2DY2 1 1 d^2(A)\/dy^2 2nd derivative. D2R 1 1 Converts Degrees to Radians. DDX 1 1 d(A)\/dx Central 1st derivative. DDY 1 1 d(A)\/dy Central 1st derivative. DILOG 1 1 dilog (A). DIV 2 1 A \/ B. DUP 1 2 Places duplicate of A on the stack. EQ 2 1 1 if A == B, else 0. ERF 1 1 Error function erf (A). ERFC 1 1 Complementary Error function erfc (A). ERFINV 1 1 Inverse error function of A. EXCH 2 2 Exchanges A and B on the stack. EXP 1 1 exp (A). EXTREMA 1 1 Local Extrema: +2\/-2 is max\/min, +1\/-1 is saddle with max\/min in x, 0 elsewhere. FACT 1 1 A! (A factorial). FCRIT 3 1 Critical value for F-distribution, with alpha = A, n1 = B, and n2 = C. FDIST 3 1 F-distribution Q(F,n1,n2), with F = A, n1 = B, and n2 = C. FLIPLR 1 1 Reverse order of values in each row. FLIPUD 1 1 Reverse order of values in each column. FLOOR 1 1 floor (A) (greatest integer <= A). FMOD 2 1 A % B (remainder after truncated division). GE 2 1 1 if A >= B, else 0. GT 2 1 1 if A > B, else 0. HYPOT 2 1 hypot (A, B) = sqrt (A*A + B*B). I0 1 1 Modified Bessel function of A (1st kind, order 0). I1 1 1 Modified Bessel function of A (1st kind, order 1). IN 2 1 Modified Bessel function of A (1st kind, order B). INRANGE 3 1 1 if B <= A <= C, else 0. INSIDE 1 1 1 when inside or on polygon(s) in A, else 0. INV 1 1 1 \/ A. ISNAN 1 1 1 if A == NaN, else 0. J0 1 1 Bessel function of A (1st kind, order 0). J1 1 1 Bessel function of A (1st kind, order 1). JN 2 1 Bessel function of A (1st kind, order B). K0 1 1 Modified Kelvin function of A (2nd kind, order 0). K1 1 1 Modified Bessel function of A (2nd kind, order 1). KEI 1 1 kei (A). KER 1 1 ker (A). KN 2 1 Modified Bessel function of A (2nd kind, order B). KURT 1 1 Kurtosis of A. LDIST 1 1 Compute distance from lines in multi-segment ASCII file A. LE 2 1 1 if A <= B, else 0. LMSSCL 1 1 LMS scale estimate (LMS STD) of A. LOG 1 1 log (A) (natural log). LOG10 1 1 log10 (A) (base 10). LOG1P 1 1 log (1+A) (accurate for small A). LOG2 1 1 log2 (A) (base 2). LOWER 1 1 The lowest (minimum) value of A. LRAND 2 1 Laplace random noise with mean A and std. deviation B. LT 2 1 1 if A < B, else 0. MAD 1 1 Median Absolute Deviation (L1 STD) of A. MAX 2 1 Maximum of A and B. MEAN 1 1 Mean value of A. MED 1 1 Median value of A. MIN 2 1 Minimum of A and B. MOD 2 1 A mod B (remainder after floored division). MODE 1 1 Mode value (Least Median of Squares) of A. MUL 2 1 A * B. NAN 2 1 NaN if A == B, else A. NEG 1 1 -A. NEQ 2 1 1 if A != B, else 0. NOT 1 1 NaN if A == NaN, 1 if A == 0, else 0. NRAND 2 1 Normal, random values with mean A and std. deviation B. OR 2 1 NaN if A or B == NaN, else A. PDIST 1 1 Compute distance from points in ASCII file A. PLM 3 1 Associated Legendre polynomial P(A) degree B order C. PLMg 3 1 Normalized associated Legendre polynomial P(A) degree B order C (geophysical convention). POP 1 0 Delete top element from the stack. POW 2 1 A ^ B. PQUANT 2 1 The B'th Quantile (0-100%) of A. PSI 1 1 Psi (or Digamma) of A. PV 3 1 Legendre function Pv(A) of degree v = real(B) + imag(C). QV 3 1 Legendre function Qv(A) of degree v = real(B) + imag(C). R2 2 1 R2 = A^2 + B^2. R2D 1 1 Convert Radians to Degrees. RAND 2 1 Uniform random values between A and B. RINT 1 1 rint (A) (nearest integer). ROTX 2 1 Rotate A by the (constant) shift B in x-direction. ROTY 2 1 Rotate A by the (constant) shift B in y-direction. SAZ 2 1 Spherical azimuth from grid nodes to stack x,y. SBAZ 2 1 Spherical backazimuth from grid nodes to stack x,y. SDIST 2 1 Spherical (Great circle) distance (in degrees) between grid nodes and stack lon,lat (A, B). SEC 1 1 sec (A) (A in radians). SECD 1 1 sec (A) (A in degrees). SIGN 1 1 sign (+1 or -1) of A. SIN 1 1 sin (A) (A in radians). SINC 1 1 sinc (A) (sin (pi*A)\/(pi*A)). SIND 1 1 sin (A) (A in degrees). SINH 1 1 sinh (A). SKEW 1 1 Skewness of A. SQR 1 1 A^2. SQRT 1 1 sqrt (A). STD 1 1 Standard deviation of A. STEP 1 1 Heaviside step function: H(A). STEPX 1 1 Heaviside step function in x: H(x-A). STEPY 1 1 Heaviside step function in y: H(y-A). SUB 2 1 A - B. TAN 1 1 tan (A) (A in radians). TAND 1 1 tan (A) (A in degrees). TANH 1 1 tanh (A). TCRIT 2 1 Critical value for Student's t-distribution, with alpha = A and n = B. TDIST 2 1 Student's t-distribution A(t,n), with t = A, and n = B. TN 2 1 Chebyshev polynomial Tn(-1<t<+1,n), with t = A, and n = B. UPPER 1 1 The highest (maximum) value of A. XOR 2 1 B if A == NaN, else A. Y0 1 1 Bessel function of A (2nd kind, order 0). Y1 1 1 Bessel function of A (2nd kind, order 1). YLM 2 2 Re and Im orthonormalized spherical harmonics degree A order B. YLMg 2 2 Cos and Sin normalized spherical harmonics degree A order B (geophysical convention). YN 2 1 Bessel function of A (2nd kind, order B). ZCRIT 1 1 Critical value for the normal-distribution, with alpha = A. ZDIST 1 1 Cumulative normal-distribution C(x), with x = A. SYMBOLS The following symbols have special meaning: PI 3.1415926... E 2.7182818... EULER 0.5772156... XMIN Minimum x value XMAX Maximum x value XINC x increment NX The number of x nodes YMIN Minimum y value YMAX Maximum y value YINC y increment NY The number of y nodes X Grid with x-coordinates Y Grid with y-coordinates Xn Grid with normalized [-1 to +1] x-coordinates Yn Grid with normalized [-1 to +1] y-coordinates","Process Name":"grdmath","Link":"https:\/\/linux.die.net\/man\/1\/grdmath"}},{"Process":{"Description":null,"Process Name":"grdpaste","Link":"https:\/\/linux.die.net\/man\/1\/grdpaste"}},{"Process":{"Description":"grdproject will do one of two things depending whether -I has been set. If set, it will transform a gridded data set from a rectangular coordinate system onto a geographical system by resampling the surface at the new nodes. If not set, it will project a geographical gridded data set onto a rectangular grid. To obtain the value at each new node, its location is inversely projected back onto the input grid after which a value is interpolated between the surrounding input grid values. By default bi-cubic interpolation is used. Aliasing is avoided by also forward projecting the input grid nodes. If two or more nodes are projected onto the same new node, their average will dominate in the calculation of the new node value. Interpolation and aliasing is controlled with the -S option. The new node spacing may be determined in one of several ways by specifying the grid spacing, number of nodes, or resolution. Nodes not constrained by input data are set to NaN. The -R option can be used to select a map region larger or smaller than that implied by the extent of the grid file. in_grdfile 2-D binary grid file to be transformed. (See GRID FILE FORMATS below.) -G Specify the name of the output grid file. (See GRID FILE FORMATS below.) -J Selects the map projection. Scale is UNIT\/degree, 1:xxxxx, or width in UNIT (upper case modifier). UNIT is cm, inch, or m, depending on the MEASURE_UNIT setting in .gmtdefaults4, but this can be overridden on the command line by appending c, i, or m to the scale\/width value. When central meridian is optional, default is center of longitude range on -R option. Default standard parallel is the equator. For map height, max dimension, or min dimension, append h, +, or - to the width, respectively. More details can be found in the psbasemap man pages. CYLINDRICAL PROJECTIONS: -Jclon0\/lat0\/scale (Cassini) -Jcyl_stere\/[lon0\/[lat0\/]]scale (Cylindrical Stereographic) -Jj[lon0\/]scale (Miller) -Jm[lon0\/[lat0\/]]scale (Mercator) -Jmlon0\/lat0\/scale (Mercator - Give meridian and standard parallel) -Jo[a]lon0\/lat0\/azimuth\/scale (Oblique Mercator - point and azimuth) -Jo[b]lon0\/lat0\/lon1\/lat1\/scale (Oblique Mercator - two points) -Joclon0\/lat0\/lonp\/latp\/scale (Oblique Mercator - point and pole) -Jq[lon0\/[lat0\/]]scale (Cylindrical Equidistant) -Jtlon0\/[lat0\/]scale (TM - Transverse Mercator) -Juzone\/scale (UTM - Universal Transverse Mercator) -Jy[lon0\/[lat0\/]]scale (Cylindrical Equal-Area) CONIC PROJECTIONS: -Jblon0\/lat0\/lat1\/lat2\/scale (Albers) -Jdlon0\/lat0\/lat1\/lat2\/scale (Conic Equidistant) -Jllon0\/lat0\/lat1\/lat2\/scale (Lambert Conic Conformal) -Jpoly\/[lon0\/[lat0\/]]scale ((American) Polyconic) AZIMUTHAL PROJECTIONS: -Jalon0\/lat0[\/horizon]\/scale (Lambert Azimuthal Equal-Area) -Jelon0\/lat0[\/horizon]\/scale (Azimuthal Equidistant) -Jflon0\/lat0[\/horizon]\/scale (Gnomonic) -Jglon0\/lat0[\/horizon]\/scale (Orthographic) -Jglon0\/lat0\/altitude\/azimuth\/tilt\/twist\/Width\/Height\/scale (General Perspective). -Jslon0\/lat0[\/horizon]\/scale (General Stereographic) MISCELLANEOUS PROJECTIONS: -Jh[lon0\/]scale (Hammer) -Ji[lon0\/]scale (Sinusoidal) -Jkf[lon0\/]scale (Eckert IV) -Jk[s][lon0\/]scale (Eckert VI) -Jn[lon0\/]scale (Robinson) -Jr[lon0\/]scale (Winkel Tripel) -Jv[lon0\/]scale (Van der Grinten) -Jw[lon0\/]scale (Mollweide) NON-GEOGRAPHICAL PROJECTIONS: -Jp[a]scale[\/origin][r|z] (Polar coordinates (theta,r)) -Jxx-scale[d|l|ppow|t|T][\/y-scale[d|l|ppow|t|T]] (Linear, log, and power scaling)","Process Name":"grdproject","Link":"https:\/\/linux.die.net\/man\/1\/grdproject"}},{"Process":{"Description":"grdraster reads a file called grdraster.info from the current working directory, the directories pointed to by the environment variables $GMT_DATADIR and $GMT_USERDIR, or in $GMT_SHAREDIR\/dbase (in that order). The file grdraster.info defines binary arrays of data stored in scan-line format in data files. Each file is given a filenumber in the info file. grdraster figures out how to load the raster data into a grid file spanning a region defined by -R. By default the grid spacing equals the raster spacing. The -I option may be used to sub-sample the raster data. No filtering or interpolating is done, however; the x_inc and y_inc of the grid must be multiples of the increments of the raster file and grdraster simply takes every n'th point. The output of grdraster is either grid or pixel registered depending on the registration of the raster used. It is up to the GMT system person to maintain the grdraster.info file in accordance with the available rasters at each site. Raster data sets are not supplied with GMT but can be obtained by anonymous ftp and on CD-ROM (see README page in dbase directory). grdraster will list the available files if no arguments are given. Finally, grdraster will write xyz-triplets to stdout if no output gridfile name is given filenumber If an integer matching one of the files listed in the grdraster.info file is given we will use that data set, else we will match the given text pattern with the data set description in order to determine the data set. -R west, east, south, and north specify the Region of interest, and you may specify them in decimal degrees or in [+-]dd:mm[:ss.xxx][W|E|S|N] format. Append r if lower left and upper right map coordinates are given instead of w\/e\/s\/n. The two shorthands -Rg and -Rd stand for global domain (0\/360 and -180\/+180 in longitude respectively, with -90\/+90 in latitude). Alternatively, specify the name of an existing grid file and the -R settings (and grid spacing, if applicable) are copied from the grid. If r is appended, you may also specify a map projection to define the shape of your region. The output region will be rounded off to the nearest whole grid-step in both dimensions.","Process Name":"grdraster","Link":"https:\/\/linux.die.net\/man\/1\/grdraster"}},{"Process":{"Description":null,"Process Name":"grdreformat","Link":"https:\/\/linux.die.net\/man\/1\/grdreformat"}},{"Process":{"Description":"grdrotater reads a geographical grid and reconstructs it given a total reconstruction rotation. Optionally, the user may supply a clipping polygon in multiple-segment format; then, only the part of the grid inside the polygon is used to determine the return grid region. The outline of the projected region is returned on stdout provided the rotated region is not the entire globe. No space between the option flag and the associated arguments. Use upper case for the option flags and lower case for modifiers. ingrdfile Name of a grid file in geographical (lon, lat) coordinates. -G Name of output grid. This is the grid with the data reconstructed according to the specified rotation. -T Finite rotation. Specify the longitude and latitude of the rotation pole and the opening angle, all in degrees.","Process Name":"grdrotater","Link":"https:\/\/linux.die.net\/man\/1\/grdrotater"}},{"Process":{"Description":null,"Process Name":"grdsample","Link":"https:\/\/linux.die.net\/man\/1\/grdsample"}},{"Process":{"Description":"grdspotter reads a grid file with residual bathymetry or gravity and calculates flowlines from each node that exceeds a minimum value using the specified rotations file. These flowlines are then convolved with the volume of the prism represented by each grid node and added up to give a Cumulative Volcano Amplitude grid (CVA). No space between the option flag and the associated arguments. Use upper case for the option flags and lower case for modifiers. grdfile Data grid to be processed, typically residual bathymetry or free-air anomalies. -E Give file with rotation parameters. This file must contain one record for each rotation; each record must be of the following format: lon lat tstart [tstop] angle [ khat a b c d e f g df ] where tstart and tstop are in Myr and lon lat angle are in degrees. tstart and tstop are the ages of the old and young ends of a stage. If -C is set then a total reconstruction rotation is expected and tstop is implicitly set to 0 and should not be specified in the file. If a covariance matrix C for the rotation is available it must be specified in a format using the nine optional terms listed in brackets. Here, C = (g\/khat)*[ a b d; b c e; d e f ] which shows C made up of three row vectors. If the degrees of freedom (df) in fitting the rotation is 0 or not given it is set to 10000. Blank lines and records whose first column contains # will be ignored. -G Specify name for output CVA grid file. -I x_inc [and optionally y_inc] is the grid spacing. Optionally, append a suffix modifier. Geographical (degrees) coordinates: Append m to indicate arc minutes or c to indicate arc seconds. If one of the units e, k, i, or n is appended instead, the increment is assumed to be given in meter, km, miles, or nautical miles, respectively, and will be converted to the equivalent degrees longitude at the middle latitude of the region (the conversion depends on ELLIPSOID). If \/ y_inc is given but set to 0 it will be reset equal to x_inc; otherwise it will be converted to degrees latitude. All coordinates: If = is appended then the corresponding max x ( east) or y ( north) may be slightly adjusted to fit exactly the given increment [by default the increment may be adjusted slightly to fit the given domain]. Finally, instead of giving an increment you may specify the number of nodes desired by appending + to the supplied integer argument; the increment is then recalculated from the number of nodes and the domain. The resulting increment value depends on whether you have selected a gridline-registered or pixel-registered grid; see Appendix B for details. Note: if -R grdfile is used then grid spacing has already been initialized; use -I to override the values. -R west, east, south, and north specify the Region of interest, and you may specify them in decimal degrees or in [+-]dd:mm[:ss.xxx][W|E|S|N] format. Append r if lower left and upper right map coordinates are given instead of w\/e\/s\/n. The two shorthands -Rg and -Rd stand for global domain (0\/360 and -180\/+180 in longitude respectively, with -90\/+90 in latitude). Alternatively, specify the name of an existing grid file and the -R settings (and grid spacing, if applicable) are copied from the grid.","Process Name":"grdspotter","Link":"https:\/\/linux.die.net\/man\/1\/grdspotter"}},{"Process":{"Description":null,"Process Name":"grdtrack","Link":"https:\/\/linux.die.net\/man\/1\/grdtrack"}},{"Process":{"Description":"grdtrend reads a 2-D grid file and fits a low-order polynomial trend to these data by [optionally weighted] least-squares. The trend surface is defined by: m1 + m2*x + m3*y + m4*x*y + m5*x*x + m6*y*y + m7*x*x*x + m8*x*x*y + m9*x*y*y + m10*y*y*y. The user must specify -Nn_model, the number of model parameters to use; thus, -N4 fits a bilinear trend, -N6 a quadratic surface, and so on. Optionally, append r to the -N option to perform a robust fit. In this case, the program will iteratively reweight the data based on a robust scale estimate, in order to converge to a solution insensitive to outliers. This may be handy when separating a \"regional\" field from a \"residual\" which should have non-zero mean, such as a local mountain on a regional surface. If data file has values set to NaN, these will be ignored during fitting; if output files are written, these will also have NaN in the same locations. No space between the option flag and the associated arguments. grdfile The name of a 2-D binary grid file. -N [ r] n_model sets the number of model parameters to fit. Append r for robust fit.","Process Name":"grdtrend","Link":"https:\/\/linux.die.net\/man\/1\/grdtrend"}},{"Process":{"Description":null,"Process Name":"grdvector","Link":"https:\/\/linux.die.net\/man\/1\/grdvector"}},{"Process":{"Description":null,"Process Name":"grdview","Link":"https:\/\/linux.die.net\/man\/1\/grdview"}},{"Process":{"Description":"grdvolume reads a 2-D binary grid file and calculates the volume contained between the surface and the plane specified by the given contour (or zero if not given) and reports the area, volume, and maximum mean height (volume\/area). Alternatively, specify a range of contours to be tried and grdvolume will determine the volume and area inside the contour for all contour values. The contour that produced the maximum mean height is reported as well. This feature may be used with grdfilter in designing an Optimal Robust Separator [ Wessel, 1998]. grdfile The name of the input 2-D binary grid file. (See GRID FILE FORMAT below.)","Process Name":"grdvolume","Link":"https:\/\/linux.die.net\/man\/1\/grdvolume"}},{"Process":{"Description":null,"Process Name":"greenspline","Link":"https:\/\/linux.die.net\/man\/1\/greenspline"}},{"Process":{"Description":"This file documents the GNU version of refer, which is part of the groff document formatting system. refer copies the contents of filename... to the standard output, except that lines between .[ and .] are interpreted as citations, and lines between .R1 and .R2 are interpreted as commands about how citations are to be processed. Each citation specifies a reference. The citation can specify a reference that is contained in a bibliographic database by giving a set of keywords that only that reference contains. Alternatively it can specify a reference by supplying a database record in the citation. A combination of these alternatives is also possible. For each citation, refer can produce a mark in the text. This mark consists of some label which can be separated from the text and from other labels in various ways. For each reference it also outputs groff commands that can be used by a macro package to produce a formatted reference for each citation. The output of refer must therefore be processed using a suitable macro package. The -ms and -me macros are both suitable. The commands to format a citation's reference can be output immediately after the citation, or the references may be accumulated, and the commands output at some later point. If the references are accumulated, then multiple citations of the same reference will produce a single formatted reference. The interpretation of lines between .R1 and .R2 as commands is a new feature of GNU refer. Documents making use of this feature can still be processed by Unix refer just by adding the lines .de R1\n.ig R2\n..\n to the beginning of the document. This will cause troff to ignore everything between .R1 and .R2. The effect of some commands can also be achieved by options. These options are supported mainly for compatibility with Unix refer. It is usually more convenient to use commands. refer generates .lf lines so that filenames and line numbers in messages produced by commands that read refer output will be correct; it also interprets lines beginning with .lf so that filenames and line numbers in the messages and .lf lines that it produces will be accurate even if the input has been preprocessed by a command such as soelim(1).","Process Name":"grefer","Link":"https:\/\/linux.die.net\/man\/1\/grefer"}},{"Process":{"Description":"The grenum program will renumber the refdes definitions read from infile.","Process Name":"grenum","Link":"https:\/\/linux.die.net\/man\/1\/grenum"}},{"Process":{"Description":"grep searches the named input FILEs (or standard input if no files are named, or if a single hyphen-minus (-) is given as file name) for lines containing a match to the given PATTERN. By default, grep prints the matching lines. In addition, two variant programs egrep and fgrep are available. egrep is the same as grep -E. fgrep is the same as grep -F. Direct invocation as either egrep or fgrep is deprecated, but is provided to allow historical applications that rely on them to run unmodified.","Process Name":"grep","Link":"https:\/\/linux.die.net\/man\/1\/grep"}},{"Process":{"Description":"grep-changelog searches the named CHANGELOGs (by default files matching the regular expressions ChangeLog and ChangeLog\\.[0-9]+) for entries matching the specified criteria. At least one option or file must be specified. This program is distributed with GNU Emacs.","Process Name":"grep-changelog","Link":"https:\/\/linux.die.net\/man\/1\/grep-changelog"}},{"Process":{"Description":null,"Process Name":"grepdiff","Link":"https:\/\/linux.die.net\/man\/1\/grepdiff"}},{"Process":{"Description":null,"Process Name":"grephistory","Link":"https:\/\/linux.die.net\/man\/1\/grephistory"}},{"Process":{"Description":"The \"grepjar\" program can be used to search files in a jar file for a pattern.","Process Name":"grepjar","Link":"https:\/\/linux.die.net\/man\/1\/grepjar"}},{"Process":{"Description":null,"Process Name":"grepmail","Link":"https:\/\/linux.die.net\/man\/1\/grepmail"}},{"Process":{"Description":"The greynetic program draws random rectangles.","Process Name":"greynetic","Link":"https:\/\/linux.die.net\/man\/1\/greynetic"}},{"Process":{"Description":null,"Process Name":"grid-ca-create","Link":"https:\/\/linux.die.net\/man\/1\/grid-ca-create"}},{"Process":{"Description":"The grid-ca-package utility creates a tarball containing an RPM spec file and the files needed to use a CA with grid tools. It optionally will also create a GPT package for distributing a CA. By default, the grid-ca-package utility displays a list of installed grid CA and prompts for which CA to package. It then creates a tarball containing the CA certificate, signing policy, CA configuration files, and an spec script to generate a binary RPM package containing the CA. If the CA hash is known prior to running grid-ca-package, it may provided as an argument to the -ca parameter to avoid prompting. In addition to generating a spec script and tarball, grid-ca-package creates a GPT package if either the -g or -b options are used on the command-line. These packages may be used to distribute a CA and configuration to systems which do not support RPM packages. The grid-ca-package utility writes the package tarballs to the current working directory. The full set of command-line options to grid-ca-package follows. -help, -h, -usage Display the command-line options to grid-ca-package and exit. -version, -versions Display the version number of the grid-ca-package command. The second form includes more details. -ca CA Use the CA whose name matches the hash string CA. When invoked with this option, grid-ca-package runs non-interactively. -g Create a GPT binary package in addition to the RPM script tarball. This package may be installed on other systems using the gpt-install program. -b Create a GPT binary package with GPT metadata located in the path expected by GPT 3.2 (used in Globus 2.0.0-5.0.x) instead of ${datadir}\/globus\/packages as used in Globus 5.2.x. This option overrides the -g command-line option. -r Create a binary RPM package for the CA. This option currently only works on RPM-based distributions. -d Create a binary Debian package for the CA. This option currently only works on Debian-based distributions.","Process Name":"grid-ca-package","Link":"https:\/\/linux.die.net\/man\/1\/grid-ca-package"}},{"Process":{"Description":null,"Process Name":"grid-ca-sign","Link":"https:\/\/linux.die.net\/man\/1\/grid-ca-sign"}},{"Process":{"Description":"The grid-cert-diagnostics program displays information about the current user's security environment, including information about security-related environment variables, security directory search path, personal key and certificates, and trusted certificates. It is intended to provide information to help diagnose problems using GSIC. By default, grid-cert-diagnostics prints out information regarding the environment and trusted certificate directory. If the -p command-line option is used, then additional information about the current user's default certificate and key will be printed. The full set of command-line options to grid-cert-diagnostics consists of: -h, -help Display a help message and exit. -p Display information about the personal certificate and key that is the current user's default credential. -n Check time synchronization with the ntpdate command. -c CERTIFICATE, -c - Check the validity of the certificate in the file named by CERTIFICATE or standard input if the parameter to -c is -.","Process Name":"grid-cert-diagnostics","Link":"https:\/\/linux.die.net\/man\/1\/grid-cert-diagnostics"}},{"Process":{"Description":null,"Process Name":"grid-cert-info","Link":"https:\/\/linux.die.net\/man\/1\/grid-cert-info"}},{"Process":{"Description":"The grid-cert-request program generates an X.509 Certificate Request and corresponding private key for the specified name, host, or service. It is intended to be used with a CA implemented using the globus_simple_ca package. The default behavior of grid-cert-request is to generate a certificate request and private key for the user running the command. The subject name is derived from the gecos information in the local system's password database, unless the -commonname, -cn, or -host command-line options are used. By default, grid-cert-request writes user certificate requests and keys to the $HOME\/.globus directory, and host and service certificate requests and keys to \/etc\/grid-security. This can be overridden by using the -dir command-line option. The full set of command-line options to grid-cert-request are: -help, -h, -?, -usage Display the command-line options to grid-cert-request and exit. -version, -versions Display the version number of the grid-cert-request command. The second form includes more details. -cn NAME, -commonname NAME Create a certificate request with the common name component of the subject set to NAME. This is used to create user identity certificates. -dir DIRECTORY Write the certificate request and key to files in the directory specified by DIRECTORY. -prefix PREFIX Use the string PREFIX as the base name of the certificate, certificate_request, and key files instead of the default. For a user certificate request, this would mean creating files $HOME\/.globus\/ PREFIXcert_request.pem, $HOME\/.globus\/ PREFIXcert.pem, and $HOME\/.globus\/ PREFIXkey.pem. -ca CA-HASH Use the certificate request configuration for the CA with the name hash CA-HASH instead of the default CA chosen by running grid-default-ca. -verbose Keep the output from the OpenSSL certificate request command visible after it completes, instead of clearing the screen.. -interactive, -int Prompt for each component of the subject name of the request, instead of generating the common name from other command-line options. Note that CAs may not sign certificates for subject names that don't match their signing policies. -force Overwrite any existing certificate request and private key with a new one. -nopw, -nodes, -nopassphrase Create an unencrypted private key for the certificate instead of prompting for a passphrase. This is the default behavior for host or service certificates, but not recommended for user certificates. -host FQDN Create a certificate request for use on a particular host. This option also causes the private key assoicated with the certificate request to be unencrypted. The FQDN argument to this option should be the fully qualified domain name of the host that will use this certificate. The subject name of the certificate will be derived from the FQDN and the service option if specified by the -service command-line option. If the host for the certificate has multiple names, then use either the -dns or -ip command-line options to add alternate names or addresses to the certificates. -service SERVICE Create a certificate request for a particular service on a host. The subject name of the certificate will be derived from the FQDN passed as the argument to the -host command-line option and the SERVICE string. -dns FQDN,... Create a certificate request containing a subjectAltName extension containing one or more host names. This is used when a certificate may be used by multiple virtual servers or if a host has different names when contacted within or outside a private network. Multiple DNS names can be included in the extension by separating then with a comma. -ip IP-ADDRESS,... Create a certificate request containing a subjectAltName extension containing the IP addresses named by the IP-ADDRESS strings. This is used when a certificate may be used by services listening on multiple networks. Multiple IP addresses can be included in the extension by separating then with a comma.","Process Name":"grid-cert-request","Link":"https:\/\/linux.die.net\/man\/1\/grid-cert-request"}},{"Process":{"Description":null,"Process Name":"grid-change-pass-phrase","Link":"https:\/\/linux.die.net\/man\/1\/grid-change-pass-phrase"}},{"Process":{"Description":"The grid-proxy-destroy program removes X.509 proxy files from the local filesystem. It overwrites the data in the files and removes the files from the filesystem. By default, it removes the current user's default proxy (either \/tmp\/x509up_uUID where UID is the current POSIX user id, or the file pointed to by the X509_USER_PROXY environment variable) unless a list of proxy file paths are included as part of the command line. Use the -- command-line option to separate a list of proxy paths from command line options if the proxy file begins with the - character. The full list of command-line options to grid-proxy-destroy are: -help, -usage Display the command-line options to grid-proxy-destroy. -version Display the version number of the grid-proxy-destroy command -debug Display verbose error messages. -dryrun Do not remove the proxy, but display the path of the files that would have been removed, or the directory where they would have been removed from if the -all command-line option is used. -default Remove the default proxy in addition to the files included on the command-line. Only needed if other paths are included on the command-line. -all Remove the default proxy and all delegated proxies in the temporary file directory.","Process Name":"grid-proxy-destroy","Link":"https:\/\/linux.die.net\/man\/1\/grid-proxy-destroy"}},{"Process":{"Description":"The grid-proxy-info program extracts information from an X.509 proxy certificates, and optionally displays or returns an exit code based on that information. The default mode of operation is to print the following facts about the current user's default proxy: subject, issuer, identity, type, strength, path, and time left. If the command-line option -exists or -e is included in the command-line, nothing is printed unless one of the print options is specified. Instead, grid-proxy-info determines if a valid proxy exists and, if so, exits with the exit code 0; if a proxy does not exist or is not valid, grid-proxy-info exits with the exit code 1. Additional validity criteria can be added by using the -valid, -v, -hours, -h, -bits, or -b command-line options. If used, these options must occur after the -e or -exists command-line options. Those options are only valid if one of the -e or -exists command-line options is used. The complete set of command-line options to grid-proxy-info are: -help, -usage Display the command-line options to grid-proxy-info. -version Display the version number of the grid-proxy-info command -debug Display verbose error messages. -file PROXYFILE, -f PROXYFILE Read the proxy located in the file PROXYFILE instead of using the default proxy. -subject, -s Display the proxy certificate's subject distinguished name. -issuer, -i Display the proxy certificate issuer's distinguished name. -identity Display the proxy certificate's identity. For non-independent proxies, the identity is the subject of the certificate which issued the first proxy in the proxy chain. -type Display the type of proxy certificate. The type string includes the format (\"legacy\", \"draft\", or RFC 3280 compliant), identity type (\"impersonation\" or \"independent\"), and policy (\"limited\" or \"full\"). See grid-proxy-init(1) for information about how to create different types of proxies. -timeleft Display the number of seconds remaining until the proxy certificate expires. -strength Display the strength (in bits) of the key associated with the proxy certificate. -all Display the default information for the proxy when also using the -e or -exists command-line option. -text Display the proxy certificate contents to standard output, including policy information, issuer, public key, and modulus. -path Display the path to the file containing the default proxy certificate. -rfc2253 Display distinguished names for the subject, issuer, and identity using the string representation described in RFC 2253, instead of the legacy format. -exists, -e Perform an existence and validity check for the proxy. If a valid proxy exists and matches the criteria described by other command-line options (if any), exit with 0; otherwise, exit with 1. This option must be before other validity check predicate in the command-line options. If this option is specified, the output of the default facts about the proxy is disabled. Use the -all option to have the information displayed as well as the exit code set. -valid HOURS : MINUTES, -v HOURS : MINUTES, -hours HOURS, -h HOURS Check that the proxy certificate is valid for at least HOURS hours and MINUTES minutes. If it is not, grid-proxy-info will exit with exit code 1. -bits BITS, -b BITS Check that the proxy certificate key strength is at least BITS bits.","Process Name":"grid-proxy-info","Link":"https:\/\/linux.die.net\/man\/1\/grid-proxy-info"}},{"Process":{"Description":"The grid-proxy-init program generates X.509 proxy certificates derived from the currently available certificate files. By default, this command generates a RFC 3820 [1] Proxy Certificate with a 512 bit key valid for 12 hours in a file named \/tmp\/x509up_uUID. Command-line options and variables can modify the format, strength, lifetime, and location of the generated proxy certificate. X.509 proxy certificates are short-lived certificates, signed usually by a user's identity certificate or another proxy certificate. The key associated with a proxy certificate is unencrypted, so applications can authenticate using a proxy identity without providing a passphrase. Proxy certificates provide a convenient alternative to constantly entering passwords, but are also less secure than the user's normal security credential. Therefore, they should always be user-readable only (this is enforced by the GSI libraries), and should be deleted after they are no longer needed. This version of grid-proxy-init supports three different proxy formats: the old proxy format used in early releases of the Globus Toolkit up to version 2.4.x, an IETF draft version of X.509 Proxy Certificate profile used in Globus Toolkit 3.0.x and 3.2.x, and the RFC 3820 profile used in Globus Toolkit Version 4.0.x and 4.2.x. By default, this version of grid-proxy-init creates an RFC 3820 compliant proxy. To create a proxy compatible with older versions of the Globus Toolkit, use the -old or -draft command-line options. The full set of command-line options to grid-proxy-init are: -help, -usage Display the command-line options to grid-proxy-init. -version Display the version number of the grid-proxy-init command -debug Display information about the path to the certificate and key used to generate the proxy certificate, the path to the trusted certificate directory, and verbose error messages -q Suppress all output from grid-proxy-init except for passphrase prompts. -verify Perform certificate chain validity checks on the generated proxy. -valid HOURS:MINUTES, -hours HOURS Create a certificate that is valid for HOURS hours and MINUTES minutes. If not specified, the default of twelve hours and no minutes is used. -cert CERTFILE, -key KEYFILE Create a proxy certificate signed by the certificate located in CERTFILE using the key located in KEYFILE. If not specified the default certificate and key will be used. This overrides the values of environment variables described below. -certdir CERTDIR Search CERTDIR for trusted certificates if verifying the proxy certificate. If not specified, the default trusted certificate search path is used. This overrides the value of the X509_CERT_DIR environment variable -out PROXYPATH Write the generated proxy certificate file to PROXYPATH instead of the default path of \/tmp\/x509up_uUID. -bits BITS When creating the proxy certificate, use a BITS bit key instead of the default 512 bit keys. -policy POLICYFILE Add the certificate policy data described in POLICYFILE as the ProxyCertInfo X.509 extension to the generated proxy certificate. -pl POLICY-OID, -policy-language POLICY-OID Set the policy language identifier of the policy data specified by the -policy command-line option to the oid specified by the POLICY-OID string. -path-length MAXIMUM Set the maximum length of the chain of proxies that can be created by the generated proxy to MAXIMUM. If not set, the default of an unlimited proxy chain length is used. -pwstdin Read the private key's passphrase from stdin instead of reading input from the controlling tty. This is useful when scripting grid-proxy-init. -limited Create a limited proxy. Limited proxies are generally refused by process-creating services, but may be used to authorize with other services. -independent Create an independent proxy. An independent proxy is not treated as an impersonation proxy but as a separate identity for authorization purposes. -draft Create a IETF draft proxy instead of the default RFC 3280-compliant proxy. This type of proxy uses a non-standard proxy policy identifier. This might be useful for authenticating with older versions of the Globus Toolkit. -old Create a legacy proxy instead of the default RFC 3280-compliant proxy. This type of proxy uses a non-standard method of indicating that the certificate is a proxy and whether it is limited. This might be useful for authenticating with older versions of the Globus Toolkit. -rfc Create an RFC 3820-compliant proxy certificate. This is the default for this version of grid-proxy-init.","Process Name":"grid-proxy-init","Link":"https:\/\/linux.die.net\/man\/1\/grid-proxy-init"}},{"Process":{"Description":"Griffith is a film collection manager. Adding items to the movie collection is as quick and easy as typing the film title and selecting a supported source. Griffith will then try to fetch all the related information from the Web.","Process Name":"griffith","Link":"https:\/\/linux.die.net\/man\/1\/griffith"}},{"Process":{"Description":null,"Process Name":"grind","Link":"https:\/\/linux.die.net\/man\/1\/grind"}},{"Process":{"Description":"grmic is a utility included with \"libgcj\" which generates stubs for remote objects. Note that this program isn't yet fully compatible with the JDK grmic. Some options, such as -classpath, are recognized but currently ignored. We have left these options undocumented for now. Long options can also be given with a GNU-style leading --. For instance, --help is accepted.","Process Name":"grmic","Link":"https:\/\/linux.die.net\/man\/1\/grmic"}},{"Process":{"Description":null,"Process Name":"grmid","Link":"https:\/\/linux.die.net\/man\/1\/grmid"}},{"Process":{"Description":"grmiregistry starts a remote object registry on the current host. If no port number is specified, then port 1099 is used.","Process Name":"grmiregistry","Link":"https:\/\/linux.die.net\/man\/1\/grmiregistry"}},{"Process":{"Description":"grn is a preprocessor for including gremlin pictures in groff input. grn writes to standard output, processing only input lines between two that start with .GS and .GE. Those lines must contain grn commands (see below). These commands request a gremlin file, and the picture in that file is converted and placed in the troff input stream. The .GS request may be followed by a C, L, or R to center, left, or right justify the whole gremlin picture (default justification is center). If no file is mentioned, the standard input is read. At the end of the picture, the position on the page is the bottom of the gremlin picture. If the grn entry is ended with .GF instead of .GE, the position is left at the top of the picture. Please note that currently only the -me macro package has support for .GS, .GE, and .GF. The following command-line options are understood: -T dev Prepare output for printer dev. The default device is ps. See groff(1) for acceptable devices. -Mdir Prepend dir to the default search path for gremlin files. The default path is (in that order) the current directory, the home directory, \/usr\/lib64\/groff\/site-tmac, \/usr\/share\/groff\/site-tmac, and \/usr\/share\/groff\/1.18.1.4\/tmac. -Fdir Search dir for subdirectories devname (name is the name of the device) for the DESC file before the default font directories \/usr\/share\/groff\/site-font, \/usr\/share\/groff\/1.18.1.4\/font, and \/usr\/lib\/font. -C Recognize .GS and .GE (and .GF) even when followed by a character other than space or newline. -v Print the version number.","Process Name":"grn","Link":"https:\/\/linux.die.net\/man\/1\/grn"}},{"Process":{"Description":"grodvi is a driver for groff that produces dvi format. Normally it should be run by groff -Tdvi. This will run troff -Tdvi; it will also input the macros \/usr\/share\/groff\/1.18.1.4\/tmac\/dvi.tmac; if the input is being preprocessed with eqn it will also input \/usr\/share\/groff\/1.18.1.4\/font\/devdvi\/eqnchar. The dvi file generated by grodvi can be printed by any correctly-written dvi driver. The troff drawing primitives are implemented using the tpic version~2 specials. If the driver does not support these, the [rs]D commands will not produce any output. There is an additional drawing command available: [rs]D'R dh dv ' Draw a rule (solid black rectangle), with one corner at the current position, and the diagonally opposite corner at the current position +( dh, dv). Afterwards the current position will be at the opposite corner. This produces a rule in the dvi file and so can be printed even with a driver that does not support the tpic specials unlike the other [rs]D commands. The groff command [rs]X'anything' is translated into the same command in the dvi file as would be produced by [rs]special{anything} in ; anything may not contain a newline. For inclusion of EPS image files, grodvi loads pspic.tmac automatically, providing the PSPIC macro. Please check grops (1) for a detailed description of this macro. Font files for grodvi can be created from tfm files using tfmtodit(1). The font description file should contain the following additional commands: :((0w'internalname'u+2n)*2u>(0u-0u)) .TP internalname name The name of the tfm file (without the .tfm extension) is name. checksum n The checksum in the tfm file is n. designsize n The designsize in the tfm file is n. These are automatically generated by tfmtodit. The default color for [rs]m and [rs]M is black. Currently, the drawing color for [rs]D commands is always black, and fill color values are translated to gray. In troff the [rs]N escape sequence can be used to access characters by their position in the corresponding tfm file; all characters in the tfm file can be accessed this way.","Process Name":"grodvi","Link":"https:\/\/linux.die.net\/man\/1\/grodvi"}},{"Process":{"Description":"This document describes the groff program, the main front-end for the groff document formatting system. The groff program and macro suite is the implementation of a roff(7) system within the free software collection GNU. The groff system has all features of the classical roff, but adds many extensions. The groff program allows to control the whole groff system by command line options. This is a great simplification in comparison to the classical case (which uses pipes only).","Process Name":"groff","Link":"https:\/\/linux.die.net\/man\/1\/groff"}},{"Process":{"Description":"The groffer program is the easiest way to use groff(1). It can display arbitrary documents written in the groff language, see groff(7), or other roff languages, see roff(7), that are compatible to the original troff language. The groffer program also includes many of the features for finding and displaying the Unix manual pages (man pages), such that it can be used as a replacement for a man(1) program. Moreover, compressed files that can be handled by gzip(1) or bzip2(1) are decompressed on-the-fly. The normal usage is quite simple by supplying a file name or name of a man page without further options. But the option handling has many possibilities for creating special behaviors. This can be done either in configuration files, with the shell environment variable $GROFFER_OPT, or on the command line. The output can be generated and viewed in several different ways available for groff. This includes the groff native X Window viewer gxditview(1), each Postcript, pdf, or dvi display program, a web browser by generating html in www mode, or several text modes in text terminals. Most of the options that must be named when running groff directly are determined automatically for groffer, due to the internal usage of the grog(1) program. But all parts can also be controlled manually by arguments. Several file names can be specified on the command line arguments. They are transformed into a single document in the normal way of groff. Option handling is done in GNU style. Options and file names can be mixed freely. The option '--' closes the option handling, all following arguments are treated as file names. Long options can be abbreviated in several ways.","Process Name":"groffer","Link":"https:\/\/linux.die.net\/man\/1\/groffer"}},{"Process":{"Description":null,"Process Name":"grog","Link":"https:\/\/linux.die.net\/man\/1\/grog"}},{"Process":{"Description":"The grohtml front end (which consists of a preprocessor, pre-grohtml, and a device driver, post-grohtml) translates the output of GNU troff to html. Users should always invoke grohtml via the groff command with a -Thtml option. If no files are given, grohtml will read the standard input. A filename of - will also cause grohtml to read the standard input. Html output is written to the standard output. When grohtml is run by groff options can be passed to grohtml using groff's -P option.","Process Name":"grohtml","Link":"https:\/\/linux.die.net\/man\/1\/grohtml"}},{"Process":{"Description":"grolbp is a driver for groff that produces output in CAPSL and VDM format suitable for Canon LBP-4 and LBP-8 printers. For compatibility with grolj4 there is an additional drawing command available: \\D'R dh dv ' Draw a rule (i.e. a solid black rectangle), with one corner at the current position, and the diagonally opposite corner at the current position +( dh, dv).","Process Name":"grolbp","Link":"https:\/\/linux.die.net\/man\/1\/grolbp"}},{"Process":{"Description":"grolj4 is a driver for groff that produces output in PCL5 format suitable for an HP Laserjet 4 printer. There is an additional drawing command available: \\D'R dh dv ' Draw a rule (solid black rectangle), with one corner at the current position, and the diagonally opposite corner at the current position +( dh, dv). Afterwards the current position will be at the opposite corner. This generates a PCL fill rectangle command, and so will work on printers that do not support HPGL\/2 unlike the other \\D commands.","Process Name":"grolj4","Link":"https:\/\/linux.die.net\/man\/1\/grolj4"}},{"Process":{"Description":null,"Process Name":"grops","Link":"https:\/\/linux.die.net\/man\/1\/grops"}},{"Process":{"Description":"grotty translates the output of GNU troff into a form suitable for typewriter-like devices. Normally grotty should be invoked by using the groff command with a -Tascii, -Tascii8, -Tlatin1, -Tnippon or -Tutf8 option on ASCII based systems, and with -Tcp1047 and -Tutf8 on EBCDIC based hosts. If no files are given, grotty will read the standard input. A filename of - will also cause grotty to read the standard input. Output is written to the standard output. By default, grotty emits SGR escape sequences (from ISO 6429, also called ANSI color escapes) to change text attributes (bold, italic, colors). This makes it possible to have eight different M[green]backgroundM[] and m[red]foregroundm[] colors; additionally, bold and italic attributes can be used BI]at the same time] (by using the BI font). The following colors are defined in tty.tmac: black, white, red, green, blue, yellow, magenta, cyan. Unknown colors are mapped to the default color (which is dependent on the settings of the terminal; in most cases, this is black for the foreground and white for the background). Use the -c switch to revert to the old behaviour, printing a bold character c with the sequence 'c BACKSPACE c' and an italic character c by the sequence '_ BACKSPACE c'. At the same time, color output is disabled. The same effect can be achieved by setting either the GROFF_NO_SGR environment variable or using the 'sgr' X command (see below). For SGR support, it is necessary to use the -R option of less(1) to disable the interpretation of grotty's old output format. Consequently, all programs which use less as the pager program have to pass this option to it. For man(1) in particular, either add -R to the $PAGER environment variable, e.g. PAGER=\"\/usr\/bin\/less -R\" export PAGER or use the -P option of man to set the pager executable and its options, or modify the configuration file of man in a similar fashion. grotty's old output format can be displayed on a terminal by piping through ul(1). Pagers such as more(1) or less(1) are also able to display these sequences. Use either -B or -U when piping into less(1); use -b when piping into more(1). There is no need to filter the output through col(1) since grotty never outputs reverse line feeds. The font description file may contain a command internalname n where n is a decimal integer. If the 01 bit in n is set, then the font will be treated as an italic font; if the 02 bit is set, then it will be treated as a bold font. The code field in the font description field gives the code which will be used to output the character. This code can also be used in the [rs]N escape sequence in troff.","Process Name":"grotty","Link":"https:\/\/linux.die.net\/man\/1\/grotty"}},{"Process":{"Description":"Print group memberships for each USERNAME or, if no USERNAME is specified, for the current process (which may differ if the groups database has changed). --help display this help and exit --version output version information and exit","Process Name":"groups","Link":"https:\/\/linux.die.net\/man\/1\/groups"}},{"Process":{"Description":null,"Process Name":"growisofs","Link":"https:\/\/linux.die.net\/man\/1\/growisofs"}},{"Process":{"Description":"GRSYNC is a simple graphical interface using GTK2 for the rsync command line program. It currently supports only a limited set of the most important rsync features, but can be used effectively for local directory synchronization.","Process Name":"grsync","Link":"https:\/\/linux.die.net\/man\/1\/grsync"}},{"Process":{"Description":"GRSYNC-BATCH is a script which can be used to automatize rsync runs using grsync sessions (see man grsync). for example it can be put into crontab for scheduled execution on terminal, and to get the results via email.","Process Name":"grsync-batch","Link":"https:\/\/linux.die.net\/man\/1\/grsync-batch"}},{"Process":{"Description":null,"Process Name":"gs","Link":"https:\/\/linux.die.net\/man\/1\/gs"}},{"Process":{"Description":"Supported Printers The ghostscript device driver pcl3 (formerly called hpdj) is a ghostscript backend for printers understanding Hewlett-Packard's Printer Command Language, level 3+ (\"PCL 3+\", also called \"PCL 3 Plus\"). The driver is intended to support in particular the following printer models: HP DeskJet\nHP DeskJet Plus\nHP DeskJet Portable\nHP DeskJet 310\nHP DeskJet 320\nHP DeskJet 340\nHP DeskJet 400\nHP DeskJet 500\nHP DeskJet 500C\nHP DeskJet 510\nHP DeskJet 520\nHP DeskJet 540\nHP DeskJet 550C\nHP DeskJet 560C\nHP DeskJet 600\nHP DeskJet 660C\nHP DeskJet 670C\nHP DeskJet 680C\nHP DeskJet 690C\nHP DeskJet 850C\nHP DeskJet 855C\nHP DeskJet 870C\nHP DeskJet 890C\nHP DeskJet 1120C The PCL dialect called \"PCL Level 3 enhanced\" is apparently a not entirely compatible modification of PCL 3+. This driver should basically work with such printers but you must be more careful which options you select and you might not be able to exploit all your printer's capabilities. The driver does not support printers understanding only Hewlett-Packard's PPA (Printing Performance Architecture) commands. If a printer's documentation does not say anything about its printer command language and you find a statement like \"... is designed for Microsoft Windows\" or \"DOS support through Windows only\", the printer is almost certainly a PPA printer and hence is intended exclusively for systems running Microsoft Windows. (These printers are also erroneously known as \"GDI printers\" because they are intended to be accessed through a manufacturer-supplied driver via Windows' GDI interface.) There exist ways of using a PPA printer with ghostscript, but not through pcl3. Different printer models usually implement model-specific subsets of all PCL-3+ commands or arguments to commands. You must therefore tell the driver by means of the Subdevice option for which model the generated PCL code is intended. The model-dependent difference in the generated code is not great. Apart from media specifications, resolutions and colour capabilities, one can consider three groups of models which are treated with significant differences: Group 1 DeskJet, DeskJet Plus, DeskJet 500 Group 2 DeskJet Portable, DeskJets 3xx, 400, 5xx except 500 and 540, Group 3 DeskJets 540, 6xx, 8xx and 1120C. The first two groups I call the \"old Deskjets\", the third group consists of \"new DeskJets\". If you have a PCL-3 printer not appearing in the list above, the likelihood is still good that it will accept the files generated by pcl3. You can specify one of the supported subdevices in these cases (it is sufficient to try one each from the groups just mentioned), or use the special subdevice names unspecold or unspec which are treated like members of the second and the third group above, respectively, with all subdevice-dependent checks having been turned off. The list of printer models for which this driver is currently known to work is: HP 2000C\nHP 2500CM\nHP DeskJet 697C\nHP DeskJet 850C\nHP DeskJet 970C\nHP DeskJet 1100C\nXerox DocuPrint M750 Details can be found in the file reports.txt in the pcl3 distribution; its latest version is available via pcl3's home page (link to URL http:\/\/home.t-online.de\/home\/Martin.Lottermoser\/pcl3.html) . If you wish to report on the hardware compatibility for a particular printer model, please read the file how-to-report.txt. Omitting models already mentioned, previous (hpdj) versions of this driver were reported to work with the following printers: HP DeskJet 340\nHP DeskJet 400 (tested for Gray only)\nHP DeskJet 420\nHP DeskJet 500\nHP DeskJet 500C (tested for Gray only)\nHP DeskJet 520\nHP DeskJet 540\nHP DeskJet 560C\nHP DeskJet 600\nHP DeskJet 610C HP DeskJet 612C HP DeskJet 640C HP DeskJet 660C\/660Cse HP DeskJet 670C HP DeskJet 672C HP DeskJet 680C HP DeskJet 690C HP DeskJet 690C+ HP DeskJet 693C HP DeskJet 694C HP DeskJet 832C HP DeskJet 855C HP DeskJet 870Cse\/870Cxi HP DeskJet 880C HP DeskJet 890C HP DeskJet 895Cse\/895Cxi HP DeskJet 932C HP DeskJet 1120C HP OfficeJet 350 HP OfficeJet 590 HP OfficeJet 600 HP OfficeJet 625 HP OfficeJet G55 HP OfficeJet T45 Lexmark 3000 Color Jetprinter Olivetti JP792 (see the option SendBlackLast) Most of the people who sent me reports did not state to which extent hpdj worked for their printer model. Colour Models Ignoring photo cartridges which are not supported by pcl3, DeskJet printers can be classified in four categories: \u2022 The printer has only a black ink cartridge. \u2022 The printer can print with either a black or a cyan\/magenta\/yellow (CMY) cartridge. \u2022 The printer holds a CMY and a black cartridge simultaneously, but the two groups of inks are chemically incompatible and should not be overlayed. (Don't worry: the printer is not going to explode if they do. You merely get poorer results because the black ink will spread further than it should. This is called \"ink bleeding\".) \u2022 The printer holds a CMY and a black cartridge simultaneously and the inks can be mixed. (Newer HP DeskJets use such bleed-proof inks.) This leads to four (process) colour models for the driver: Gray Print in black only. CMY Print with cyan, magenta and yellow. In this mode, \"composite black\" consisting of all three inks is used to stand in for true black. CMY+K Print with all four inks, but never mix black with one of the others. CMYK Print with all four inks. As a printer with both, a black and a CMY cartridge, can usually also print, e.g., with black only, the printer's \"cartridge state\" merely identifies one of these models as the maximal one. Depending on the category of the printer, the driver will therefore accept one or more models. The possibilities are: The subdevices unspecold and unspec also permit all colour models. A printer capable only of CMY might accept CMY+K or CMYK data, remapping them to CMY, and a printer capable of CMY+K might remap CMY data to CMY+K. The colour model CMY+K is not useful if you have a CMYK printer. In contrast, if you have a CMY+K or CMYK printer and the two cartridges support different resolutions, the colour models Gray or CMY become interesting as well. In most of these cases the black cartridge can print at a higher resolution than the CMY cartridge, although the converse does also occur. In addition, ghostscript is generally fastest for Gray. PCL 3+ also supports the colour model RGB although Hewlett-Packard discourages its use. For this model the printer internally converts the RGB data it receives into CMY data for printing. Note that not everything which can be demanded when using a CMY palette in PCL 3+ is also permitted when using RGB. Because of its limited usefulness, pcl3 accepts the colour model RGB only for the subdevices unspecold and unspec. Media Sizes and Orientations A PostScript document describes its visible content with respect to a coordinate system called default user space. Almost all PostScript devices are page devices which paint only a restricted rectangular area in default user space. Part of the state of a page device is therefore the current page size, two numbers specifying the width and height of the sheet to be printed on. These values must be interpreted from default user space, hence the page size not only describes the \"sheet size\" (extension irrespective of orientation) but also the orientation between page contents and sheet (portrait if width ≤ height, landscape otherwise). The page size is requested by the user or the document, and it is one of the jobs of the device to satisfy this request. Ghostscript looks at several sources to determine the page size: \u2022 the default size configured for gs (usually US Letter or ISO A4 in portrait orientation), \u2022 the value given to the option PAPERSIZE in the invocation, \u2022 the size requested by the document, unless you specify -dFIXEDMEDIA. The last applicable item in this list overrides the others, hence the current page size can change at runtime. The pcl3 driver splits the page size into sheet size and page orientation and passes the sheet size to the printer. This works only if the printer accepts this size (accepted sizes are listed in your printer's manual). For the explicitly supported printers, the driver knows which sizes are accepted and will refuse to print if an unsupported one is requested. (If you suspect that pcl3 is in error concerning what is supported, check the list of supported sizes in the PPD file for the subdevice you are using.) Group-3 printers also accept a custom page size command which permits printing on arbitrarily-sized media but only within certain limits which are also known to the driver. Unlike the sheet size the page orientation is irrelevant for deciding whether a particular page size is supported or not. The driver will adapt itself as required by the PostScript language and rotate the output if necessary. (I know of only one other ghostscript driver capable of this.) In setting up the PostScript default user space, pcl3 does not treat envelope sizes differently from other sizes. The subdevice unspecold accepts all sizes supported by the HP DeskJet 560C, unspec supports all discrete sizes known to the HP DeskJets 850C\/855C\/870C\/890C and treats in addition every other size request as a custom page size without imposing any limits. If using any of these two subdevices you should change the list of supported sizes to fit your printer's capabilities; see the CONFIGURATION section below for details. In order for a document to be printed correctly a sheet of appropriate size must be provided and the driver must know what its orientation with respect to the printing mechanism is. The latter is usually specified by reference to the feeding direction as \"short edge first\" or \"long edge first\". Don't confuse this kind of orientation with the portrait\/landscape orientation: the former (\"sheet orientation\") refers to the orientation of the sheet with respect to the feeding direction, the latter (\"page orientation\") describes the orientation of the sheet with respect to the page contents (default user space). These orientations are logically independent: people inserting paper into the printer need to know about the first, people composing documents only care about the latter. Because pcl3 has no information about the actual dimension or orientation of the medium in the input tray, you must ensure yourself that this is appropriate. By default, the driver assumes the dimension to be that requested via the page size, but you can override this assumption with an InputAttributes definition (see the Media Sources and Destinations subsection in the CONFIGURATION section below). There is no command in PCL 3+ to tell the printer about the sheet's orientation in the input tray, therefore it cannot be chosen and the manufacturer must prescribe it. I am not aware of any precise and complete statement from Hewlett-Packard about what is required in this respect, hence you should check your printer's manual whether the assumptions made by pcl3 are correct or not: the driver assumes that media are always fed short edge first except when using the subdevices hpdj, hpdjplus, hpdj400, hpdj500 or hpdj500c for printing on envelope sizes (US no. 10 and ISO DL). In these cases you should insert the medium long edge first. If you find that pcl3's default behaviour is incorrect, you can override it with the option LeadingEdge or a media configuration file (see the CONFIGURATION section below). Print Quality and Media Properties With the introduction of the DeskJet 540, HP added two new PCL commands to the language: \"Print Quality\" and \"Media Type\". For older DeskJets (groups 1 and 2), similar effects can be achieved by specifying some technical aspects of the printing process in detail. You can use the PrintQuality and Medium options to adapt the driver to the desired output quality and those properties of the medium it must know about, independent of which kind of subdevice you select. If it corresponds to a printer understanding the new commands, the option values are passed through to the printer, otherwise these specifications are mapped to the older Depletion, Shingling, and Raster Graphics Quality commands based on recommendations from HP. If you are not satisfied with the result in the latter case, use the options Depletion, Shingling and RasterGraphicsQuality to explicitly set these values. Diagnostic Messages Error messages issued by this driver start with \" ? component :\" and warnings with \" ?-W component :\". The component can be eprn, pcl3, or pclgen, corresponding to the driver's three internal layers: the eprn device extends ghostscript without knowing PCL, pclgen is a module generating PCL without being aware of ghostscript, and pcl3 is the driver proper connecting the other two layers. All these messages are written on the standard error stream.","Process Name":"gs-pcl3","Link":"https:\/\/linux.die.net\/man\/1\/gs-pcl3"}},{"Process":{"Description":null,"Process Name":"gsbj","Link":"https:\/\/linux.die.net\/man\/1\/gsbj"}},{"Process":{"Description":"gscan2pdf has the following command-line options: --device=<device> Specifies the device to use, instead of getting the list of devices from via the SANE API . This can be useful if the scanner is on a remote computer which is not broadcasting its existence. --help Displays this help page and exits. --log=<log file> Specifies a file to store logging messages. --(debug|info|warn|error|fatal) Defines the log level. If a log file is specified, this defaults to 'debug', otherwise 'warn'. --version Displays the program version and exits. Scanning is handled with SANE via scanimage. PDF conversion is done by PDF::API2 . TIFF export is handled by libtiff (faster and smaller memory footprint for multipage files).","Process Name":"gscan2pdf","Link":"https:\/\/linux.die.net\/man\/1\/gscan2pdf"}},{"Process":{"Description":null,"Process Name":"gschem","Link":"https:\/\/linux.die.net\/man\/1\/gschem"}},{"Process":{"Description":"This utility provides functionality approximately equivalent to the Unix enscript(1) program. It prints plain text files using a single font. It currently handles tabs and formfeeds, but not backspaces. It will line-wrap when using fixed-pitch fonts. It will also do kerning and width adjustment. The default device (-sDEVICE=) and resolution (-r) are as follows: gslp      epson      180\ngsbj      bj10e      180\ngsdj      deskjet    300\ngsdj500   djet500    300\ngslj      laserjet   300 By default the current date is formatted as the center header.","Process Name":"gsdj","Link":"https:\/\/linux.die.net\/man\/1\/gsdj"}},{"Process":{"Description":"This utility provides functionality approximately equivalent to the Unix enscript(1) program. It prints plain text files using a single font. It currently handles tabs and formfeeds, but not backspaces. It will line-wrap when using fixed-pitch fonts. It will also do kerning and width adjustment. The default device (-sDEVICE=) and resolution (-r) are as follows: gslp      epson      180\ngsbj      bj10e      180\ngsdj      deskjet    300\ngsdj500   djet500    300\ngslj      laserjet   300 By default the current date is formatted as the center header.","Process Name":"gsdj500","Link":"https:\/\/linux.die.net\/man\/1\/gsdj500"}},{"Process":{"Description":null,"Process Name":"gserialver","Link":"https:\/\/linux.die.net\/man\/1\/gserialver"}},{"Process":{"Description":"This manual page briefly documents the gsf command. gsf is a simple archive utility, somewhat similar to tar(1). It operates on files following one of the structured file formats understood by the G Structured File library, for example, Microsoft Excel(TM) files.","Process Name":"gsf","Link":"https:\/\/linux.die.net\/man\/1\/gsf"}},{"Process":{"Description":null,"Process Name":"gsf-office-thumbnailer","Link":"https:\/\/linux.die.net\/man\/1\/gsf-office-thumbnailer"}},{"Process":{"Description":"This manual page briefly documents the gsf-vba-dump command. Various Microsoft binary data formats, including Excel (.xls), Word (.doc) and PowerPoint (.ppt) can embed macro code streams. These macro streams are in P-code (intermediate language) compiled from Visual Basic for Applications (VBA). The gsf-vba-dump utility extracts these macro streams.","Process Name":"gsf-vba-dump","Link":"https:\/\/linux.die.net\/man\/1\/gsf-vba-dump"}},{"Process":{"Description":null,"Process Name":"gsftopk","Link":"https:\/\/linux.die.net\/man\/1\/gsftopk"}},{"Process":{"Description":"The idea behind this tool originally came from wanting to do something on each machine in our network. Existing scripts would serially go to each machine run the command, wait for it to finish, and continue to the next machine. There was no reason why this couldn't be done in parallel. The problems, however, were many. First of all, the output from finishing parallel jobs needs to be buffered in such a way that different machines wouldn't output their results on top of eachother. A final bit was added because it was nice to have output alphabetical rather than first-done, first-seen. The result is a parallel job spawner that displays output from the machines alphabetically, as soon as it is available. If \"alpha\" take longer than \"zebra\", there will be no output past \"alpha\" until it is finished. As soon as \"alpha\" is finished, though, everyone's output is printed. Sending a SIGUSR1 to gsh(1) will cause it to report which machines are still pending. (Effectively turns on --debug for one cycle.)","Process Name":"gsh","Link":"https:\/\/linux.die.net\/man\/1\/gsh"}},{"Process":{"Description":"gshhg reads the binary coastline (GSHHS) or political boundary or river (WDBII) files and extracts an ASCII listing. It automatically handles byte-swabbing between different architectures. Optionally, only segment header info can be displayed. The output header information has the format ID npoints hierarchical-level source area f_area west east south north container ancestor, where hierarchical levels for coastline polygons go from 1 (shoreline) to 4 (lake inside island inside lake inside land). Source is either W (World Vector Shoreline) or C (CIA World Data Bank II); lower case is used if a lake is a river-lake (a portion of a river that is so wide it is better represented by a closed polygon). The west east south north is the enclosing rectangle, area is the polygon area in km^2 while f_area is the actual area of the ancestor polygon (at full resolution), container is the ID of the polygon that contains this polygon (-1 if none), and ancestor is the ID of the polygon in the full resolution set that was reduced to yield this polygon (-1 if full resolution since there is no ancestor). For river and border data the header is simply ID npoints hierarchical-level source west east south north. For more information about the file formats, see TECHNICAL INFORMATION below. binaryfile.b GSHHS or WDBII binary data file as distributed with the GSHHS data supplement. Any of the 5 standard resolutions (full, high, intermediate, low, crude) can be used. -A Only output polygons whose area equals or exceeds the area value in km^2 [Default outputs all polygons]. -G Write output that can be imported into GNU Octave or Matlab by ending each segment with a NaN-record. -I Only output information for the polygon that matches id. Use -Ic to get all the continents only [Default outputs all polygons]. -L Only output a listing of polygon or line segment headers [Default outputs headers and data records]. -M Start all header records with the GMT multiple segment indicator '>' [Default uses P for polygons and L for lines]. -N Only output features whose level matches the given level [Default will output all levels]. -Q Control what to do with river-lakes (river sections large enough to be stored as closed polygons). Use -Qe to exclude them and -Qi to exclude everything else instead [Default outputs all polygons].","Process Name":"gshhg","Link":"https:\/\/linux.die.net\/man\/1\/gshhg"}},{"Process":{"Description":null,"Process Name":"gshhg_dp","Link":"https:\/\/linux.die.net\/man\/1\/gshhg_dp"}},{"Process":{"Description":null,"Process Name":"gshhgtograss","Link":"https:\/\/linux.die.net\/man\/1\/gshhgtograss"}},{"Process":{"Description":"gshhs reads the binary coastline (GSHHS) or political boundary or river (WDBII) files and extracts an ASCII listing. It automatically handles byte-swabbing between different architectures. Optionally, only segment header info can be displayed. The header info has the format ID npoints hierarchical-level source area f_area west east south north container ancestor, where hierarchical levels for coastline polygons go from 1 (shoreline) to 4 (lake inside island inside lake inside land). Source is either W (World Vector Shoreline) or C (CIA World Data Bank II); lower case is used if a lake is a river-lake. The west east south north is the enclosing rectangle, area is the polygon area in km^2 while f_area is the actual area of the ancestor polygon, container is the ID of the polygon that contains this polygon (-1 if none), and ancestor is the ID of the polygon in the full resolution set that was reduced to yield this polygon (-1 if full resolution). For line data the header is simply ID npoints hierarchical-level source west east south north binaryfile.b GSHHS or WDBII binary data file as distributed with the GSHHS data supplement. Any of the 5 standard resolutions (full, high, intermediate, low, crude) can be used. -I Only output information for the polygon that matches id [default outputs all polygons]. -L Only output a listing of polygon or line segment headers [default outputs headers and data records]. -M Start all header records with the GMT multiple segment indicator '>' [Default uses P for polygons and L for lines].","Process Name":"gshhs","Link":"https:\/\/linux.die.net\/man\/1\/gshhs"}},{"Process":{"Description":null,"Process Name":"gshhs_dp","Link":"https:\/\/linux.die.net\/man\/1\/gshhs_dp"}},{"Process":{"Description":null,"Process Name":"gshhstograss","Link":"https:\/\/linux.die.net\/man\/1\/gshhstograss"}},{"Process":{"Description":"This manual page briefly documents the gsi and gsc commands. The Gambit-C programming system is a full implementation of the Scheme language which conforms to the R4RS, R5RS, and IEEE Scheme standards. It consists of two main programs: gsi, the Gambit Scheme interpreter, and gsc, the Gambit Scheme compiler. The interpreter provides an interactive environment with a debugger, while the compiler can produce stand-alone executables or modules which can be linked or loaded at run time. Interpreted code and compiled code can be freely mixed. C modules can be linked with Scheme modules. The interpreter is executed in interactive mode when no file or '-' or '-e' option is given on the command line. When at least one file or '-' or '-e' option is present the interpreter is executed in batch mode. Entering ,q or (exit) at the interpreter's command prompt will exit the interpreter. Entering ,? at the interpreter's command prompt will display a list of special commands. In batch mode the command line arguments denote files to be loaded, REPL interactions to start ('-' option), and expressions to be evaluated ('-e' option). Note that the '-' and '-e' options can be interspersed with the files on the command line and can occur multiple times. The interpreter processes the command line arguments from left to right, loading files with the 'load' procedure and evaluating expressions with the 'eval' procedure in the global interaction environment. After this processing the interpreter exits. gsc-cc-o.bat is a script called by gsc to generate object files during compilation. It takes its data from environment variables and is not intended for user interaction.","Process Name":"gsi","Link":"https:\/\/linux.die.net\/man\/1\/gsi"}},{"Process":{"Description":null,"Process Name":"gsiscp","Link":"https:\/\/linux.die.net\/man\/1\/gsiscp"}},{"Process":{"Description":"sftp is an interactive file transfer program, similar to ftp(1), which performs all operations over an encrypted ssh(1) transport. It may also use many features of ssh, such as public key authentication and compression. sftp connects and logs into the specified host, then enters an interactive command mode. The second usage format will retrieve files automatically if a non-interactive authentication method is used; otherwise it will do so after successful interactive authentication. The third usage format allows sftp to start in a remote directory. The final usage format allows for automated sessions using the -b option. In such cases, it is necessary to configure non-interactive authentication to obviate the need to enter a password at connection time (see sshd(8) and ssh-keygen(1) for details). The options are as follows:       -1'        Specify the use of protocol version 1. -B buffer_size Specify the size of the buffer that sftp uses when transferring files. Larger buffers require fewer round trips at the cost of higher memory consumption. The default is 32768 bytes. -b batchfile Batch mode reads a series of commands from an input batchfile instead of stdin. Since it lacks user interaction it should be used in conjunction with non-interactive authentication. A batchfile of '-' may be used to indicate standard input. sftp will abort if any of the following commands fail: get, put, rename, ln, rm, mkdir, chdir, ls, lchdir, chmod, chown, chgrp, lpwd, df, and lmkdir. Termination on error can be suppressed on a command by command basis by prefixing the command with a '-' character (for example, -rm \/tmp\/blah*). -C' Enables compression (via ssh's -C flag). -F ssh_config Specifies an alternative per-user configuration file for ssh(1). This option is directly passed to ssh(1). -o ssh_option Can be used to pass options to ssh in the format used in ssh_config(5). This is useful for specifying options for which there is no separate sftp command-line flag. For example, to specify an alternate port use: sftp -oPort=24. For full details of the options listed below, and their possible values, see ssh_config(5). AddressFamily BatchMode BindAddress ChallengeResponseAuthentication CheckHostIP Cipher Ciphers Compression CompressionLevel ConnectionAttempts ConnectTimeout ControlMaster ControlPath GlobalKnownHostsFile GSSAPIAuthentication GSSAPIDelegateCredentials HashKnownHosts Host' HostbasedAuthentication HostKeyAlgorithms HostKeyAlias HostName IdentityFile IdentitiesOnly KbdInteractiveDevices LogLevel MACs' NoHostAuthenticationForLocalhost NumberOfPasswordPrompts PasswordAuthentication Port' PreferredAuthentications Protocol ProxyCommand PubkeyAuthentication RekeyLimit RhostsRSAAuthentication RSAAuthentication SendEnv ServerAliveInterval ServerAliveCountMax SmartcardDevice StrictHostKeyChecking TCPKeepAlive UsePrivilegedPort User' UserKnownHostsFile VerifyHostKeyDNS -P sftp_server_path Connect directly to a local sftp server (rather than via ssh(1)). This option may be useful in debugging the client and server. -R num_requests Specify how many requests may be outstanding at any one time. Increasing this may slightly improve file transfer speed but will increase memory usage. The default is 64 outstanding requests. -S program Name of the program to use for the encrypted connection. The program must understand ssh(1) options. -s subsystem | sftp_server Specifies the SSH2 subsystem or the path for an sftp server on the remote host. A path is useful for using sftp over protocol version 1, or when the remote sshd(8) does not have an sftp subsystem configured. -v' Raise logging level. This option is also passed to ssh.","Process Name":"gsisftp","Link":"https:\/\/linux.die.net\/man\/1\/gsisftp"}},{"Process":{"Description":null,"Process Name":"gsislogin","Link":"https:\/\/linux.die.net\/man\/1\/gsislogin"}},{"Process":{"Description":"ssh (SSH client) is a program for logging into a remote machine and for executing commands on a remote machine. It is intended to replace rlogin and rsh, and provide secure encrypted communications between two untrusted hosts over an insecure network. X11 connections and arbitrary TCP ports can also be forwarded over the secure channel. ssh connects and logs into the specified hostname (with optional user name). The user must prove his\/her identity to the remote machine using one of several methods depending on the protocol version used (see below). If command is specified, it is executed on the remote host instead of a login shell. The options are as follows:       -1'        Forces ssh to try protocol version 1 only. -2' Forces ssh to try protocol version 2 only. -4' Forces ssh to use IPv4 addresses only. -6' Forces ssh to use IPv6 addresses only. -A' Enables forwarding of the authentication agent connection. This can also be specified on a per-host basis in a configuration file. Agent forwarding should be enabled with caution. Users with the ability to bypass file permissions on the remote host (for the agent's Unix-domain socket) can access the local agent through the forwarded connection. An attacker cannot obtain key material from the agent, however they can perform operations on the keys that enable them to authenticate using the identities loaded into the agent. -a' Disables forwarding of the authentication agent connection. -b bind_address Use bind_address on the local machine as the source address of the connection. Only useful on systems with more than one address. -C' Requests compression of all data (including stdin, stdout, stderr, and data for forwarded X11 and TCP connections). The compression algorithm is the same used by gzip(1), and the ''level'' can be controlled by the CompressionLevel option for protocol version 1. Compression is desirable on modem lines and other slow connections, but will only slow down things on fast networks. The default value can be set on a host-by-host basis in the configuration files; see the Compression option. -c cipher_spec Selects the cipher specification for encrypting the session. Protocol version 1 allows specification of a single cipher. The supported values are ''3des'', ''blowfish'', and ''des''. 3des (triple-des) is an encrypt-decrypt-encrypt triple with three different keys. It is believed to be secure. blowfish is a fast block cipher; it appears very secure and is much faster than 3des. des is only supported in the ssh client for interoperability with legacy protocol 1 implementations that do not support the 3des cipher. Its use is strongly discouraged due to cryptographic weaknesses. The default is ''3des''. For protocol version 2, cipher_spec is a comma-separated list of ciphers listed in order of preference. See the Ciphers keyword for more information. -D [ bind_address:]port Specifies a local ''dynamic'' application-level port forwarding. This works by allocating a socket to listen to port on the local side, optionally bound to the specified bind_address. Whenever a connection is made to this port, the connection is forwarded over the secure channel, and the application protocol is then used to determine where to connect to from the remote machine. Currently the SOCKS4 and SOCKS5 protocols are supported, and ssh will act as a SOCKS server. Only root can forward privileged ports. Dynamic port forwardings can also be specified in the configuration file. IPv6 addresses can be specified with an alternative syntax: [bind_address\/]port or by enclosing the address in square brackets. Only the superuser can forward privileged ports. By default, the local port is bound in accordance with the GatewayPorts setting. However, an explicit bind_address may be used to bind the connection to a specific address. The bind_address of ''localhost'' indicates that the listening port be bound for local use only, while an empty address or '*' indicates that the port should be available from all interfaces. -e escape_char Sets the escape character for sessions with a pty (default: '~'). The escape character is only recognized at the beginning of a line. The escape character followed by a dot ('.') closes the connection; followed by control-Z suspends the connection; and followed by itself sends the escape character once. Setting the character to ''none'' disables any escapes and makes the session fully transparent. -F configfile Specifies an alternative per-user configuration file. If a configuration file is given on the command line, the system-wide configuration file (\/etc\/gsissh\/ssh_config) will be ignored. The default for the per-user configuration file is ~\/.ssh\/config. -f' Requests ssh to go to background just before command execution. This is useful if ssh is going to ask for passwords or passphrases, but the user wants it in the background. This implies -n. The recommended way to start X11 programs at a remote site is with something like ssh -f host xterm. If the ExitOnForwardFailure configuration option is set to ''yes'', then a client started with -f will wait for all remote port forwards to be successfully established before placing itself in the background. -g' Allows remote hosts to connect to local forwarded ports. -I smartcard_device Specify the device ssh should use to communicate with a smartcard used for storing the user's private RSA key. This option is only available if support for smartcard devices is compiled in (default is no support). -i identity_file Selects a file from which the identity (private key) for RSA or DSA authentication is read. The default is ~\/.ssh\/identity for protocol version 1, and ~\/.ssh\/id_rsa and ~\/.ssh\/id_dsa for protocol version 2. Identity files may also be specified on a per-host basis in the configuration file. It is possible to have multiple -i options (and multiple identities specified in configuration files). -K' Enables GSSAPI-based authentication and forwarding (delegation) of GSSAPI credentials to the server. -k' Disables forwarding (delegation) of GSSAPI credentials to the server. -L [ bind_address:]port:host:hostport Specifies that the given port on the local (client) host is to be forwarded to the given host and port on the remote side. This works by allocating a socket to listen to port on the local side, optionally bound to the specified bind_address. Whenever a connection is made to this port, the connection is forwarded over the secure channel, and a connection is made to host port hostport from the remote machine. Port forwardings can also be specified in the configuration file. IPv6 addresses can be specified with an alternative syntax: [bind_address\/]port\/host\/hostport or by enclosing the address in square brackets. Only the superuser can forward privileged ports. By default, the local port is bound in accordance with the GatewayPorts setting. However, an explicit bind_address may be used to bind the connection to a specific address. The bind_address of ''localhost'' indicates that the listening port be bound for local use only, while an empty address or '*' indicates that the port should be available from all interfaces. -l login_name Specifies the user to log in as on the remote machine. This also may be specified on a per-host basis in the configuration file. -M' Places the ssh client into ''master'' mode for connection sharing. Multiple -M options places ssh into ''master'' mode with confirmation required before slave connections are accepted. Refer to the description of ControlMaster in ssh_config(5) for details. -m mac_spec Additionally, for protocol version 2 a comma-separated list of MAC (message authentication code) algorithms can be specified in order of preference. See the MACs keyword for more information. -N' Do not execute a remote command. This is useful for just forwarding ports (protocol version 2 only). -n' Redirects stdin from \/dev\/null (actually, prevents reading from stdin). This must be used when ssh is run in the background. A common trick is to use this to run X11 programs on a remote machine. For example, ssh -n shadows.cs.hut.fi emacs & will start an emacs on shadows.cs.hut.fi, and the X11 connection will be automatically forwarded over an encrypted channel. The ssh program will be put in the background. (This does not work if ssh needs to ask for a password or passphrase; see also the -f option.) -O ctl_cmd Control an active connection multiplexing master process. When the -O option is specified, the ctl_cmd argument is interpreted and passed to the master process. Valid commands are: ''check'' (check that the master process is running) and ''exit'' (request the master to exit). -o option Can be used to give options in the format used in the configuration file. This is useful for specifying options for which there is no separate command-line flag. For full details of the options listed below, and their possible values, see ssh_config(5). AddressFamily BatchMode BindAddress ChallengeResponseAuthentication CheckHostIP Cipher Ciphers ClearAllForwardings Compression CompressionLevel ConnectionAttempts ConnectTimeout ControlMaster ControlPath DynamicForward EscapeChar ExitOnForwardFailure ForwardAgent ForwardX11 ForwardX11Trusted GatewayPorts GlobalKnownHostsFile GSSAPIAuthentication GSSAPIDelegateCredentials HashKnownHosts Host' HostbasedAuthentication HostKeyAlgorithms HostKeyAlias HostName IdentityFile IdentitiesOnly KbdInteractiveDevices LocalCommand LocalForward LogLevel MACs' NoHostAuthenticationForLocalhost NumberOfPasswordPrompts PasswordAuthentication PermitLocalCommand Port' PreferredAuthentications Protocol ProxyCommand PubkeyAuthentication RekeyLimit RemoteForward RhostsRSAAuthentication RSAAuthentication SendEnv ServerAliveInterval ServerAliveCountMax SmartcardDevice StrictHostKeyChecking TCPKeepAlive Tunnel TunnelDevice UsePrivilegedPort User' UserKnownHostsFile VerifyHostKeyDNS VisualHostKey XAuthLocation -p port Port to connect to on the remote host. This can be specified on a per-host basis in the configuration file. -q' Quiet mode. Causes most warning and diagnostic messages to be suppressed. -R [ bind_address:]port:host:hostport Specifies that the given port on the remote (server) host is to be forwarded to the given host and port on the local side. This works by allocating a socket to listen to port on the remote side, and whenever a connection is made to this port, the connection is forwarded over the secure channel, and a connection is made to host port hostport from the local machine. Port forwardings can also be specified in the configuration file. Privileged ports can be forwarded only when logging in as root on the remote machine. IPv6 addresses can be specified by enclosing the address in square braces or using an alternative syntax: [bind_address\/]host\/port\/hostport. By default, the listening socket on the server will be bound to the loopback interface only. This may be overridden by specifying a bind_address. An empty bind_address, or the address '*', indicates that the remote socket should listen on all interfaces. Specifying a remote bind_address will only succeed if the server's GatewayPorts option is enabled (see sshd_config(5)). If the port argument is '0', the listen port will be dynamically allocated on the server and reported to the client at run time. -S ctl_path Specifies the location of a control socket for connection sharing. Refer to the description of ControlPath and ControlMaster in ssh_config(5) for details. -s' May be used to request invocation of a subsystem on the remote system. Subsystems are a feature of the SSH2 protocol which facilitate the use of SSH as a secure transport for other applications (eg. sftp(1)). The subsystem is specified as the remote command. -T' Disable pseudo-tty allocation. -t' Force pseudo-tty allocation. This can be used to execute arbitrary screen-based programs on a remote machine, which can be very useful, e.g. when implementing menu services. Multiple -t options force tty allocation, even if ssh has no local tty. -V' Display the version number and exit. -v' Verbose mode. Causes ssh to print debugging messages about its progress. This is helpful in debugging connection, authentication, and configuration problems. Multiple -v options increase the verbosity. The maximum is 3. -W host:port Requests that standard input and output on the client be forwarded to host on port over the secure channel. Implies -N, -T, ExitOnForwardFailure and ClearAllForwardings and works with Protocol version 2 only. -w local_tun[:remote_tun] Requests tunnel device forwarding with the specified tun(4) devices between the client (local_tun) and the server (remote_tun). The devices may be specified by numerical ID or the keyword ''any'', which uses the next available tunnel device. If remote_tun is not specified, it defaults to ''any''. See also the Tunnel and TunnelDevice directives in ssh_config(5). If the Tunnel directive is unset, it is set to the default tunnel mode, which is ''point-to-point''. -X' Enables X11 forwarding. This can also be specified on a per-host basis in a configuration file. X11 forwarding should be enabled with caution. Users with the ability to bypass file permissions on the remote host (for the user's X authorization database) can access the local X11 display through the forwarded connection. An attacker may then be able to perform activities such as keystroke monitoring. For this reason, X11 forwarding is subjected to X11 SECURITY extension restrictions by default. Please refer to the ssh -Y option and the ForwardX11Trusted directive in ssh_config(5) for more information. -x' Disables X11 forwarding. -Y' Enables trusted X11 forwarding. Trusted X11 forwardings are not subjected to the X11 SECURITY extension controls. -y' Send log information using the syslog(3) system module. By default this information is sent to stderr. ssh may additionally obtain configuration data from a per-user configuration file and a system-wide configuration file. The file format and configuration options are described in ssh_config(5). ssh exits with the exit status of the remote command or with 255 if an error occurred.","Process Name":"gsissh","Link":"https:\/\/linux.die.net\/man\/1\/gsissh"}},{"Process":{"Description":"ssh-keygen generates, manages and converts authentication keys for ssh(1). ssh-keygen can create RSA keys for use by SSH protocol version 1 and RSA or DSA keys for use by SSH protocol version 2. The type of key to be generated is specified with the -t option. If invoked without any arguments, ssh-keygen will generate an RSA key for use in SSH protocol 2 connections. ssh-keygen is also used to generate groups for use in Diffie-Hellman group exchange (DH-GEX). See the MODULI GENERATION section for details. Normally each user wishing to use SSH with RSA or DSA authentication runs this once to create the authentication key in ~\/.ssh\/identity, ~\/.ssh\/id_dsa or ~\/.ssh\/id_rsa. Additionally, the system administrator may use this to generate host keys, as seen in \/etc\/rc. Normally this program generates the key and asks for a file in which to store the private key. The public key is stored in a file with the same name but ''.pub'' appended. The program also asks for a passphrase. The passphrase may be empty to indicate no passphrase (host keys must have an empty passphrase), or it may be a string of arbitrary length. A passphrase is similar to a password, except it can be a phrase with a series of words, punctuation, numbers, whitespace, or any string of characters you want. Good passphrases are 10-30 characters long, are not simple sentences or otherwise easily guessable (English prose has only 1-2 bits of entropy per character, and provides very bad passphrases), and contain a mix of upper and lowercase letters, numbers, and non-alphanumeric characters. The passphrase can be changed later by using the -p option. There is no way to recover a lost passphrase. If the passphrase is lost or forgotten, a new key must be generated and copied to the corresponding public key to other machines. For RSA1 keys, there is also a comment field in the key file that is only for convenience to the user to help identify the key. The comment can tell what the key is for, or whatever is useful. The comment is initialized to ''user@host'' when the key is created, but can be changed using the -c option. After a key is generated, instructions below detail where the keys should be placed to be activated. The options are as follows:        -a trials Specifies the number of primality tests to perform when screening DH-GEX candidates using the -T command. -B' Show the bubblebabble digest of specified private or public key file. -b bits Specifies the number of bits in the key to create. For RSA keys, the minimum size is 768 bits and the default is 2048 bits. Generally, 2048 bits is considered sufficient. DSA keys must be exactly 1024 bits as specified by FIPS 186-2. -C comment Provides a new comment. -c' Requests changing the comment in the private and public key files. This operation is only supported for RSA1 keys. The program will prompt for the file containing the private keys, for the passphrase if the key has one, and for the new comment. -D reader Download the RSA public key stored in the smartcard in reader. -e' This option will read a private or public OpenSSH key file and print the key in RFC 4716 SSH Public Key File Format to stdout. This option allows exporting keys for use by several commercial SSH implementations. -F hostname Search for the specified hostname in a known_hosts file, listing any occurrences found. This option is useful to find hashed host names or addresses and may also be used in conjunction with the -H option to print found keys in a hashed format. -f filename Specifies the filename of the key file. -G output_file Generate candidate primes for DH-GEX. These primes must be screened for safety (using the -T option) before use. -g' Use generic DNS format when printing fingerprint resource records using the -r command. -H' Hash a known_hosts file. This replaces all hostnames and addresses with hashed representations within the specified file; the original content is moved to a file with a .old suffix. These hashes may be used normally by ssh and sshd, but they do not reveal identifying information should the file's contents be disclosed. This option will not modify existing hashed hostnames and is therefore safe to use on files that mix hashed and non-hashed names. -i' This option will read an unencrypted private (or public) key file in SSH2-compatible format and print an OpenSSH compatible private (or public) key to stdout. ssh-keygen also reads the RFC 4716 SSH Public Key File Format. This option allows importing keys from several commercial SSH implementations. -l' Show fingerprint of specified public key file. Private RSA1 keys are also supported. For RSA and DSA keys ssh-keygen tries to find the matching public key file and prints its fingerprint. If combined with -v, an ASCII art representation of the key is supplied with the fingerprint. -M memory Specify the amount of memory to use (in megabytes) when generating candidate moduli for DH-GEX. -n' Extract the public key from smartcard. -N new_passphrase Provides the new passphrase. -P passphrase Provides the (old) passphrase. -p' Requests changing the passphrase of a private key file instead of creating a new private key. The program will prompt for the file containing the private key, for the old passphrase, and twice for the new passphrase. -q' Silence ssh-keygen. Used by \/etc\/rc when creating a new key. -R hostname Removes all keys belonging to hostname from a known_hosts file. This option is useful to delete hashed hosts (see the -H option above). -r hostname Print the SSHFP fingerprint resource record named hostname for the specified public key file. -S start Specify start point (in hex) when generating candidate moduli for DH-GEX. -T output_file Test DH group exchange candidate primes (generated using the -G option) for safety. -t type Specifies the type of key to create. The possible values are ''rsa1'' for protocol version 1 and ''rsa'' or ''dsa'' for protocol version 2. -U reader Upload an existing RSA private key into the smartcard in reader. -v' Verbose mode. Causes ssh-keygen to print debugging messages about its progress. This is helpful for debugging moduli generation. Multiple -v options increase the verbosity. The maximum is 3. -W generator Specify desired generator when testing candidate moduli for DH-GEX. -y' This option will read a private OpenSSH format file and print an OpenSSH public key to stdout.","Process Name":"gsissh-keygen","Link":"https:\/\/linux.die.net\/man\/1\/gsissh-keygen"}},{"Process":{"Description":null,"Process Name":"gsl-config","Link":"https:\/\/linux.die.net\/man\/1\/gsl-config"}},{"Process":{"Description":"gsl-histogram is a demonstration program for the GNU Scientific Library. It takes three arguments, specifying the upper and lower bounds of the histogram and the number of bins. It then reads numbers from 'stdin', one line at a time, and adds them to the histogram. When there is no more data to read it prints out the accumulated histogram using gsl_histogram_fprintf. If n is unspecified then bins of integer width are used.","Process Name":"gsl-histogram","Link":"https:\/\/linux.die.net\/man\/1\/gsl-histogram"}},{"Process":{"Description":null,"Process Name":"gsl-randist","Link":"https:\/\/linux.die.net\/man\/1\/gsl-randist"}},{"Process":{"Description":"This utility provides functionality approximately equivalent to the Unix enscript(1) program. It prints plain text files using a single font. It currently handles tabs and formfeeds, but not backspaces. It will line-wrap when using fixed-pitch fonts. It will also do kerning and width adjustment. The default device (-sDEVICE=) and resolution (-r) are as follows: gslp      epson      180\ngsbj      bj10e      180\ngsdj      deskjet    300\ngsdj500   djet500    300\ngslj      laserjet   300 By default the current date is formatted as the center header.","Process Name":"gslj","Link":"https:\/\/linux.die.net\/man\/1\/gslj"}},{"Process":{"Description":null,"Process Name":"gslp","Link":"https:\/\/linux.die.net\/man\/1\/gslp"}},{"Process":{"Description":"GSmartControl is a graphical user interface for smartctl (from smartmontools), which is a tool for querying and controlling SMART (Self-Monitoring, Analysis, and Reporting Technology) data on modern hard disk drives. It allows you to inspect the drive's SMART data to determine its health, as well as run various tests on it. This manual page documents briefly the gsmartcontrol command.","Process Name":"gsmartcontrol","Link":"https:\/\/linux.die.net\/man\/1\/gsmartcontrol"}},{"Process":{"Description":null,"Process Name":"gsmartcontrol-root","Link":"https:\/\/linux.die.net\/man\/1\/gsmartcontrol-root"}},{"Process":{"Description":"This script simply invokes gs(1) with the -NODISPLAY flag, followed by any other arguments from the command-line.","Process Name":"gsnd","Link":"https:\/\/linux.die.net\/man\/1\/gsnd"}},{"Process":{"Description":null,"Process Name":"gsoelim","Link":"https:\/\/linux.die.net\/man\/1\/gsoelim"}},{"Process":{"Description":"This utility can be used to print out a single directory or path in order for external applications to know what paths the GNUstep system is using. It was written to provide information for the GNUstep make package. The gspath usility is called with exactly one argument, which determines what it prints.","Process Name":"gspath","Link":"https:\/\/linux.die.net\/man\/1\/gspath"}},{"Process":{"Description":"gst-complete is a utility enabling bash to provide context sensitive tab completion for gst-launch command-lines. See the man page for gst-launch for the syntax of gst-launch command-lines. You must have a version of bash which supports programmable completion. Versions of bash which support this provide the complete builtin command.","Process Name":"gst-complete-0.8","Link":"https:\/\/linux.die.net\/man\/1\/gst-complete-0.8"}},{"Process":{"Description":null,"Process Name":"gst-compprep-0.8","Link":"https:\/\/linux.die.net\/man\/1\/gst-compprep-0.8"}},{"Process":{"Description":"gst-feedback is a utility that scans the system for various information that is useful to GStreamer developers to help diagnose user problem. Depending on the type of bug report, it may be useful to attach the output of gst-feedback when you contact the developers. NOTE: The current version of this script scans many dirs: \/usr\/lib \/usr\/local\/lib \/home \/usr\/include \/usr\/local\/include \/home (again) for various GStreamer installations. Depending on your system you may not want to do this. Improvements to the script are welcome.","Process Name":"gst-feedback-0.10","Link":"https:\/\/linux.die.net\/man\/1\/gst-feedback-0.10"}},{"Process":{"Description":"gst-feedback is a utility that scans the system for various information that is useful to GStreamer developers to help diagnose user problem. Depending on the type of bug report, it may be useful to attach the output of gst-feeback when you contact the developers. NOTE: The current version of this script scans many dirs: \/usr\/lib \/usr\/local\/lib \/home \/usr\/include \/usr\/local\/include \/home (again) for various GStreamer installations. Depending on your system you may not want to do this. Improvements to the script are welcome.","Process Name":"gst-feedback-0.8","Link":"https:\/\/linux.die.net\/man\/1\/gst-feedback-0.8"}},{"Process":{"Description":null,"Process Name":"gst-inspect-0.10","Link":"https:\/\/linux.die.net\/man\/1\/gst-inspect-0.10"}},{"Process":{"Description":"gst-inspect is a tool that prints out information on available GStreamer plugins, information about a particular plugin, or information about a particular element. When executed with no PLUGIN or ELEMENT argument, gst-inspect will print a list of all plugins and elements. When executed with a PLUGIN or ELEMENT argument, gst-inspect will print information about that plug-in or element.","Process Name":"gst-inspect-0.8","Link":"https:\/\/linux.die.net\/man\/1\/gst-inspect-0.8"}},{"Process":{"Description":"gst-launch is a tool that builds and runs basic GStreamer pipelines. In simple form, a PIPELINE-DESCRIPTION is a list of elements separated by exclamation marks (!). Properties may be appended to elements, in the form property=value. For a complete description of possible PIPELINE-DESCRIPTIONS see the section pipeline description below or consult the GStreamer documentation. Please note that gst-launch is primarily a debugging tool for developers and users. You should not build applications on top of it. For applications, use the gst_parse_launch() function of the GStreamer API as an easy way to construct pipelines from pipeline descriptions.","Process Name":"gst-launch-0.10","Link":"https:\/\/linux.die.net\/man\/1\/gst-launch-0.10"}},{"Process":{"Description":"gst-launch is a tool that builds and runs basic GStreamer pipelines. In simple form, a PIPELINE-DESCRIPTION is a list of elements separated by exclamation marks (!). Properties may be appended to elements, in the form property=value. For a complete description of possible PIPELINE-DESCRIPTIONS see above under pipeline description or the GStreamer documentation.","Process Name":"gst-launch-0.8","Link":"https:\/\/linux.die.net\/man\/1\/gst-launch-0.8"}},{"Process":{"Description":null,"Process Name":"gst-launch-ext-0.8","Link":"https:\/\/linux.die.net\/man\/1\/gst-launch-ext-0.8"}},{"Process":{"Description":"gst-md5sum generates MD5 checksums of the data generated by a GStreamer pipeline. In theory, running gst-md5sum-0.8 filesrc location=music.mp3 should print out the same checksum as md5sum music.mp3 If the pipeline contains an md5sink element, gst-md5sum-0.8 will query it for the md5sum at the end of pipeline iteration. If it doesn't contain an md5sink element, gst-md5sum-0.8 will automatically connect an md5sink to the right hand side of the given pipeline. See the man page for gst-launch or the GStreamer docuementation for more information on how to create a PARTIAL-PIPELINE-DESCRIPTION.","Process Name":"gst-md5sum-0.8","Link":"https:\/\/linux.die.net\/man\/1\/gst-md5sum-0.8"}},{"Process":{"Description":"gst-register is a tool that is used to register all the GStreamer plugins on your system. It creates a listing of their properties such that on startup of GStreamer based application do not need to load plugins until they need them.","Process Name":"gst-register-0.8","Link":"https:\/\/linux.die.net\/man\/1\/gst-register-0.8"}},{"Process":{"Description":"gst-typefind uses the GStreamer type finding system to determine the relevant GStreamer plugin to parse or decode file, and the corresponding MIME type.","Process Name":"gst-typefind-0.10","Link":"https:\/\/linux.die.net\/man\/1\/gst-typefind-0.10"}},{"Process":{"Description":null,"Process Name":"gst-typefind-0.8","Link":"https:\/\/linux.die.net\/man\/1\/gst-typefind-0.8"}},{"Process":{"Description":"gst-visualise is a tool that is used to run a basic GStreamer pipeline, to display a graphical visualisation of an audio stream. By default, the audio stream is read from ESD (the Enlightened Sound Daemon), but this can be changed by setting the AUDIOSRC parameter in ~\/.gst. For example, you might set \"AUDIOSRC=osssrc\" to display a visualisation of the sound input to your soundcard. You can select a visualiser by providing a parameter naming the visualiser. For example: gst-visualise synaesthesia will use the synaesthesia plugin. If no visualiser is named, the VISUALIZER property in ~\/.gst will be used. If this is not specified either, the goom visualiser will be used. The videosink to use to display the visualisation will be read from the VIDEOSINK parameter in ~\/.gst, defaulting to sdlvideosink.","Process Name":"gst-visualise-0.10","Link":"https:\/\/linux.die.net\/man\/1\/gst-visualise-0.10"}},{"Process":{"Description":null,"Process Name":"gst-visualise-0.8","Link":"https:\/\/linux.die.net\/man\/1\/gst-visualise-0.8"}},{"Process":{"Description":"gst-xmlinspect is a tool that prints out information on available GStreamer plugins, information about a particular plugin, or information about a particular element. When executed with no PLUGIN or ELEMENT argument, gst-xmlinspect will print a list of all plugins and elements. When executed with a PLUGIN or ELEMENT argument, gst-xmlinspect will print information about that plug-in or element as an XML document.","Process Name":"gst-xmlinspect-0.10","Link":"https:\/\/linux.die.net\/man\/1\/gst-xmlinspect-0.10"}},{"Process":{"Description":null,"Process Name":"gst-xmlinspect-0.8","Link":"https:\/\/linux.die.net\/man\/1\/gst-xmlinspect-0.8"}},{"Process":{"Description":"gst-xmllaunch is a tool that is used to build and run a basic GStreamer pipeline, loading it from an XML description. You can produce the XML description using gst-launch-0.10(1) with the -o option or by calling gst_xml_write_file() in your own app. A simple commandline looks like: gst-xmllaunch my-pipeline.xml filesrc0.location=music.mp3 This sets the location property of the element named filesrc0 to the value \"music.mp3\". See gst-launch(1) for syntax on setting element names, and gst-inspect to see what properties various elements have. You can pass \"-\" as the XML-FILE to read from stdin. XML-FILE can be a URI as well, thanks to the wizardry of libxml. I'm not really sure what all is supported, it seems http works just fine though.","Process Name":"gst-xmllaunch-0.10","Link":"https:\/\/linux.die.net\/man\/1\/gst-xmllaunch-0.10"}},{"Process":{"Description":null,"Process Name":"gst-xmllaunch-0.8","Link":"https:\/\/linux.die.net\/man\/1\/gst-xmllaunch-0.8"}},{"Process":{"Description":"gstack attaches to the active process named by the pid on the command line, and prints out an execution stack trace. If ELF symbols exist in the binary (usually the case unless you have run strip(1)), then symbolic addresses are printed as well. If the process is part of a thread group, then gstack will print out a stack trace for each of the threads in the group.","Process Name":"gstack","Link":"https:\/\/linux.die.net\/man\/1\/gstack"}},{"Process":{"Description":null,"Process Name":"gstat","Link":"https:\/\/linux.die.net\/man\/1\/gstat"}},{"Process":{"Description":"gsymcheck is a symbol checker for gEDA\/gaf. Here is a list of checks that gsymcheck performs: [bu] Checks for graphical= attribute (if present does varied tests) [bu] Checks for the device= attribute [bu] Checks for various other missing attributes (pinlabel, pintype, pinseq, footprint, refdes) [bu] Checks to make sure that the active pin end is on the 100 unit grid. [bu] Checks the uses of the various attributes (duplicates, incorrect syntax, invalid values, etc...) [bu] Checks for obsolete, old, and forbidden attributes [bu] Checks for any nets or buses within a symbol [bu] Checks for any other connections within a symbol [bu] Checks to make sure the number of pins is the correct number","Process Name":"gsymcheck","Link":"https:\/\/linux.die.net\/man\/1\/gsymcheck"}},{"Process":{"Description":"gtab-merge is used internally only. It is not recommended to call from shells.","Process Name":"gtab-merge","Link":"https:\/\/linux.die.net\/man\/1\/gtab-merge"}},{"Process":{"Description":null,"Process Name":"gtar","Link":"https:\/\/linux.die.net\/man\/1\/gtar"}},{"Process":{"Description":"This manual page describes the GNU version of tbl, which is part of the groff document formatting system. tbl compiles descriptions of tables embedded within troff input files into commands that are understood by troff. Normally, it should be invoked using the -t option of groff. It is highly compatible with Unix tbl. The output generated by GNU tbl cannot be processed with Unix troff; it must be processed with GNU troff. If no files are given on the command line, the standard input will be read. A filename of - will cause the standard input to be read.","Process Name":"gtbl","Link":"https:\/\/linux.die.net\/man\/1\/gtbl"}},{"Process":{"Description":null,"Process Name":"gtester","Link":"https:\/\/linux.die.net\/man\/1\/gtester"}},{"Process":{"Description":"gtester-report is a script which converts the XML output generated by gtester into HTML. Options -h, --help print help and exit -v, --version print version information and exit","Process Name":"gtester-report","Link":"https:\/\/linux.die.net\/man\/1\/gtester-report"}},{"Process":{"Description":"Gtf is a utility for calculating VESA GTF modes. Given the desired horizontal and vertical resolutions and refresh rate (in Hz), the parameters for a matching VESA GTF mode are printed out. Two output formats are supported: mode lines suitable for the Xorg xorg.conf(5) file, and mode parameters suitable for the Linux fbset(8) utility.","Process Name":"gtf","Link":"https:\/\/linux.die.net\/man\/1\/gtf"}},{"Process":{"Description":"With gThumb you can browse your hard disk viewing thumbnails of image files. It also lets you view single files (including GIF animations), add comments to images, organize images in catalogs, print images, view slideshows, set your desktop background, and more.","Process Name":"gthumb","Link":"https:\/\/linux.die.net\/man\/1\/gthumb"}},{"Process":{"Description":null,"Process Name":"gtick","Link":"https:\/\/linux.die.net\/man\/1\/gtick"}},{"Process":{"Description":"As the name suggests, this little program lets you change your Gtk+ 2.0 theme. The aim is to make theme preview and selection as slick as possible. Themes installed on the system are presented for selection and previewed on the fly. A large variety of widgets provides a comprehensive demonstration.","Process Name":"gtk-chtheme","Link":"https:\/\/linux.die.net\/man\/1\/gtk-chtheme"}},{"Process":{"Description":null,"Process Name":"gtk-config","Link":"https:\/\/linux.die.net\/man\/1\/gtk-config"}},{"Process":{"Description":"gtk-gnutella is a GTK+ Gnutella client for Unix. It supports both searching and sharing with the 0.6 version of the Gnutella protocol and includes support for numerous protocol improvements and additions that have been added to the Gnutella network since. The Gnutella network allows users to search for files and to advertise files shared locally, but is not handling file transfers which is done through HTTP. The addition of Gnutella-specific HTTP headers makes each Gnutella client a powerful file swarmer where even firewalled hosts can contribute. Great care has been devoted into making gtk-gnutella powerful, efficient and reliable.","Process Name":"gtk-gnutella","Link":"https:\/\/linux.die.net\/man\/1\/gtk-gnutella"}},{"Process":{"Description":null,"Process Name":"gtk-query-immodules-2.0","Link":"https:\/\/linux.die.net\/man\/1\/gtk-query-immodules-2.0"}},{"Process":{"Description":"gtk-update-icon-cache creates mmap()able cache files for icon themes. It expects to be given the path to a icon theme directory containing an index.theme, e.g. \/usr\/share\/icons\/hicolor, and writes a icon-theme.cache containing cached information about the icons in the directory tree below the given directory. GTK+ can use the cache files created by gtk-update-icon-cache to avoid a lot of system call and disk seek overhead when the application starts. Since the format of the cache files allows them to be mmap()ed shared between multiple applications, the overall memory consumption is reduced as well.","Process Name":"gtk-update-icon-cache","Link":"https:\/\/linux.die.net\/man\/1\/gtk-update-icon-cache"}},{"Process":{"Description":"gtkada-config is a tool that is used to configure to determine the compiler and linker flags that should be used to compile and link programs that use GTKADA.","Process Name":"gtkada-config","Link":"https:\/\/linux.die.net\/man\/1\/gtkada-config"}},{"Process":{"Description":null,"Process Name":"gtksql","Link":"https:\/\/linux.die.net\/man\/1\/gtksql"}},{"Process":{"Description":"gtkterm is a simple GTK+ terminal used to communicate with the serial port.","Process Name":"gtkterm","Link":"https:\/\/linux.die.net\/man\/1\/gtkterm"}},{"Process":{"Description":null,"Process Name":"gtkwave","Link":"https:\/\/linux.die.net\/man\/1\/gtkwave"}},{"Process":{"Description":"To be written ...","Process Name":"gtnameserv","Link":"https:\/\/linux.die.net\/man\/1\/gtnameserv"}},{"Process":{"Description":null,"Process Name":"gtroff","Link":"https:\/\/linux.die.net\/man\/1\/gtroff"}},{"Process":{"Description":"This manual page documents briefly the gts-config command.","Process Name":"gts-config","Link":"https:\/\/linux.die.net\/man\/1\/gts-config"}},{"Process":{"Description":null,"Process Name":"gts2dxf","Link":"https:\/\/linux.die.net\/man\/1\/gts2dxf"}},{"Process":{"Description":"This manual page documents briefly the gts2oogl command.","Process Name":"gts2oogl","Link":"https:\/\/linux.die.net\/man\/1\/gts2oogl"}},{"Process":{"Description":null,"Process Name":"gts2stl","Link":"https:\/\/linux.die.net\/man\/1\/gts2stl"}},{"Process":{"Description":"This manual page documents briefly the gtscheck command.","Process Name":"gtscheck","Link":"https:\/\/linux.die.net\/man\/1\/gtscheck"}},{"Process":{"Description":null,"Process Name":"gtscompare","Link":"https:\/\/linux.die.net\/man\/1\/gtscompare"}},{"Process":{"Description":"This manual page documents briefly the delaunay command. This command constructs the constrained Delaunay triangulation of the input","Process Name":"gtsdelaunay","Link":"https:\/\/linux.die.net\/man\/1\/gtsdelaunay"}},{"Process":{"Description":"This manual page documents briefly the happrox command.","Process Name":"gtshapprox","Link":"https:\/\/linux.die.net\/man\/1\/gtshapprox"}},{"Process":{"Description":null,"Process Name":"gtstemplate","Link":"https:\/\/linux.die.net\/man\/1\/gtstemplate"}},{"Process":{"Description":"This manual page documents briefly the transform command.","Process Name":"gtstransform","Link":"https:\/\/linux.die.net\/man\/1\/gtstransform"}},{"Process":{"Description":"gtv is an MPEG audio and video player that uses the SDL MPEG Player Library. It can play back MPEG audio (layer 1, 2 and 3), MPEG video (MPEG-1) and MPEG system (audio and video combined) files. MPEG-2 video files (as found on DVDs) are not supported. The video player works best on a 16 bit color depth X11 display, it works on other color depths with reduced speed as well. You'll need a CPU with 300 MHz or more to play back an MPEG system stream with 25 frames per secons (fps) at full speed.","Process Name":"gtv","Link":"https:\/\/linux.die.net\/man\/1\/gtv"}},{"Process":{"Description":null,"Process Name":"guake","Link":"https:\/\/linux.die.net\/man\/1\/guake"}},{"Process":{"Description":"The script reads a configuration file that may contain so-called guards, file names, and comments, and writes those file names that satisfy all guards to standard output. The script takes a list of symbols as its arguments. Each line in the configuration file is processed separately. Lines may start with a number of guards. The following guards are defined: + xxx Include the file(s) on this line if the symbol xxx is defined. -xxx Exclude the file(s) on this line if the symbol xxx is defined. +!xxx Include the file(s) on this line if the symbol xxx is not defined. -!xxx Exclude the file(s) on this line if the symbol xxx is not defined. - Exclude this file. Used to avoid spurious --check messages. The guards are processed left to right. The last guard that matches determines if the file is included. If no guard is specified, the --default setting determines if the file is included. If no configuration file is specified, the script reads from standard input. The --check option is used to compare the specification file against the file system. If files are referenced in the specification that do not exist, or if files are not enlisted in the specification file warnings are printed. The --path option can be used to specify which directory or directories to scan. Multiple directories are eparated by a colon (\":\") character. The --prefix option specifies the location of the files.","Process Name":"guards","Link":"https:\/\/linux.die.net\/man\/1\/guards"}},{"Process":{"Description":null,"Process Name":"guestfish","Link":"https:\/\/linux.die.net\/man\/1\/guestfish"}},{"Process":{"Description":"This page contains recipes for and links to things you can do using libguestfs, guestfish(1) and the virt tools.","Process Name":"guestfs-recipes","Link":"https:\/\/linux.die.net\/man\/1\/guestfs-recipes"}},{"Process":{"Description":"This page has manual tests you can try on libguestfs. Everyone has a slightly different combination of platform, hardware and guests, so this testing is very valuable. Thanks for helping out! These tests require libguestfs ≥ 1.14. Tests marked with a * (asterisk) can destroy data if you're not careful. The others are safe and won't modify anything. You can report bugs you find through this link: https:\/\/bugzilla.redhat.com\/enter_bug.cgi?component=libguestfs&product=Virtualization+Tools or post on the mailing list (registration is not required, but if you're not registered then you'll have to wait for a moderator to manually approve your message): https:\/\/www.redhat.com\/mailman\/listinfo\/libguestfs","Process Name":"guestfs-testing","Link":"https:\/\/linux.die.net\/man\/1\/guestfs-testing"}},{"Process":{"Description":null,"Process Name":"guestmount","Link":"https:\/\/linux.die.net\/man\/1\/guestmount"}},{"Process":{"Description":"GNU Guile is an interpreter for the Scheme programming language. It implements R5RS, providing additional features necessary for real-world use. It is extremely simple to embed guile into a C program, calling C from Scheme and Scheme from C. Guile's design makes it very suitable for use as an \"extension\" or \"glue\" language, but it also works well as a stand-alone scheme development environment. The guile executable itself provides a stand-alone interpreter for scheme programs, for either interactive use or executing scripts. This manpage provides only brief instruction in invoking guile from the command line. Please consult the guile info documentation (type info guile at a command prompt) for more information. There is also a tutorial (info guile-tut) available.","Process Name":"guile","Link":"https:\/\/linux.die.net\/man\/1\/guile"}},{"Process":{"Description":"Adds the files specified to git using git-add making it available to guilt.","Process Name":"guilt-add","Link":"https:\/\/linux.die.net\/man\/1\/guilt-add"}},{"Process":{"Description":null,"Process Name":"guilt-applied","Link":"https:\/\/linux.die.net\/man\/1\/guilt-applied"}},{"Process":{"Description":null,"Process Name":"guilt-branch","Link":"https:\/\/linux.die.net\/man\/1\/guilt-branch"}},{"Process":{"Description":"","Process Name":"guilt-commit","Link":"https:\/\/linux.die.net\/man\/1\/guilt-commit"}},{"Process":{"Description":null,"Process Name":"guilt-delete","Link":"https:\/\/linux.die.net\/man\/1\/guilt-delete"}},{"Process":{"Description":"Outputs top-most applied diff and any additional changes in the working directory to standard output.","Process Name":"guilt-diff","Link":"https:\/\/linux.die.net\/man\/1\/guilt-diff"}},{"Process":{"Description":null,"Process Name":"guilt-export","Link":"https:\/\/linux.die.net\/man\/1\/guilt-export"}},{"Process":{"Description":"Print the list of files that the topmost patch changes.","Process Name":"guilt-files","Link":"https:\/\/linux.die.net\/man\/1\/guilt-files"}},{"Process":{"Description":null,"Process Name":"guilt-fold","Link":"https:\/\/linux.die.net\/man\/1\/guilt-fold"}},{"Process":{"Description":"Create a copy of the top most patch, and replace the entry in the series file to use this new patch file.","Process Name":"guilt-fork","Link":"https:\/\/linux.die.net\/man\/1\/guilt-fork"}},{"Process":{"Description":"Create a dot(1) directed graph showing the dependencies between applied patches.","Process Name":"guilt-graph","Link":"https:\/\/linux.die.net\/man\/1\/guilt-graph"}},{"Process":{"Description":null,"Process Name":"guilt-guard","Link":"https:\/\/linux.die.net\/man\/1\/guilt-guard"}},{"Process":{"Description":"Prints either the topmost patch's header or the header of a specified patch. -e Open the header in an editor, instead of printing it. -E Open the raw patch in an editor, instead of printing it. <patchname> Name of the patch.","Process Name":"guilt-header","Link":"https:\/\/linux.die.net\/man\/1\/guilt-header"}},{"Process":{"Description":null,"Process Name":"guilt-help","Link":"https:\/\/linux.die.net\/man\/1\/guilt-help"}},{"Process":{"Description":"Import a specified patch file into the series, placing it after the current topmost patch.","Process Name":"guilt-import","Link":"https:\/\/linux.die.net\/man\/1\/guilt-import"}},{"Process":{"Description":"Import one or more commits as individual patches.","Process Name":"guilt-import-commit","Link":"https:\/\/linux.die.net\/man\/1\/guilt-import-commit"}},{"Process":{"Description":"Initialize a git repository for use with guilt","Process Name":"guilt-init","Link":"https:\/\/linux.die.net\/man\/1\/guilt-init"}},{"Process":{"Description":"Create a new patch and push it on top of the stack. An optional patch description can be supplied either interactively on via the command line.","Process Name":"guilt-new","Link":"https:\/\/linux.die.net\/man\/1\/guilt-new"}},{"Process":{"Description":"Output the name of next patch to be pushed. If linkguilt:guilt-push[1] is issued, this patch would be the topmost patch.","Process Name":"guilt-next","Link":"https:\/\/linux.die.net\/man\/1\/guilt-next"}},{"Process":{"Description":null,"Process Name":"guilt-patchbomb","Link":"https:\/\/linux.die.net\/man\/1\/guilt-patchbomb"}},{"Process":{"Description":"","Process Name":"guilt-pop","Link":"https:\/\/linux.die.net\/man\/1\/guilt-pop"}},{"Process":{"Description":"Output name of second topmost applied patch. If linkguilt:guilt-pop[1] is used, this patch will become the topmost patch.","Process Name":"guilt-prev","Link":"https:\/\/linux.die.net\/man\/1\/guilt-prev"}},{"Process":{"Description":null,"Process Name":"guilt-push","Link":"https:\/\/linux.die.net\/man\/1\/guilt-push"}},{"Process":{"Description":"Rebase pushed patches against an upstream branch.","Process Name":"guilt-rebase","Link":"https:\/\/linux.die.net\/man\/1\/guilt-rebase"}},{"Process":{"Description":null,"Process Name":"guilt-refresh","Link":"https:\/\/linux.die.net\/man\/1\/guilt-refresh"}},{"Process":{"Description":"Perform various repository repairs. You must specify one mode of repair: --full Tries to repair everything possible. Additionally, it resets the repository state by reseting the HEAD to what's believed to be the proper commit, and reseting the applied stack. The end result should be a repository with no patches applied. WARNING: Running this command may result in commits and working\ndirectory changes being lost. You may want to create a new reference\n(e.g., branch, or reflog) to the original HEAD before using\nguilt-repair. --status Upgrade the status file from old format to new.","Process Name":"guilt-repair","Link":"https:\/\/linux.die.net\/man\/1\/guilt-repair"}},{"Process":{"Description":null,"Process Name":"guilt-rm","Link":"https:\/\/linux.die.net\/man\/1\/guilt-rm"}},{"Process":{"Description":"Select guards to apply when pushing patches. Guards are selected without the + or - prefix. Patches are applied in the following way: \u2022 An unguarded patch is always applied. \u2022 A patch with a positive guard is applied only if the guard is selected with guilt-select. \u2022 A patch with a negative guard is applied unless the guard is selected with guilt-select.","Process Name":"guilt-select","Link":"https:\/\/linux.die.net\/man\/1\/guilt-select"}},{"Process":{"Description":null,"Process Name":"guilt-series","Link":"https:\/\/linux.die.net\/man\/1\/guilt-series"}},{"Process":{"Description":"Each file is printed with a prefix that describes it's status. A   added\nC   copied\nD   deleted\nM   modified\nR   renamed\nT   type change (e.g., mode)\nU   unmerged\n?   untracked","Process Name":"guilt-status","Link":"https:\/\/linux.die.net\/man\/1\/guilt-status"}},{"Process":{"Description":null,"Process Name":"guilt-top","Link":"https:\/\/linux.die.net\/man\/1\/guilt-top"}},{"Process":{"Description":"List all unapplied patches.","Process Name":"guilt-unapplied","Link":"https:\/\/linux.die.net\/man\/1\/guilt-unapplied"}},{"Process":{"Description":"Gzip reduces the size of the named files using Lempel-Ziv coding (LZ77). Whenever possible, each file is replaced by one with the extension .gz, while keeping the same ownership modes, access and modification times. (The default extension is -gz for VMS, z for MSDOS, OS\/2 FAT, Windows NT FAT and Atari.) If no files are specified, or if a file name is \"-\", the standard input is compressed to the standard output. Gzip will only attempt to compress regular files. In particular, it will ignore symbolic links. If the compressed file name is too long for its file system, gzip truncates it. Gzip attempts to truncate only the parts of the file name longer than 3 characters. (A part is delimited by dots.) If the name consists of small parts only, the longest parts are truncated. For example, if file names are limited to 14 characters, gzip.msdos.exe is compressed to gzi.msd.exe.gz. Names are not truncated on systems which do not have a limit on file name length. By default, gzip keeps the original file name and timestamp in the compressed file. These are used when decompressing the file with the -N option. This is useful when the compressed file name was truncated or when the time stamp was not preserved after a file transfer. Compressed files can be restored to their original form using gzip -d or gunzip or zcat. If the original name saved in the compressed file is not suitable for its file system, a new name is constructed from the original one to make it legal. gunzip takes a list of files on its command line and replaces each file whose name ends with .gz, -gz, .z, -z, _z or .Z and which begins with the correct magic number with an uncompressed file without the original extension. gunzip also recognizes the special extensions .tgz and .taz as shorthands for .tar.gz and .tar.Z respectively. When compressing, gzip uses the .tgz extension if necessary instead of truncating a file with a .tar extension. gunzip can currently decompress files created by gzip, zip, compress, compress -H or pack. The detection of the input format is automatic. When using the first two formats, gunzip checks a 32 bit CRC. For pack, gunzip checks the uncompressed length. The standard compress format was not designed to allow consistency checks. However gunzip is sometimes able to detect a bad .Z file. If you get an error when uncompressing a .Z file, do not assume that the .Z file is correct simply because the standard uncompress does not complain. This generally means that the standard uncompress does not check its input, and happily generates garbage output. The SCO compress -H format (lzh compression method) does not include a CRC but also allows some consistency checks. Files created by zip can be uncompressed by gzip only if they have a single member compressed with the 'deflation' method. This feature is only intended to help conversion of tar.zip files to the tar.gz format. To extract a zip file with a single member, use a command like gunzip <foo.zip or gunzip -S .zip foo.zip. To extract zip files with several members, use unzip instead of gunzip. zcat is identical to gunzip -c. (On some systems, zcat may be installed as gzcat to preserve the original link to compress.) zcat uncompresses either a list of files on the command line or its standard input and writes the uncompressed data on standard output. zcat will uncompress files that have the correct magic number whether they have a .gz suffix or not. Gzip uses the Lempel-Ziv algorithm used in zip and PKZIP. The amount of compression obtained depends on the size of the input and the distribution of common substrings. Typically, text such as source code or English is reduced by 60-70%. Compression is generally much better than that achieved by LZW (as used in compress), Huffman coding (as used in pack), or adaptive Huffman coding (compact). Compression is always performed, even if the compressed file is slightly larger than the original. The worst case expansion is a few bytes for the gzip file header, plus 5 bytes every 32K block, or an expansion ratio of 0.015% for large files. Note that the actual number of used disk blocks almost never increases. gzip preserves the mode, ownership and timestamps of files when compressing or decompressing. The gzip file format is specified in P. Deutsch, GZIP file format specification version 4.3, <ftp:\/\/ftp.isi.edu\/in-notes\/rfc1952.txt>, Internet RFC 1952 (May 1996). The zip deflation format is specified in P. Deutsch, DEFLATE Compressed Data Format Specification version 1.3, <ftp:\/\/ftp.isi.edu\/in-notes\/rfc1951.txt>, Internet RFC 1951 (May 1996).","Process Name":"gunzip","Link":"https:\/\/linux.die.net\/man\/1\/gunzip"}},{"Process":{"Description":null,"Process Name":"gv","Link":"https:\/\/linux.die.net\/man\/1\/gv"}},{"Process":{"Description":null,"Process Name":"gv-update-userconfig","Link":"https:\/\/linux.die.net\/man\/1\/gv-update-userconfig"}},{"Process":{"Description":"gxl2gv converts between graphs represented in GXL and in the GV language. Unless a conversion type is specified using a flag, gxl2gv will deduce the type of conversion from the suffix of the input file, a \".gv\" (or a \".dot\") suffix causing a conversion from GV to GXL, and a \".gxl\" suffix causing a conversion from GXL to GV. If no suffix is available, e.g. when the input is from a pipe, and no flags are used then gxl2gv assumes the type of the input file from its executable name so that gxl2gv converts from GXL to GV, and gv2gxl converts from GV to GXL. GXL supports a much richer graph model than GV. gxl2gv will attempt to map GXL constructs into the analogous GV construct when this is possible. If not, the GXL information is stored as an attribute. The intention is that applying gxl2gv|gv2gxl is semantically equivalent to the identity operator.","Process Name":"gv2gxl","Link":"https:\/\/linux.die.net\/man\/1\/gv2gxl"}},{"Process":{"Description":"gvcolor is a filter that sets node colors from initial seed values. Colors flow along edges from tail to head, and are averaged (as HSB vectors) at nodes. The graph must already have been processed by dot. Appropriate choice of initial colors yields drawings in which node colors help to emphasize logical relationships between nodes, even when they are spread far apart in the layout. Initial colors must be set externally, using the color attribute of a node. It is often effective to assign colors to a few key source or sink nodes, manually setting their colors by editing the graph file. Color names are as in dot(1): symbolic names or RGB triples. It is best to choose some easily-distinguished but related colors; not necessarily spaced evenly around the color wheel. For example, blue_green, green, and light_yellow looks better than red, green, blue. Certain graph attributes control the gvcolor algorithm. flow=back reverses the flow of colors from heads to tails. saturation=.1,.9 (or any two numbers between 0 and 1) adjusts the color saturation linearly from least to greatest rank. If Defcolor is set, this color value is applied to any node not otherwise colored.","Process Name":"gvcolor","Link":"https:\/\/linux.die.net\/man\/1\/gvcolor"}},{"Process":{"Description":"gvgen generates a variety of simple, regularly-structured abstract graphs.","Process Name":"gvgen","Link":"https:\/\/linux.die.net\/man\/1\/gvgen"}},{"Process":{"Description":null,"Process Name":"gvhdl","Link":"https:\/\/linux.die.net\/man\/1\/gvhdl"}},{"Process":{"Description":"Vim is a text editor that is upwards compatible to Vi. It can be used to edit all kinds of plain text. It is especially useful for editing programs. There are a lot of enhancements above Vi: multi level undo, multi windows and buffers, syntax highlighting, command line editing, filename completion, on-line help, visual selection, etc.. See \":help vi_diff.txt\" for a summary of the differences between Vim and Vi. While running Vim a lot of help can be obtained from the on-line help system, with the \":help\" command. See the ON-LINE HELP section below. Most often Vim is started to edit a single file with the command vim file More generally Vim is started with: vim [options] [filelist] If the filelist is missing, the editor will start with an empty buffer. Otherwise exactly one out of the following four may be used to choose one or more files to be edited. file .. A list of filenames. The first one will be the current file and read into the buffer. The cursor will be positioned on the first line of the buffer. You can get to the other files with the \":next\" command. To edit a file that starts with a dash, precede the filelist with \"--\". - The file to edit is read from stdin. Commands are read from stderr, which should be a tty. -t {tag} The file to edit and the initial cursor position depends on a \"tag\", a sort of goto label. {tag} is looked up in the tags file, the associated file becomes the current file and the associated command is executed. Mostly this is used for C programs, in which case {tag} could be a function name. The effect is that the file containing that function becomes the current file and the cursor is positioned on the start of the function. See \":help tag-commands\". -q [errorfile] Start in quickFix mode. The file [errorfile] is read and the first error is displayed. If [errorfile] is omitted, the filename is obtained from the 'errorfile' option (defaults to \"AztecC.Err\" for the Amiga, \"errors.err\" on other systems). Further errors can be jumped to with the \":cn\" command. See \":help quickfix\". Vim behaves differently, depending on the name of the command (the executable may still be the same file). vim The \"normal\" way, everything is default. ex Start in Ex mode. Go to Normal mode with the \":vi\" command. Can also be done with the \"-e\" argument. view Start in read-only mode. You will be protected from writing the files. Can also be done with the \"-R\" argument. gvim gview The GUI version. Starts a new window. Can also be done with the \"-g\" argument. evim eview The GUI version in easy mode. Starts a new window. Can also be done with the \"-y\" argument. rvim rview rgvim rgview Like the above, but with restrictions. It will not be possible to start shell commands, or suspend Vim. Can also be done with the \"-Z\" argument.","Process Name":"gvim","Link":"https:\/\/linux.die.net\/man\/1\/gvim"}},{"Process":{"Description":null,"Process Name":"gvimdiff","Link":"https:\/\/linux.die.net\/man\/1\/gvimdiff"}},{"Process":{"Description":"gvpack reads in a stream of graphs, combines the graphs into a single layout, and produces a single graph serving as the union of the input graphs. The input graphs must be in dot format, and must have all necessary layout information. Acceptable input is produced by applying a Graphviz layout program, such as dot or neato, with no -T flag. By default, the packing is done at the cluster level. Thus, parts of one graph will not intrude into any top-level clusters or overlap any nodes or edges of another. The output of gvpack can be used to produce concrete output by applying neato -s -n2 with the desired -T flag.","Process Name":"gvpack","Link":"https:\/\/linux.die.net\/man\/1\/gvpack"}},{"Process":{"Description":null,"Process Name":"gvpr","Link":"https:\/\/linux.die.net\/man\/1\/gvpr"}},{"Process":{"Description":"Gwenview is an image viewer for KDE. It features a folder tree window and a file list window to provide easy navigation of your file hierarchy. Gwenview uses docked windows, so you can alter its layout any way you wish. You can also browse your images in full-screen mode, or embedded within Konqueror using the Gwenview Image Browser View and Kpart. Image loading is handled by the Qt library, so Gwenview supports all image formats your Qt installation supports. Gwenview correctly displays images with an alpha channel. Gwenview supports the displaying and editing of EXIF comments, if the necessary JPEG kfile-plugin is installed. This comes as part of the kdegraphics package. Lossless JPEG transforms are also supported.","Process Name":"gwenview","Link":"https:\/\/linux.die.net\/man\/1\/gwenview"}},{"Process":{"Description":null,"Process Name":"gxine","Link":"https:\/\/linux.die.net\/man\/1\/gxine"}},{"Process":{"Description":"This is the command line client used to connect and control the gxine video player.","Process Name":"gxine_client","Link":"https:\/\/linux.die.net\/man\/1\/gxine_client"}},{"Process":{"Description":null,"Process Name":"gxint","Link":"https:\/\/linux.die.net\/man\/1\/gxint"}},{"Process":{"Description":"gxl2dot converts between graphs represented in GXL and in the DOT language. Unless a conversion type is specified using a flag, gxl2dot will deduce the type of conversion from the suffix of the input file, a \".dot\" suffix causing a conversion from DOT to GXL, and a \".gxl\" suffix causing a conversion from GXL to DOT. If no suffix is available, e.g. when the input is from a pipe, and no flags are used then gxl2dot assumes the type of the input file from its executable name so that gxl2dot converts from GXL to DOT, and dot2gxl converts from DOT to GXL. GXL supports a much richer graph model than DOT. gxl2dot will attempt to map GXL constructs into the analogous DOT construct when this is possible. If not, the GXL information is stored as an attribute. The intention is that applying gxl2dot|dot2gxl is semantically equivalent to the identity operator.","Process Name":"gxl2dot","Link":"https:\/\/linux.die.net\/man\/1\/gxl2dot"}},{"Process":{"Description":null,"Process Name":"gxl2gv","Link":"https:\/\/linux.die.net\/man\/1\/gxl2gv"}},{"Process":{"Description":"The gzexe utility allows you to compress executables in place and have them automatically uncompress and execute when you run them (at a penalty in performance). For example if you execute ''gzexe \/usr\/bin\/gdb'' it will create the following two files: -rwxr-xr-x  1 root root 1026675 Jun  7 13:53 \/usr\/bin\/gdb\n-rwxr-xr-x  1 root root 2304524 May 30 13:02 \/usr\/bin\/gdb~ \/usr\/bin\/gdb~ is the original file and \/usr\/bin\/gdb is the self-uncompressing executable file. You can remove \/usr\/bin\/gdb~ once you are sure that \/usr\/bin\/gdb works properly. This utility is most useful on systems with very small disks.","Process Name":"gzexe","Link":"https:\/\/linux.die.net\/man\/1\/gzexe"}},{"Process":{"Description":"Gzip reduces the size of the named files using Lempel-Ziv coding (LZ77). Whenever possible, each file is replaced by one with the extension .gz, while keeping the same ownership modes, access and modification times. (The default extension is -gz for VMS, z for MSDOS, OS\/2 FAT, Windows NT FAT and Atari.) If no files are specified, or if a file name is \"-\", the standard input is compressed to the standard output. Gzip will only attempt to compress regular files. In particular, it will ignore symbolic links. If the compressed file name is too long for its file system, gzip truncates it. Gzip attempts to truncate only the parts of the file name longer than 3 characters. (A part is delimited by dots.) If the name consists of small parts only, the longest parts are truncated. For example, if file names are limited to 14 characters, gzip.msdos.exe is compressed to gzi.msd.exe.gz. Names are not truncated on systems which do not have a limit on file name length. By default, gzip keeps the original file name and timestamp in the compressed file. These are used when decompressing the file with the -N option. This is useful when the compressed file name was truncated or when the time stamp was not preserved after a file transfer. Compressed files can be restored to their original form using gzip -d or gunzip or zcat. If the original name saved in the compressed file is not suitable for its file system, a new name is constructed from the original one to make it legal. gunzip takes a list of files on its command line and replaces each file whose name ends with .gz, -gz, .z, -z, _z or .Z and which begins with the correct magic number with an uncompressed file without the original extension. gunzip also recognizes the special extensions .tgz and .taz as shorthands for .tar.gz and .tar.Z respectively. When compressing, gzip uses the .tgz extension if necessary instead of truncating a file with a .tar extension. gunzip can currently decompress files created by gzip, zip, compress, compress -H or pack. The detection of the input format is automatic. When using the first two formats, gunzip checks a 32 bit CRC. For pack, gunzip checks the uncompressed length. The standard compress format was not designed to allow consistency checks. However gunzip is sometimes able to detect a bad .Z file. If you get an error when uncompressing a .Z file, do not assume that the .Z file is correct simply because the standard uncompress does not complain. This generally means that the standard uncompress does not check its input, and happily generates garbage output. The SCO compress -H format (lzh compression method) does not include a CRC but also allows some consistency checks. Files created by zip can be uncompressed by gzip only if they have a single member compressed with the 'deflation' method. This feature is only intended to help conversion of tar.zip files to the tar.gz format. To extract a zip file with a single member, use a command like gunzip <foo.zip or gunzip -S .zip foo.zip. To extract zip files with several members, use unzip instead of gunzip. zcat is identical to gunzip -c. (On some systems, zcat may be installed as gzcat to preserve the original link to compress.) zcat uncompresses either a list of files on the command line or its standard input and writes the uncompressed data on standard output. zcat will uncompress files that have the correct magic number whether they have a .gz suffix or not. Gzip uses the Lempel-Ziv algorithm used in zip and PKZIP. The amount of compression obtained depends on the size of the input and the distribution of common substrings. Typically, text such as source code or English is reduced by 60-70%. Compression is generally much better than that achieved by LZW (as used in compress), Huffman coding (as used in pack), or adaptive Huffman coding (compact). Compression is always performed, even if the compressed file is slightly larger than the original. The worst case expansion is a few bytes for the gzip file header, plus 5 bytes every 32K block, or an expansion ratio of 0.015% for large files. Note that the actual number of used disk blocks almost never increases. gzip preserves the mode, ownership and timestamps of files when compressing or decompressing. The gzip file format is specified in P. Deutsch, GZIP file format specification version 4.3, <ftp:\/\/ftp.isi.edu\/in-notes\/rfc1952.txt>, Internet RFC 1952 (May 1996). The zip deflation format is specified in P. Deutsch, DEFLATE Compressed Data Format Specification version 1.3, <ftp:\/\/ftp.isi.edu\/in-notes\/rfc1951.txt>, Internet RFC 1951 (May 1996).","Process Name":"gzip","Link":"https:\/\/linux.die.net\/man\/1\/gzip"}}]