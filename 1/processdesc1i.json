[{"Process":{"Description":"i2f is a utility for converting IRAF images to FITS images. If you want to change the number of bits per pixel, use imrot. The BITPIX, NAXIS, NAXIS1, NAXIS2, OBJECT, and PIXFILE keywords are set from the binary portion of the header. IMHFILE and PIXFILE are set from the actual filenames for internal purposes.","Process Name":"i2f","Link":"https:\/\/linux.die.net\/man\/1\/i2f"}},{"Process":{"Description":"INTRODUCTION i3 was created because wmii, our favorite window manager at the time, didn't provide some features we wanted (multi-monitor done right, for example), had some bugs, didn't progress since quite some time and wasn't easy to hack at all (source code comments\/documentation completely lacking). Still, we think the wmii developers and contributors did a great job. Thank you for inspiring us to create i3. Please be aware that i3 is primarily targeted at advanced users and developers. IMPORTANT NOTE TO nVidia BINARY DRIVER USERS If you are using the nVidia binary graphics driver (also known as blob) you need to use the --force-xinerama flag (in your ~\/.xsession) when starting i3, like so: exec i3 --force-xinerama -V >>~\/.i3\/i3log 2>&1 See also docs\/multi-monitor for the full explanation. TERMINOLOGY Tree i3 keeps your layout in a tree data structure. Window An X11 window, like the Firefox browser window or a terminal emulator. Split container A split container contains multiple other split containers or windows. Containers can be used in various layouts. The default mode is called \"default\" and just resizes each client equally so that it fits. Workspace A workspace is a set of containers. Other window managers call this \"Virtual Desktops\". In i3, each workspace is assigned to a specific virtual screen. By default, screen 1 has workspace 1, screen 2 has workspace 2 and so on... However, when you create a new workspace (by simply switching to it), it'll be assigned the screen you are currently on. Output Using XRandR, you can have an X11 screen spanning multiple real monitors. Furthermore, you can set them up in cloning mode or with positions (monitor 1 is left of monitor 2). i3 uses the RandR API to query which outputs are available and which screens are connected to these outputs.","Process Name":"i3","Link":"https:\/\/linux.die.net\/man\/1\/i3"}},{"Process":{"Description":"i3-config-wizard is started by i3 in its default config, unless \/.i3\/config exists. i3-config-wizard creates a keysym based i3 config file (based on \/etc\/i3\/config.keycodes) in \/.i3\/config. The advantage of using keysyms is that the config file is easy to read, understand and modify. However, if we shipped with a keysym based default config file, the key positions would not be consistent across different keyboard layouts (take for example the homerow for movement). Therefore, we ship with a keycode based default config and let the wizard transform it according to your current keyboard layout.","Process Name":"i3-config-wizard","Link":"https:\/\/linux.die.net\/man\/1\/i3-config-wizard"}},{"Process":{"Description":"i3-input is a tool to take commands (or parts of a command) composed by the user, and send it\/them to i3. This is useful, for example, for the mark\/goto command.","Process Name":"i3-input","Link":"https:\/\/linux.die.net\/man\/1\/i3-input"}},{"Process":{"Description":"i3-ipc can be used to communicate with i3, the improved tiling window manager, through the provided ipc socket. Useful for scripting the window manager. Currently implemented message types of i3 are the following: 0 (COMMAND) The payload of the message is a command for i3 (like the commands you can bind to keys in the configuration file) The command will be executed directly after receiving it. The reply will be always {\"succes\":true} for now. 1 (GET_WORKSPACES) Gets the current workspaces. The reply will be a JSON-encoded list of workspaces. 2 (SUBSCRIBE) Subscribes your connection to the workspace event. 3 (GET_OUTPUTS) Gets the current outputs. The reply will be a JSON-encoded list of outputs.","Process Name":"i3-ipc","Link":"https:\/\/linux.die.net\/man\/1\/i3-ipc"}},{"Process":{"Description":"i3-migrate-config-to-v4 is a Perl script which migrates your old (< version 4) configuration files to a version 4 config file. The most significant changes are the new commands (see the release notes). This script will automatically be run by i3 when it detects an old config file. Please migrate your config file as soon as possible. We plan to include this script in all i3 release until 2012-08-01. Afterwards, old config files will no longer be supported.","Process Name":"i3-migrate-config-to-v4","Link":"https:\/\/linux.die.net\/man\/1\/i3-migrate-config-to-v4"}},{"Process":{"Description":"i3-msg is a sample implementation for a client using the unix socket IPC interface to i3. At the moment, it can only be used for sending commands (like in configuration file for key bindings), but this may change in the future (staying backwards-compatible, of course).","Process Name":"i3-msg","Link":"https:\/\/linux.die.net\/man\/1\/i3-msg"}},{"Process":{"Description":"i3-nagbar is used by i3 to tell you about errors in your configuration file (for example). While these errors are logged to the logfile (if any), the past has proven that users are either not aware of their logfile or do not check it after modifying the configuration file.","Process Name":"i3-nagbar","Link":"https:\/\/linux.die.net\/man\/1\/i3-nagbar"}},{"Process":{"Description":"","Process Name":"i3-wsbar","Link":"https:\/\/linux.die.net\/man\/1\/i3-wsbar"}},{"Process":{"Description":"i3lock locks your screen by making it white (default). To quit i3lock just enter your password.","Process Name":"i3lock","Link":"https:\/\/linux.die.net\/man\/1\/i3lock"}},{"Process":{"Description":"i3status is a small program (less than 1000 SLOC) for generating a status bar for dzen2, xmobar or similar programs. It is designed to be very efficient by issuing a very small number of systemcalls, as one generally wants to update such a status line every second. This ensures that even under high load, your status bar is updated correctly. Also, it saves a bit of energy by not hogging your CPU as much as spawning the corresponding amount of shell commands would.","Process Name":"i3status","Link":"https:\/\/linux.die.net\/man\/1\/i3status"}},{"Process":{"Description":"addr2line translates addresses into file names and line numbers. Given an address in an executable or an offset in a section of a relocatable object, it uses the debugging information to figure out which file name and line number are associated with it. The executable or relocatable object to use is specified with the -e option. The default is the file a.out. The section in the relocatable object to use is specified with the -j option. addr2line has two modes of operation. In the first, hexadecimal addresses are specified on the command line, and addr2line displays the file name and line number for each address. In the second, addr2line reads hexadecimal addresses from standard input, and prints the file name and line number for each address on standard output. In this mode, addr2line may be used in a pipe to convert dynamically chosen addresses. The format of the output is FILENAME:LINENO . The file name and line number for each address is printed on a separate line. If the -f option is used, then each FILENAME:LINENO line is preceded by a FUNCTIONNAME line which is the name of the function containing the address. If the file name or function name can not be determined, addr2line will print two question marks in their place. If the line number can not be determined, addr2line will print 0.","Process Name":"i686-pc-mingw32-addr2line","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-addr2line"}},{"Process":{"Description":"The GNU ar program creates, modifies, and extracts from archives. An archive is a single file holding a collection of other files in a structure that makes it possible to retrieve the original individual files (called members of the archive). The original files' contents, mode (permissions), timestamp, owner, and group are preserved in the archive, and can be restored on extraction. GNU ar can maintain archives whose members have names of any length; however, depending on how ar is configured on your system, a limit on member-name length may be imposed for compatibility with archive formats maintained with other tools. If it exists, the limit is often 15 characters (typical of formats related to a.out) or 16 characters (typical of formats related to coff). ar is considered a binary utility because archives of this sort are most often used as libraries holding commonly needed subroutines. ar creates an index to the symbols defined in relocatable object modules in the archive when you specify the modifier s. Once created, this index is updated in the archive whenever ar makes a change to its contents (save for the q update operation). An archive with such an index speeds up linking to the library, and allows routines in the library to call each other without regard to their placement in the archive. You may use nm -s or nm --print-armap to list this index table. If an archive lacks the table, another form of ar called ranlib can be used to add just the table. GNU ar can optionally create a thin archive, which contains a symbol index and references to the original copies of the member files of the archives. Such an archive is useful for building libraries for use within a local build, where the relocatable objects are expected to remain available, and copying the contents of each object would only waste time and space. Thin archives are also flattened, so that adding one or more archives to a thin archive will add the elements of the nested archive individually. The paths to the elements of the archive are stored relative to the archive itself. GNU ar is designed to be compatible with two different facilities. You can control its activity using command-line options, like the different varieties of ar on Unix systems; or, if you specify the single command-line option -M, you can control it with a script supplied via standard input, like the MRI \"librarian\" program.","Process Name":"i686-pc-mingw32-ar","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-ar"}},{"Process":{"Description":"GNU as is really a family of assemblers. If you use (or have used) the GNU assembler on one architecture, you should find a fairly similar environment when you use it on another architecture. Each version has much in common with the others, including object file formats, most assembler directives (often called pseudo-ops) and assembler syntax. as is primarily intended to assemble the output of the GNU C compiler \"gcc\" for use by the linker \"ld\". Nevertheless, we've tried to make as assemble correctly everything that other assemblers for the same machine would assemble. Any exceptions are documented explicitly. This doesn't mean as always uses the same syntax as another assembler for the same architecture; for example, we know of several incompatible versions of 680x0 assembly language syntax. Each time you run as it assembles exactly one source program. The source program is made up of one or more files. (The standard input is also a file.) You give as a command line that has zero or more input file names. The input files are read (from left file name to right). A command line argument (in any position) that has no special meaning is taken to be an input file name. If you give as no file names it attempts to read one input file from the as standard input, which is normally your terminal. You may have to type ctl-D to tell as there is no more program to assemble. Use -- if you need to explicitly name the standard input file in your command line. If the source is empty, as produces a small, empty object file. as may write warnings and error messages to the standard error file (usually your terminal). This should not happen when a compiler runs as automatically. Warnings report an assumption made so that as could keep assembling a flawed program; errors report a grave problem that stops the assembly. If you are invoking as via the GNU C compiler, you can use the -Wa option to pass arguments through to the assembler. The assembler arguments must be separated from each other (and the -Wa) by commas. For example: gcc -c -g -O -Wa,-alh,-L file.c This passes two options to the assembler: -alh (emit a listing to standard output with high-level and assembly source) and -L (retain local symbols in the symbol table). Usually you do not need to use this -Wa mechanism, since many compiler command-line options are automatically passed to the assembler by the compiler. (You can call the GNU compiler driver with the -v option to see precisely what options it passes to each compilation pass, including the assembler.)","Process Name":"i686-pc-mingw32-as","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-as"}},{"Process":{"Description":"The C ++ and Java languages provide function overloading, which means that you can write many functions with the same name, providing that each function takes parameters of different types. In order to be able to distinguish these similarly named functions C ++ and Java encode them into a low-level assembler name which uniquely identifies each different version. This process is known as mangling. The c++filt [1] program does the inverse mapping: it decodes (demangles) low-level names into user-level names so that they can be read. Every alphanumeric word (consisting of letters, digits, underscores, dollars, or periods) seen in the input is a potential mangled name. If the name decodes into a C ++ name, the C ++ name replaces the low-level name in the output, otherwise the original word is output. In this way you can pass an entire assembler source file, containing mangled names, through c++filt and see the same source file containing demangled names. You can also use c++filt to decipher individual symbols by passing them on the command line: c++filt <symbol> If no symbol arguments are given, c++filt reads symbol names from the standard input instead. All the results are printed on the standard output. The difference between reading names from the command line versus reading names from the standard input is that command line arguments are expected to be just mangled names and no checking is performed to separate them from surrounding text. Thus for example: c++filt -n _Z1fv will work and demangle the name to \"f()\" whereas: c++filt -n _Z1fv, will not work. (Note the extra comma at the end of the mangled name which makes it invalid). This command however will work: echo _Z1fv, | c++filt -n and will display \"f(),\", i.e., the demangled name followed by a trailing comma. This behaviour is because when the names are read from the standard input it is expected that they might be part of an assembler source file where there might be extra, extraneous characters trailing after a mangled name. For example: .type   _Z1fv, @function","Process Name":"i686-pc-mingw32-c++filt","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-c++filt"}},{"Process":{"Description":"The C preprocessor, often known as cpp, is a macro processor that is used automatically by the C compiler to transform your program before compilation. It is called a macro processor because it allows you to define macros, which are brief abbreviations for longer constructs. The C preprocessor is intended to be used only with C, C ++ , and Objective-C source code. In the past, it has been abused as a general text processor. It will choke on input which does not obey C's lexical rules. For example, apostrophes will be interpreted as the beginning of character constants, and cause errors. Also, you cannot rely on it preserving characteristics of the input which are not significant to C-family languages. If a Makefile is preprocessed, all the hard tabs will be removed, and the Makefile will not work. Having said that, you can often get away with using cpp on things which are not C. Other Algol-ish programming languages are often safe (Pascal, Ada, etc.) So is assembly, with caution. -traditional-cpp mode preserves more white space, and is otherwise more permissive. Many of the problems can be avoided by writing C or C ++ style comments instead of native language comments, and keeping macros simple. Wherever possible, you should use a preprocessor geared to the language you are writing in. Modern versions of the GNU assembler have macro facilities. Most high level programming languages have their own conditional compilation and inclusion mechanism. If all else fails, try a true general text processor, such as GNU M4. C preprocessors vary in some details. This manual discusses the GNU C preprocessor, which provides a small superset of the features of ISO Standard C. In its default mode, the GNU C preprocessor does not do a few things required by the standard. These are features which are rarely, if ever, used, and may cause surprising changes to the meaning of a program which does not expect them. To get strict ISO Standard C, you should use the -std=c89 or -std=c99 options, depending on which version of the standard you want. To get all the mandatory diagnostics, you must also use -pedantic. This manual describes the behavior of the ISO preprocessor. To minimize gratuitous differences, where the ISO preprocessor's behavior does not conflict with traditional semantics, the traditional preprocessor should behave the same way. The various differences that do exist are detailed in the section Traditional Mode. For clarity, unless noted otherwise, references to CPP in this manual refer to GNU CPP .","Process Name":"i686-pc-mingw32-cpp","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-cpp"}},{"Process":{"Description":"dlltool reads its inputs, which can come from the -d and -b options as well as object files specified on the command line. It then processes these inputs and if the -e option has been specified it creates a exports file. If the -l option has been specified it creates a library file and if the -z option has been specified it creates a def file. Any or all of the -e, -l and -z options can be present in one invocation of dlltool. When creating a DLL , along with the source for the DLL , it is necessary to have three other files. dlltool can help with the creation of these files. The first file is a .def file which specifies which functions are exported from the DLL , which functions the DLL imports, and so on. This is a text file and can be created by hand, or dlltool can be used to create it using the -z option. In this case dlltool will scan the object files specified on its command line looking for those functions which have been specially marked as being exported and put entries for them in the .def file it creates. In order to mark a function as being exported from a DLL , it needs to have an -export:<name_of_function> entry in the .drectve section of the object file. This can be done in C by using the asm() operator: asm (\".section .drectve\");\nasm (\".ascii \\\"-export:my_func\\\"\");\n\nint my_func (void) { ... } The second file needed for DLL creation is an exports file. This file is linked with the object files that make up the body of the DLL and it handles the interface between the DLL and the outside world. This is a binary file and it can be created by giving the -e option to dlltool when it is creating or reading in a .def file. The third file needed for DLL creation is the library file that programs will link with in order to access the functions in the DLL (an 'import library'). This file can be created by giving the -l option to dlltool when it is creating or reading in a .def file. If the -y option is specified, dlltool generates a delay-import library that can be used instead of the normal import library to allow a program to link to the dll only as soon as an imported function is called for the first time. The resulting executable will need to be linked to the static delayimp library containing __delayLoadHelper2(), which in turn will import LoadLibraryA and GetProcAddress from kernel32. dlltool builds the library file by hand, but it builds the exports file by creating temporary files containing assembler statements and then assembling these. The -S command line option can be used to specify the path to the assembler that dlltool will use, and the -f option can be used to pass specific flags to that assembler. The -n can be used to prevent dlltool from deleting these temporary assembler files when it is done, and if -n is specified twice then this will prevent dlltool from deleting the temporary object files it used to build the library. Here is an example of creating a DLL from a source file dll.c and also creating a program (from an object file called program.o) that uses that DLL: gcc -c dll.c\ndlltool -e exports.o -l dll.lib dll.o\ngcc dll.o exports.o -o dll.dll\ngcc program.o dll.lib -o program dlltool may also be used to query an existing import library to determine the name of the DLL to which it is associated. See the description of the -I or --identify option.","Process Name":"i686-pc-mingw32-dlltool","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-dlltool"}},{"Process":{"Description":"When you invoke GCC , it normally does preprocessing, compilation, assembly and linking. The \"overall options\" allow you to stop this process at an intermediate stage. For example, the -c option says not to run the linker. Then the output consists of object files output by the assembler. Other options are passed on to one stage of processing. Some options control the preprocessor and others the compiler itself. Yet other options control the assembler and linker; most of these are not documented here, since you rarely need to use any of them. Most of the command line options that you can use with GCC are useful for C programs; when an option is only useful with another language (usually C ++ ), the explanation says so explicitly. If the description for a particular option does not mention a source language, you can use that option with all supported languages. The gcc program accepts options and file names as operands. Many options have multi-letter names; therefore multiple single-letter options may not be grouped: -dv is very different from -d -v. You can mix options and other arguments. For the most part, the order you use doesn't matter. Order does matter when you use several options of the same kind; for example, if you specify -L more than once, the directories are searched in the order specified. Also, the placement of the -l option is significant. Many options have long names starting with -f or with -W---for example, -fmove-loop-invariants, -Wformat and so on. Most of these have both positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"i686-pc-mingw32-g++","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-g++"}},{"Process":{"Description":"When you invoke GCC , it normally does preprocessing, compilation, assembly and linking. The \"overall options\" allow you to stop this process at an intermediate stage. For example, the -c option says not to run the linker. Then the output consists of object files output by the assembler. Other options are passed on to one stage of processing. Some options control the preprocessor and others the compiler itself. Yet other options control the assembler and linker; most of these are not documented here, since you rarely need to use any of them. Most of the command line options that you can use with GCC are useful for C programs; when an option is only useful with another language (usually C ++ ), the explanation says so explicitly. If the description for a particular option does not mention a source language, you can use that option with all supported languages. The gcc program accepts options and file names as operands. Many options have multi-letter names; therefore multiple single-letter options may not be grouped: -dv is very different from -d -v. You can mix options and other arguments. For the most part, the order you use doesn't matter. Order does matter when you use several options of the same kind; for example, if you specify -L more than once, the directories are searched in the order specified. Also, the placement of the -l option is significant. Many options have long names starting with -f or with -W---for example, -fmove-loop-invariants, -Wformat and so on. Most of these have both positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"i686-pc-mingw32-gcc","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-gcc"}},{"Process":{"Description":"gcov is a test coverage program. Use it in concert with GCC to analyze your programs to help create more efficient, faster running code and to discover untested parts of your program. You can use gcov as a profiling tool to help discover where your optimization efforts will best affect your code. You can also use gcov along with the other profiling tool, gprof, to assess which parts of your code use the greatest amount of computing time. Profiling tools help you analyze your code's performance. Using a profiler such as gcov or gprof, you can find out some basic performance statistics, such as: \u2022 how often each line of code executes \u2022 what lines of code are actually executed \u2022 how much computing time each section of code uses Once you know these things about how your code works when compiled, you can look at each module to see which modules should be optimized. gcov helps you determine where to work on optimization. Software developers also use coverage testing in concert with testsuites, to make sure software is actually good enough for a release. Testsuites can verify that a program works as expected; a coverage program tests to see how much of the program is exercised by the testsuite. Developers can then determine what kinds of test cases need to be added to the testsuites to create both better testing and a better final product. You should compile your code without optimization if you plan to use gcov because the optimization, by combining some lines of code into one function, may not give you as much information as you need to look for 'hot spots' where the code is using a great deal of computer time. Likewise, because gcov accumulates statistics by line (at the lowest resolution), it works best with a programming style that places only one statement on each line. If you use complicated macros that expand to loops or to other control structures, the statistics are less helpful---they only report on the line where the macro call appears. If your complex macros behave like functions, you can replace them with inline functions to solve this problem. gcov creates a logfile called sourcefile.gcov which indicates how many times each line of a source file sourcefile.c has executed. You can use these logfiles along with gprof to aid in fine-tuning the performance of your programs. gprof gives timing information you can use along with the information you get from gcov. gcov works only on code compiled with GCC . It is not compatible with any other profiling or test coverage mechanism.","Process Name":"i686-pc-mingw32-gcov","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-gcov"}},{"Process":{"Description":"The gfortran command supports all the options supported by the gcc command. Only options specific to GNU Fortran are documented here. All GCC and GNU Fortran options are accepted both by gfortran and by gcc (as well as any other drivers built at the same time, such as g++), since adding GNU Fortran to the GCC distribution enables acceptance of GNU Fortran options by all of the relevant drivers. In some cases, options have positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"i686-pc-mingw32-gfortran","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-gfortran"}},{"Process":{"Description":"\"gprof\" produces an execution profile of C, Pascal, or Fortran77 programs. The effect of called routines is incorporated in the profile of each caller. The profile data is taken from the call graph profile file (gmon.out default) which is created by programs that are compiled with the -pg option of \"cc\", \"pc\", and \"f77\". The -pg option also links in versions of the library routines that are compiled for profiling. \"Gprof\" reads the given object file (the default is \"a.out\") and establishes the relation between its symbol table and the call graph profile from gmon.out. If more than one profile file is specified, the \"gprof\" output shows the sum of the profile information in the given profile files. \"Gprof\" calculates the amount of time spent in each routine. Next, these times are propagated along the edges of the call graph. Cycles are discovered, and calls into a cycle are made to share the time of the cycle. Several forms of output are available from the analysis. The flat profile shows how much time your program spent in each function, and how many times that function was called. If you simply want to know which functions burn most of the cycles, it is stated concisely here. The call graph shows, for each function, which functions called it, which other functions it called, and how many times. There is also an estimate of how much time was spent in the subroutines of each function. This can suggest places where you might try to eliminate function calls that use a lot of time. The annotated source listing is a copy of the program's source code, labeled with the number of times each line of the program was executed.","Process Name":"i686-pc-mingw32-gprof","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-gprof"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"i686-pc-mingw32-ld","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-ld"}},{"Process":{"Description":"nlmconv converts the relocatable i386 object file infile into the NetWare Loadable Module outfile, optionally reading headerfile for NLM header information. For instructions on writing the NLM command file language used in header files, see the linkers section, NLMLINK in particular, of the NLM Development and Tools Overview, which is part of the NLM Software Developer's Kit (\" NLM SDK \"), available from Novell, Inc. nlmconv uses the GNU Binary File Descriptor library to read infile; nlmconv can perform a link step. In other words, you can list more than one object file for input if you list them in the definitions file (rather than simply specifying one input file on the command line). In this case, nlmconv calls the linker for you.","Process Name":"i686-pc-mingw32-nlmconv","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-nlmconv"}},{"Process":{"Description":"GNU nm lists the symbols from object files objfile.... If no object files are listed as arguments, nm assumes the file a.out. For each symbol, nm shows: \u2022 The symbol value, in the radix selected by options (see below), or hexadecimal by default. \u2022 The symbol type. At least the following types are used; others are, as well, depending on the object file format. If lowercase, the symbol is local; if uppercase, the symbol is global (external). \"A\" The symbol's value is absolute, and will not be changed by further linking. \"B\" \"b\" The symbol is in the uninitialized data section (known as BSS ). \"C\" The symbol is common. Common symbols are uninitialized data. When linking, multiple common symbols may appear with the same name. If the symbol is defined anywhere, the common symbols are treated as undefined references. \"D\" \"d\" The symbol is in the initialized data section. \"G\" \"g\" The symbol is in an initialized data section for small objects. Some object file formats permit more efficient access to small data objects, such as a global int variable as opposed to a large global array. \"i\" For PE format files this indicates that the symbol is in a section specific to the implementation of DLLs. For ELF format files this indicates that the symbol is an indirect function. This is a GNU extension to the standard set of ELF symbol types. It indicates a symbol which if referenced by a relocation does not evaluate to its address, but instead must be invoked at runtime. The runtime execution will then return the value to be used in the relocation. \"N\" The symbol is a debugging symbol. \"p\" The symbols is in a stack unwind section. \"R\" \"r\" The symbol is in a read only data section. \"S\" \"s\" The symbol is in an uninitialized data section for small objects. \"T\" \"t\" The symbol is in the text (code) section. \"U\" The symbol is undefined. \"u\" The symbol is a unique global symbol. This is a GNU extension to the standard set of ELF symbol bindings. For such a symbol the dynamic linker will make sure that in the entire process there is just one symbol with this name and type in use. \"V\" \"v\" The symbol is a weak object. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the weak symbol becomes zero with no error. On some systems, uppercase indicates that a default value has been specified. \"W\" \"w\" The symbol is a weak symbol that has not been specifically tagged as a weak object symbol. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the symbol is determined in a system-specific manner without error. On some systems, uppercase indicates that a default value has been specified. \"-\" The symbol is a stabs symbol in an a.out object file. In this case, the next values printed are the stabs other field, the stabs desc field, and the stab type. Stabs symbols are used to hold debugging information. \"?\" The symbol type is unknown, or object file format specific. \u2022 The symbol name.","Process Name":"i686-pc-mingw32-nm","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-nm"}},{"Process":{"Description":"The GNU objcopy utility copies the contents of an object file to another. objcopy uses the GNU BFD Library to read and write the object files. It can write the destination object file in a format different from that of the source object file. The exact behavior of objcopy is controlled by command-line options. Note that objcopy should be able to copy a fully linked file between any two formats. However, copying a relocatable object file between any two formats may not work as expected. objcopy creates temporary files to do its translations and deletes them afterward. objcopy uses BFD to do all its translation work; it has access to all the formats described in BFD and thus is able to recognize most formats without being told explicitly. objcopy can be used to generate S-records by using an output target of srec (e.g., use -O srec). objcopy can be used to generate a raw binary file by using an output target of binary (e.g., use -O binary). When objcopy generates a raw binary file, it will essentially produce a memory dump of the contents of the input object file. All symbols and relocation information will be discarded. The memory dump will start at the load address of the lowest section copied into the output file. When generating an S-record or a raw binary file, it may be helpful to use -S to remove sections containing debugging information. In some cases -R will be useful to remove sections which contain information that is not needed by the binary file. Note---objcopy is not able to change the endianness of its input files. If the input format has an endianness (some formats do not), objcopy can only copy the inputs into file formats that have the same endianness or which have no endianness (e.g., srec). (However, see the --reverse-bytes option.)","Process Name":"i686-pc-mingw32-objcopy","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-objcopy"}},{"Process":{"Description":"objdump displays information about one or more object files. The options control what particular information to display. This information is mostly useful to programmers who are working on the compilation tools, as opposed to programmers who just want their program to compile and work. objfile... are the object files to be examined. When you specify archives, objdump shows information on each of the member object files.","Process Name":"i686-pc-mingw32-objdump","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-objdump"}},{"Process":{"Description":"ranlib generates an index to the contents of an archive and stores it in the archive. The index lists each symbol defined by a member of an archive that is a relocatable object file. You may use nm -s or nm --print-armap to list this index. An archive with such an index speeds up linking to the library and allows routines in the library to call each other without regard to their placement in the archive. The GNU ranlib program is another form of GNU ar; running ranlib is completely equivalent to executing ar -s.","Process Name":"i686-pc-mingw32-ranlib","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-ranlib"}},{"Process":{"Description":"readelf displays information about one or more ELF format object files. The options control what particular information to display. elffile... are the object files to be examined. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files. This program performs a similar function to objdump but it goes into more detail and it exists independently of the BFD library, so if there is a bug in BFD then readelf will not be affected.","Process Name":"i686-pc-mingw32-readelf","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-readelf"}},{"Process":{"Description":"The GNU size utility lists the section sizes---and the total size---for each of the object or archive files objfile in its argument list. By default, one line of output is generated for each object file or each module in an archive. objfile... are the object files to be examined. If none are specified, the file \"a.out\" will be used.","Process Name":"i686-pc-mingw32-size","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-size"}},{"Process":{"Description":"For each file given, GNU strings prints the printable character sequences that are at least 4 characters long (or the number given with the options below) and are followed by an unprintable character. By default, it only prints the strings from the initialized and loaded sections of object files; for other types of files, it prints the strings from the whole file. strings is mainly useful for determining the contents of non-text files.","Process Name":"i686-pc-mingw32-strings","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-strings"}},{"Process":{"Description":"GNU strip discards all symbols from object files objfile. The list of object files may include archives. At least one object file must be given. strip modifies the files named in its argument, rather than writing modified copies under different names.","Process Name":"i686-pc-mingw32-strip","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-strip"}},{"Process":{"Description":"windmc reads message definitions from an input file (.mc) and translate them into a set of output files. The output files may be of four kinds: \"h\" A C header file containing the message definitions. \"rc\" A resource file compilable by the windres tool. \"bin\" One or more binary files containing the resource data for a specific message language. \"dbg\" A C include file that maps message id's to their symbolic name. The exact description of these different formats is available in documentation from Microsoft. When windmc converts from the \"mc\" format to the \"bin\" format, \"rc\", \"h\", and optional \"dbg\" it is acting like the Windows Message Compiler.","Process Name":"i686-pc-mingw32-windmc","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-windmc"}},{"Process":{"Description":"windres reads resources from an input file and copies them into an output file. Either file may be in one of three formats: \"rc\" A text format read by the Resource Compiler. \"res\" A binary format generated by the Resource Compiler. \"coff\" A COFF object or executable. The exact description of these different formats is available in documentation from Microsoft. When windres converts from the \"rc\" format to the \"res\" format, it is acting like the Windows Resource Compiler. When windres converts from the \"res\" format to the \"coff\" format, it is acting like the Windows \"CVTRES\" program. When windres generates an \"rc\" file, the output is similar but not identical to the format expected for the input. When an input \"rc\" file refers to an external filename, an output \"rc\" file will instead include the file contents. If the input or output format is not specified, windres will guess based on the file name, or, for the input file, the file contents. A file with an extension of .rc will be treated as an \"rc\" file, a file with an extension of .res will be treated as a \"res\" file, and a file with an extension of .o or .exe will be treated as a \"coff\" file. If no output file is specified, windres will print the resources in \"rc\" format to standard output. The normal use is for you to write an \"rc\" file, use windres to convert it to a COFF object file, and then link the COFF file into your application. This will make the resources described in the \"rc\" file available to Windows.","Process Name":"i686-pc-mingw32-windres","Link":"https:\/\/linux.die.net\/man\/1\/i686-pc-mingw32-windres"}},{"Process":{"Description":"This manual page documents briefly the i810rotate, command. This manual page was written for the Debian GNU\/Linux distribution because the original program does not have a manual page. i810rotate is a program that toggles the currently selected video output for i810 based video cards, as found on several laptops. It loops through three states: LCD only output, LCD and CRT output, and CRT only output.","Process Name":"i810rotate","Link":"https:\/\/linux.die.net\/man\/1\/i810rotate"}},{"Process":{"Description":"This manual page documents briefly the i810switch, command. This manual page was written for the Debian GNU\/Linux distribution because the original program does not have a manual page. i810switch is a program that enables\/disables the output to the CRT display and LCD, depending on the i810 graphics controller hardware. With no options, it displays the current output status.","Process Name":"i810switch","Link":"https:\/\/linux.die.net\/man\/1\/i810switch"}},{"Process":{"Description":"addr2line translates addresses into file names and line numbers. Given an address in an executable or an offset in a section of a relocatable object, it uses the debugging information to figure out which file name and line number are associated with it. The executable or relocatable object to use is specified with the -e option. The default is the file a.out. The section in the relocatable object to use is specified with the -j option. addr2line has two modes of operation. In the first, hexadecimal addresses are specified on the command line, and addr2line displays the file name and line number for each address. In the second, addr2line reads hexadecimal addresses from standard input, and prints the file name and line number for each address on standard output. In this mode, addr2line may be used in a pipe to convert dynamically chosen addresses. The format of the output is FILENAME:LINENO . The file name and line number for each input address is printed on separate lines. If the -f option is used, then each FILENAME:LINENO line is preceded by FUNCTIONNAME which is the name of the function containing the address. If the -i option is used and the code at the given address is present there because of inlining by the compiler then the { FUNCTIONNAME } FILENAME:LINENO information for the inlining function will be displayed afterwards. This continues recursively until there is no more inlining to report. If the -a option is used then the output is prefixed by the input address. If the -p option is used then the output for each input address is displayed on one, possibly quite long, line. If -p is not used then the output is broken up into multiple lines, based on the paragraphs above. If the file name or function name can not be determined, addr2line will print two question marks in their place. If the line number can not be determined, addr2line will print 0.","Process Name":"ia64-linux-gnu-addr2line","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-addr2line"}},{"Process":{"Description":"The GNU ar program creates, modifies, and extracts from archives. An archive is a single file holding a collection of other files in a structure that makes it possible to retrieve the original individual files (called members of the archive). The original files' contents, mode (permissions), timestamp, owner, and group are preserved in the archive, and can be restored on extraction. GNU ar can maintain archives whose members have names of any length; however, depending on how ar is configured on your system, a limit on member-name length may be imposed for compatibility with archive formats maintained with other tools. If it exists, the limit is often 15 characters (typical of formats related to a.out) or 16 characters (typical of formats related to coff). ar is considered a binary utility because archives of this sort are most often used as libraries holding commonly needed subroutines. ar creates an index to the symbols defined in relocatable object modules in the archive when you specify the modifier s. Once created, this index is updated in the archive whenever ar makes a change to its contents (save for the q update operation). An archive with such an index speeds up linking to the library, and allows routines in the library to call each other without regard to their placement in the archive. You may use nm -s or nm --print-armap to list this index table. If an archive lacks the table, another form of ar called ranlib can be used to add just the table. GNU ar can optionally create a thin archive, which contains a symbol index and references to the original copies of the member files of the archives. Such an archive is useful for building libraries for use within a local build, where the relocatable objects are expected to remain available, and copying the contents of each object would only waste time and space. Thin archives are also flattened, so that adding one or more archives to a thin archive will add the elements of the nested archive individually. The paths to the elements of the archive are stored relative to the archive itself. GNU ar is designed to be compatible with two different facilities. You can control its activity using command-line options, like the different varieties of ar on Unix systems; or, if you specify the single command-line option -M, you can control it with a script supplied via standard input, like the MRI \"librarian\" program.","Process Name":"ia64-linux-gnu-ar","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-ar"}},{"Process":{"Description":"GNU as is really a family of assemblers. If you use (or have used) the GNU assembler on one architecture, you should find a fairly similar environment when you use it on another architecture. Each version has much in common with the others, including object file formats, most assembler directives (often called pseudo-ops) and assembler syntax. as is primarily intended to assemble the output of the GNU C compiler \"gcc\" for use by the linker \"ld\". Nevertheless, we've tried to make as assemble correctly everything that other assemblers for the same machine would assemble. Any exceptions are documented explicitly. This doesn't mean as always uses the same syntax as another assembler for the same architecture; for example, we know of several incompatible versions of 680x0 assembly language syntax. Each time you run as it assembles exactly one source program. The source program is made up of one or more files. (The standard input is also a file.) You give as a command line that has zero or more input file names. The input files are read (from left file name to right). A command line argument (in any position) that has no special meaning is taken to be an input file name. If you give as no file names it attempts to read one input file from the as standard input, which is normally your terminal. You may have to type ctl-D to tell as there is no more program to assemble. Use -- if you need to explicitly name the standard input file in your command line. If the source is empty, as produces a small, empty object file. as may write warnings and error messages to the standard error file (usually your terminal). This should not happen when a compiler runs as automatically. Warnings report an assumption made so that as could keep assembling a flawed program; errors report a grave problem that stops the assembly. If you are invoking as via the GNU C compiler, you can use the -Wa option to pass arguments through to the assembler. The assembler arguments must be separated from each other (and the -Wa) by commas. For example: gcc -c -g -O -Wa,-alh,-L file.c This passes two options to the assembler: -alh (emit a listing to standard output with high-level and assembly source) and -L (retain local symbols in the symbol table). Usually you do not need to use this -Wa mechanism, since many compiler command-line options are automatically passed to the assembler by the compiler. (You can call the GNU compiler driver with the -v option to see precisely what options it passes to each compilation pass, including the assembler.)","Process Name":"ia64-linux-gnu-as","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-as"}},{"Process":{"Description":"The C ++ and Java languages provide function overloading, which means that you can write many functions with the same name, providing that each function takes parameters of different types. In order to be able to distinguish these similarly named functions C ++ and Java encode them into a low-level assembler name which uniquely identifies each different version. This process is known as mangling. The c++filt [1] program does the inverse mapping: it decodes (demangles) low-level names into user-level names so that they can be read. Every alphanumeric word (consisting of letters, digits, underscores, dollars, or periods) seen in the input is a potential mangled name. If the name decodes into a C ++ name, the C ++ name replaces the low-level name in the output, otherwise the original word is output. In this way you can pass an entire assembler source file, containing mangled names, through c++filt and see the same source file containing demangled names. You can also use c++filt to decipher individual symbols by passing them on the command line: c++filt <symbol> If no symbol arguments are given, c++filt reads symbol names from the standard input instead. All the results are printed on the standard output. The difference between reading names from the command line versus reading names from the standard input is that command line arguments are expected to be just mangled names and no checking is performed to separate them from surrounding text. Thus for example: c++filt -n _Z1fv will work and demangle the name to \"f()\" whereas: c++filt -n _Z1fv, will not work. (Note the extra comma at the end of the mangled name which makes it invalid). This command however will work: echo _Z1fv, | c++filt -n and will display \"f(),\", i.e., the demangled name followed by a trailing comma. This behaviour is because when the names are read from the standard input it is expected that they might be part of an assembler source file where there might be extra, extraneous characters trailing after a mangled name. For example: .type   _Z1fv, @function","Process Name":"ia64-linux-gnu-c++filt","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-c++filt"}},{"Process":{"Description":"The C preprocessor, often known as cpp, is a macro processor that is used automatically by the C compiler to transform your program before compilation. It is called a macro processor because it allows you to define macros, which are brief abbreviations for longer constructs. The C preprocessor is intended to be used only with C, C ++ , and Objective-C source code. In the past, it has been abused as a general text processor. It will choke on input which does not obey C's lexical rules. For example, apostrophes will be interpreted as the beginning of character constants, and cause errors. Also, you cannot rely on it preserving characteristics of the input which are not significant to C-family languages. If a Makefile is preprocessed, all the hard tabs will be removed, and the Makefile will not work. Having said that, you can often get away with using cpp on things which are not C. Other Algol-ish programming languages are often safe (Pascal, Ada, etc.) So is assembly, with caution. -traditional-cpp mode preserves more white space, and is otherwise more permissive. Many of the problems can be avoided by writing C or C ++ style comments instead of native language comments, and keeping macros simple. Wherever possible, you should use a preprocessor geared to the language you are writing in. Modern versions of the GNU assembler have macro facilities. Most high level programming languages have their own conditional compilation and inclusion mechanism. If all else fails, try a true general text processor, such as GNU M4. C preprocessors vary in some details. This manual discusses the GNU C preprocessor, which provides a small superset of the features of ISO Standard C. In its default mode, the GNU C preprocessor does not do a few things required by the standard. These are features which are rarely, if ever, used, and may cause surprising changes to the meaning of a program which does not expect them. To get strict ISO Standard C, you should use the -std=c90, -std=c99 or -std=c11 options, depending on which version of the standard you want. To get all the mandatory diagnostics, you must also use -pedantic. This manual describes the behavior of the ISO preprocessor. To minimize gratuitous differences, where the ISO preprocessor's behavior does not conflict with traditional semantics, the traditional preprocessor should behave the same way. The various differences that do exist are detailed in the section Traditional Mode. For clarity, unless noted otherwise, references to CPP in this manual refer to GNU CPP .","Process Name":"ia64-linux-gnu-cpp","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-cpp"}},{"Process":{"Description":"dlltool reads its inputs, which can come from the -d and -b options as well as object files specified on the command line. It then processes these inputs and if the -e option has been specified it creates a exports file. If the -l option has been specified it creates a library file and if the -z option has been specified it creates a def file. Any or all of the -e, -l and -z options can be present in one invocation of dlltool. When creating a DLL , along with the source for the DLL , it is necessary to have three other files. dlltool can help with the creation of these files. The first file is a .def file which specifies which functions are exported from the DLL , which functions the DLL imports, and so on. This is a text file and can be created by hand, or dlltool can be used to create it using the -z option. In this case dlltool will scan the object files specified on its command line looking for those functions which have been specially marked as being exported and put entries for them in the .def file it creates. In order to mark a function as being exported from a DLL , it needs to have an -export:<name_of_function> entry in the .drectve section of the object file. This can be done in C by using the asm() operator: asm (\".section .drectve\");\nasm (\".ascii \\\"-export:my_func\\\"\");\n\nint my_func (void) { ... } The second file needed for DLL creation is an exports file. This file is linked with the object files that make up the body of the DLL and it handles the interface between the DLL and the outside world. This is a binary file and it can be created by giving the -e option to dlltool when it is creating or reading in a .def file. The third file needed for DLL creation is the library file that programs will link with in order to access the functions in the DLL (an 'import library'). This file can be created by giving the -l option to dlltool when it is creating or reading in a .def file. If the -y option is specified, dlltool generates a delay-import library that can be used instead of the normal import library to allow a program to link to the dll only as soon as an imported function is called for the first time. The resulting executable will need to be linked to the static delayimp library containing __delayLoadHelper2(), which in turn will import LoadLibraryA and GetProcAddress from kernel32. dlltool builds the library file by hand, but it builds the exports file by creating temporary files containing assembler statements and then assembling these. The -S command line option can be used to specify the path to the assembler that dlltool will use, and the -f option can be used to pass specific flags to that assembler. The -n can be used to prevent dlltool from deleting these temporary assembler files when it is done, and if -n is specified twice then this will prevent dlltool from deleting the temporary object files it used to build the library. Here is an example of creating a DLL from a source file dll.c and also creating a program (from an object file called program.o) that uses that DLL: gcc -c dll.c\ndlltool -e exports.o -l dll.lib dll.o\ngcc dll.o exports.o -o dll.dll\ngcc program.o dll.lib -o program dlltool may also be used to query an existing import library to determine the name of the DLL to which it is associated. See the description of the -I or --identify option.","Process Name":"ia64-linux-gnu-dlltool","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-dlltool"}},{"Process":{"Description":"elfedit updates the ELF header of ELF files which have the matching ELF machine and file types. The options control how and which fields in the ELF header should be updated. elffile... are the ELF files to be updated. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files.","Process Name":"ia64-linux-gnu-elfedit","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-elfedit"}},{"Process":{"Description":"When you invoke GCC , it normally does preprocessing, compilation, assembly and linking. The \"overall options\" allow you to stop this process at an intermediate stage. For example, the -c option says not to run the linker. Then the output consists of object files output by the assembler. Other options are passed on to one stage of processing. Some options control the preprocessor and others the compiler itself. Yet other options control the assembler and linker; most of these are not documented here, since you rarely need to use any of them. Most of the command-line options that you can use with GCC are useful for C programs; when an option is only useful with another language (usually C ++ ), the explanation says so explicitly. If the description for a particular option does not mention a source language, you can use that option with all supported languages. The gcc program accepts options and file names as operands. Many options have multi-letter names; therefore multiple single-letter options may not be grouped: -dv is very different from -d -v. You can mix options and other arguments. For the most part, the order you use doesn't matter. Order does matter when you use several options of the same kind; for example, if you specify -L more than once, the directories are searched in the order specified. Also, the placement of the -l option is significant. Many options have long names starting with -f or with -W---for example, -fmove-loop-invariants, -Wformat and so on. Most of these have both positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"ia64-linux-gnu-gcc","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-gcc"}},{"Process":{"Description":"gcov is a test coverage program. Use it in concert with GCC to analyze your programs to help create more efficient, faster running code and to discover untested parts of your program. You can use gcov as a profiling tool to help discover where your optimization efforts will best affect your code. You can also use gcov along with the other profiling tool, gprof, to assess which parts of your code use the greatest amount of computing time. Profiling tools help you analyze your code's performance. Using a profiler such as gcov or gprof, you can find out some basic performance statistics, such as: \u2022 how often each line of code executes \u2022 what lines of code are actually executed \u2022 how much computing time each section of code uses Once you know these things about how your code works when compiled, you can look at each module to see which modules should be optimized. gcov helps you determine where to work on optimization. Software developers also use coverage testing in concert with testsuites, to make sure software is actually good enough for a release. Testsuites can verify that a program works as expected; a coverage program tests to see how much of the program is exercised by the testsuite. Developers can then determine what kinds of test cases need to be added to the testsuites to create both better testing and a better final product. You should compile your code without optimization if you plan to use gcov because the optimization, by combining some lines of code into one function, may not give you as much information as you need to look for 'hot spots' where the code is using a great deal of computer time. Likewise, because gcov accumulates statistics by line (at the lowest resolution), it works best with a programming style that places only one statement on each line. If you use complicated macros that expand to loops or to other control structures, the statistics are less helpful---they only report on the line where the macro call appears. If your complex macros behave like functions, you can replace them with inline functions to solve this problem. gcov creates a logfile called sourcefile.gcov which indicates how many times each line of a source file sourcefile.c has executed. You can use these logfiles along with gprof to aid in fine-tuning the performance of your programs. gprof gives timing information you can use along with the information you get from gcov. gcov works only on code compiled with GCC . It is not compatible with any other profiling or test coverage mechanism.","Process Name":"ia64-linux-gnu-gcov","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-gcov"}},{"Process":{"Description":"\"gprof\" produces an execution profile of C, Pascal, or Fortran77 programs. The effect of called routines is incorporated in the profile of each caller. The profile data is taken from the call graph profile file (gmon.out default) which is created by programs that are compiled with the -pg option of \"cc\", \"pc\", and \"f77\". The -pg option also links in versions of the library routines that are compiled for profiling. \"Gprof\" reads the given object file (the default is \"a.out\") and establishes the relation between its symbol table and the call graph profile from gmon.out. If more than one profile file is specified, the \"gprof\" output shows the sum of the profile information in the given profile files. \"Gprof\" calculates the amount of time spent in each routine. Next, these times are propagated along the edges of the call graph. Cycles are discovered, and calls into a cycle are made to share the time of the cycle. Several forms of output are available from the analysis. The flat profile shows how much time your program spent in each function, and how many times that function was called. If you simply want to know which functions burn most of the cycles, it is stated concisely here. The call graph shows, for each function, which functions called it, which other functions it called, and how many times. There is also an estimate of how much time was spent in the subroutines of each function. This can suggest places where you might try to eliminate function calls that use a lot of time. The annotated source listing is a copy of the program's source code, labeled with the number of times each line of the program was executed.","Process Name":"ia64-linux-gnu-gprof","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-gprof"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"ia64-linux-gnu-ld","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-ld"}},{"Process":{"Description":"nlmconv converts the relocatable i386 object file infile into the NetWare Loadable Module outfile, optionally reading headerfile for NLM header information. For instructions on writing the NLM command file language used in header files, see the linkers section, NLMLINK in particular, of the NLM Development and Tools Overview, which is part of the NLM Software Developer's Kit (\" NLM SDK \"), available from Novell, Inc. nlmconv uses the GNU Binary File Descriptor library to read infile; nlmconv can perform a link step. In other words, you can list more than one object file for input if you list them in the definitions file (rather than simply specifying one input file on the command line). In this case, nlmconv calls the linker for you.","Process Name":"ia64-linux-gnu-nlmconv","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-nlmconv"}},{"Process":{"Description":"GNU nm lists the symbols from object files objfile.... If no object files are listed as arguments, nm assumes the file a.out. For each symbol, nm shows: \u2022 The symbol value, in the radix selected by options (see below), or hexadecimal by default. \u2022 The symbol type. At least the following types are used; others are, as well, depending on the object file format. If lowercase, the symbol is usually local; if uppercase, the symbol is global (external). There are however a few lowercase symbols that are shown for special global symbols (\"u\", \"v\" and \"w\"). \"A\" The symbol's value is absolute, and will not be changed by further linking. \"B\" \"b\" The symbol is in the uninitialized data section (known as BSS ). \"C\" The symbol is common. Common symbols are uninitialized data. When linking, multiple common symbols may appear with the same name. If the symbol is defined anywhere, the common symbols are treated as undefined references. \"D\" \"d\" The symbol is in the initialized data section. \"G\" \"g\" The symbol is in an initialized data section for small objects. Some object file formats permit more efficient access to small data objects, such as a global int variable as opposed to a large global array. \"i\" For PE format files this indicates that the symbol is in a section specific to the implementation of DLLs. For ELF format files this indicates that the symbol is an indirect function. This is a GNU extension to the standard set of ELF symbol types. It indicates a symbol which if referenced by a relocation does not evaluate to its address, but instead must be invoked at runtime. The runtime execution will then return the value to be used in the relocation. \"N\" The symbol is a debugging symbol. \"p\" The symbols is in a stack unwind section. \"R\" \"r\" The symbol is in a read only data section. \"S\" \"s\" The symbol is in an uninitialized data section for small objects. \"T\" \"t\" The symbol is in the text (code) section. \"U\" The symbol is undefined. \"u\" The symbol is a unique global symbol. This is a GNU extension to the standard set of ELF symbol bindings. For such a symbol the dynamic linker will make sure that in the entire process there is just one symbol with this name and type in use. \"V\" \"v\" The symbol is a weak object. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the weak symbol becomes zero with no error. On some systems, uppercase indicates that a default value has been specified. \"W\" \"w\" The symbol is a weak symbol that has not been specifically tagged as a weak object symbol. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the symbol is determined in a system-specific manner without error. On some systems, uppercase indicates that a default value has been specified. \"-\" The symbol is a stabs symbol in an a.out object file. In this case, the next values printed are the stabs other field, the stabs desc field, and the stab type. Stabs symbols are used to hold debugging information. \"?\" The symbol type is unknown, or object file format specific. \u2022 The symbol name.","Process Name":"ia64-linux-gnu-nm","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-nm"}},{"Process":{"Description":"The GNU objcopy utility copies the contents of an object file to another. objcopy uses the GNU BFD Library to read and write the object files. It can write the destination object file in a format different from that of the source object file. The exact behavior of objcopy is controlled by command-line options. Note that objcopy should be able to copy a fully linked file between any two formats. However, copying a relocatable object file between any two formats may not work as expected. objcopy creates temporary files to do its translations and deletes them afterward. objcopy uses BFD to do all its translation work; it has access to all the formats described in BFD and thus is able to recognize most formats without being told explicitly. objcopy can be used to generate S-records by using an output target of srec (e.g., use -O srec). objcopy can be used to generate a raw binary file by using an output target of binary (e.g., use -O binary). When objcopy generates a raw binary file, it will essentially produce a memory dump of the contents of the input object file. All symbols and relocation information will be discarded. The memory dump will start at the load address of the lowest section copied into the output file. When generating an S-record or a raw binary file, it may be helpful to use -S to remove sections containing debugging information. In some cases -R will be useful to remove sections which contain information that is not needed by the binary file. Note---objcopy is not able to change the endianness of its input files. If the input format has an endianness (some formats do not), objcopy can only copy the inputs into file formats that have the same endianness or which have no endianness (e.g., srec). (However, see the --reverse-bytes option.)","Process Name":"ia64-linux-gnu-objcopy","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-objcopy"}},{"Process":{"Description":"objdump displays information about one or more object files. The options control what particular information to display. This information is mostly useful to programmers who are working on the compilation tools, as opposed to programmers who just want their program to compile and work. objfile... are the object files to be examined. When you specify archives, objdump shows information on each of the member object files.","Process Name":"ia64-linux-gnu-objdump","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-objdump"}},{"Process":{"Description":"ranlib generates an index to the contents of an archive and stores it in the archive. The index lists each symbol defined by a member of an archive that is a relocatable object file. You may use nm -s or nm --print-armap to list this index. An archive with such an index speeds up linking to the library and allows routines in the library to call each other without regard to their placement in the archive. The GNU ranlib program is another form of GNU ar; running ranlib is completely equivalent to executing ar -s.","Process Name":"ia64-linux-gnu-ranlib","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-ranlib"}},{"Process":{"Description":"readelf displays information about one or more ELF format object files. The options control what particular information to display. elffile... are the object files to be examined. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files. This program performs a similar function to objdump but it goes into more detail and it exists independently of the BFD library, so if there is a bug in BFD then readelf will not be affected.","Process Name":"ia64-linux-gnu-readelf","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-readelf"}},{"Process":{"Description":"The GNU size utility lists the section sizes---and the total size---for each of the object or archive files objfile in its argument list. By default, one line of output is generated for each object file or each module in an archive. objfile... are the object files to be examined. If none are specified, the file \"a.out\" will be used.","Process Name":"ia64-linux-gnu-size","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-size"}},{"Process":{"Description":"For each file given, GNU strings prints the printable character sequences that are at least 4 characters long (or the number given with the options below) and are followed by an unprintable character. By default, it only prints the strings from the initialized and loaded sections of object files; for other types of files, it prints the strings from the whole file. strings is mainly useful for determining the contents of non-text files.","Process Name":"ia64-linux-gnu-strings","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-strings"}},{"Process":{"Description":"GNU strip discards all symbols from object files objfile. The list of object files may include archives. At least one object file must be given. strip modifies the files named in its argument, rather than writing modified copies under different names.","Process Name":"ia64-linux-gnu-strip","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-strip"}},{"Process":{"Description":"windmc reads message definitions from an input file (.mc) and translate them into a set of output files. The output files may be of four kinds: \"h\" A C header file containing the message definitions. \"rc\" A resource file compilable by the windres tool. \"bin\" One or more binary files containing the resource data for a specific message language. \"dbg\" A C include file that maps message id's to their symbolic name. The exact description of these different formats is available in documentation from Microsoft. When windmc converts from the \"mc\" format to the \"bin\" format, \"rc\", \"h\", and optional \"dbg\" it is acting like the Windows Message Compiler.","Process Name":"ia64-linux-gnu-windmc","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-windmc"}},{"Process":{"Description":"windres reads resources from an input file and copies them into an output file. Either file may be in one of three formats: \"rc\" A text format read by the Resource Compiler. \"res\" A binary format generated by the Resource Compiler. \"coff\" A COFF object or executable. The exact description of these different formats is available in documentation from Microsoft. When windres converts from the \"rc\" format to the \"res\" format, it is acting like the Windows Resource Compiler. When windres converts from the \"res\" format to the \"coff\" format, it is acting like the Windows \"CVTRES\" program. When windres generates an \"rc\" file, the output is similar but not identical to the format expected for the input. When an input \"rc\" file refers to an external filename, an output \"rc\" file will instead include the file contents. If the input or output format is not specified, windres will guess based on the file name, or, for the input file, the file contents. A file with an extension of .rc will be treated as an \"rc\" file, a file with an extension of .res will be treated as a \"res\" file, and a file with an extension of .o or .exe will be treated as a \"coff\" file. If no output file is specified, windres will print the resources in \"rc\" format to standard output. The normal use is for you to write an \"rc\" file, use windres to convert it to a COFF object file, and then link the COFF file into your application. This will make the resources described in the \"rc\" file available to Windows.","Process Name":"ia64-linux-gnu-windres","Link":"https:\/\/linux.die.net\/man\/1\/ia64-linux-gnu-windres"}},{"Process":{"Description":"This manual page documents briefly the iasl command. The option list is taken from the iasl interactive help. iasl is an ASL compiler and decompiler.","Process Name":"iasl","Link":"https:\/\/linux.die.net\/man\/1\/iasl"}},{"Process":{"Description":"ib_acme provides assistance configuring and testing the ibacm service. The first usage of the service will test that the ibacm is running and operating correctly. The second usage model will automatically create address and configuration files for the ibacm service.","Process Name":"ib_acme","Link":"https:\/\/linux.die.net\/man\/1\/ib_acme"}},{"Process":{"Description":"The IB ACM implements and provides a framework for name, address, and route (path) resolution services over InfiniBand. It is intended to address connection setup scalability issues running MPI applications on large clusters. The IB ACM provides information needed to establish a connection, but does not implement the CM protocol. A primary user of the ibacm service is the librdmacm library. This enables applications to make use of the ibacm service without code changes or needing to be aware that the service is in use. librdmacm versions 1.0.12 - 1.0.15 can invoke IB ACM services when built using the --with-ib_acm option. Version 1.0.16 and newer of librdmacm will automatically use the IB ACM if it is installed. The IB ACM services tie in under the rdma_resolve_addr, rdma_resolve_route, and rdma_getaddrinfo routines. For maximum benefit, the rdma_getaddrinfo routine should be used, however existing applications should still see significant connection scaling benefits using the calls available in librdmacm 1.0.11 and previous releases. The IB ACM is focused on being scalable and efficient. The current implementation limits network traffic, SA interactions, and centralized services. ACM supports multiple resolution protocols in order to handle different fabric topologies. The IB ACM package is comprised of two components: the ibacm service and a test\/configuration utility - ib_acme. Both are userspace components and are available for Linux and Windows. Additional details are given below.","Process Name":"ibacm","Link":"https:\/\/linux.die.net\/man\/1\/ibacm"}},{"Process":{"Description":"ibdiagnet scans the fabric using directed route packets and extracts all the available information regarding its connectivity and devices. It then produces the following files in the output directory (which defined by the -o option (see below)).","Process Name":"ibdiagnet","Link":"https:\/\/linux.die.net\/man\/1\/ibdiagnet"}},{"Process":{"Description":"ibdiagpath traces a path between two end-points and provides information regarding the nodes and ports traversed along the path. It utilizes device specific health queries for the different devices along the traversed path. The way ibdiagpath operates depends on the addressing mode used on the command line. If directed route adressing is used, the local node is the source node and the route to the destination port is known apriori. On the other hand, if LID route (or by-name) addressing is imployed, then the source and destination ports of a route are specified by their LIDs (or by the names defined in the topology file). In this case, the actual path from the local port to the source port, and from the source port to the destination port, is defined by means of Subnet Management Linear Forwarding Table queries of the switch nodes along those paths. Therefore, the path cannot be predicted as it may change. The tool allows omitting the source node, in which case the local port on the machine running the tool is assumed to be the source. Note: When ibdiagpath queries for the performance counters along the path between the source and destination ports, it always traverses the LID route, even if a directed route is specified. If along the LID route one or more links are not in the ACTIVE state, ibdiagpath reports an error. Checks for path validity from partitions, IPoIB and QoS perspectives are also provided. ibdiagpath.log - A dump of all the application reports generate according to the provided flags ibdiagnet.pm - A dump of the pm Counters values, of the fabric links","Process Name":"ibdiagpath","Link":"https:\/\/linux.die.net\/man\/1\/ibdiagpath"}},{"Process":{"Description":"ibdiagui is a GUI wrapper for ibdiagnet.\nIts main features: 1. Display a graph of the discovered fabric (with optional names annotattion) 2. Hyperlink the ibdiagnet log to the graph objects 3. Show each object properties and object type specific actions on a properties pannel.","Process Name":"ibdiagui","Link":"https:\/\/linux.die.net\/man\/1\/ibdiagui"}},{"Process":{"Description":"IBDM topology file ibdm-topo-file provide means to describe the IB fabric using a set of predefined systems. A system definition is provided in a single file in IBNL format that describes the internal InfiniBand connectivity of the system in terms of boards and devices. When IBDM starts it parses all the available system definition files before it handles the topology file. The files are located in the following directory relative to the installation prefix: <prefix>\/lib\/ibdm1.0\/ibnl. This man page describes the IBNL file format used to define an arbitrary IB system internal IB connectivity. It outlines the main concepts used by the file, provides details for how to write such a file and provides a formal definition of the file syntax in BNF like format (YACC\/Bison readable).","Process Name":"ibdm-ibnl-file","Link":"https:\/\/linux.die.net\/man\/1\/ibdm-ibnl-file"}},{"Process":{"Description":"","Process Name":"ibdm-topo-file","Link":"https:\/\/linux.die.net\/man\/1\/ibdm-topo-file"}},{"Process":{"Description":"","Process Name":"ibdmchk","Link":"https:\/\/linux.die.net\/man\/1\/ibdmchk"}},{"Process":{"Description":"ibdmsh is a TCL shell extended with interface for the IB data model. To use this shell you will write TCL code that directly access the IB data model objects and functions. The following sub sections provide detailed definition for those objects and API .","Process Name":"ibdmsh","Link":"https:\/\/linux.die.net\/man\/1\/ibdmsh"}},{"Process":{"Description":"This utility parses a cabling list or topology file describing the systems connections that make a fabric. Then it start following the direct route provided and print out the systems and nodes on the route.","Process Name":"ibdmtr","Link":"https:\/\/linux.die.net\/man\/1\/ibdmtr"}},{"Process":{"Description":"ibis is a TCL shell extended with interface for sending and receiving IB management datagrams ( MADS ). To use this shell you will write TCL code that excersizes the regular TCL command and the special API provided by this extension. Interactive use is also possible and is greatly enhanced if tclreadline package is available on the machine. The following sub sections provide detailed definition for the API and global objects defined by the extension. The different MADs APIs are group by the management class.","Process Name":"ibis","Link":"https:\/\/linux.die.net\/man\/1\/ibis"}},{"Process":{"Description":"The ibm_hosts file contains information regarding IBM hosts on the network. An IBM host is a host which can communicate with a 3270 terminal emulator such as x3270 or c3270. Each line defines a name in the following (optional fields are shown in brackets): name type [opt:]...[luname@]hostname[:port] [actions] Items are separated by any number of blanks and\/or TAB characters. A line beginning with # is taken as a comment (note that # anywhere else on a line does not indicate a comment). The name field is a mnemonic used to identify the host. The type field is a keyword that indicates the type of entry. The value primary means that the name will be included in host-selection menus that may be displayed by a 3270 emulator. The value alias means that the name will not be included in menus, but will still be accepted as valid input when a host name is required. The hostname field is the Internet hostname or dot-notation Internet address of the host. The hostname can include 's:' or 'p:' prefixes, e.g., s:finicky (see the x3270(1) or c3270(1) man page sfor details). It can also include an LU name, separated by an '@' character, e.g., oddlu@bluehost. Finally, it can include a non-default port number, appended to the hostname with a colon ':' character, e.g., bluehost:97. (For compatability with earlier versions of x3270, the port can also be separated by a slash '\/' character.) The optional actions field specifies actions to be taken once the connection is made and a data-entry field is defined. If the text looks like an action, e.g., PF(1), it is unmodified; otherwise it is taken as the parameter to the String() action. The actions are not intended for entering usernames and passwords; rather they provide an automated way of specifying a front-end menu option.","Process Name":"ibm_hosts","Link":"https:\/\/linux.die.net\/man\/1\/ibm_hosts"}},{"Process":{"Description":"Simulates the Fabric as defined by the given topology file and start a server to handle MAD requests from clients like OpenSM.","Process Name":"ibmgtsim","Link":"https:\/\/linux.die.net\/man\/1\/ibmgtsim"}},{"Process":{"Description":"Causes the IB management simulator at the given host and port to quit","Process Name":"ibmsquit","Link":"https:\/\/linux.die.net\/man\/1\/ibmsquit"}},{"Process":{"Description":"ibmssh is a TCL shell extended with interface for the IB Management Simulator facilities. Normally you will not run this shell directly - but instead it will be invoked by the IBMgtSim utility. However, in order to write simulation flows that interacts closely with the simulated model, a simulation flow should be provided. This flow is loaded by IBMgtSim and excersize the API provided by this shell. The following sub sections provide detailed definition for those objects and API .","Process Name":"ibmssh","Link":"https:\/\/linux.die.net\/man\/1\/ibmssh"}},{"Process":{"Description":"Ibod is a ISDN MPPP bandwidth on demand daemon designed to operate in conjunction with isdn4linux. It is normally started at boot time, but can be started and stopped at any time. The program monitors inbound and outbound traffic on the ISDN interface. When the required bandwidth exceeds the capacity for one IDSN B-channel (64kbps) a second (slave) channel is connected according to the MPPP protocol. When the traffic decreases below one channel capacity, the slave channel is disconnected. A configuration file \/etc\/ppp\/ibod.cf is read initially. The device \/dev\/isdninfo is monitored for the current state and throughput of the ISDN interface. All operation on the ISDN interface is made through the \/dev\/isdnctrl interface. Ibod is also listening on TCP port 6050 for eventual connection from control panel","Process Name":"ibod","Link":"https:\/\/linux.die.net\/man\/1\/ibod"}},{"Process":{"Description":"List InfiniBand SCSI RDMA Protocol (SRP) targets on an IB fabric.","Process Name":"ibsrpdm","Link":"https:\/\/linux.die.net\/man\/1\/ibsrpdm"}},{"Process":{"Description":"This utility performs matching between a given topology file (see man ibdm-topology-file) and a subnet.lst file (this file provides a dump of all links in the discovered fabric and generated by ibdiagnet or OpenSM).","Process Name":"ibtopodiff","Link":"https:\/\/linux.die.net\/man\/1\/ibtopodiff"}},{"Process":{"Description":"Display asynchronous events forwarded to userspace for an RDMA device.","Process Name":"ibv_asyncwatch","Link":"https:\/\/linux.die.net\/man\/1\/ibv_asyncwatch"}},{"Process":{"Description":"List RDMA devices available for use from userspace.","Process Name":"ibv_devices","Link":"https:\/\/linux.die.net\/man\/1\/ibv_devices"}},{"Process":{"Description":"Print information about RDMA devices available for use from userspace.","Process Name":"ibv_devinfo","Link":"https:\/\/linux.die.net\/man\/1\/ibv_devinfo"}},{"Process":{"Description":"Run a simple ping-pong test over InfiniBand via the reliable connected (RC) transport.","Process Name":"ibv_rc_pingpong","Link":"https:\/\/linux.die.net\/man\/1\/ibv_rc_pingpong"}},{"Process":{"Description":"Run a simple ping-pong test over InfiniBand via the reliable connected (RC) transport, using multiple queue pairs (QPs) and a single shared receive queue (SRQ).","Process Name":"ibv_srq_pingpong","Link":"https:\/\/linux.die.net\/man\/1\/ibv_srq_pingpong"}},{"Process":{"Description":"Run a simple ping-pong test over InfiniBand via the reliable connected (RC) transport.","Process Name":"ibv_uc_pingpong","Link":"https:\/\/linux.die.net\/man\/1\/ibv_uc_pingpong"}},{"Process":{"Description":"Run a simple ping-pong test over InfiniBand via the unreliable datagram (UD) transport.","Process Name":"ibv_ud_pingpong","Link":"https:\/\/linux.die.net\/man\/1\/ibv_ud_pingpong"}},{"Process":{"Description":"lcms is a standalone CMM engine, which deals with the color management. It implements a fast transformation between ICC profiles. icc2ps is little cms PostScript converter.","Process Name":"icc2ps","Link":"https:\/\/linux.die.net\/man\/1\/icc2ps"}},{"Process":{"Description":"lcms is a standalone CMM engine, which deals with the color management. It implements a fast transformation between ICC profiles. icclink is little cms device link generator. Links two or more profiles into a single devicelink profile. Colorspaces must be paired except Lab\/XYZ, that can be interchanged.","Process Name":"icclink","Link":"https:\/\/linux.die.net\/man\/1\/icclink"}},{"Process":{"Description":"ice-gdb (respectively ice-insight) starts avr-gdb (respectively avr-insight) and avarice to do on-chip debugging on Atmel AVR processors using their JTAG-based on-chip debug facilities, via the Atmel JTAG ICE pod. The executable for the program being debugged is downloaded to the processor unless the --capture option is specified. A number of extra commands are available within gdb when started, see their description in NEW GDB COMMANDS below. Restrictions on debugging with the JTAG ICE pod are discussed in more detail in DEBUGGING WITH JTAG ICE below. The arguments passed to avarice include the value of the AVARICE_ARGS environment variable. This is typically used to specify the serial port to which the JTAG ICE pod is connected by setting AVARICE_ARGS to \"-j <serial port name>\".","Process Name":"ice-gdb","Link":"https:\/\/linux.die.net\/man\/1\/ice-gdb"}},{"Process":{"Description":"ice-gdb (respectively ice-insight) starts avr-gdb (respectively avr-insight) and avarice to do on-chip debugging on Atmel AVR processors using their JTAG-based on-chip debug facilities, via the Atmel JTAG ICE pod. The executable for the program being debugged is downloaded to the processor unless the --capture option is specified. A number of extra commands are available within gdb when started, see their description in NEW GDB COMMANDS below. Restrictions on debugging with the JTAG ICE pod are discussed in more detail in DEBUGGING WITH JTAG ICE below. The arguments passed to avarice include the value of the AVARICE_ARGS environment variable. This is typically used to specify the serial port to which the JTAG ICE pod is connected by setting AVARICE_ARGS to \"-j <serial port name>\".","Process Name":"ice-insight","Link":"https:\/\/linux.die.net\/man\/1\/ice-insight"}},{"Process":{"Description":"The iceauth program is used to edit and display the authorization information used in connecting with ICE. This program is usually used to extract authorization records from one machine and merge them in on another (as is the case when using remote logins or granting access to other users). Commands (described below) may be entered interactively, on the iceauth command line, or in scripts.","Process Name":"iceauth","Link":"https:\/\/linux.die.net\/man\/1\/iceauth"}},{"Process":{"Description":"icecast is an audio broadcasting system that streams music in Ogg Vorbis and\/or MPEG 1 Layer III format. It accepts stream input from sources like ices0 and ices2, and broadcasts to clients like XMMS.","Process Name":"icecast","Link":"https:\/\/linux.die.net\/man\/1\/icecast"}},{"Process":{"Description":"icecc is the Icecream compiler stub. It gets called in place of the actual compiler and transparently routes the compile requests to the Icecream network. You shouldn't call icecc directly, but place the specific compiler stubs in your path: export PATH=\/opt\/icecream\/bin:$PATH.","Process Name":"icecc","Link":"https:\/\/linux.die.net\/man\/1\/icecc"}},{"Process":{"Description":"The Icecream scheduler is the central instance of an Icecream compile network. It distributes the compile jobs and provides the data for the monitors.","Process Name":"icecc-scheduler","Link":"https:\/\/linux.die.net\/man\/1\/icecc-scheduler"}},{"Process":{"Description":"The Icecream daemon has to run on all nodes being part of the Icecream compile cluster. It receives compile jobs and executes them in a chroot environment. The compile clients send their compile environment the first time they send a job to a particular daemon, so that the environment of the daemon doesn't have to match the one of the client. The daemon also has to run on clients sending compile jobs to the Icecream network. If a node should be able to send compile jobs, but never receive any, start the daemon with the option -m 0. All Icecream daemons need to have contact to the Icecream scheduler which controls the distribution of data between compile nodes. Normally the daemon will automatically find the right scheduler. If this is not the case you can explicitly specify the name of the Icecream network and the host running the scheduler.","Process Name":"iceccd","Link":"https:\/\/linux.die.net\/man\/1\/iceccd"}},{"Process":{"Description":"icedax stands for InCrEdible Digital Audio eXtractor. It can retrieve audio tracks (CDDA) from CDROM drives that are capable of reading audio data digitally to the host (see README for a list of drives).","Process Name":"icedax","Link":"https:\/\/linux.die.net\/man\/1\/icedax"}},{"Process":{"Description":"IceWM is lightweight X11 window manager. The goal of IceWM is to provide a small, fast and familiar window manager for the X11 window system. Compatibility with the window manager is desired and will be implemented where appropriate. It was originally designed to emulate the look of Motif, OS\/2 Warp 4, OS\/2 Warp 3 and Windows 95. Since it has a theming engine (hint: http:\/\/www.icewm.org\/) others styles are possible. It also tries to combine the feel of the above systems whenever it is compatible. Generally, it tries to make all functions available both by keyboard and by mouse (this is not currently possible when using mouse focus). Extreme configurability similar to fvwm and many other window managers is NOT the goal. However IceWM configurability is very good throught its various preferences files. IceWM consists of several parts: icewm - the actual window manager binary. This is the one you need to get window decorations. icewmbg - the background setting applications. It can assign plain background color or images in different formats to the X background, shared or separated for different workspaces. This program should be started before IceWM startup. icewmtray - catches the Docklet objects installed by various applications like PSI. icewm-session - runs all of the above when needed. Implements basic session management. icesh - could be used to manage IceWM internals from command line. icewmhint - used internaly. icesound - plays audio files on GUI events raised by IceWM.","Process Name":"icewm","Link":"https:\/\/linux.die.net\/man\/1\/icewm"}},{"Process":{"Description":"Icmpinfo is a tool for looking at the ICMP messages received on the running host. It can be used to detect and record 'bombs' as well as various network problems. The output format is as follows (all on one line): MMM DD HH:MM:SS ICMP_type[sub-type] < sender_ip [sender_name] > unreach_ip [unreach_name] sp=source_port dp=dest_port seq=sequence sz=packet_size In normal operation, icmpinfo will only report on \"weird\" packets, mainly icmp_unreachable.","Process Name":"icmpinfo","Link":"https:\/\/linux.die.net\/man\/1\/icmpinfo"}},{"Process":{"Description":"Ico displays a wire-frame rotating polyhedron, with hidden lines removed, or a solid-fill polyhedron with hidden faces removed. There are a number of different polyhedra available; adding a new polyhedron to the program is quite simple.","Process Name":"ico","Link":"https:\/\/linux.die.net\/man\/1\/ico"}},{"Process":{"Description":"This program is part of Netpbm(1). icontopbm reads a Sun icon as input and produces a PBM image as output.","Process Name":"icontopbm","Link":"https:\/\/linux.die.net\/man\/1\/icontopbm"}},{"Process":{"Description":"The iconv program converts the encoding of characters in inputfile from one coded character set to another. The result is written to standard output unless otherwise specified by the --output option. --from-code, -f encoding Convert characters from encoding --to-code, -t encoding Convert characters to encoding --list List known coded character sets --output, -o file Specify output file (instead of stdout) --verbose Print progress information.","Process Name":"iconv","Link":"https:\/\/linux.die.net\/man\/1\/iconv"}},{"Process":{"Description":"This program is part of the OpenImageIO (http:\/\/www.openimageio.org) tool suite. Detailed documentation is avaliable in pdf format with the OpenImageIO distribution.","Process Name":"iconvert","Link":"https:\/\/linux.die.net\/man\/1\/iconvert"}},{"Process":{"Description":"This manual page document describes the icotool command. The icotool program converts and creates icon (.ico) and cursor (.cur) files. At the moment icons can only be created from and extracted into PNG files. This is done using libpng. Icon and cursor files are used mainly on the Microsoft Windows(R) platform. Each icons or cursors file may contain multiple images of various resolutions and with different number of colors. Cursor files differ from icon files in that they also contain information about the hotspot of each image. Recent versions of Microsoft's Internet Explorer use icons for small site logotypes. The browser fetches a file called favicon.ico from a web site, and uses the images in this file to represent the site in menus and site lists. (This file is placed in the web site's root directory, like any other file.) Browsers like Galeon have copied this behaviour and now also fetches .ico files and use them for site logotypes. As each icon or cursor file may contains multiple images of different dimensions and depth, a conversion may result in multiple PNG files being created. Correspondingly, multiple PNG files can be specified when creating an icon\/cursor file.","Process Name":"icotool","Link":"https:\/\/linux.die.net\/man\/1\/icotool"}},{"Process":{"Description":"ictrans is the user interface to the Computer Graphics Metafile ( CGM ) translator ctrans. ictrans will enter command interpreter mode upon invocation and await instructions from the user. When waiting for commands from the user, ictrans displays the prompt 'ictrans>'. Upon invocation ictrans performs a configuration of its spooled device table. The table is configured by processing several sources. ictrans first searches for the file ncarv_spool in the $NCARG_ROOT\/lib\/ncarg directory. If the file exists ictrans will load it into the spooled device table. Next, ictrans searches for the file .ncarv_spool in the user's home directory. If found, its contents are merged into the spooler table. Finally, the NCARV_SPOOL environment variable may contain the definition for a single spooler. If this variable is set, its contents also are merged into the table. Each entry in the above set of sources is identified by a name. If a conflict in names exists then the last entry encountered takes precedence. i.e. the previous entry of the same name is overridden.","Process Name":"ictrans","Link":"https:\/\/linux.die.net\/man\/1\/ictrans"}},{"Process":{"Description":"icu-config simplifies the task of building and linking against ICU as compared to manually configuring user makefiles or equivalent. Because icu-config is an executable script, it also solves the problem of locating the ICU libraries and headers, by allowing the system PATH to locate it.","Process Name":"icu-config","Link":"https:\/\/linux.die.net\/man\/1\/icu-config"}},{"Process":{"Description":"Print user and group information for the specified USERNAME, or (when USERNAME omitted) for the current user. -a ignore, for compatibility with other versions -Z, --context print only the security context of the current user -g, --group print only the effective group ID -G, --groups print all group IDs -n, --name print a name instead of a number, for -ugG -r, --real print the real ID instead of the effective ID, with -ugG -u, --user print only the effective user ID --help display this help and exit --version output version information and exit Without any OPTION, print some useful set of identified information.","Process Name":"id","Link":"https:\/\/linux.die.net\/man\/1\/id"}},{"Process":{"Description":"ident searches for all instances of the pattern $keyword: text $ in the named files or, if no files are named, the standard input. These patterns are normally inserted automatically by the RCS command co(1), but can also be inserted manually. The option -q suppresses the warning given if there are no patterns in a file. The option -V prints ident's version number. ident works on text files as well as object files and dumps. For example, if the C program in f.c contains #include <stdio.h> static char const rcsid[] = \"$Id: f.c,v 5.4 1993\/11\/09 17:40:15 eggert Exp $\"; int main() { return printf(\"%s\\n\", rcsid) == EOF; } and f.c is compiled into f.o, then the command ident f.c f.o will output        f.c:\n           $Id: f.c,v 5.4 1993\/11\/09 17:40:15 eggert Exp $\n       f.o:\n           $Id: f.c,v 5.4 1993\/11\/09 17:40:15 eggert Exp $\n If a C program defines a string like rcsid above but does not use it, lint(1) may complain, and some C compilers will optimize away the string. The most reliable solution is to have the program use the rcsid string, as shown in the example above. ident finds all instances of the $keyword: text $ pattern, even if keyword is not actually an RCS -supported keyword. This gives you information about nonstandard keywords like $XConsortium$.","Process Name":"ident","Link":"https:\/\/linux.die.net\/man\/1\/ident"}},{"Process":{"Description":"Image Settings: -antialias remove pixel-aliasing -authenticate value decrypt image with this password -channel type apply option to select image channels -crop geometry cut out a rectangular region of the image -define format:option define one or more image format options -define unique=true return the number of unique colors in the image -density geometry horizontal and vertical density of the image -depth value image depth -extract geometry extract area from image -format \"string\" output formatted image characteristics -fuzz distance colors within this distance are considered equal -interlace type type of image interlacing scheme -interpolate method pixel color interpolation method -limit type value pixel cache resource limit -list type Color, Configure, Delegate, Format, Magic, Module, Resource, or Type -matte store matte channel if the image has one -monitor monitor progress -ping efficiently determine image attributes -quiet suppress all warning messages -regard-warnings pay attention to warning messages -sampling-factor geometry horizontal and vertical sampling factor -seed value seed a new sequence of pseudo-random numbers -set attribute value set an image attribute -size geometry width and height of image -strip strip image of all profiles and comments -units type the units of image resolution -verbose print detailed information about the image -virtual-pixel method virtual pixel access method Miscellaneous Options: -debug events display copious debugging information -help print program options -log format format of debugging information -list type print a list of supported option arguments -version print version information By default, the image format of 'file' is determined by its magic number. To specify a particular image format, precede the filename with an image format name and a colon (i.e. ps:image) or specify the image type as the filename suffix (i.e. image.ps). Specify 'file' as '-' for standard input or output.","Process Name":"identify","Link":"https:\/\/linux.die.net\/man\/1\/identify"}},{"Process":{"Description":"This program is part of the OpenImageIO (http:\/\/www.openimageio.org) tool suite. Detailed documentation is avaliable in pdf format with the OpenImageIO distribution.","Process Name":"idiff","Link":"https:\/\/linux.die.net\/man\/1\/idiff"}},{"Process":{"Description":"idl2wrs is a program that takes a user specified CORBA IDL file and generates \"C\" source code for an Wireshark \"plugin\". This resulting file can be compiled as an Wireshark plugin, and used to monitor GIOP\/IIOP traffic that is using this IDL . idl2wrs is actually a shell script wrapper for two Python programs. These programs are: wireshark_be.py - Contains the main IDL Visitor Class wireshark_gen.py - Contains the Source Code Generator Class idl2wrs supports heuristic dissection of GIOP\/IIOP traffic, and some experimental code for explicit dissection, based on Object Key <-> Repository Id mapping. However, code for heuristic based plugins is generated by default, and users should consider this the preferred method unless you have some namespace collisions.","Process Name":"idl2wrs","Link":"https:\/\/linux.die.net\/man\/1\/idl2wrs"}},{"Process":{"Description":"The IDL-to-Java Compiler generates the Java bindings for a given IDL file. For binding details, see the OMG IDL to Java Language Language Mapping Specification. Some previous releases of the IDL-to-Java compiler were named idltojava. Emitting Client and Server Bindings To generate Java bindings for an IDL file named My.idl: idlj My.idl This generates the client-side bindings and is equivalent to: idlj -fclient My.idl The client-side bindings do not include the server-side skeleton. If you want to generate the server-side bindings for the interfaces: idlj -fserver My.idl Server-side bindings include the client-side bindings plus the skeleton, all of which are POA (that is, Inheritance Model) classes. If you want to generate both client and server-side bindings, use one of the following (equivalent) commands: idlj -fclient -fserver My.idl idlj -fall My.idl There are two possible server-side models: the Inheritance Model and the Tie Delegation Model. The default server-side model is the Portable Servant Inheritance Model. Given an interface My defined in My.idl, the file MyPOA.java is generated. You must provide the implementation for My and it must inherit from MyPOA. MyPOA.java is a stream-based skeleton that extends org.omg.PortableServer.Servant and implements the InvokeHandler interface and the operations interface associated with the IDL interface the skeleton implements. The PortableServer module for the Portable Object Adapter (POA) defines the native Servant type. In the Java programming language, the Servant type is mapped to the Java org.omg.PortableServer.Servant class. It serves as the base class for all POA servant implementations and provides a number of methods that may be invoked by the application programmer, as well as methods which are invoked by the POA itself and may be overridden by the user to control aspects of servant behavior. Another option for the Inheritance Model is to use the -oldImplBase flag in order to generate server-side bindings that are compatible with versions of the Java programming language prior to J2SE 1.4. Note that using the -oldImplBase flag is non-standard: these APIs are being deprecated. You would use this flag ONLY for compatibility with existing servers written in J2SE 1.3. In that case, you would need to modify an existing MAKEFILE to add the -oldImplBase flag to the idlj compiler, otherwise POA-based server-side mappings will be generated. To generate server-side bindings that are backwards compatible: idlj -fclient -fserver -oldImplBase My.idl idlj -fall -oldImplBase My.idl Given an interface My defined in My.idl, the file _MyImplBase.java is generated. You must provide the implementation for My and it must inherit from _MyImplBase. The other server-side model is called the Tie Model. This is a delegation model. Because it is not possible to generate ties and skeletons at the same time, they must be generated separately. The following commands generate the bindings for the Tie Model: idlj -fall My.idl idlj -fallTIE My.idl For the interface My, the second command generates MyPOATie.java. The constructor to MyPOATie takes a delegate. In this example, using the default POA model, the constructor also needs a poa. You must provide the implementation for delegate, but it does not have to inherit from any other class, only the interface MyOperations. But to use it with the ORB, you must wrap your implementation within MyPOATie. For instance:     ORB orb = ORB.init(args, System.getProperties());\n\n   \/\/ Get reference to rootpoa & activate the POAManager\n    POA rootpoa = (POA)orb.resolve_initial_references(\"RootPOA\");\n    rootpoa.the_POAManager().activate();\n\n   \/\/ create servant and register it with the ORB\n    MyServant myDelegate = new MyServant();\n    myDelegate.setORB(orb);\n\n   \/\/ create a tie, with servant being the delegate.\n    MyPOATie tie = new MyPOATie(myDelegate, rootpoa);\n\n   \/\/ obtain the objectRef for the tie\n    My ref = tie._this(orb);\n\n You might want to use the Tie model instead of the typical Inheritance model if your implementation must inherit from some other implementation. Java allows any number of interface inheritance, but there is only one slot for class inheritance. If you use the inheritance model, that slot is used up . By using the Tie Model, that slot is freed up for your own use. The drawback is that it introduces a level of indirection: one extra method call occurs when invoking a method. To generate server-side, Tie model bindings that are compatible with versions of the IDL to Java language mapping in versions prior to J2SE 1.4. idlj -oldImplBase -fall My.idl idlj -oldImplBase -fallTIE My.idl For the interface My, this will generate My_Tie.java. The constructor to My_Tie takes a impl. You must provide the implementation for impl, but it does not have to inherit from any other class, only the interface HelloOperations. But to use it with the ORB, you must wrap your implementation within My_Tie. For instance:     ORB orb = ORB.init(args, System.getProperties());\n\n   \/\/ create servant and register it with the ORB\n    MyServant myDelegate = new MyServant();\n    myDelegate.setORB(orb);\n\n   \/\/ create a tie, with servant being the delegate.\n    MyPOATie tie = new MyPOATie(myDelegate);\n\n   \/\/ obtain the objectRef for the tie\n    My ref = tie._this(orb);\n\n Specifying Alternate Locations for Emitted Files If you want to direct the emitted files to a directory other than the current directory, invoke the compiler as: idlj -td \/altdir My.idl For the interface My, the bindings will be emitted to \/altdir\/My.java, etc., instead of .\/My.java. Specifying Alternate Locations for Include Files If My.idl included another idl file, MyOther.idl, the compiler assumes that MyOther.idl resides in the local directory. If it resides in \/includes, for example, then you would invoke the compiler with the following command: idlj -i \/includes My.idl If My.idl also included Another.idl that resided in \/moreIncludes, for example, then you would invoke the compiler with the following command: idlj -i \/includes -i \/moreIncludes My.idl Since this form of include can become irritatingly long, another means of indicating to the compiler where to search for included files is provided. This technique is similar to the idea of an environment variable. Create a file named idl.config in a directory that is listed in your CLASSPATH. Inside of idl.config, provide a line with the following form: includes=\/includes;\/moreIncludes The compiler will find this file and read in the includes list. Note that in this example the separator character between the two directories is a semicolon (;). This separator character is platform dependent. On the Windows platform, use a semicolon, on the Unix platform, use a colon, etc. For more information on includes, read the CLASSPATH (Solaris) or CLASSPATH (Windows) documentation. Emitting Bindings for Include Files By default, only those interfaces, structs, etc, that are defined in the idl file on the command line have Java bindings generated for them. The types defined in included files are not generated. For example, assume the following two idl files: My.idl #include <MyOther.idl> interface My { }; MyOther.idl interface MyOther { }; The following command will only generate the java bindings for My: idlj My.idl To generate all of the types in My.idl and all of the types in the files that My.idl includes (in this example, MyOther.idl), use the following command: idlj -emitAll My.idl There is a caveat to the default rule. #include statements which appear at global scope are treated as described. These #include statements can be thought of as import statements. #include statements which appear within some enclosing scope are treated as true #include statements, meaning that the code within the included file is treated as if it appeared in the original file and, therefore, Java bindings are emitted for it. Here is an example: My.idl #include <MyOther.idl> interface My { #include <Embedded.idl> }; MyOther.idl interface MyOther { }; Embedded.idl enum E {one, two, three}; Running the following command: idlj My.idl will generate the following list of Java files: .\/MyHolder.java Notice that MyOther.java was not generated because it is defined in an import-like #include. But E.java was generated because it was defined in a true #include. Also notice that since Embedded.idl was included within the scope of the interface My, it appears within the scope of My (that is,in MyPackage). If the -emitAll flag had been used in the previous example, then all types in all included files would be emitted. Inserting Package Prefixes Suppose that you work for a company named ABC that has constructed the following IDL file: Widgets.idl module Widgets { interface W1 {...}; interface W2 {...}; }; Running this file through the IDL-to-Java compiler will place the Java bindings for W1 and W2 within the package Widgets. But there is an industry convention that states that a company's packages should reside within a package named com.<company name>. The Widgets package is not good enough. To follow convention, it should be com.abc.Widgets. To place this package prefix onto the Widgets module, execute the following: idlj -pkgPrefix Widgets com.abc Widgets.idl If you have an IDL file which includes Widgets.idl, the -pkgPrefix flag must appear in that command also. If it does not, then your IDL file will be looking for a Widgets package rather than a com.abc.Widgets package. If you have a number of these packages that require prefixes, it might be easier to place them into the idl.config file described above. Each package prefix line should be of the form: PkgPrefix.<type>=<prefix> So the line for the above example would be: PkgPrefix.Widgets=com.abc The use of this option does not affect the Repository ID. Defining Symbols Before Compilation You may need to define a symbol for compilation that is not defined within the IDL file, perhaps to include debugging code in the bindings. The command idlj -d MYDEF My.idl is the equivalent of putting the line #define MYDEF inside My.idl. Preserving Pre-Existing Bindings If the Java binding files already exist, the -keep flag will keep the compiler from overwriting them. The default is to generate all files without considering if they already exist. If you've customized those files (which you should not do unless you are very comfortable with their contents), then the -keep option is very useful. The command idlj -keep My.idl emit all client-side bindings that do not already exist. Viewing Progress of Compilation The IDL-to-Java compiler will generate status messages as it progresses through its phases of execution. Use the -v option to activate this \"verbose\" mode: idlj -v My.idl By default the compiler does not operate in verbose mode. Displaying Version Information To display the build version of the IDL-to-Java compiler, specify the -version option on the command-line: idlj -version Version information also appears within the bindings generated by the compiler. Any additional options appearing on the command-line are ignored.","Process Name":"idlj-java-1.6.0-openjdk","Link":"https:\/\/linux.die.net\/man\/1\/idlj-java-1.6.0-openjdk"}},{"Process":{"Description":"The IDL-to-Java Compiler generates the Java bindings for a given IDL file. For binding details, see the OMG IDL to Java Language Language Mapping Specification @ http:\/\/docs.oracle.com\/javase\/7\/docs\/technotes\/guides\/idl\/mapping\/jidlMapping.html. Some previous releases of the IDL-to-Java compiler were named idltojava. Emitting Client and Server Bindings To generate Java bindings for an IDL file named My.idl: idlj My.idl\n\n This generates the client-side bindings and is equivalent to: idlj -fclient My.idl The client-side bindings do not include the server-side skeleton. If you want to generate the server-side bindings for the interfaces: idlj -fserver My.idl Server-side bindings include the client-side bindings plus the skeleton, all of which are POA (that is, Inheritance Model) classes. If you want to generate both client and server-side bindings, use one of the following (equivalent) commands: idlj -fclient -fserver My.idl\nidlj -fall My.idl There are two possible server-side models: the Inheritance Model and the Tie Delegation Model. The default server-side model is the Portable Servant Inheritance Model. Given an interface My defined in My.idl, the file MyPOA.java is generated. You must provide the implementation for My and it must inherit from MyPOA. MyPOA.java is a stream-based skeleton that extends org.omg.PortableServer.Servant @ http:\/\/docs.oracle.com\/javase\/7\/docs\/api\/org\/omg\/PortableServer\/Servant.html and implements the InvokeHandler interface and the operations interface associated with the IDL interface the skeleton implements. The PortableServer module for the Portable Object Adapter (POA) @ http:\/\/docs.oracle.com\/javase\/7\/docs\/technotes\/guides\/idl\/POA.html defines the native Servant type. In the Java programming language, the Servant type is mapped to the Java org.omg.PortableServer.Servant class. It serves as the base class for all POA servant implementations and provides a number of methods that may be invoked by the application programmer, as well as methods which are invoked by the POA itself and may be overridden by the user to control aspects of servant behavior. Another option for the Inheritance Model is to use the -oldImplBase flag in order to generate server-side bindings that are compatible with versions of the Java programming language prior to J2SE 1.4. Note that using the -oldImplBase flag is non-standard: these APIs are being deprecated. You would use this flag ONLY for compatibility with existing servers written in J2SE 1.3. In that case, you would need to modify an existing MAKEFILE to add the -oldImplBase flag to the idlj compiler, otherwise POA-based server-side mappings will be generated. To generate server-side bindings that are backwards compatible: idlj -fclient -fserver -oldImplBase My.idl\nidlj -fall -oldImplBase My.idl Given an interface My defined in My.idl, the file _MyImplBase.java is generated. You must provide the implementation for My and it must inherit from _MyImplBase. The other server-side model is called the Tie Model. This is a delegation model. Because it is not possible to generate ties and skeletons at the same time, they must be generated separately. The following commands generate the bindings for the Tie Model: idlj -fall My.idl\nidlj -fallTIE My.idl For the interface My, the second command generates MyPOATie.java. The constructor to MyPOATie takes a delegate. In this example, using the default POA model, the constructor also needs a poa. You must provide the implementation for delegate, but it does not have to inherit from any other class, only the interface MyOperations. But to use it with the ORB, you must wrap your implementation within MyPOATie. For instance:     ORB orb = ORB.init(args, System.getProperties());\n\n   \/\/ Get reference to rootpoa & activate the POAManager\n    POA rootpoa = (POA)orb.resolve_initial_references(\"RootPOA\");\n    rootpoa.the_POAManager().activate();\n\n   \/\/ create servant and register it with the ORB\n    MyServant myDelegate = new MyServant();\n    myDelegate.setORB(orb);\n\n   \/\/ create a tie, with servant being the delegate.\n    MyPOATie tie = new MyPOATie(myDelegate, rootpoa);\n\n   \/\/ obtain the objectRef for the tie\n    My ref = tie._this(orb);\n\n You might want to use the Tie model instead of the typical Inheritance model if your implementation must inherit from some other implementation. Java allows any number of interface inheritance, but there is only one slot for class inheritance. If you use the inheritance model, that slot is used up . By using the Tie Model, that slot is freed up for your own use. The drawback is that it introduces a level of indirection: one extra method call occurs when invoking a method. To generate server-side, Tie model bindings that are compatible with versions of the IDL to Java language mapping in versions prior to J2SE 1.4. idlj -oldImplBase -fall My.idl\nidlj -oldImplBase -fallTIE My.idl For the interface My, this will generate My_Tie.java. The constructor to My_Tie takes a impl. You must provide the implementation for impl, but it does not have to inherit from any other class, only the interface HelloOperations. But to use it with the ORB, you must wrap your implementation within My_Tie. For instance:     ORB orb = ORB.init(args, System.getProperties());\n\n   \/\/ create servant and register it with the ORB\n    MyServant myDelegate = new MyServant();\n    myDelegate.setORB(orb);\n\n   \/\/ create a tie, with servant being the delegate.\n    MyPOATie tie = new MyPOATie(myDelegate);\n\n   \/\/ obtain the objectRef for the tie\n    My ref = tie._this(orb);\n\n Specifying Alternate Locations for Emitted Files If you want to direct the emitted files to a directory other than the current directory, invoke the compiler as: idlj -td \/altdir My.idl For the interface My, the bindings will be emitted to \/altdir\/My.java, etc., instead of .\/My.java. Specifying Alternate Locations for Include Files If My.idl included another idl file, MyOther.idl, the compiler assumes that MyOther.idl resides in the local directory. If it resides in \/includes, for example, then you would invoke the compiler with the following command: idlj -i \/includes My.idl If My.idl also included Another.idl that resided in \/moreIncludes, for example, then you would invoke the compiler with the following command: idlj -i \/includes -i \/moreIncludes My.idl Since this form of include can become irritatingly long, another means of indicating to the compiler where to search for included files is provided. This technique is similar to the idea of an environment variable. Create a file named idl.config in a directory that is listed in your CLASSPATH. Inside of idl.config, provide a line with the following form: includes=\/includes;\/moreIncludes\n\n The compiler will find this file and read in the includes list. Note that in this example the separator character between the two directories is a semicolon (;). This separator character is platform dependent. On the Windows platform, use a semicolon, on the Unix platform, use a colon, etc. For more information on includes, see the Setting the Classpath @ http:\/\/docs.oracle.com\/javase\/7\/docs\/technotes\/tools\/index.html#general. Emitting Bindings for Include Files By default, only those interfaces, structs, etc, that are defined in the idl file on the command line have Java bindings generated for them. The types defined in included files are not generated. For example, assume the following two idl files: My.idl #include <MyOther.idl>\ninterface My\n{\n};\n\n MyOther.idl interface MyOther\n{\n};\n\n The following command will only generate the java bindings for My: idlj My.idl\n\n To generate all of the types in My.idl and all of the types in the files that My.idl includes (in this example, MyOther.idl), use the following command: idlj -emitAll My.idl There is a caveat to the default rule. #include statements which appear at global scope are treated as described. These #include statements can be thought of as import statements. #include statements which appear within some enclosing scope are treated as true #include statements, meaning that the code within the included file is treated as if it appeared in the original file and, therefore, Java bindings are emitted for it. Here is an example: My.idl #include <MyOther.idl>\ninterface My\n{\n  #include <Embedded.idl>\n};\n\n MyOther.idl interface MyOther\n{\n};\n\n Embedded.idl enum E {one, two, three};\n\n Running the following command: idlj My.idl\n\n will generate the following list of Java files: Notice that MyOther.java was not generated because it is defined in an import-like #include. But E.java was generated because it was defined in a true #include. Also notice that since Embedded.idl was included within the scope of the interface My, it appears within the scope of My (that is,in MyPackage). If the -emitAll flag had been used in the previous example, then all types in all included files would be emitted. Inserting Package Prefixes Suppose that you work for a company named ABC that has constructed the following IDL file: Widgets.idl module Widgets\n{\n  interface W1 {...};\n  interface W2 {...};\n};\n\n Running this file through the IDL-to-Java compiler will place the Java bindings for W1 and W2 within the package Widgets. But there is an industry convention that states that a company's packages should reside within a package named com.<company name>. The Widgets package is not good enough. To follow convention, it should be com.abc.Widgets. To place this package prefix onto the Widgets module, execute the following: idlj -pkgPrefix Widgets com.abc Widgets.idl If you have an IDL file which includes Widgets.idl, the -pkgPrefix flag must appear in that command also. If it does not, then your IDL file will be looking for a Widgets package rather than a com.abc.Widgets package. If you have a number of these packages that require prefixes, it might be easier to place them into the idl.config file described above. Each package prefix line should be of the form: PkgPrefix.<type>=<prefix>\n\n So the line for the above example would be: PkgPrefix.Widgets=com.abc\n\n The use of this option does not affect the Repository ID. Defining Symbols Before Compilation You may need to define a symbol for compilation that is not defined within the IDL file, perhaps to include debugging code in the bindings. The command idlj -d MYDEF My.idl is the equivalent of putting the line #define MYDEF inside My.idl. Preserving Pre-Existing Bindings If the Java binding files already exist, the -keep flag will keep the compiler from overwriting them. The default is to generate all files without considering if they already exist. If you've customized those files (which you should not do unless you are very comfortable with their contents), then the -keep option is very useful. The command idlj -keep My.idl emits all client-side bindings that do not already exist. Viewing Progress of Compilation The IDL-to-Java compiler will generate status messages as it progresses through its phases of execution. Use the -v option to activate this \"verbose\" mode: idlj -v My.idl By default the compiler does not operate in verbose mode. Displaying Version Information To display the build version of the IDL-to-Java compiler, specify the -version option on the command-line: idlj -version\n\n Version information also appears within the bindings generated by the compiler. Any additional options appearing on the command-line are ignored.","Process Name":"idlj-java-1.7.0-openjdk","Link":"https:\/\/linux.die.net\/man\/1\/idlj-java-1.7.0-openjdk"}},{"Process":{"Description":"Internationalized Domain Name (IDN) convert STRINGS, or standard input. Command line interface to the internationalized domain name library. All strings are expected to be encoded in the preferred charset used by your locale. Use '--debug' to find out what this charset is. You can override the charset used by setting environment variable CHARSET. To process a string that starts with '-', for example '-foo', use '--' to signal the end of parameters, as in 'idn --quiet -a -- -foo'. Mandatory arguments to long options are mandatory for short options too. -h, --help Print help and exit -V, --version Print version and exit -s, --stringprep Prepare string according to nameprep profile -d, --punycode-decode Decode Punycode -e, --punycode-encode Encode Punycode -a, --idna-to-ascii Convert to ACE according to IDNA (default mode) -u, --idna-to-unicode Convert from ACE according to IDNA --allow-unassigned Toggle IDNA AllowUnassigned flag (default off) --usestd3asciirules Toggle IDNA UseSTD3ASCIIRules flag (default off) --no-tld Don't check string for TLD specific rules Only for --idna-to-ascii and --idna-to-unicode -n, --nfkc Normalize string according to Unicode v3.2 NFKC -p, --profile= STRING Use specified stringprep profile instead Valid stringprep profiles: 'Nameprep', 'iSCSI', 'Nodeprep', 'Resourceprep', 'trace', 'SASLprep' --debug Print debugging information --quiet Silent operation","Process Name":"idn","Link":"https:\/\/linux.die.net\/man\/1\/idn"}},{"Process":{"Description":"Internationalized Domain Name (IDNA2008) convert STRINGS, or standard input. Command line interface to the Libidn2 implementation of IDNA2008. All strings are expected to be encoded in the locale charset. To process a string that starts with '-', for example '-foo', use '--' to signal the end of parameters, as in 'idn2 --quiet -- -foo'. Mandatory arguments to long options are mandatory for short options too. -h, --help Print help and exit -V, --version Print version and exit -l, --lookup Lookup domain name (default) -r, --register Register label --debug Print debugging information --quiet Silent operation","Process Name":"idn2","Link":"https:\/\/linux.die.net\/man\/1\/idn2"}},{"Process":{"Description":"idnlookup is a utility to ask an Internet host for its fully qualified domain name over ICMP. If successful, it will print the host's domain name to standard output. hostname is the address of the Internet host to query. It can be either a symbolic hostname, an IPv4 address in decimal notation, or an IPv6 address in hexadecimal notation. If a symbolic hostname is specified, all its addresses are looked up and tried in order. The remote host must be running software which can respond to the requests for hostname. To the knowledge of the author, current BSD-based systems can do this in the kernel. Linux systems must, as of this writing, be running icmpdnd(8).","Process Name":"idnlookup","Link":"https:\/\/linux.die.net\/man\/1\/idnlookup"}},{"Process":{"Description":"idt provides a graphical user interface to the NCAR View interactive metafile translator ictrans. idt supports a subset of the ictrans command interface. idt provides two types of command panels for interacting with imagery. The first type is the control panel, which you see when you initially invoke idt. The control panel displays messages from the translators, provides a metafile selection utility and is responsible for instantiating the second type of command panel, the display panel. The display panel provides mechanisms for controlling translators. Each display panel manages a single metafile translator. There can be multiple display panels in existence at the same time but only one control panel.","Process Name":"idt","Link":"https:\/\/linux.die.net\/man\/1\/idt"}},{"Process":{"Description":"This manual page documents briefly the iec16022 command. iec16022 generates 2d barcodes conforming to the ISO\/IEC 16022 standard (which is also known as Data Matrix and Semacode).","Process Name":"iec16022","Link":"https:\/\/linux.die.net\/man\/1\/iec16022"}},{"Process":{"Description":"iecset is a small utility to set or dump the IEC958 (or so-called \"S\/PDIF\") status bits of the specified sound card via ALSA control API. When iecset is started without arguments except for options, it will show the current IEC958 status in a human-readable form. When the commands are given in the arguments, they are parsed and the IEC958 status bits are updated. The resultant status is shown as well. The commands consist of the command directive and the argument. As the boolean argument, yes, no, true, false, or a digit number is allowed.","Process Name":"iecset","Link":"https:\/\/linux.die.net\/man\/1\/iecset"}},{"Process":{"Description":"ietf2datebook is a Perl script that converts IETF (http:\/\/www.ietf.org) agenda to format suitable for the install-datebook program. The agenda file is given to it as standard input and the output file is sent to standard output. Please remove all headers and footers from the agenda file before giving it to ietf2datebook.","Process Name":"ietf2datebook","Link":"https:\/\/linux.die.net\/man\/1\/ietf2datebook"}},{"Process":{"Description":"","Process Name":"if","Link":"https:\/\/linux.die.net\/man\/1\/if"}},{"Process":{"Description":"ifdata can be used to check for the existence of a network interface, or to get information abut the interface, such as its IP address. Unlike ifconfig or ip, ifdata has simple to parse output that is designed to be easily used by a shell script.","Process Name":"ifdata","Link":"https:\/\/linux.die.net\/man\/1\/ifdata"}},{"Process":{"Description":"Scan all of the C source FILES (or the standard input, if none are given) and write to the standard output a sorted list of all the identifiers that appear in those files in '#if', '#elif', '#ifdef', or '#ifndef' directives. Print each identifier on a line, followed by a space-separated list of the files in which that identifier occurs. -h, --help print this help, then exit -V, --version print version number, then exit","Process Name":"ifnames","Link":"https:\/\/linux.die.net\/man\/1\/ifnames"}},{"Process":{"Description":"ifne runs the following command if and only if the standard input is not empty.","Process Name":"ifne","Link":"https:\/\/linux.die.net\/man\/1\/ifne"}},{"Process":{"Description":"The ifs program draws spinning, colliding iterated-function-system images.","Process Name":"ifs","Link":"https:\/\/linux.die.net\/man\/1\/ifs"}},{"Process":{"Description":"Ifstat is a little tool to report interface activity, just like iostat\/vmstat do for other system statistics.","Process Name":"ifstat","Link":"https:\/\/linux.die.net\/man\/1\/ifstat"}},{"Process":{"Description":"Igawk is a simple shell script that adds the ability to have ''include files'' to gawk(1). AWK programs for igawk are the same as for gawk, except that, in addition, you may have lines like @include getopt.awk in your program to include the file getopt.awk from either the current directory or one of the other directories in the search path.","Process Name":"igawk","Link":"https:\/\/linux.die.net\/man\/1\/igawk"}},{"Process":{"Description":"This program is part of the OpenImageIO (http:\/\/www.openimageio.org) tool suite. Detailed documentation is avaliable in pdf format with the OpenImageIO distribution.","Process Name":"igrep","Link":"https:\/\/linux.die.net\/man\/1\/igrep"}},{"Process":{"Description":"This program is part of the OpenImageIO (http:\/\/www.openimageio.org) tool suite. Detailed documentation is avaliable in pdf format with the OpenImageIO distribution.","Process Name":"iinfo","Link":"https:\/\/linux.die.net\/man\/1\/iinfo"}},{"Process":{"Description":null,"Process Name":"ijs-config","Link":"https:\/\/linux.die.net\/man\/1\/ijs-config"}},{"Process":{"Description":"ijsgimpprint provides Ghostscript with a Gimp-Print driver, supporting all printers supported by libgimpprint. IJS is an initiative to improve the quality and ease of use of printing with Ghostscript. It permits adding or upgrading drivers without recompiling Ghostscript. An IJS driver runs in a separate process that communicates with Ghostscript via an IPC channel. The Gimp-Print IJS driver may be used with AFPL Ghostscript, as it runs in a separate process. The options for this driver are very complex. We strongly recommend use of a printer management system such as Foomatic rather than configuring a spooler manually with this driver. The driver name used by Foomatic is gimp-print-ijs.","Process Name":"ijsgimpprint","Link":"https:\/\/linux.die.net\/man\/1\/ijsgimpprint"}},{"Process":{"Description":"ijsgutenprint provides Ghostscript with a Gutenprint driver, supporting all printers supported by libgutenprint. NOTE: ijsgutenprint should never be called directly. Ghostscript will run it if the IjsServer parameter is set to IjsServer=ijsgutenprint when invoking the IJS driver. NOTE: The only supported method of running ijsgutenprint is via Foomatic. Users running ghostscript directly will not be supported. The driver name used by Foomatic is gutenprint-ijs.5.2.","Process Name":"ijsgutenprint","Link":"https:\/\/linux.die.net\/man\/1\/ijsgutenprint"}},{"Process":{"Description":"ike-scan discovers IKE hosts and can also fingerprint them using the retransmission backoff pattern. ike-scan does two things: Discovery: Determine which hosts are running IKE. This is done by displaying those hosts which respond to the IKE requests sent by ike-scan. Fingerprinting: Determine which IKE implementation the hosts are using. There are several ways to do this: (a) Backoff fingerprinting - recording the times of the IKE response packets from the target hosts and comparing the observed retransmission backoff pattern against known patterns; (b) vendor id fingerprinting - matching the vendor-specific vendor IDs against known vendor ID patterns; and (c) proprietary notify message codes. The retransmission backoff fingerprinting concept is discussed in more detail in the UDP backoff fingerprinting paper which should be included in the ike-scan kit as udp-backoff-fingerprinting-paper.txt. The program sends IKE Phase-1 requests to the specified hosts and displays any responses that are received. It handles retry and retransmission with backoff to cope with packet loss. It also limits the amount of bandwidth used by the outbound IKE packets. IKE is the Internet Key Exchange protocol which is the key exchange and authentication mechanism used by IPsec. Just about all modern VPN systems implement IPsec, and the vast majority of IPsec VPNs use IKE for key exchange. Phase-1 has two modes: Main Mode and Aggressive Mode. ike-scan supports both Main and Aggressive mode, and uses Main Mode by default. RFC 2409 (IKE) section 5 specifies that main mode must be implemented, therefore all IKE implementations can be expected to support main mode.","Process Name":"ike-scan","Link":"https:\/\/linux.die.net\/man\/1\/ike-scan"}},{"Process":{"Description":"The ikea Qt GUI application is used to manage remote Site Configurations. A remote Site Configuration describes all the parameters used to establish an IPsec VPN Client connection with a remote gateway. In addition to the creation, removal and modificaion of remote Site Configurations, the ikea application also allows a user to launch the ikec(1) ( Shrew Soft IKE Connect ) Qt GUI application for the selected Site.","Process Name":"ikea","Link":"https:\/\/linux.die.net\/man\/1\/ikea"}},{"Process":{"Description":"The ikec Qt GUI application provides a simple interface for users to interact with iked(8) ( Shrew Soft IKE Daemon ). This interface allows a user to control an IPsec VPN Client connection with a remote gateway. The parameters used for the connection are described in a Site Configuration file. For more information about managing Site Configuration files, please see the ikea(1) man page. Please note, the ikec application is not typically started directly by a user but as a sub-process of the ikea(1) Qt GUI application. The options are as follows:       -r name'            Specify the Site Configuration name. -u username Specify the Xauth username for the connection. -p password Specify the Xauth password for the connection. -a' Automatically initiate the connection.","Process Name":"ikec","Link":"https:\/\/linux.die.net\/man\/1\/ikec"}},{"Process":{"Description":null,"Process Name":"ilasm","Link":"https:\/\/linux.die.net\/man\/1\/ilasm"}},{"Process":{"Description":"This program is part of Netpbm(1). ilbmtoppm reads an IFF ILBM file as input and produces a PPM image as output. ilbmtoppm can handle the following ILBM types: \u2022 Normal ILBMs with 1-16 planes. \u2022 Amiga Extra_Halfbrite (EHB) \u2022 Amiga HAM with 3-16 planes. \u2022 24 bit. \u2022 Multiplatte (normal or HAM) pictures. \u2022 Color map (BMHD + CMAP chunk only, nPlanes = 0). \u2022 Unofficial direct color. 1-16 planes for each color component. ilbmtoppm uses these ILBM chunks: BMHD, CMAP, CAMG (only HAM & EHB flags used), PCHG, BODY unofficial DCOL chunk to identify direct color ILBM. It ignores these chunks: GRAB, DEST, SPRT, CRNG, CCRT, CLUT, DPPV, DRNG, EPSF. It ignores, but displays in verbose mode, these: NAME, AUTH, (c), ANNO, DPI. It skips chunks whose type it doesn't recognize.","Process Name":"ilbmtoppm","Link":"https:\/\/linux.die.net\/man\/1\/ilbmtoppm"}},{"Process":{"Description":"illegal2vrml convert Virtual Reality Modeling Language (VRML97) files with illegal extensions to valid VRML97, according to ISO\/IEC 14772 Annex F. Annex F decribe, how extensions to the VRML language should be implemented: using a EXTERNPROTO with a \"urn\" (fake URL) that sign the VRML browser the extension and not to use the following URL. illegal2vrml read the file protofile.wrl which should contain such EXTERNPROTOs. The names of the EXTERNPROTOs should start with the same prefix. The prefix is needed to deal with additional illegal field names in valid VRML97 nodes. In the next step illegal2vrml read the file file.wrl and replace all node names which are identical to the EXTERNPROTOs without the prefix with the names of the EXTERNPROTOs (with the prefix). In the last step, the result is written to standard output.","Process Name":"illegal2vrml","Link":"https:\/\/linux.die.net\/man\/1\/illegal2vrml"}},{"Process":{"Description":"Run Instance Messenger, the Tkinter twisted.words client","Process Name":"im","Link":"https:\/\/linux.die.net\/man\/1\/im"}},{"Process":{"Description":"","Process Name":"imagemagick","Link":"https:\/\/linux.die.net\/man\/1\/imagemagick"}},{"Process":{"Description":null,"Process Name":"imake","Link":"https:\/\/linux.die.net\/man\/1\/imake"}},{"Process":{"Description":"imapfilter is a mail filtering utility. It connects to remote mail servers using the Internet Message Access Protocol (IMAP), sends searching queries to the server and processes mailboxes based on the results. It can be used to delete, copy, move, flag, etc. messages residing in mailboxes at the same or different mail servers. The 4rev1 and 4 versions of the IMAP protocol are supported. The command line options of imapfilter(1) are as follows:        -c configfile Path to the configuration file. The default is $HOME\/.imapfilter\/config.lua. -d debugfile File that contains debugging information about the full communication with the server, along with other inner workings' details. -e 'command' May be used to enter ''one line'' of configuration, while it is also possible to pipe a full configuration as a string. When this options is used, a configuration file will not be loaded. -i' Enters interactive mode after executing the configuration file. -l logfile File that contains logs of error messages produced. -V' Displays version and copyright information. -v' Enables printing of some brief details of the communication with the server.","Process Name":"imapfilter","Link":"https:\/\/linux.die.net\/man\/1\/imapfilter"}},{"Process":{"Description":null,"Process Name":"imapsync","Link":"https:\/\/linux.die.net\/man\/1\/imapsync"}},{"Process":{"Description":null,"Process Name":"imcat","Link":"https:\/\/linux.die.net\/man\/1\/imcat"}},{"Process":{"Description":null,"Process Name":"imcom","Link":"https:\/\/linux.die.net\/man\/1\/imcom"}},{"Process":{"Description":"img2google reads a 1x1 minute Mercator surface relief img file and creates a Google Earth overlay KML file and associated PNG tile for the specified region. If no input file is given we use topo.11.1.img. -R west, east, south, and north specify the Region of interest, and you may specify them in decimal degrees or in [+-]dd:mm[:ss.xxx][W|E|S|N] format. Append r if lower left and upper right map coordinates are given instead of w\/e\/s\/n. The two shorthands -Rg and -Rd stand for global domain (0\/360 and -180\/+180 in longitude respectively, with -90\/+90 in latitude). Alternatively, specify the name of an existing grid file and the -R settings (and grid spacing, if applicable) are copied from the grid.","Process Name":"img2google","Link":"https:\/\/linux.die.net\/man\/1\/img2google"}},{"Process":{"Description":"img2grd is a front-end to img2mercgrd which reads an img format file and creates a grid file. The -M option dictates whether or not the Spherical Mercator projection of the img file is preserved. imgfile An img format file such as the marine gravity or seafloor topography fields estimated from satellite altimeter data by Sandwell and Smith. If the user has set an environment variable $GMT_DATADIR, then img2mercgrd will try to find imgfile in $GMT_DATADIR; else it will try to open imgfile directly. -G grdfile is the name of the output grid file. -R west, east, south, and north specify the Region of interest, and you may specify them in decimal degrees or in [+-]dd:mm[:ss.xxx][W|E|S|N] format. Append r if lower left and upper right map coordinates are given instead of w\/e\/s\/n. The two shorthands -Rg and -Rd stand for global domain (0\/360 and -180\/+180 in longitude respectively, with -90\/+90 in latitude). Alternatively, specify the name of an existing grid file and the -R settings (and grid spacing, if applicable) are copied from the grid. -T type handles the encoding of constraint information. type = 0 indicates that no such information is encoded in the img file (used for pre-1995 versions of the gravity data) and gets all data. type > 0 indicates that constraint information is encoded (1995 and later (current) versions of the img files) so that one may produce a grid file as follows: -T 1 gets data values at all points, -T 2 gets data values at constrained points and NaN at interpolated points; -T 3 gets 1 at constrained points and 0 at interpolated points.","Process Name":"img2grd","Link":"https:\/\/linux.die.net\/man\/1\/img2grd"}},{"Process":{"Description":null,"Process Name":"img2mercgrd","Link":"https:\/\/linux.die.net\/man\/1\/img2mercgrd"}},{"Process":{"Description":null,"Process Name":"img2txt","Link":"https:\/\/linux.die.net\/man\/1\/img2txt"}},{"Process":{"Description":"The imgcmp command compares two images of the same geometry with respect to a given metric. Please use the --help command line switch and the JasPer Software Reference Manual for more information.","Process Name":"imgcmp","Link":"https:\/\/linux.die.net\/man\/1\/imgcmp"}},{"Process":{"Description":"The imginfo command displays information about an image. Please use the --help command line switch and the JasPer Software Reference Manual for more information.","Process Name":"imginfo","Link":"https:\/\/linux.die.net\/man\/1\/imginfo"}},{"Process":{"Description":null,"Process Name":"imgsc","Link":"https:\/\/linux.die.net\/man\/1\/imgsc"}},{"Process":{"Description":"No-brainer to size an image supplied on the command-line. All the real work is done in Image::Size","Process Name":"imgsize","Link":"https:\/\/linux.die.net\/man\/1\/imgsize"}},{"Process":{"Description":null,"Process Name":"imgtoppm","Link":"https:\/\/linux.die.net\/man\/1\/imgtoppm"}},{"Process":{"Description":"imhead is a utility for listing IRAF and FITS image headers. It converts IRAF .imh image headers to a FITS format before listing them. For IRAF files, the BITPIX, NAXIS, NAXIS1, NAXIS2, OBJECT, and PIXFILE keywords are set from the binary portion of the header. IMHFILE is set from the actual filename for internal purposes.","Process Name":"imhead","Link":"https:\/\/linux.die.net\/man\/1\/imhead"}},{"Process":{"Description":"imlib-config is a script that is used to display what compiler flags and libraries were used when Imlib and GDK-Imlib were built. Note that imlib-config is not the same as the imlib_config program.","Process Name":"imlib-config","Link":"https:\/\/linux.die.net\/man\/1\/imlib-config"}},{"Process":{"Description":null,"Process Name":"imlib_config","Link":"https:\/\/linux.die.net\/man\/1\/imlib_config"}},{"Process":{"Description":"This manual page documents briefly the implantisomd5 command. implantisomd5 is a program that embeds an MD5 checksum in an unused section of and ISO9660 (.iso) image. This checksum can later be compared to the .iso, or a block device, using the corresponding checkisomd5 command.","Process Name":"implantisomd5","Link":"https:\/\/linux.die.net\/man\/1\/implantisomd5"}},{"Process":{"Description":null,"Process Name":"import","Link":"https:\/\/linux.die.net\/man\/1\/import"}},{"Process":{"Description":null,"Process Name":"import-ferm","Link":"https:\/\/linux.die.net\/man\/1\/import-ferm"}},{"Process":{"Description":null,"Process Name":"impressive","Link":"https:\/\/linux.die.net\/man\/1\/impressive"}},{"Process":{"Description":"imrot is a utility for rotating and\/or reflecting FITS or IRAF images. Images can be rotated only in multiples of 90 degrees. The image may be output with a different data type than that in which it is input, and IRAF files may be written out as FITS files.","Process Name":"imrot","Link":"https:\/\/linux.die.net\/man\/1\/imrot"}},{"Process":{"Description":"imsize is a utility for printing the region covered by an image using the world coordinate system parameters in IRAF and FITS image headers. It converts IRAF .imh image headers to a FITS format before extracting the information. For IRAF files, the image dimensions and the object name are set from the binary portion of the header.","Process Name":"imsize","Link":"https:\/\/linux.die.net\/man\/1\/imsize"}},{"Process":{"Description":null,"Process Name":"imsmap","Link":"https:\/\/linux.die.net\/man\/1\/imsmap"}},{"Process":{"Description":"","Process Name":"imstar","Link":"https:\/\/linux.die.net\/man\/1\/imstar"}},{"Process":{"Description":null,"Process Name":"imtest","Link":"https:\/\/linux.die.net\/man\/1\/imtest"}},{"Process":{"Description":"Search USNO-A2.0 Catalog within the area described by the world coordinate system in an image header. This is a link to imcat rather than a separate executable.","Process Name":"imua2","Link":"https:\/\/linux.die.net\/man\/1\/imua2"}},{"Process":{"Description":null,"Process Name":"imusa2","Link":"https:\/\/linux.die.net\/man\/1\/imusa2"}},{"Process":{"Description":"This manual page documents briefly the imvirt script. imvirt tries to detect if it runs on a physical machine or on a virtualized one. If it detects that it is a virtualized one it also tries to find out which virtualization technology is used.","Process Name":"imvirt","Link":"https:\/\/linux.die.net\/man\/1\/imvirt"}},{"Process":{"Description":null,"Process Name":"imwcs","Link":"https:\/\/linux.die.net\/man\/1\/imwcs"}},{"Process":{"Description":"include_server.py starts an include server process. This process answers queries from distcc(1) clients about what files to include in C\/C++ compilations. The include_server.py command itself terminates as soon as the include server has been spawned. The INCLUDE_SERVER_PORT argument is the name of a socket used for all communication between distcc clients and the include server. The pump(1) command is responsible for creating the socket location, for passing it to this script, and for passing it to all distcc clients via the environment variable named INCLUDE_SERVER_PORT. The protocol used by the include server uses distcc's RPC implementation. Each distcc request consists of (1) the current directory and (2) the list of arguments of the compilation command. If the include server is able to process the request, then it answers the distcc client by sending a list of filepaths. The filepaths are those of the compressed source and header files found to be necessary for compilation through include analysis. The list also comprises symbolic links and even dummy files needed for the compilation server to construct an accurate replica of the parts of the filesystem needed for compilation. In this way, a needed header file like \/path\/foo.h is compressed, renamed, and stored in a temporary location, such as \/dev\/shm\/tmpiAvfGv.include_server-9368-1\/path\/foo.h.lzo. The distcc client will pass these files on to a compilation server, where they will be uncompressed and mounted temporarily. If the include server is not able to process the request, then it returns the empty list to the distcc client. There are two kinds of failures that relate to the include server. The include server may fail to compute the includes or fail in other ways, see section INCLUDE SERVER SYMPTOMS. Also, the compilation on the remove server may fail due to inadequacy of the calculated include closure, but then succeed when locally retried, see section DISTCC DISCREPANCY SYMPTOMS.","Process Name":"include_server","Link":"https:\/\/linux.die.net\/man\/1\/include_server"}},{"Process":{"Description":null,"Process Name":"includemocs","Link":"https:\/\/linux.die.net\/man\/1\/includemocs"}},{"Process":{"Description":"Includeres includes resources (fonts, procsets, patterns, files, etc) in place of %%IncludeResource comments in a PostScript document. The resources are searched for in the current directory and the system default directory under the resource name, and with an appropriate extension. The pipeline extractres file.ps | includeres >out.ps will move all resources appearing in a document to the document prologue, removing redundant copies. The output file can then be put through page re-arrangement filters such as psnup or pstops safely.","Process Name":"includeres","Link":"https:\/\/linux.die.net\/man\/1\/includeres"}},{"Process":{"Description":"incrontab is a table manipulator for the inotify cron (incron) system. It creates, removes, modifies and lists user tables ( incrontab(5)). Each user (including system users even they haven't home directories) has an incron table which can't be manipulated directly (only root can effectively change these tables and is NOT recommended to do so). All informational messages of this program are printed to the standard error output (stderr). If \/etc\/incron.allow exists only users listed here may use incron. Otherwise if \/etc\/incron.deny exists only users NOT listed here may use incron. If none of these files exists everyone is allowed to use incron. (Important note: This behavior is insecure and will be probably changed to be compatible with the style used by ISC Cron.) Location of these files can be changed in the configuration. The first form of this command imports a file, validates it and stores to the table. \"-\" can be used for loading from the standard input. -u (or --user) option overrides the current (real) user to the given one. This option is intended for manipulation with system users' tables (such as apache, postfix, daemon etc.). It can be used only if the current user has root's effective rights. -l (or --list) option causes the current table is printed to the standard output. -r (or --remove) option causes the current table (if any) is permanently remove without any warning or confirmation. Use with caution! -e (or --edit) option causes executing an editor for editting the user table (see below for the information about editor selection). You can edit your incron table now. If the table is changed it stores the modified version. -t (or --types) option causes the list of supported event types (delimited by commas) is printed to the standard output. This feature is intended for front-end applications to find out which event types was compiled in. -d (or --reload) option causes reloading the current table by incrond(8). It is done through \"touching\" the table (writing into it without modifying it). This feature is intended e.g. for creating watches on newly created files (with already existing rules) or for rearming IN_ONESHOT watches. -f <FILE> (or --config=<FILE>) option specifies another location for the configuration file (\/etc\/incron.conf is used by default). This feature requires root privileges. There is a few complex algorithm how to determine which editor will be user for editting. If any of the following rule succeeds the appropriate editor is used: 1. EDITOR environment variable 2. VISUAL environment variable 3. configuration value 4. etc\/alternatives\/editor 5. hard-wired editor (vim by default) It's not recommended to use graphical editors (such as gVim, KEdit etc.) due to possible problems with connecting to the X server.","Process Name":"incrontab","Link":"https:\/\/linux.die.net\/man\/1\/incrontab"}},{"Process":{"Description":"This man page is generated from the file indent.texinfo. This is Edition of \"The indent Manual\", for Indent Version , last updated . The indent program can be used to make code easier to read. It can also convert from one style of writing C to another. indent understands a substantial amount about the syntax of C, but it also attempts to cope with incomplete and misformed syntax. In version 1.2 and more recent versions, the GNU style of indenting is the default.","Process Name":"indent","Link":"https:\/\/linux.die.net\/man\/1\/indent"}},{"Process":{"Description":"indexcon allows the user to index the file contexts on a SELinux system, beginning with the root directory ( \/ ) and recursing into subdirectories. The index will be written to FILE. The index can be searched using apol or findcon.","Process Name":"indexcon","Link":"https:\/\/linux.die.net\/man\/1\/indexcon"}},{"Process":{"Description":"Indexdump prints to standard output the content of the index file. The type of the index is one of those supported by the XBase::Index Perl module (cdx, idx, ntx, ndx, mdx). The output contains the index key and the value, which is the record number in the correcponding dbf file. For mulitag index files (like cdx), you need to specify the tag name to get the actual data.","Process Name":"indexdump","Link":"https:\/\/linux.die.net\/man\/1\/indexdump"}},{"Process":{"Description":"indexer is a part of mnoGoSearch - search engine. The purpose of indexer is to walk through HTTP, HTTPS, FTP, NEWS servers as well as local file system, recursively grabbing all the documents and storing metadata about documents into SQL or built-in database in a smart and effective manner. Since every document is referenced by its corresponding URL, metadata collected by indexer is used later in a search process. The behaviour of indexer is controlled mainly via configuration file indexer.conf (5) , which it reads on startup. There is a compiled-in default for configuration file name and location, so you don't need to specify it every time you run indexer , but you can specify alternative configuration file as the last argument. indexer supports HTML-formatted (text\/html MIME type), XML-formated (text\/xml MIME type) and plain text (text\/plain MIME type) documents. Support for other data types is provided by using external programs, which are called \"parsers\". Parser should get data of some type from stdin and put text\/html or text\/plain data to stdout. See indexer.conf(5) for details. You may run indexer regularly from cron (8) to keep metadata up-to-date. indexer is also used to manipulate database. It may be used to clear some data from database, to output some statistics and to calculate popolarity ranking.","Process Name":"indexer","Link":"https:\/\/linux.die.net\/man\/1\/indexer"}},{"Process":{"Description":null,"Process Name":"indexmaker","Link":"https:\/\/linux.die.net\/man\/1\/indexmaker"}},{"Process":{"Description":"indxbib makes an inverted index for the bibliographic databases in filename... for use with refer(1), lookbib(1), and lkbib(1). The index will be named filename.i; the index is written to a temporary file which is then renamed to this. If no filenames are given on the command line because the -f option has been used, and no -o option is given, the index will be named Ind.i. Bibliographic databases are divided into records by blank lines. Within a record, each fields starts with a % character at the beginning of a line. Fields have a one letter name which follows the % character. The values set by the -c, -n, -l and -t options are stored in the index; when the index is searched, keys will be discarded and truncated in a manner appropriate to these options; the original keys will be used for verifying that any record found using the index actually contains the keys. This means that a user of an index need not know whether these options were used in the creation of the index, provided that not all the keys to be searched for would have been discarded during indexing and that the user supplies at least the part of each key that would have remained after being truncated during indexing. The value set by the -i option is also stored in the index and will be used in verifying records found using the index.","Process Name":"indxbib","Link":"https:\/\/linux.die.net\/man\/1\/indxbib"}},{"Process":{"Description":null,"Process Name":"inews","Link":"https:\/\/linux.die.net\/man\/1\/inews"}},{"Process":{"Description":"infadd is used to measure performance of the add operation. It can span multiple threads in order to test the performance under heavy locking.","Process Name":"infadd","Link":"https:\/\/linux.die.net\/man\/1\/infadd"}},{"Process":{"Description":null,"Process Name":"info","Link":"https:\/\/linux.die.net\/man\/1\/info"}},{"Process":{"Description":"infocmp can be used to compare a binary terminfo entry with other terminfo entries, rewrite a terminfo description to take advantage of the use= terminfo field, or print out a terminfo description from the binary file (term) in a variety of formats. In all cases, the boolean fields will be printed first, followed by the numeric fields, followed by the string fields. Default Options If no options are specified and zero or one termnames are specified, the -I option will be assumed. If more than one termname is specified, the -d option will be assumed. Comparison Options [-d] [-c] [-n] infocmp compares the terminfo description of the first terminal termname with each of the descriptions given by the entries for the other terminal's termnames. If a capability is defined for only one of the terminals, the value returned will depend on the type of the capability: F for boolean variables, -1 for integer variables, and NULL for string variables. The -d option produces a list of each capability that is different between two entries. This option is useful to show the difference between two entries, created by different people, for the same or similar terminals. The -c option produces a list of each capability that is common between two entries. Capabilities that are not set are ignored. This option can be used as a quick check to see if the -u option is worth using. The -n option produces a list of each capability that is in neither entry. If no termnames are given, the environment variable TERM will be used for both of the termnames. This can be used as a quick check to see if anything was left out of a description. Source Listing Options [-I] [-L] [-C] [-r] The -I, -L, and -C options will produce a source listing for each terminal named. If no termnames are given, the environment variable TERM will be used for the terminal name. The source produced by the -C option may be used directly as a termcap entry, but not all parameterized strings can be changed to the termcap format. infocmp will attempt to convert most of the parameterized information, and anything not converted will be plainly marked in the output and commented out. These should be edited by hand. All padding information for strings will be collected together and placed at the beginning of the string where termcap expects it. Mandatory padding (padding information with a trailing '\/') will become optional. All termcap variables no longer supported by terminfo, but which are derivable from other terminfo variables, will be output. Not all terminfo capabilities will be translated; only those variables which were part of termcap will normally be output. Specifying the -r option will take off this restriction, allowing all capabilities to be output in termcap form. Note that because padding is collected to the beginning of the capability, not all capabilities are output. Mandatory padding is not supported. Because termcap strings are not as flexible, it is not always possible to convert a terminfo string capability into an equivalent termcap format. A subsequent conversion of the termcap file back into terminfo format will not necessarily reproduce the original terminfo source. Some common terminfo parameter sequences, their termcap equivalents, and some terminal types which commonly have such sequences, are: Use= Option [-u] The -u option produces a terminfo source description of the first terminal termname which is relative to the sum of the descriptions given by the entries for the other terminals termnames. It does this by analyzing the differences between the first termname and the other termnames and producing a description with use= fields for the other terminals. In this manner, it is possible to retrofit generic terminfo entries into a terminal's description. Or, if two similar terminals exist, but were coded at different times or by different people so that each description is a full description, using infocmp will show what can be done to change one description to be relative to the other. A capability will get printed with an at-sign (@) if it no longer exists in the first termname, but one of the other termname entries contains a value for it. A capability's value gets printed if the value in the first termname is not found in any of the other termname entries, or if the first of the other termname entries that has this capability gives a different value for the capability than that in the first termname. The order of the other termname entries is significant. Since the terminfo compiler tic does a left-to-right scan of the capabilities, specifying two use= entries that contain differing entries for the same capabilities will produce different results depending on the order that the entries are given in. infocmp will flag any such inconsistencies between the other termname entries as they are found. Alternatively, specifying a capability after a use= entry that contains that capability will cause the second specification to be ignored. Using infocmp to recreate a description can be a useful check to make sure that everything was specified correctly in the original source description. Another error that does not cause incorrect compiled files, but will slow down the compilation time, is specifying extra use= fields that are superfluous. infocmp will flag any other termname use= fields that were not needed. Changing Databases [-A directory] [-B directory] The location of the compiled terminfo database is taken from the environment variable TERMINFO . If the variable is not defined, or the terminal is not found in that location, the system terminfo database, in \/usr\/share\/terminfo, will be used. The options -A and -B may be used to override this location. The -A option will set TERMINFO for the first termname and the -B option will set TERMINFO for the other termnames. With this, it is possible to compare descriptions for a terminal with the same name located in two different databases. This is useful for comparing descriptions for the same terminal created by different people. Other Options -1 causes the fields to be printed out one to a line. Otherwise, the fields will be printed several to a line to a maximum width of 60 characters. -a tells infocmp to retain commented-out capabilities rather than discarding them. Capabilities are commented by prefixing them with a period. -E Dump the capabilities of the given terminal as tables, needed in the C initializer for a TERMTYPE structure (the terminal capability structure in the <term.h>). This option is useful for preparing versions of the curses library hardwired for a given terminal type. The tables are all declared static, and are named according to the type and the name of the corresponding terminal entry. Before ncurses 5.0, the split between the -e and -E options was not needed; but support for extended names required making the arrays of terminal capabilities separate from the TERMTYPE structure. -e Dump the capabilities of the given terminal as a C initializer for a TERMTYPE structure (the terminal capability structure in the <term.h>). This option is useful for preparing versions of the curses library hardwired for a given terminal type. -F compare terminfo files. This assumes that two following arguments are filenames. The files are searched for pairwise matches between entries, with two entries considered to match if any of their names do. The report printed to standard output lists entries with no matches in the other file, and entries with more than one match. For entries with exactly one match it includes a difference report. Normally, to reduce the volume of the report, use references are not resolved before looking for differences, but resolution can be forced by also specifying -r. -f Display complex terminfo strings which contain if\/then\/else\/endif expressions indented for readability. -G Display constant literals in decimal form rather than their character equivalents. -g Display constant character literals in quoted form rather than their decimal equivalents. -i Analyze the initialization (is1, is2, is3), and reset (rs1, rs2, rs3), strings in the entry. For each string, the code tries to analyze it into actions in terms of the other capabilities in the entry, certain X3.64\/ISO 6429\/ECMA-48 capabilities, and certain DEC VT-series private modes (the set of recognized special sequences has been selected for completeness over the existing terminfo database). Each report line consists of the capability name, followed by a colon and space, followed by a printable expansion of the capability string with sections matching recognized actions translated into {}-bracketed descriptions. Here is a list of the DEC\/ANSI special sequences recognized: i. It also recognizes a SGR action corresponding to ANSI\/ISO 6429\/ECMA Set Graphics Rendition, with the values NORMAL, BOLD, UNDERLINE, BLINK, and REVERSE. All but NORMAL may be prefixed with '+' (turn on) or '-' (turn off). An SGR0 designates an empty highlight sequence (equivalent to {SGR:NORMAL}). -l Set output format to terminfo. -p Ignore padding specifications when comparing strings. -q Make the comparison listing shorter by omitting subheadings, and using \"-\" for absent capabilities, \"@\" for canceled rather than \"NULL\". -R subset Restrict output to a given subset. This option is for use with archaic versions of terminfo like those on SVr1, Ultrix, or HP\/UX that do not support the full set of SVR4\/XSI Curses terminfo; and variants such as AIX that have their own extensions incompatible with SVr4\/XSI. Available terminfo subsets are \"SVr1\", \"Ultrix\", \"HP\", and \"AIX\"; see terminfo(5) for details. You can also choose the subset \"BSD\" which selects only capabilities with termcap equivalents recognized by 4.4BSD. -s [d|i|l|c] The -s option sorts the fields within each type according to the argument below: d leave fields in the order that they are stored in the terminfo database. i sort by terminfo name. l sort by the long C variable name. c sort by the termcap name. If the -s option is not given, the fields printed out will be sorted alphabetically by the terminfo name within each type, except in the case of the -C or the -L options, which cause the sorting to be done by the termcap name or the long C variable name, respectively. -T eliminates size-restrictions on the generated text. This is mainly useful for testing and analysis, since the compiled descriptions are limited (e.g., 1023 for termcap, 4096 for terminfo). -t tells tic to discard commented-out capabilities. Normally when translating from terminfo to termcap, untranslatable capabilities are commented-out. -U tells infocmp to not post-process the data after parsing the source file. This feature helps when comparing the actual contents of two source files, since it excludes the inferences that infocmp makes to fill in missing data. -V reports the version of ncurses which was used in this program, and exits. -v n prints out tracing information on standard error as the program runs. Higher values of n induce greater verbosity. -w width changes the output to width characters. -x print information for user-defined capabilities. These are extensions to the terminfo repertoire which can be loaded using the -x option of tic.","Process Name":"infocmp","Link":"https:\/\/linux.die.net\/man\/1\/infocmp"}},{"Process":{"Description":null,"Process Name":"infokey","Link":"https:\/\/linux.die.net\/man\/1\/infokey"}},{"Process":{"Description":"infotocap looks in file for terminfo descriptions. For each one found, an equivalent termcap description is written to standard output. Terminfo use capabilities are translated directly to termcap tc capabilities. -v print out tracing information on standard error as the program runs. -V print out the version of the program in use on standard error and exit. -1 cause the fields to print out one to a line. Otherwise, the fields will be printed several to a line to a maximum width of 60 characters. -w change the output to width characters.","Process Name":"infotocap","Link":"https:\/\/linux.die.net\/man\/1\/infotocap"}},{"Process":{"Description":null,"Process Name":"infotopam","Link":"https:\/\/linux.die.net\/man\/1\/infotopam"}},{"Process":{"Description":"","Process Name":"ini2po","Link":"https:\/\/linux.die.net\/man\/1\/ini2po"}},{"Process":{"Description":"Metafont reads the program in the specified files and outputs font rasters (in gf format) and font metrics (in tfm format). The Metafont language is described in The Metafontbook. Like TeX, Metafont is normally used with a large body of precompiled macros, and font generation in particular requires the support of several macro files. This version of Metafont looks at its command line to see what name it was called under. Both inimf and virmf are symlinks to the mf executable. When called as inimf (or when the -ini option is given) it can be used to precompile macros into a .base file. When called as virmf it will use the plain base. When called under any other name, Metafont will use that name as the name of the base to use. For example, when called as mf the mf base is used, which is identical to the plain base. Other bases than plain are rarely used. The commands given on the command line to the Metafont program are passed to it as the first input line. (But it is often easier to type extended arguments as the first input line, since UNIX shells tend to gobble up or misinterpret Metafont's favorite symbols, like semicolons, unless you quote them.) As described in The Metafontbook, that first line should begin with a filename, a \\controlsequence, or a &basename. The normal usage is to say mf '\\mode=<printengine>; [mag=magstep( n);]' input font to start processing font.mf. The single quotes are the best way of keeping the Unix shell from misinterpreting the semicolons and from removing the \\ character, which is needed here to keep Metafont from thinking that you want to produce a font called mode. (Or you can just say mf and give the other stuff on the next line, without quotes.) Other control sequences, such as batchmode (for silent operation) can also appear. The name font will be the ''jobname'', and is used in forming output file names. If Metafont doesn't get a file name in the first line, the jobname is mfput. The default extension, .mf, can be overridden by specifying an extension explicitly. A log of error messages goes into the file jobname.log. The output files are jobname.tfm and jobname.<number>gf, where <number> depends on the resolution and magnification of the font. The mode in this example is shown generically as <printengine>, a symbolic term for which the name of an actual device or, most commonly, the name localfont (see below) must be substituted. If the mode is not specified or is not valid for your site, Metafont will default to proof mode which produces large character images for use in font design and refinement. Proof mode can be recognized by the suffix .2602gf after the jobname. Examples of proof mode output can be found in Computer Modern Typefaces (Volume E of Computers and Typesetting). The system of magsteps is identical to the system used by TeX, with values generally in the range 0.5, 1.0, 2.0, 3.0, 4.0 and 5.0. A listing of gf numbers for 118-dpi, 240-dpi and 300-dpi fonts is shown below. Magnification can also be specified not as a magstep but as an arbitrary value, such as 1.315, to create special character sizes. Before font production can begin, it is necessary to set up the appropriate base files. The minimum set of components for font production for a given print-engine is the plain.mf macro file and the local mode_def file. The macros in plain.mf can be studied in an appendix to the Metafontbook; they were developed by Donald E. Knuth, and this file should never be altered except when it is officially upgraded. Each mode_def specification helps adapt fonts to a particular print-engine. There is a regular discussion of mode_defs in TUGboat, the journal of the TeX Users Group. The local ones in use on this computer should be in modes.mf. The e response to Metafont's error-recovery mode invokes the system default editor at the erroneous line of the source file. There is an environment variable, MFEDIT, that overrides the default editor. It should contain a string with \"%s\" indicating where the filename goes and \"%d\" indicating where the decimal linenumber (if any) goes. For example, an MFEDIT string for the vi editor can be set with the csh command setenv MFEDIT \"vi +%d %s\" A convenient file in the library is null.mf, containing nothing. When mf can't find the file it thinks you want to input, it keeps asking you for another file name; responding 'null' gets you out of the loop if you don't want to input anything.","Process Name":"inimf","Link":"https:\/\/linux.die.net\/man\/1\/inimf"}},{"Process":{"Description":null,"Process Name":"inimpost","Link":"https:\/\/linux.die.net\/man\/1\/inimpost"}},{"Process":{"Description":"Run the Omega typesetter on file, usually creating file.dvi. If the file argument has no extension, \".tex\" will be appended to it. Instead of a filename, a set of Omega commands can be given, the first of which must start with a backslash. With a &format argument Omega uses a different set of precompiled commands, contained in format.fmt; it is usually better to use the -fmt format option instead. Omega is a version of the TeX program modified for multilingual typesetting. It uses unicode, and has additional primitives for (among other things) bidirectional typesetting. The iniomega and viromega commands are Omega's analogues to the initex and virtex commands. In this installation, they are symlinks to the omega executable. Omega's command line options are similar to those of TeX. Omega is experimental software.","Process Name":"iniomega","Link":"https:\/\/linux.die.net\/man\/1\/iniomega"}},{"Process":{"Description":null,"Process Name":"initdb","Link":"https:\/\/linux.die.net\/man\/1\/initdb"}},{"Process":{"Description":"Run the TeX typesetter on file, usually creating file.dvi. If the file argument has no extension, \".tex\" will be appended to it. Instead of a filename, a set of TeX commands can be given, the first of which must start with a backslash. With a &format argument TeX uses a different set of precompiled commands, contained in format.fmt; it is usually better to use the -fmt format option instead. TeX formats the interspersed text and commands contained in the named files and outputs a typesetter independent file (called DVI, which is short for DeVice Independent). TeX's capabilities and language are described in The TeX for nroffbook. TeX is normally used with a large body of precompiled macros, and there are several specific formatting systems, such as LaTeX, which require the support of several macro files. This version of TeX looks at its command line to see what name it was called under. If they exist, then both initex and virtex are symbolic links to the tex executable. When called as initex (or when the -ini option is given) it can be used to precompile macros into a .fmt file. When called as virtex it will use the plain format. When called under any other name, TeX will use that name as the name of the format to use. For example, when called as tex the tex format is used, which is identical to the plain format. The commands defined by the plain format are documented in The TeX for nroffbook. Other formats that are often available include latex and amstex. The non-option command line arguments to the TeX program are passed to it as the first input line. (But it is often easier to type extended arguments as the first input line, since UNIX shells tend to gobble up or misinterpret TeX's favorite symbols, like backslashes, unless you quote them.) As described in The TeX for nroffbook, that first line should begin with a filename, a \\controlsequence, or a &formatname. The normal usage is to say tex paper to start processing paper.tex. The name paper will be the ''jobname'', and is used in forming output filenames. If TeX doesn't get a filename in the first line, the jobname is texput. When looking for a file, TeX looks for the name with and without the default extension ( .tex) appended, unless the name already contains that extension. If paper is the ''jobname'', a log of error messages, with rather more detail than normally appears on the screen, will appear in paper.log, and the output file will be in paper.dvi. This version of TeX can look in the first line of the file paper.tex to see if it begins with the magic sequence %&. If the first line begins with %&format -translate-file tcxname then TeX will use the named format and transation table tcxname to process the source file. Either the format name or the -translate-file specification may be omitted, but not both. This overrides the format selection based on the name by which the program is invoked. The -parse-first-line option or the parse_first_line configuration variable controls whether this behaviour is enabled. The e response to TeX's error prompt causes the system default editor to start up at the current line of the current file. The environment variable TEXEDIT can be used to change the editor used. It may contain a string with \"%s\" indicating where the filename goes and \"%d\" indicating where the decimal line number (if any) goes. For example, a TEXEDIT string for emacs can be set with the sh command TEXEDIT=\"emacs +%d %s\"; export TEXEDIT\n A convenient file in the library is null.tex, containing nothing. When TeX can't find a file it thinks you want to input, it keeps asking you for another filename; responding 'null' gets you out of the loop if you don't want to input anything. You can also type your EOF character (usually control-D).","Process Name":"initex","Link":"https:\/\/linux.die.net\/man\/1\/initex"}},{"Process":{"Description":null,"Process Name":"initlocation","Link":"https:\/\/linux.die.net\/man\/1\/initlocation"}},{"Process":{"Description":null,"Process Name":"initlog","Link":"https:\/\/linux.die.net\/man\/1\/initlog"}},{"Process":{"Description":"Inkscape is a GUI editor for Scalable Vector Graphics ( SVG ) format drawing files, with capabilities similar to Adobe Illustrator, CorelDraw, Xara Xtreme, etc. Inkscape features include versatile shapes, bezier paths, freehand drawing, multi-line text, text on path, alpha blending, arbitrary affine transforms, gradient and pattern fills, node editing, many export and import formats including PNG and PDF , grouping, layers, live clones, and a lot more. The interface is designed to be comfortable and efficient for skilled users, while remaining conformant to GNOME standards so that users familiar with other GNOME applications can learn its interface rapidly. SVG is a W3C standard XML format for 2D vector drawing. It allows defining objects in the drawing using points, paths, and primitive shapes. Colors, fonts, stroke width, and so forth are specified as 'style' attributes to these objects. The intent is that since SVG is a standard, and since its files are text\/xml, it will be possible to use SVG files in a sizeable number of programs and for a wide range of uses. Inkscape uses SVG as its native document format, and has the goal of becoming the most fully compliant drawing program for SVG files available in the Open Source community.","Process Name":"inkscape","Link":"https:\/\/linux.die.net\/man\/1\/inkscape"}},{"Process":{"Description":null,"Process Name":"inkview","Link":"https:\/\/linux.die.net\/man\/1\/inkview"}},{"Process":{"Description":"\"inline2test\" is the Test::Inline 2 test compiler. It's job is to scan through an arbitrary tree of Perl source code files, locate inline test sections, extract them, convert them to test scripts, and write them to an output path. Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"inline2test","Link":"https:\/\/linux.die.net\/man\/1\/inline2test"}},{"Process":{"Description":"innconfval normally prints the values of the parameters specified on the command line. By default, it just prints the parameter values, but if -p, -s, or -t are given, it instead prints the parameter and value in the form of a variable assignment in Perl, Bourne shell, or Tcl respectively. If no parameters are specifically requested, innconfval prints out all parameter values (this isn't particularly useful unless one of -p, -s, or -t were specified). All parameters are taken from inn.conf except for version, which is always the version string of INN . If given the -C option, innconfval instead checks inn.conf, reporting any problems found to standard error. innconfval will exit with status 0 if no problems are found and with status 1 otherwise.","Process Name":"innconfval","Link":"https:\/\/linux.die.net\/man\/1\/innconfval"}},{"Process":{"Description":null,"Process Name":"innfeed","Link":"https:\/\/linux.die.net\/man\/1\/innfeed"}},{"Process":{"Description":"innmail is a Perl script intended to provide the non-interactive mail-sending functionality of mail(1) while avoiding nasty security problems. It takes the body of a mail message on standard input and sends it to the specified addresses by invoking the value of mta in inn.conf. At least one address (formatted for the MTA specified in inn.conf, if it matters) is required. innmail will sanitize the addresses so that they contain only alphanumerics and the symbols \"@\", \".\", \"-\", \"+\", \"_\", and \"%\". innmail was written to be suitable for the mailcmd setting in inn.conf.","Process Name":"innmail","Link":"https:\/\/linux.die.net\/man\/1\/innmail"}},{"Process":{"Description":"innochecksum prints checksums for InnoDB files. This tool reads an InnoDB tablespace file, calculates the checksum for each page, compares the calculated checksum to the stored checksum, and reports mismatches, which indicate damaged pages. It was originally developed to speed up verifying the integrity of tablespace files after power outages but can also be used after file copies. Because checksum mismatches will cause InnoDB to deliberately shut down a running server, it can be preferable to use this tool rather than waiting for a server in production usage to encounter the damaged pages. innochecksum cannot be used on tablespace files that the server already has open. For such files, you should use CHECK TABLE to check tables within the tablespace. If checksum mismatches are found, you would normally restore the tablespace from backup or start the server and attempt to use mysqldump to make a backup of the tables within the tablespace. Invoke innochecksum like this: shell> innochecksum [options] file_name\n innochecksum supports the following options. For options that refer to page numbers, the numbers are zero-based. \u2022 -c Print a count of the number of pages in the file. \u2022 -d Debug mode; prints checksums for each page. \u2022 -e num End at this page number. \u2022 -p num Check only this page number. \u2022 -s num Start at this page number. \u2022 -v Verbose mode; print a progress indicator every five seconds.","Process Name":"innochecksum","Link":"https:\/\/linux.die.net\/man\/1\/innochecksum"}},{"Process":{"Description":null,"Process Name":"innotop","Link":"https:\/\/linux.die.net\/man\/1\/innotop"}},{"Process":{"Description":"inotifywait efficiently waits for changes to files using Linux's inotify(7) interface. It is suitable for waiting for changes to files from shell scripts. It can either exit once an event occurs, or continually execute and output events as they occur.","Process Name":"inotifywait","Link":"https:\/\/linux.die.net\/man\/1\/inotifywait"}},{"Process":{"Description":"inotifywatch listens for filesystem events using Linux's inotify(7) interface, then outputs a summary count of the events received on each file or directory.","Process Name":"inotifywatch","Link":"https:\/\/linux.die.net\/man\/1\/inotifywatch"}},{"Process":{"Description":null,"Process Name":"inputattach","Link":"https:\/\/linux.die.net\/man\/1\/inputattach"}},{"Process":{"Description":"This install program copies files (often just compiled) into destination locations you choose. If you want to download and install a ready-to-use package on a GNU\/Linux system, you should instead be using a package manager like yum(1) or apt-get(1). In the first three forms, copy SOURCE to DEST or multiple SOURCE(s) to the existing DIRECTORY, while setting permission modes and owner\/group. In the 4th form, create all components of the given DIRECTORY(ies). Mandatory arguments to long options are mandatory for short options too. --backup[= CONTROL] make a backup of each existing destination file -b like --backup but does not accept an argument -c (ignored) -C, --compare compare each pair of source and destination files, and in some cases, do not modify the destination at all -d, --directory treat all arguments as directory names; create all components of the specified directories -D create all leading components of DEST except the last, then copy SOURCE to DEST -g, --group= GROUP set group ownership, instead of process' current group -m, --mode= MODE set permission mode (as in chmod), instead of rwxr-xr-x -o, --owner= OWNER set ownership (super-user only) -p, --preserve-timestamps apply access\/modification times of SOURCE files to corresponding destination files -s, --strip strip symbol tables --strip-program= PROGRAM program used to strip binaries -S, --suffix= SUFFIX override the usual backup suffix -t, --target-directory= DIRECTORY copy all SOURCE arguments into DIRECTORY -T, --no-target-directory treat DEST as a normal file -v, --verbose print the name of each directory as it is created -P, --preserve-context (SELinux) preserve security context -Z, --context= CONTEXT (SELinux) set security context of files and directories --help display this help and exit --version output version information and exit The backup suffix is '~', unless set with --suffix or SIMPLE_BACKUP_SUFFIX. The version control method may be selected via the --backup option or through the VERSION_CONTROL environment variable. Here are the values: none, off never make backups (even if --backup is given) numbered, t make numbered backups existing, nil numbered if numbered backups exist, simple otherwise simple, never always make simple backups","Process Name":"install","Link":"https:\/\/linux.die.net\/man\/1\/install"}},{"Process":{"Description":null,"Process Name":"install-datebook","Link":"https:\/\/linux.die.net\/man\/1\/install-datebook"}},{"Process":{"Description":"install-expenses allows a Palm handheld with the Expense application to have expense records synchronized to it from the commandline. You can pass any of the fields (except categories, currently) to install-expenses and create new expense records on your Palm handheld.","Process Name":"install-expenses","Link":"https:\/\/linux.die.net\/man\/1\/install-expenses"}},{"Process":{"Description":"Use this program to install \"ExtUtils::FindFunctions\" in an embedded directory of your distribution, the target directory need not to exist: install-extutils-findfunctions inc will copy the module in the inc\/ directory, typically used by other embedded modules such.","Process Name":"install-extutils-findfunctions","Link":"https:\/\/linux.die.net\/man\/1\/install-extutils-findfunctions"}},{"Process":{"Description":null,"Process Name":"install-hinote","Link":"https:\/\/linux.die.net\/man\/1\/install-hinote"}},{"Process":{"Description":"Add or remove entries in INFO-FILE from the Info directory DIR-FILE.","Process Name":"install-info","Link":"https:\/\/linux.die.net\/man\/1\/install-info"}},{"Process":{"Description":null,"Process Name":"install-memo","Link":"https:\/\/linux.die.net\/man\/1\/install-memo"}},{"Process":{"Description":"install-netsync allows the user to read or change the Network Preferences stored on the Palm.","Process Name":"install-netsync","Link":"https:\/\/linux.die.net\/man\/1\/install-netsync"}},{"Process":{"Description":"install-todo allows the user to install one new ToDo list entry with specified paramters passed to install-todo and add a Note entry using text found in a filename < filename> passed to install-todo.","Process Name":"install-todo","Link":"https:\/\/linux.die.net\/man\/1\/install-todo"}},{"Process":{"Description":null,"Process Name":"install-user","Link":"https:\/\/linux.die.net\/man\/1\/install-user"}},{"Process":{"Description":"installsieve is a utility that allows users to manage their sieve scripts kept on the server.","Process Name":"installsieve","Link":"https:\/\/linux.die.net\/man\/1\/installsieve"}},{"Process":{"Description":"","Process Name":"instant-clean","Link":"https:\/\/linux.die.net\/man\/1\/instant-clean"}},{"Process":{"Description":null,"Process Name":"instant-showcache","Link":"https:\/\/linux.die.net\/man\/1\/instant-showcache"}},{"Process":{"Description":null,"Process Name":"instmodsh","Link":"https:\/\/linux.die.net\/man\/1\/instmodsh"}},{"Process":{"Description":null,"Process Name":"intel_audio_dump","Link":"https:\/\/linux.die.net\/man\/1\/intel_audio_dump"}},{"Process":{"Description":null,"Process Name":"intel_bios_dumper","Link":"https:\/\/linux.die.net\/man\/1\/intel_bios_dumper"}},{"Process":{"Description":"intel_bios_reader is a tool to parse the contents of an Intel video BIOS file. The file can come from intel_bios_dumper. This can be used for quick debugging of video bios table handling, which is harder when done inside of the kernel graphics driver.","Process Name":"intel_bios_reader","Link":"https:\/\/linux.die.net\/man\/1\/intel_bios_reader"}},{"Process":{"Description":null,"Process Name":"intel_error_decode","Link":"https:\/\/linux.die.net\/man\/1\/intel_error_decode"}},{"Process":{"Description":"intel_gpu_dump is a tool to log the current state of an Intel GPU when it is hung, for later analysis. It requires kernel 2.6.30rc1 or newer, debugfs mounted on \/sys\/kernel\/debug or \/debug, and root privilege for mapping the device to inspect it. Options filename Decodes just one batchbuffer or ringbuffer dump, rather than dumping all of the GPU state. Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"intel_gpu_dump","Link":"https:\/\/linux.die.net\/man\/1\/intel_gpu_dump"}},{"Process":{"Description":"intel_gpu_top is a tool to display usage information of an Intel GPU. It requires root privilege to map the graphics device. Note that idle units are not displayed, so an entirely idle GPU will only display the ring status and header.","Process Name":"intel_gpu_top","Link":"https:\/\/linux.die.net\/man\/1\/intel_gpu_top"}},{"Process":{"Description":"intel_gtt is a tool to view the contents of the GTT on an Intel GPU. The GTT is the page table that maps between GPU addresses and system memory. This tool can be useful in debugging the Linux AGP driver initialization of the chip or in debugging later overwriting of the GTT with garbage data. Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"intel_gtt","Link":"https:\/\/linux.die.net\/man\/1\/intel_gtt"}},{"Process":{"Description":null,"Process Name":"intel_lid","Link":"https:\/\/linux.die.net\/man\/1\/intel_lid"}},{"Process":{"Description":null,"Process Name":"intel_reg_dumper","Link":"https:\/\/linux.die.net\/man\/1\/intel_reg_dumper"}},{"Process":{"Description":"intel_reg_read is a tool to read Intel GPU registers, for use in debugging. The register argument is given as hexadecimal.","Process Name":"intel_reg_read","Link":"https:\/\/linux.die.net\/man\/1\/intel_reg_read"}},{"Process":{"Description":null,"Process Name":"intel_reg_write","Link":"https:\/\/linux.die.net\/man\/1\/intel_reg_write"}},{"Process":{"Description":"intel_stepping is a tool to print the stepping information for an Intel GPU, along with the PCI ID and revision used to determine it. It requires root privilege to map the graphics device.","Process Name":"intel_stepping","Link":"https:\/\/linux.die.net\/man\/1\/intel_stepping"}},{"Process":{"Description":"intel_upload_blit_large is a microbenchmark tool for DRM performance. It should be run with kernel modesetting enabled, and may require root privilege for correct operation. It does not require X to be running. Given that it is a microbenchmark, its utility is largely for regression testing of the kernel, and not for general conclusions on graphics performance. Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"intel_upload_blit_large","Link":"https:\/\/linux.die.net\/man\/1\/intel_upload_blit_large"}},{"Process":{"Description":null,"Process Name":"intel_upload_blit_large_gtt","Link":"https:\/\/linux.die.net\/man\/1\/intel_upload_blit_large_gtt"}},{"Process":{"Description":"intel_upload_blit_large_map is a microbenchmark tool for DRM performance. It should be run with kernel modesetting enabled, and may require root privilege for correct operation. It does not require X to be running. Given that it is a microbenchmark, its utility is largely for regression testing of the kernel, and not for general conclusions on graphics performance. Site Search Library linux docs linux man pages page load time Toys world sunlight moon phase trace explorer","Process Name":"intel_upload_blit_large_map","Link":"https:\/\/linux.die.net\/man\/1\/intel_upload_blit_large_map"}},{"Process":{"Description":null,"Process Name":"intel_upload_blit_small","Link":"https:\/\/linux.die.net\/man\/1\/intel_upload_blit_small"}},{"Process":{"Description":null,"Process Name":"interdiff","Link":"https:\/\/linux.die.net\/man\/1\/interdiff"}},{"Process":{"Description":"Another color-field hack, this one works by computing decaying sinusoidal waves, and allowing them to interfere with each other as their origins move.","Process Name":"interference","Link":"https:\/\/linux.die.net\/man\/1\/interference"}},{"Process":{"Description":null,"Process Name":"intro","Link":"https:\/\/linux.die.net\/man\/1\/intro"}},{"Process":{"Description":"","Process Name":"introu","Link":"https:\/\/linux.die.net\/man\/1\/introu"}},{"Process":{"Description":null,"Process Name":"iodbc-config","Link":"https:\/\/linux.die.net\/man\/1\/iodbc-config"}},{"Process":{"Description":"The iodbctest program and iodbtestw programs are simple ODBC sample programs, showing the strength of the ODBC API to connect to any ODBC enabled database, issue SQL commands and retrieve the query results. The iodbctest program uses the standard ODBC API calls to connect using any DSN, but retrieves all results in ASCII mode. The iodbctestw program uses the ODBC Unicode API calls to connect using any DSN, and retrieves all results in Unicode mode.","Process Name":"iodbctest","Link":"https:\/\/linux.die.net\/man\/1\/iodbctest"}},{"Process":{"Description":"The iodbctest program and iodbtestw programs are simple ODBC sample programs, showing the strength of the ODBC API to connect to any ODBC enabled database, issue SQL commands and retrieve the query results. The iodbctest program uses the standard ODBC API calls to connect using any DSN, but retrieves all results in ASCII mode. The iodbctestw program uses the ODBC Unicode API calls to connect using any DSN, and retrieves all results in Unicode mode.","Process Name":"iodbctestw","Link":"https:\/\/linux.die.net\/man\/1\/iodbctestw"}},{"Process":{"Description":null,"Process Name":"iok","Link":"https:\/\/linux.die.net\/man\/1\/iok"}},{"Process":{"Description":"This program sets or gets the io scheduling class and priority for a program. If no arguments or just -p is given, ionice will query the current io scheduling class and priority for that process. As of this writing, a process can be in one of three scheduling classes: Idle A program running with idle io priority will only get disk time when no other program has asked for disk io for a defined grace period. The impact of idle io processes on normal system activity should be zero. This scheduling class does not take a priority argument. Presently, this scheduling class is permitted for an ordinary user (since kernel 2.6.25). Best effort This is the effective scheduling class for any process that has not asked for a specific io priority. This class takes a priority argument from 0-7, with lower number being higher priority. Programs running at the same best effort priority are served in a round-robin fashion. Note that before kernel 2.6.26 a process that has not asked for an io priority formally uses \"none\" as scheduling class, but the io scheduler will treat such processes as if it were in the best effort class. The priority within the best effort class will be dynamically derived from the cpu nice level of the process: io_priority = (cpu_nice + 20) \/ 5. For kernels after 2.6.26 with CFQ io scheduler a process that has not asked for an io priority inherits CPU scheduling class. The io priority is derived from the cpu nice level of the process (same as before kernel 2.6.26). Real time The RT scheduling class is given first access to the disk, regardless of what else is going on in the system. Thus the RT class needs to be used with some care, as it can starve other processes. As with the best effort class, 8 priority levels are defined denoting how big a time slice a given process will receive on each scheduling window. This scheduling class is not permitted for an ordinary (i.e., non-root) user.","Process Name":"ionice","Link":"https:\/\/linux.die.net\/man\/1\/ionice"}},{"Process":{"Description":null,"Process Name":"iostat","Link":"https:\/\/linux.die.net\/man\/1\/iostat"}},{"Process":{"Description":"iostat2pcp reads a text file created with iostat(1) (infile) and translates this into a Performance Co-Pilot ( PCP ) archive with the basename outfile. If infile is \"-\" then iostat2pcp reads for standard input, allowing easy preprocessing of the iostat(1) output with sed(1) or similar. The resultant PCP achive may be used with all the PCP client tools to graph subsets of the data using pmchart(1), perform data reduction and reporting, filter with the PCP inference engine pmie(1), etc. A series of physical files will be created with the prefix outfile. These are outfile.0 (the performance data), outfile.meta (the metadata that describes the performance data) and outfile.index (a temporal index to improve efficiency of replay operations for the archive). If any of these files exists already, then iostat2pcp will not overwrite them and will exit with an error message. The first output sample from iostat(1) contains a statistical summary since boot time and is ignored by iostat2pcp, so the first real data set is the second one in the iostat(1) output. The best results are obtained when iostat(1) was run with its own -t flag, so each output sample is prefixed with a timestamp. Even better is -t with $S_TIME_FORMAT=ISO set in environment when iostat(1) is run, in which case the timestamp includes the timezone. If there are no timestamps in the input stream, iostat2pcp will try and deduce the sample interval if basic Disk data (-d option for iostat(1)) is found. If this fails, then the -t option may be used to specify the sample interval in seconds. This option is ignored if timestamps are found in the input stream. The -S option may be used to specify as start time for the first real sample in infile, where start must have the format HH:MM:SS . This option is ignored if timestamps are found in the input stream. The -Z option may be used to specify a timezone. It must have the format +HHMM (for hours and minutes East of UTC ) or -HHMM (for hours and minutes West of UTC ). Note in particular that neither the zoneinfo (aka Olson) format, e.g. Europe\/Paris, nor the Posix TZ format, e.g. EST+5 is allowed for the -Z option. This option is ignored if ISO timestamps are found in the input stream. If the timezone is not specified and cannot be deduced, it defaults to \" UTC \". Some additional diagnostic output is generated with the -v option. iostat2pcp is a Perl script that uses the PCP::LogImport Perl wrapper around the PCP libpcp_import library, and as such could be used as an example to develop new tools to import other types of performance data and create PCP archives.","Process Name":"iostat2pcp","Link":"https:\/\/linux.die.net\/man\/1\/iostat2pcp"}},{"Process":{"Description":null,"Process Name":"iotop","Link":"https:\/\/linux.die.net\/man\/1\/iotop"}},{"Process":{"Description":null,"Process Name":"iozone","Link":"https:\/\/linux.die.net\/man\/1\/iozone"}},{"Process":{"Description":"Ip2cc is a program to lookup countries of IP addresses. Ip2cc has two modes: interactive and non-interactive. Interactive mode allows the user to query more than one hostname. Non-interactive mode is used to print just the country for a single host.","Process Name":"ip2cc","Link":"https:\/\/linux.die.net\/man\/1\/ip2cc"}},{"Process":{"Description":null,"Process Name":"ip6sic","Link":"https:\/\/linux.die.net\/man\/1\/ip6sic"}},{"Process":{"Description":"IPA is an integrated security information management solution based on 389 Directory Server (formerly know as Fedora Directory Server), MIT Kerberos, Dogtag Certificate System, NTP and DNS. It includes a web interface and command-line administration tools for managing identity data. This manual page focuses on the ipa script that serves as the main command-line interface (CLI) for IPA administration. More information about the project is available on its homepage located at http:\/\/www.freeipa.org.","Process Name":"ipa","Link":"https:\/\/linux.die.net\/man\/1\/ipa"}},{"Process":{"Description":null,"Process Name":"ipa-adtrust-install","Link":"https:\/\/linux.die.net\/man\/1\/ipa-adtrust-install"}},{"Process":{"Description":"Adds a CA as an IPA-managed service. This requires that the IPA server is already installed and configured. The replica_file is created using the ipa-replica-prepare utility and should be the same one used when originally installing the replica.","Process Name":"ipa-ca-install","Link":"https:\/\/linux.die.net\/man\/1\/ipa-ca-install"}},{"Process":{"Description":null,"Process Name":"ipa-client-automount","Link":"https:\/\/linux.die.net\/man\/1\/ipa-client-automount"}},{"Process":{"Description":"Configures a client machine to use IPA for authentication and identity services. By default this configures SSSD to connect to an IPA server for authentication and authorization. Optionally one can instead configure PAM and NSS (Name Switching Service) to work with an IPA server over Kerberos and LDAP. An authorized user is required to join a client machine to IPA. This can take the form of a kerberos principal or a one-time password associated with the machine. This same tool is used to unconfigure IPA and attempts to return the machine to its previous state. Part of this process is to unenroll the host from the IPA server. Unenrollment consists of disabling the prinicipal key on the IPA server so that it may be re-enrolled. The machine principal in \/etc\/krb5.keytab (host\/<fqdn>@REALM) is used to authenticate to the IPA server to unenroll itself. If this principal does not exist then unenrollment will fail and an administrator will need to disable the host principal (ipa host-disable <fqdn>). Hostname Requirements Client must use a static hostname. If the machine hostname changes for example due to a dynamic hostname assignment by a DHCP server, client enrollment to IPA server breaks and user then would not be able to perform Kerberos authentication. --hostname option may be used to specify a static hostname that persists over reboot.","Process Name":"ipa-client-install","Link":"https:\/\/linux.die.net\/man\/1\/ipa-client-install"}},{"Process":{"Description":null,"Process Name":"ipa-compat-manage","Link":"https:\/\/linux.die.net\/man\/1\/ipa-compat-manage"}},{"Process":{"Description":"Manages the CA replication agreements of an IPA server. connect [SERVER_A] <SERVER_B> - Adds a new replication agreement between SERVER_A\/localhost and SERVER_B disconnect [SERVER_A] <SERVER_B> - Removes a replication agreement between SERVER_A\/localhost and SERVER_B del <SERVER> - Removes all replication agreements and data about SERVER list [SERVER] - Lists all the servers or the list of agreements of SERVER re-initialize - Forces a full re-initialization of the IPA CA server retrieving data from the server specified with the --from option force-sync - Immediately flush any data to be replicated from a server specified with the --from option The connect and disconnect options are used to manage the replication topology. When a replica is created it is only connected with the master that created it. The connect option may be used to connect it to other existing replicas. The disconnect option cannot be used to remove the last link of a replica. To remove a replica from the topology use the del option. If a replica is deleted and then re-added within a short time-frame then the 389-ds instance on the master that created it should be restarted before re-installing the replica. The master will have the old service principals cached which will cause replication to fail.","Process Name":"ipa-csreplica-manage","Link":"https:\/\/linux.die.net\/man\/1\/ipa-csreplica-manage"}},{"Process":{"Description":null,"Process Name":"ipa-dns-install","Link":"https:\/\/linux.die.net\/man\/1\/ipa-dns-install"}},{"Process":{"Description":"The ipa-getcert tool issues requests to a org.fedorahosted.certmonger service on behalf of the invoking user. It can ask the service to begin enrollment, optionally generating a key pair to use, it can ask the service to begin monitoring a certificate in a specified location for expiration, and optionally to refresh it when expiration nears, it can list the set of certificates that the service is already monitoring, or it can list the set of CAs that the service is capable of using. If no command is given as the first command-line argument, ipa-getcert will print short usage information for each of its functions. The ipa-getcert tool behaves identically to the generic getcert tool when it is used with the -c IPA option.","Process Name":"ipa-getcert","Link":"https:\/\/linux.die.net\/man\/1\/ipa-getcert"}},{"Process":{"Description":null,"Process Name":"ipa-getkeytab","Link":"https:\/\/linux.die.net\/man\/1\/ipa-getkeytab"}},{"Process":{"Description":"Joins a host to an IPA realm and retrieves a kerberos keytab for the host service principal, or unenrolls an enrolled host from an IPA server. Kerberos keytabs are used for services (like sshd) to perform kerberos authentication. A keytab is a file with one or more secrets (or keys) for a kerberos principal. The ipa-join command will create and retrieve a service principal for host\/foo.example.com@EXAMPLE.COM and place it by default into \/etc\/krb5.keytab. The location can be overridden with the -k option. The IPA server to contact is set in \/etc\/ipa\/default.conf by default and can be overridden using the -s,--server option. In order to join the machine needs to be authenticated. This can happen in one of two ways: * Authenticate using the current kerberos principal * Provide a password to authenticate with If a client host has already been joined to the IPA realm the ipa-join command will fail. The host will need to be removed from the server using 'ipa host-del FQDN' in order to join the client to the realm. This command is normally executed by the ipa-client-install command as part of the enrollment process. The reverse is unenrollment. Unenrolling a host removes the Kerberos key on the IPA server. This prepares the host to be re-enrolled. This uses the host principal stored in \/etc\/krb5.conf to authenticate to the IPA server to perform the unenrollment. Please note, that while the ipa-join option removes the client from the domain, it does not actually uninstall the client or properly remove all of the IPA-related configuration. The only way to uninstall a client completely is to use ipa-client-install --uninstall (see ipa-client-install(1)).","Process Name":"ipa-join","Link":"https:\/\/linux.die.net\/man\/1\/ipa-join"}},{"Process":{"Description":null,"Process Name":"ipa-ldap-updater","Link":"https:\/\/linux.die.net\/man\/1\/ipa-ldap-updater"}},{"Process":{"Description":"Run the command with the enable option to enable the Managed Entry plugin. Run the command with the disable option to disable the Managed Entry plugin. Run the command with the status to determine the current status of the Managed Entry plugin. In all cases the user will be prompted to provide the Directory Manager's password unless option -p is used. Directory Server will need to be restarted after the Managed Entry plugin has been enabled.","Process Name":"ipa-managed-entries","Link":"https:\/\/linux.die.net\/man\/1\/ipa-managed-entries"}},{"Process":{"Description":null,"Process Name":"ipa-nis-manage","Link":"https:\/\/linux.die.net\/man\/1\/ipa-nis-manage"}},{"Process":{"Description":"When an IPA replica is being installed a network connection between a replica machine and a replicated IPA master machine has to be prepared for master-replica communication. In case of a flawed connection the installation may fail with inconvenient error messages. A common connection problem is a misconfigured firewall with closed required port on a replica or master machine. The connection is checked by running a set of tests from both master and replica machines. The program is incorporated to ipa-replica-install(1) but can be also run separately.","Process Name":"ipa-replica-conncheck","Link":"https:\/\/linux.die.net\/man\/1\/ipa-replica-conncheck"}},{"Process":{"Description":"Configures a new IPA server that is a replica of the server that generated it. Once it has been created it is an exact copy of the original IPA server and is an equal master. Changes made to any master are automatically replicated to other masters. The replica_file is created using the ipa-replica-prepare utility. If the installation fails you may need to run ipa-server-install --uninstall before running ipa-replica-install again. The installation will fail if the host you are installing the replica on exists as a host in IPA or an existing replication agreement exists (for example, from a previously failed installation). A replica should only be installed on the same or higher version of IPA on the remote system.","Process Name":"ipa-replica-install","Link":"https:\/\/linux.die.net\/man\/1\/ipa-replica-install"}},{"Process":{"Description":"Manages the replication agreements of an IPA server. connect [SERVER_A] <SERVER_B> - Adds a new replication agreement between SERVER_A\/localhost and SERVER_B disconnect [SERVER_A] <SERVER_B> - Removes a replication agreement between SERVER_A\/localhost and SERVER_B del <SERVER> - Removes all replication agreements and data about SERVER list [SERVER] - Lists all the servers or the list of agreements of SERVER re-initialize - Forces a full re-initialization of the IPA server retrieving data from the server specified with the --from option force-sync - Immediately flush any data to be replicated from a server specified with the --from option list-ruv - List the replication IDs on this server. clean-ruv [REPLICATION_ID] - Run the CLEANALLRUV task to remove a replication ID. abort-clean-ruv [REPLICATION_ID] - Abort a running CLEANALLRUV task. list-clean-ruv - List all running CLEANALLRUV and abort CLEANALLRUV tasks. The connect and disconnect options are used to manage the replication topology. When a replica is created it is only connected with the master that created it. The connect option may be used to connect it to other existing replicas. The disconnect option cannot be used to remove the last link of a replica. To remove a replica from the topology use the del option. If a replica is deleted and then re-added within a short time-frame then the 389-ds instance on the master that created it should be restarted before re-installing the replica. The master will have the old service principals cached which will cause replication to fail. Each IPA master server has a unique replication ID. This ID is used by 389-ds-base when storing information about replication status. The output consists of the masters and their respective replication ID. See clean-ruv When a master is removed, all other masters need to remove its replication ID from the list of masters. Normally this occurs automatically when a master is deleted with ipa-replica-manage. If one or more masters was down or unreachable when ipa-replica-manage was executed then this replica ID may still exist. The clean-ruv command may be used to clean up an unused replication ID. NOTE: clean-ruv is VERY DANGEROUS. Execution against the wrong replication ID can result in inconsistent data on that master. The master should be re-initialized from another if this happens. The replication topology is examined when a master is deleted and will attempt to prevent a master from being orphaned. For example, if your topology is A <-> B <-> C and you attempt to delete master B it will fail because that would leave masters and A and C orphaned. The list of masters is stored in cn=masters,cn=ipa,cn=etc,dc=example,dc=com. This should be cleaned up automatically when a master is deleted. If it occurs that you have deleted the master and all the agreements but these entries still exist then you will not be able to re-install IPA on it, the installation will fail with: An IPA master host cannot be deleted or disabled using standard commands (host-del, for example). An orphaned master may be cleaned up using the del directive with the --cleanup option. This will remove the entries from cn=masters,cn=ipa,cn=etc that otherwise prevent host-del from working, its dna profile, s4u2proxy configuration, service principals and remove it from the default DUA profile defaultServerList.","Process Name":"ipa-replica-manage","Link":"https:\/\/linux.die.net\/man\/1\/ipa-replica-manage"}},{"Process":{"Description":null,"Process Name":"ipa-replica-prepare","Link":"https:\/\/linux.die.net\/man\/1\/ipa-replica-prepare"}},{"Process":{"Description":"Removes a kerberos principal from a keytab. Kerberos keytabs are used for services (like sshd) to perform kerberos authentication. A keytab is a file with one or more secrets (or keys) for a kerberos principal. A kerberos service principal is a kerberos identity that can be used for authentication. Service principals contain the name of the service, the hostname of the server, and the realm name. ipa-rmkeytab provides two ways to remove principals. A specific principal can be removed or all principals for a given realm can be removed. All encryption types and versions of a principal are removed. The realm may be included when removing a specific principal but it is not required. NOTE: removing a principal from the keytab does not affect the Kerberos principal stored in the IPA server. It merely removes the entry from the local keytab.","Process Name":"ipa-rmkeytab","Link":"https:\/\/linux.die.net\/man\/1\/ipa-rmkeytab"}},{"Process":{"Description":null,"Process Name":"ipa-server-certinstall","Link":"https:\/\/linux.die.net\/man\/1\/ipa-server-certinstall"}},{"Process":{"Description":"Configures the services needed by an IPA server. This includes setting up a Kerberos Key Distribution Center (KDC) and a Kadmin daemon with an LDAP back-end, configuring Apache, configuring NTP and optionally configuring and starting an LDAP-backed DNS server. By default a dogtag-based CA will be configured to issue server certificates.","Process Name":"ipa-server-install","Link":"https:\/\/linux.die.net\/man\/1\/ipa-server-install"}},{"Process":{"Description":null,"Process Name":"ipcalc","Link":"https:\/\/linux.die.net\/man\/1\/ipcalc"}},{"Process":{"Description":"ipcclean removes all shared memory segments and semaphore sets owned by the current user. It is intended to be used for cleaning up after a crashed PostgreSQL server (postmaster(1)). Note that immediately restarting the server will also clean up shared memory and semaphores, so this command is of little real utility. Only the database administrator should execute this program as it can cause bizarre behavior (i.e., crashes) if run during multiuser execution. If this command is executed while a server is running, the shared memory and semaphores allocated by that server will be deleted, which would have rather severe consequences for that server.","Process Name":"ipcclean","Link":"https:\/\/linux.die.net\/man\/1\/ipcclean"}},{"Process":{"Description":null,"Process Name":"ipcluster","Link":"https:\/\/linux.die.net\/man\/1\/ipcluster"}},{"Process":{"Description":"ipcmk allows you to create shared memory segments, message queues or semaphore arrays.","Process Name":"ipcmk","Link":"https:\/\/linux.die.net\/man\/1\/ipcmk"}},{"Process":{"Description":null,"Process Name":"ipcontroller","Link":"https:\/\/linux.die.net\/man\/1\/ipcontroller"}},{"Process":{"Description":null,"Process Name":"ipcrm","Link":"https:\/\/linux.die.net\/man\/1\/ipcrm"}},{"Process":{"Description":"ipcs provides information on the ipc facilities for which the calling process has read access. The -i option allows a specific resource id to be specified. Only information on this id will be printed. Resources may be specified as follows: -m shared memory segments -q message queues -s semaphore arrays -a all (this is the default) The output format may be specified as follows: -t time -p pid -c creator -l limits -u summary","Process Name":"ipcs","Link":"https:\/\/linux.die.net\/man\/1\/ipcs"}},{"Process":{"Description":null,"Process Name":"ipengine","Link":"https:\/\/linux.die.net\/man\/1\/ipengine"}},{"Process":{"Description":"iperf is a tool for performing network throughput measurements. It can test either TCP or UDP throughput. To perform an iperf test the user must establish both a server (to discard traffic) and a client (to generate traffic).","Process Name":"iperf","Link":"https:\/\/linux.die.net\/man\/1\/iperf"}},{"Process":{"Description":null,"Process Name":"ipmi_ui","Link":"https:\/\/linux.die.net\/man\/1\/ipmi_ui"}},{"Process":{"Description":"The ipmicmd program allows a user to execute direct IPMI commands. It can work with direct interface with the OpenIPMI driver or with IPMI LAN interfaces.","Process Name":"ipmicmd","Link":"https:\/\/linux.die.net\/man\/1\/ipmicmd"}},{"Process":{"Description":null,"Process Name":"ipmish","Link":"https:\/\/linux.die.net\/man\/1\/ipmish"}},{"Process":{"Description":"This program lets you manage Intelligent Platform Management Interface (IPMI) functions of either the local system, via a kernel device driver, or a remote system, using IPMI V1.5 and IPMI v2.0. These functions include printing FRU information, LAN configuration, sensor readings, and remote chassis power control. IPMI management of a local system interface requires a compatible IPMI kernel driver to be installed and configured. On Linux this driver is called OpenIPMI and it is included in standard distributions. On Solaris this driver is called BMC and is inclued in Solaris 10. Management of a remote station requires the IPMI-over-LAN interface to be enabled and configured. Depending on the particular requirements of each system it may be possible to enable the LAN interface using ipmitool over the system interface.","Process Name":"ipmitool","Link":"https:\/\/linux.die.net\/man\/1\/ipmitool"}},{"Process":{"Description":"This program is part of the OpenImageIO (http:\/\/www.openimageio.org) tool suite. Detailed documentation is avaliable in pdf format with the OpenImageIO distribution.","Process Name":"iprocess","Link":"https:\/\/linux.die.net\/man\/1\/iprocess"}},{"Process":{"Description":null,"Process Name":"ipv4calc","Link":"https:\/\/linux.die.net\/man\/1\/ipv4calc"}},{"Process":{"Description":"An interactive Python shell with automatic history (input and output), dynamic object introspection, easier configuration, command completion, access to the system shell, integration with numerical and scientific computing tools, and more.","Process Name":"ipython","Link":"https:\/\/linux.die.net\/man\/1\/ipython"}},{"Process":{"Description":"ipython-wx is, compared to ipythonx, a more advanced graphical frontend to IPython. It is implemented using WxWidgets.","Process Name":"ipython-wx","Link":"https:\/\/linux.die.net\/man\/1\/ipython-wx"}},{"Process":{"Description":null,"Process Name":"ipythonx","Link":"https:\/\/linux.die.net\/man\/1\/ipythonx"}},{"Process":{"Description":"ir-keytable is a tool that lists the Remote Controller devices, allows to get\/set IR keycode\/scancode tables, test events generated by IR, and to adjust other Remote Controller options. Note: You need to have read permissions on \/dev\/input for most of the options to work.","Process Name":"ir-keytable","Link":"https:\/\/linux.die.net\/man\/1\/ir-keytable"}},{"Process":{"Description":"","Process Name":"irb","Link":"https:\/\/linux.die.net\/man\/1\/irb"}},{"Process":{"Description":null,"Process Name":"ircat","Link":"https:\/\/linux.die.net\/man\/1\/ircat"}},{"Process":{"Description":"This program lets you execute arbitrary commands on an IR signal decoded by lircd, the LIRC daemon. You can give irexec a command line parameter which has to be a name of a valid config file. If no command line parameters are given irexec reads the default config file which is usually ~\/.lircrc. If irexec executes a program it will wait until this program terminates. So append a '&' to the command string if you don't want that. The config string consists of the command to be run. -h --help display usage summary -v --version display version -d --daemon run in background -n --name use this program name","Process Name":"irexec","Link":"https:\/\/linux.die.net\/man\/1\/irexec"}},{"Process":{"Description":null,"Process Name":"irpty","Link":"https:\/\/linux.die.net\/man\/1\/irpty"}},{"Process":{"Description":"The purpose of irqbalance is distribute hardware interrupts across processors on a multiprocessor system in order to increase performance.","Process Name":"irqbalance","Link":"https:\/\/linux.die.net\/man\/1\/irqbalance"}},{"Process":{"Description":null,"Process Name":"irrecord","Link":"https:\/\/linux.die.net\/man\/1\/irrecord"}},{"Process":{"Description":"Asks the lircd daemon to send one or more CIR (Consumer Infra-Red) commands. This is intended for remote control of electronic devices such as TV boxes, HiFi sets, etc. DIRECTIVE can be: SEND_ONCE         - send CODE [CODE ...] once\nSEND_START        - start repeating CODE\nSEND_STOP         - stop repeating CODE\nLIST              - list configured remote items\nSET_TRANSMITTERS  - set transmitters NUM [NUM ...]\nSIMULATE          - simulate IR event REMOTE is the name of a remote, as described in the lircd configuration file. CODE is the name of a remote control key of REMOTE, as it appears in the lircd configuration file. NUM is the transmitter number of the hardware device. For the LIST DIRECTIVE, REMOTE and\/or CODE can be empty: LIST   \"\"    \"\"   - list all configured remote names\nLIST REMOTE  \"\"   - list all codes of REMOTE\nLIST REMOTE CODE  - list only CODE of REMOTE The SIMULATE command only works if it has been explicitly enabled in lircd. -h --help display usage summary -v --version display version -d --device use given lircd socket [\/var\/run\/lirc\/lircd] -a --address= host[:port] connect to lircd at this address -# --count= n send command n times","Process Name":"irsend","Link":"https:\/\/linux.die.net\/man\/1\/irsend"}},{"Process":{"Description":null,"Process Name":"irsim","Link":"https:\/\/linux.die.net\/man\/1\/irsim"}},{"Process":{"Description":"Irssi is a modular Internet Relay Chat client. It is highly extensible and very secure. Being a fullscreen, termcap based client with many features, Irssi is easily extensible through scripts and modules.","Process Name":"irssi","Link":"https:\/\/linux.die.net\/man\/1\/irssi"}},{"Process":{"Description":"irunner is an interface to the various interactive runners available in IPython's irunner module. The already implemented runners are listed below; adding one for a new program is a trivial task, see the source for examples.","Process Name":"irunner","Link":"https:\/\/linux.die.net\/man\/1\/irunner"}},{"Process":{"Description":null,"Process Name":"irw","Link":"https:\/\/linux.die.net\/man\/1\/irw"}},{"Process":{"Description":"Irxevent is a program that I wrote to send button clicks and key presses to X applications triggered by a LIRC driven remote control. You can control your favorite CD\/MP3 player or your TV tuner program or any other X application that responds to keyboard or mouse input. If you like to you can send emacs ^X^S from your armchair. Irxevent is a complement to irexec and irpty. -d --daemon fork and run in background -h --help display usage summary -V --version display version","Process Name":"irxevent","Link":"https:\/\/linux.die.net\/man\/1\/irxevent"}},{"Process":{"Description":null,"Process Name":"isamchk","Link":"https:\/\/linux.die.net\/man\/1\/isamchk"}},{"Process":{"Description":"myisamlog processes the contents of a MyISAM log file. Invoke myisamlog like this: shell> myisamlog [options] [log_file [tbl_name] ...]\nshell> isamlog [options] [log_file [tbl_name] ...]\n The default operation is update ( -u). If a recovery is done ( -r), all writes and possibly updates and deletes are done and errors are only counted. The default log file name is myisam.log for myisamlog and isam.log for isamlog if no log_file argument is given. If tables are named on the command line, only those tables are updated. myisamlog supports the following options: \u2022 -?, -I Display a help message and exit. \u2022 -c N Execute only N commands. \u2022 -f N Specify the maximum number of open files. \u2022 -i Display extra information before exiting. \u2022 -o offset Specify the starting offset. \u2022 -p N Remove N components from path. \u2022 -r Perform a recovery operation. \u2022 -R record_pos_file record_pos Specify record position file and record position. \u2022 -u Perform an update operation. \u2022 -v Verbose mode. Print more output about what the program does. This option can be given multiple times to produce more and more output. \u2022 -w write_file Specify the write file. \u2022 -V Display version information.","Process Name":"isamlog","Link":"https:\/\/linux.die.net\/man\/1\/isamlog"}},{"Process":{"Description":null,"Process Name":"isatty","Link":"https:\/\/linux.die.net\/man\/1\/isatty"}},{"Process":{"Description":"isc-config.sh prints information related to the installed version of ISC BIND, such as the compiler and linker flags required to compile and link programs that use ISC BIND libraries. The optional libraries are used to report specific details for compiling and linking for the listed libraries. The allowed choices are: isc, isccc, isccfg, dns, lwres, and bind9. Multiple libraries may be listed on the command line. (Some libraries require other libraries, so are implied.)","Process Name":"isc-config.sh","Link":"https:\/\/linux.die.net\/man\/1\/isc-config.sh"}},{"Process":{"Description":null,"Process Name":"iscsi-ls","Link":"https:\/\/linux.die.net\/man\/1\/iscsi-ls"}},{"Process":{"Description":null,"Process Name":"isdnbill","Link":"https:\/\/linux.die.net\/man\/1\/isdnbill"}},{"Process":{"Description":"isdnconf can manipulate or read the file \/etc\/isdn\/callerid.conf as well as ~\/.isdn. Entries can be added or removed from these files. Additionally, entries can be searched for and displayed in a way similar to grep. An entry can be an own MSN ( [MSN]) or a phone number ( [NUMBER]). You can use this program to build your own phonebook. These files are used by many of the other ISDN utilities that use phone numbers, to display a number symbolicly instead of as a plain number.","Process Name":"isdnconf","Link":"https:\/\/linux.die.net\/man\/1\/isdnconf"}},{"Process":{"Description":null,"Process Name":"isdnrate","Link":"https:\/\/linux.die.net\/man\/1\/isdnrate"}},{"Process":{"Description":null,"Process Name":"isdnrep","Link":"https:\/\/linux.die.net\/man\/1\/isdnrep"}},{"Process":{"Description":"The bibutils program set inter-converts between various bibliography formats using Library of Congress [1] 's Metadata Object Description Schema (MODS) [2] version 3.1. For example, one can convert RIS-format files to Bibtex by doing two transformations: RIS->MODS->Bibtex.","Process Name":"isi2xml","Link":"https:\/\/linux.die.net\/man\/1\/isi2xml"}},{"Process":{"Description":null,"Process Name":"isic","Link":"https:\/\/linux.die.net\/man\/1\/isic"}},{"Process":{"Description":"The isistest command is for managing information stored in the distributed P2P Grid information cloud of ISIS services.","Process Name":"isistest","Link":"https:\/\/linux.die.net\/man\/1\/isistest"}},{"Process":{"Description":"-d, --debug= INT Set debugging to LEVEL -i, --input[= FILE] Filename to read ISO-9960 image from -f Generate output similar to 'find . -print' -l, --iso9660 Generate output similar to 'ls -lR' --no-header Don't display header and copyright (for regression testing) --no-joliet Don't use Joliet-extension information --no-rock-ridge Don't use Rock-Ridge-extension information --no-xa Don't use XA-extension information -q, --quiet Don't produce warning output -V, --version display version and copyright information and exit Help options: -?, --help Show this help message --usage Display brief usage message","Process Name":"iso-info","Link":"https:\/\/linux.die.net\/man\/1\/iso-info"}},{"Process":{"Description":null,"Process Name":"iso-read","Link":"https:\/\/linux.die.net\/man\/1\/iso-read"}},{"Process":{"Description":"Isodebug reads the debug info written by genisoimage(8) from within a ISO-9660 file system image and prints them.","Process Name":"isodebug","Link":"https:\/\/linux.die.net\/man\/1\/isodebug"}},{"Process":{"Description":null,"Process Name":"isogmt","Link":"https:\/\/linux.die.net\/man\/1\/isogmt"}},{"Process":{"Description":"devdump is a crude utility to interactively display the contents of device or filesystem images. The initial screen is a display of the first 256 bytes of the first 2048 byte sector. The commands are the same as with isodump. isodump is a crude utility to interactively display the contents of iso9660 images in order to verify directory integrity. The initial screen is a display of the first part of the root directory, and the prompt shows you the extent number and offset in the extent. You can use the 'a' and 'b' commands to move backwards and forwards within the image. The 'g' command allows you to goto an arbitrary extent, and the 'f' command specifies a search string to be used. The '+' command searches forward for the next instance of the search string, and the 'q' command exits devdump or isodump. isoinfo is a utility to perform directory like listings of iso9660 images. isovfy is a utility to verify the integrity of an iso9660 image. Most of the tests in isovfy were added after bugs were discovered in early versions of genisoimage. It isn't all that clear how useful this is anymore, but it doesn't hurt to have this around.","Process Name":"isoinfo","Link":"https:\/\/linux.die.net\/man\/1\/isoinfo"}},{"Process":{"Description":"ISO Master is an open-source, easy to use, graphical CD image editor for Linux and BSD. Basically you can use this program to extract files from an ISO, add files to an ISO, and create bootable ISOs - all in a graphical user interface. ISO Master can open ISO, NRG, and some MDF files but can only save as ISO.","Process Name":"isomaster","Link":"https:\/\/linux.die.net\/man\/1\/isomaster"}},{"Process":{"Description":"The issndfile program checks the specified file to see if it is in one of the recognized audio file formats and, if it is, returns a zero (true) exit status; otherwise, a non-zero (false) exit status is returned.","Process Name":"issndfile","Link":"https:\/\/linux.die.net\/man\/1\/issndfile"}},{"Process":{"Description":"isutf8 checks whether files are syntactically valid UTF-8. Input is either files named on the command line, or the standard input. Notices about files with invalid UTF-8 are printed to standard output.","Process Name":"isutf8","Link":"https:\/\/linux.die.net\/man\/1\/isutf8"}},{"Process":{"Description":"isympy is a Python shell for SymPy. It is just a normal python shell (ipython shell if you have the ipython package installed) that executes the following commands so that you don't have to: >>> from __future__ import division\n>>> from sympy import *\n>>> x, y, z = symbols(\"xyz\")\n>>> k, m, n = symbols(\"kmn\", integer=True) So starting isympy is equivalent to starting python (or ipython) and executing the above commands by hand. It is intended for easy and quick experimentation with SymPy. For more complicated programs, it is recommended to write a script and import things explicitly (using the \"from sympy import sin, log, Symbol, ...\" idiom).","Process Name":"isympy","Link":"https:\/\/linux.die.net\/man\/1\/isympy"}},{"Process":{"Description":"isync is a command line application which synchronizes local Maildir mailboxes with remote IMAP4 mailboxes, suitable for use in IMAP-disconnected mode. Multiple copies of the remote IMAP4 mailboxes can be maintained, and all flags are synchronized. isync is only a wrapper binary around mbsync to simplify upgrades. It will automatically migrate the UID mapping from previous versions of isync (even before 0.8) to the new format, and transparently call mbsync. If you were using isync version 0.8 or 0.9.x you might want to use mdconvert to convert the mailboxes to the more efficient native UID storage scheme after migrating them.","Process Name":"isync","Link":"https:\/\/linux.die.net\/man\/1\/isync"}},{"Process":{"Description":"itstool extracts messages from XML files and outputs PO template files, then merges translations from MO files to create translated XML files. It determines what to translate and how to chunk it into messages using the W3C Internationalization Tag Set (ITS). To extract messages from XML files FILES and output them to OUT.pot: itstool -o OUT.pot FILES After merging with existing translations or translating strings, generate an MO file with msgfmt(1), then output translated files to the directory DIR: itstool -m OUT.mo -o DIR FILES ITS definitions are loaded from the built-in rules, rules embedded in the source XML files, files passed with the -i option, and ITS attributes in the source XML files. Later definitions take precedence.","Process Name":"itstool","Link":"https:\/\/linux.die.net\/man\/1\/itstool"}},{"Process":{"Description":null,"Process Name":"iv","Link":"https:\/\/linux.die.net\/man\/1\/iv"}},{"Process":{"Description":"iverilog is a compiler that translates Verilog source code into executable programs for simulation, or other netlist formats for further processing. The currently supported targets are vvp for simulation, and fpga for synthesis. Other target types are added as code generators are implemented.","Process Name":"iverilog","Link":"https:\/\/linux.die.net\/man\/1\/iverilog"}},{"Process":{"Description":null,"Process Name":"iverilog-vpi","Link":"https:\/\/linux.die.net\/man\/1\/iverilog-vpi"}},{"Process":{"Description":null,"Process Name":"ivstools","Link":"https:\/\/linux.die.net\/man\/1\/ivstools"}}]