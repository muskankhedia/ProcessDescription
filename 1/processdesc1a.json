[{"Process":{"Description":"A2p takes an awk script specified on the command line (or from standard input) and produces a comparable perl script on the standard output. OPTIONS Options include: -D<number> sets debugging flags. -F<character> tells a2p that this awk script is always invoked with this -F switch. -n<fieldlist> specifies the names of the input fields if input does not have to be split into an array. If you were translating an awk script that processes the password file, you might say: a2p -7 -nlogin.password.uid.gid.gcos.shell.home Any delimiter can be used to separate the field names. -<number> causes a2p to assume that input will always have that many fields. -o tells a2p to use old awk behavior. The only current differences are: \u2022 Old awk always has a line loop, even if there are no line actions, whereas new awk does not. \u2022 In old awk, sprintf is extremely greedy about its arguments. For example, given the statement print sprintf(some_args), extra_args; old awk considers extra_args to be arguments to \"sprintf\"; new awk considers them arguments to \"print\". \"Considerations\" A2p cannot do as good a job translating as a human would, but it usually does pretty well. There are some areas where you may want to examine the perl script produced and tweak it some. Here are some of them, in no particular order. There is an awk idiom of putting int() around a string expression to force numeric interpretation, even though the argument is always integer anyway. This is generally unneeded in perl, but a2p can't tell if the argument is always going to be integer, so it leaves it in. You may wish to remove it. Perl differentiates numeric comparison from string comparison. Awk has one operator for both that decides at run time which comparison to do. A2p does not try to do a complete job of awk emulation at this point. Instead it guesses which one you want. It's almost always right, but it can be spoofed. All such guesses are marked with the comment \"\"#???\"\". You should go through and check them. You might want to run at least once with the -w switch to perl, which will warn you if you use == where you should have used eq. Perl does not attempt to emulate the behavior of awk in which nonexistent array elements spring into existence simply by being referenced. If somehow you are relying on this mechanism to create null entries for a subsequent for...in, they won't be there in perl. If a2p makes a split line that assigns to a list of variables that looks like (Fld1, Fld2, Fld3...) you may want to rerun a2p using the -n option mentioned above. This will let you name the fields throughout the script. If it splits to an array instead, the script is probably referring to the number of fields somewhere. The exit statement in awk doesn't necessarily exit; it goes to the END block if there is one. Awk scripts that do contortions within the END block to bypass the block under such circumstances can be simplified by removing the conditional in the END block and just exiting directly from the perl script. Perl has two kinds of array, numerically-indexed and associative. Perl associative arrays are called \"hashes\". Awk arrays are usually translated to hashes, but if you happen to know that the index is always going to be numeric you could change the {...} to [...]. Iteration over a hash is done using the keys() function, but iteration over an array is NOT . You might need to modify any loop that iterates over such an array. Awk starts by assuming OFMT has the value %.6g. Perl starts by assuming its equivalent, $#, to have the value %.20g. You'll want to set $# explicitly if you use the default value of OFMT . Near the top of the line loop will be the split operation that is implicit in the awk script. There are times when you can move this down past some conditionals that test the entire record so that the split is not done as often. For aesthetic reasons you may wish to change the array base $[ from 1 back to perl's default of 0, but remember to change all array subscripts AND all substr() and index() operations to match. Cute comments that say \"# Here is a workaround because awk is dumb\" are passed through unmodified. Awk scripts are often embedded in a shell script that pipes stuff into and out of awk. Often the shell script wrapper can be incorporated into the perl script, since perl can start up pipes into and out of itself, and can do other things that awk can't do by itself. Scripts that refer to the special variables RSTART and RLENGTH can often be simplified by referring to the variables $', $& and $', as long as they are within the scope of the pattern match that sets them. The produced perl script may have subroutines defined to deal with awk's semantics regarding getline and print. Since a2p usually picks correctness over efficiency. it is almost always possible to rewrite such code to be more efficient by discarding the semantic sugar. For efficiency, you may wish to remove the keyword from any return statement that is the last statement executed in a subroutine. A2p catches the most common case, but doesn't analyze embedded blocks for subtler cases. ARGV[0] translates to $ARGV0, but ARGV[n] translates to $ARGV[$n]. A loop that tries to iterate over ARGV[0] won't find it.","Process Name":"a2p","Link":"https:\/\/linux.die.net\/man\/1\/a2p"}},{"Process":{"Description":"Convert FILE(s) or standard input to PostScript. By default, the output is sent to the default printer. An output file may be specified with -o. Mandatory arguments to long options are mandatory for short options too. Long options marked with * require a yes\/no argument, corresponding short options stand for 'yes'. Tasks: --version display version --help display this help --guess report guessed types of FILES --which report the full path of library files named FILES --glob report the full path of library files matching FILES --list= defaults display default settings and parameters --list= TOPIC detailed list on TOPIC (delegations, encodings, features, variables, media, ppd, printers, prologues, style-sheets, user-options) After having performed the task, exit successfully. Detailed lists may provide additional help on specific features. Global: -q, --quiet, --silent be really quiet -v, --verbose[= LEVEL] set verbosity on, or to LEVEL -=, --user-option= OPTION use the user defined shortcut OPTION --debug enable debugging features -D, --define= KEY[:VALUE] unset variable KEY or set to VALUE Sheets: -M, --medium= NAME use output medium NAME -r, --landscape print in landscape mode -R, --portrait print in portrait mode --columns= NUM number of columns per sheet --rows= NUM number of rows per sheet --major= DIRECTION first fill (DIRECTION=) rows, or columns -1, -2, ..., -9 predefined font sizes and layouts for 1.. 9 virtuals -A, --file-align= MODE align separate files according to MODE (fill, rank page, sheet, or a number) -j, --borders* print borders around columns --margin[= NUM] define an interior margin of size NUM The options -1.. -9 affect several primitive parameters to set up predefined layouts with 80 columns. Therefore the order matters: '-R -f40 -2' is equivalent to '-2'. To modify the layout, use '-2Rf40', or compose primitive options ('--columns', '--font-size' etc.). Virtual pages: --line-numbers= NUM precede each NUM lines with its line number -C alias for --line-numbers=5 -f, --font-size= SIZE use font SIZE (float) for the body text -L, --lines-per-page= NUM scale the font to print NUM lines per virtual -l, --chars-per-line= NUM scale the font to print NUM columns per virtual -m, --catman process FILE as a man page (same as -L66) -T, --tabsize= NUM set tabulator size to NUM --non-printable-format= FMT specify how non-printable chars are printed Headings: -B, --no-header no page headers at all -b, --header[= TEXT] set page header -u, --underlay[= TEXT] print TEXT under every page --center-title[= TEXT] set page title to TITLE --left-title[= TEXT] set left and right page title to TEXT --right-title[= TEXT] --left-footer[= TEXT] set sheet footers to TEXT --footer[= TEXT] --right-footer[= TEXT] The TEXTs may use special escapes. Input: -a, --pages[= RANGE] select the pages to print -c, --truncate-lines* cut long lines -i, --interpret* interpret tab, bs and ff chars --end-of-line= TYPE specify the eol char (TYPE: r, n, nr, rn, any) -X, --encoding= NAME use input encoding NAME -t, --title= NAME set the name of the job --stdin= NAME set the name of the input file stdin --print-anyway* force binary printing -Z, --delegate* delegate files to another application --toc[= TEXT] generate a table of content When delegations are enabled, a2ps may use other applications to handle the processing of files that should not be printed as raw information, e.g., HTML PostScript, PDF etc. Pretty-printing: -E, --pretty-print[= LANG] enable pretty-printing (set style to LANG) --highlight-level= LEVEL set pretty printing highlight LEVEL LEVEL can be none, normal or heavy -g alias for --highlight-level=heavy --strip-level= NUM level of comments stripping Output: -o, --output= FILE leave output to file FILE. If FILE is '-', leave output to stdout. --version-control= WORD override the usual version control --suffix= SUFFIX override the usual backup suffix -P, --printer= NAME send output to printer NAME -d send output to the default printer (this is the default behavior) PostScript: --prologue= FILE include FILE.pro as PostScript prologue --ppd[= KEY] automatic PPD selection or set to KEY -n, --copies= NUM print NUM copies of each page -s, --sides= MODE set the duplex MODE ('1' or 'simplex', '2' or 'duplex', 'tumble') -S, --setpagedevice= K[:V] pass a page device definition to output --statusdict= K[:[:]V] pass a statusdict definition to the output -k, --page-prefeed enable page prefeed -K, --no-page-prefeed disable page prefeed By default a2ps is tuned to do what you want to, so trust it. To pretty print the content of the 'src' directory and a table of content, and send the result to the printer 'lw', $ a2ps -P lw --toc src\/* To process the files 'sample.ps' and 'sample.html' and display the result, $ a2ps -P display sample.ps sample.html To process a mailbox in 4 up, $ a2ps -=mail -4 mailbox To print as a booklet on the default printer, which is Duplex capable, $ a2ps -=book paper.dvi.gz -d News, updates and documentation: visit http:\/\/www.gnu.org\/software\/a2ps\/.","Process Name":"a2ps","Link":"https:\/\/linux.die.net\/man\/1\/a2ps"}},{"Process":{"Description":"A DocBook toolchain wrapper script that translates an AsciiDoc text file FILE to PDF, DVI, PS, LaTeX, XHTML (single page or chunked), man page, HTML Help or plain text formats. PDF, XHTML, man page and HTML Help formats are are generated using the asciidoc(1), xsltproc(1), DocBook XSL Stylesheets, dblatex (or FOP) toolchain. Plain text is produced by passing asciidoc(1) generated HTML through lynx(1). The htmlhelp format option generates .hhp, .hhc and .html files suitable for compilation to an HTML Help .chm file.","Process Name":"a2x","Link":"https:\/\/linux.die.net\/man\/1\/a2x"}},{"Process":{"Description":"'a52dec' plays ATSC A\/52 audio streams. Input is from stdin if no file is given. -h display help and available audio modes -s use program stream demultiplexer, track 0-7 or 0x80-0x87 -t pid use transport stream demultiplexer, pid 0x10-0x1ffe -c use c implementation, disables all accelerations -r disable dynamic range compression -o mode use audio output driver 'mode'. A list of modes is available using the -h option.","Process Name":"a52dec","Link":"https:\/\/linux.die.net\/man\/1\/a52dec"}},{"Process":{"Description":"This manual page documents briefly the aafire, aainfo, aasavefont, and aatest programs. All of these programs exist to demonstrate the capabilities of the aalib library, an ascii art library. aafire displays burning ascii art flames. aainfo displays information about what drivers aalib will use for the display, keyboard, and mouse, and what parameters, such as screen size those drivers will use. aasavefont saves a font to a file. aatest tests the capabilities of aalib.","Process Name":"aafire","Link":"https:\/\/linux.die.net\/man\/1\/aafire"}},{"Process":{"Description":"addr2line translates addresses into file names and line numbers. Given an address in an executable or an offset in a section of a relocatable object, it uses the debugging information to figure out which file name and line number are associated with it. The executable or relocatable object to use is specified with the -e option. The default is the file a.out. The section in the relocatable object to use is specified with the -j option. addr2line has two modes of operation. In the first, hexadecimal addresses are specified on the command line, and addr2line displays the file name and line number for each address. In the second, addr2line reads hexadecimal addresses from standard input, and prints the file name and line number for each address on standard output. In this mode, addr2line may be used in a pipe to convert dynamically chosen addresses. The format of the output is FILENAME:LINENO . The file name and line number for each input address is printed on separate lines. If the -f option is used, then each FILENAME:LINENO line is preceded by FUNCTIONNAME which is the name of the function containing the address. If the -i option is used and the code at the given address is present there because of inlining by the compiler then the { FUNCTIONNAME } FILENAME:LINENO information for the inlining function will be displayed afterwards. This continues recursively until there is no more inlining to report. If the -a option is used then the output is prefixed by the input address. If the -p option is used then the output for each input address is displayed on one, possibly quite long, line. If -p is not used then the output is broken up into multiple lines, based on the paragraphs above. If the file name or function name can not be determined, addr2line will print two question marks in their place. If the line number can not be determined, addr2line will print 0.","Process Name":"aarch64-linux-gnu-addr2line","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-addr2line"}},{"Process":{"Description":"The GNU ar program creates, modifies, and extracts from archives. An archive is a single file holding a collection of other files in a structure that makes it possible to retrieve the original individual files (called members of the archive). The original files' contents, mode (permissions), timestamp, owner, and group are preserved in the archive, and can be restored on extraction. GNU ar can maintain archives whose members have names of any length; however, depending on how ar is configured on your system, a limit on member-name length may be imposed for compatibility with archive formats maintained with other tools. If it exists, the limit is often 15 characters (typical of formats related to a.out) or 16 characters (typical of formats related to coff). ar is considered a binary utility because archives of this sort are most often used as libraries holding commonly needed subroutines. ar creates an index to the symbols defined in relocatable object modules in the archive when you specify the modifier s. Once created, this index is updated in the archive whenever ar makes a change to its contents (save for the q update operation). An archive with such an index speeds up linking to the library, and allows routines in the library to call each other without regard to their placement in the archive. You may use nm -s or nm --print-armap to list this index table. If an archive lacks the table, another form of ar called ranlib can be used to add just the table. GNU ar can optionally create a thin archive, which contains a symbol index and references to the original copies of the member files of the archives. Such an archive is useful for building libraries for use within a local build, where the relocatable objects are expected to remain available, and copying the contents of each object would only waste time and space. Thin archives are also flattened, so that adding one or more archives to a thin archive will add the elements of the nested archive individually. The paths to the elements of the archive are stored relative to the archive itself. GNU ar is designed to be compatible with two different facilities. You can control its activity using command-line options, like the different varieties of ar on Unix systems; or, if you specify the single command-line option -M, you can control it with a script supplied via standard input, like the MRI \"librarian\" program.","Process Name":"aarch64-linux-gnu-ar","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-ar"}},{"Process":{"Description":"GNU as is really a family of assemblers. If you use (or have used) the GNU assembler on one architecture, you should find a fairly similar environment when you use it on another architecture. Each version has much in common with the others, including object file formats, most assembler directives (often called pseudo-ops) and assembler syntax. as is primarily intended to assemble the output of the GNU C compiler \"gcc\" for use by the linker \"ld\". Nevertheless, we've tried to make as assemble correctly everything that other assemblers for the same machine would assemble. Any exceptions are documented explicitly. This doesn't mean as always uses the same syntax as another assembler for the same architecture; for example, we know of several incompatible versions of 680x0 assembly language syntax. Each time you run as it assembles exactly one source program. The source program is made up of one or more files. (The standard input is also a file.) You give as a command line that has zero or more input file names. The input files are read (from left file name to right). A command line argument (in any position) that has no special meaning is taken to be an input file name. If you give as no file names it attempts to read one input file from the as standard input, which is normally your terminal. You may have to type ctl-D to tell as there is no more program to assemble. Use -- if you need to explicitly name the standard input file in your command line. If the source is empty, as produces a small, empty object file. as may write warnings and error messages to the standard error file (usually your terminal). This should not happen when a compiler runs as automatically. Warnings report an assumption made so that as could keep assembling a flawed program; errors report a grave problem that stops the assembly. If you are invoking as via the GNU C compiler, you can use the -Wa option to pass arguments through to the assembler. The assembler arguments must be separated from each other (and the -Wa) by commas. For example: gcc -c -g -O -Wa,-alh,-L file.c This passes two options to the assembler: -alh (emit a listing to standard output with high-level and assembly source) and -L (retain local symbols in the symbol table). Usually you do not need to use this -Wa mechanism, since many compiler command-line options are automatically passed to the assembler by the compiler. (You can call the GNU compiler driver with the -v option to see precisely what options it passes to each compilation pass, including the assembler.)","Process Name":"aarch64-linux-gnu-as","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-as"}},{"Process":{"Description":"The C ++ and Java languages provide function overloading, which means that you can write many functions with the same name, providing that each function takes parameters of different types. In order to be able to distinguish these similarly named functions C ++ and Java encode them into a low-level assembler name which uniquely identifies each different version. This process is known as mangling. The c++filt [1] program does the inverse mapping: it decodes (demangles) low-level names into user-level names so that they can be read. Every alphanumeric word (consisting of letters, digits, underscores, dollars, or periods) seen in the input is a potential mangled name. If the name decodes into a C ++ name, the C ++ name replaces the low-level name in the output, otherwise the original word is output. In this way you can pass an entire assembler source file, containing mangled names, through c++filt and see the same source file containing demangled names. You can also use c++filt to decipher individual symbols by passing them on the command line: c++filt <symbol> If no symbol arguments are given, c++filt reads symbol names from the standard input instead. All the results are printed on the standard output. The difference between reading names from the command line versus reading names from the standard input is that command line arguments are expected to be just mangled names and no checking is performed to separate them from surrounding text. Thus for example: c++filt -n _Z1fv will work and demangle the name to \"f()\" whereas: c++filt -n _Z1fv, will not work. (Note the extra comma at the end of the mangled name which makes it invalid). This command however will work: echo _Z1fv, | c++filt -n and will display \"f(),\", i.e., the demangled name followed by a trailing comma. This behaviour is because when the names are read from the standard input it is expected that they might be part of an assembler source file where there might be extra, extraneous characters trailing after a mangled name. For example: .type   _Z1fv, @function","Process Name":"aarch64-linux-gnu-c++filt","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-c++filt"}},{"Process":{"Description":"The C preprocessor, often known as cpp, is a macro processor that is used automatically by the C compiler to transform your program before compilation. It is called a macro processor because it allows you to define macros, which are brief abbreviations for longer constructs. The C preprocessor is intended to be used only with C, C ++ , and Objective-C source code. In the past, it has been abused as a general text processor. It will choke on input which does not obey C's lexical rules. For example, apostrophes will be interpreted as the beginning of character constants, and cause errors. Also, you cannot rely on it preserving characteristics of the input which are not significant to C-family languages. If a Makefile is preprocessed, all the hard tabs will be removed, and the Makefile will not work. Having said that, you can often get away with using cpp on things which are not C. Other Algol-ish programming languages are often safe (Pascal, Ada, etc.) So is assembly, with caution. -traditional-cpp mode preserves more white space, and is otherwise more permissive. Many of the problems can be avoided by writing C or C ++ style comments instead of native language comments, and keeping macros simple. Wherever possible, you should use a preprocessor geared to the language you are writing in. Modern versions of the GNU assembler have macro facilities. Most high level programming languages have their own conditional compilation and inclusion mechanism. If all else fails, try a true general text processor, such as GNU M4. C preprocessors vary in some details. This manual discusses the GNU C preprocessor, which provides a small superset of the features of ISO Standard C. In its default mode, the GNU C preprocessor does not do a few things required by the standard. These are features which are rarely, if ever, used, and may cause surprising changes to the meaning of a program which does not expect them. To get strict ISO Standard C, you should use the -std=c90, -std=c99 or -std=c11 options, depending on which version of the standard you want. To get all the mandatory diagnostics, you must also use -pedantic. This manual describes the behavior of the ISO preprocessor. To minimize gratuitous differences, where the ISO preprocessor's behavior does not conflict with traditional semantics, the traditional preprocessor should behave the same way. The various differences that do exist are detailed in the section Traditional Mode. For clarity, unless noted otherwise, references to CPP in this manual refer to GNU CPP .","Process Name":"aarch64-linux-gnu-cpp","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-cpp"}},{"Process":{"Description":"dlltool reads its inputs, which can come from the -d and -b options as well as object files specified on the command line. It then processes these inputs and if the -e option has been specified it creates a exports file. If the -l option has been specified it creates a library file and if the -z option has been specified it creates a def file. Any or all of the -e, -l and -z options can be present in one invocation of dlltool. When creating a DLL , along with the source for the DLL , it is necessary to have three other files. dlltool can help with the creation of these files. The first file is a .def file which specifies which functions are exported from the DLL , which functions the DLL imports, and so on. This is a text file and can be created by hand, or dlltool can be used to create it using the -z option. In this case dlltool will scan the object files specified on its command line looking for those functions which have been specially marked as being exported and put entries for them in the .def file it creates. In order to mark a function as being exported from a DLL , it needs to have an -export:<name_of_function> entry in the .drectve section of the object file. This can be done in C by using the asm() operator: asm (\".section .drectve\");\nasm (\".ascii \\\"-export:my_func\\\"\");\n\nint my_func (void) { ... } The second file needed for DLL creation is an exports file. This file is linked with the object files that make up the body of the DLL and it handles the interface between the DLL and the outside world. This is a binary file and it can be created by giving the -e option to dlltool when it is creating or reading in a .def file. The third file needed for DLL creation is the library file that programs will link with in order to access the functions in the DLL (an 'import library'). This file can be created by giving the -l option to dlltool when it is creating or reading in a .def file. If the -y option is specified, dlltool generates a delay-import library that can be used instead of the normal import library to allow a program to link to the dll only as soon as an imported function is called for the first time. The resulting executable will need to be linked to the static delayimp library containing __delayLoadHelper2(), which in turn will import LoadLibraryA and GetProcAddress from kernel32. dlltool builds the library file by hand, but it builds the exports file by creating temporary files containing assembler statements and then assembling these. The -S command line option can be used to specify the path to the assembler that dlltool will use, and the -f option can be used to pass specific flags to that assembler. The -n can be used to prevent dlltool from deleting these temporary assembler files when it is done, and if -n is specified twice then this will prevent dlltool from deleting the temporary object files it used to build the library. Here is an example of creating a DLL from a source file dll.c and also creating a program (from an object file called program.o) that uses that DLL: gcc -c dll.c\ndlltool -e exports.o -l dll.lib dll.o\ngcc dll.o exports.o -o dll.dll\ngcc program.o dll.lib -o program dlltool may also be used to query an existing import library to determine the name of the DLL to which it is associated. See the description of the -I or --identify option.","Process Name":"aarch64-linux-gnu-dlltool","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-dlltool"}},{"Process":{"Description":"elfedit updates the ELF header of ELF files which have the matching ELF machine and file types. The options control how and which fields in the ELF header should be updated. elffile... are the ELF files to be updated. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files.","Process Name":"aarch64-linux-gnu-elfedit","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-elfedit"}},{"Process":{"Description":"When you invoke GCC , it normally does preprocessing, compilation, assembly and linking. The \"overall options\" allow you to stop this process at an intermediate stage. For example, the -c option says not to run the linker. Then the output consists of object files output by the assembler. Other options are passed on to one stage of processing. Some options control the preprocessor and others the compiler itself. Yet other options control the assembler and linker; most of these are not documented here, since you rarely need to use any of them. Most of the command-line options that you can use with GCC are useful for C programs; when an option is only useful with another language (usually C ++ ), the explanation says so explicitly. If the description for a particular option does not mention a source language, you can use that option with all supported languages. The gcc program accepts options and file names as operands. Many options have multi-letter names; therefore multiple single-letter options may not be grouped: -dv is very different from -d -v. You can mix options and other arguments. For the most part, the order you use doesn't matter. Order does matter when you use several options of the same kind; for example, if you specify -L more than once, the directories are searched in the order specified. Also, the placement of the -l option is significant. Many options have long names starting with -f or with -W---for example, -fmove-loop-invariants, -Wformat and so on. Most of these have both positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"aarch64-linux-gnu-gcc","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-gcc"}},{"Process":{"Description":"gcov is a test coverage program. Use it in concert with GCC to analyze your programs to help create more efficient, faster running code and to discover untested parts of your program. You can use gcov as a profiling tool to help discover where your optimization efforts will best affect your code. You can also use gcov along with the other profiling tool, gprof, to assess which parts of your code use the greatest amount of computing time. Profiling tools help you analyze your code's performance. Using a profiler such as gcov or gprof, you can find out some basic performance statistics, such as: \u2022 how often each line of code executes \u2022 what lines of code are actually executed \u2022 how much computing time each section of code uses Once you know these things about how your code works when compiled, you can look at each module to see which modules should be optimized. gcov helps you determine where to work on optimization. Software developers also use coverage testing in concert with testsuites, to make sure software is actually good enough for a release. Testsuites can verify that a program works as expected; a coverage program tests to see how much of the program is exercised by the testsuite. Developers can then determine what kinds of test cases need to be added to the testsuites to create both better testing and a better final product. You should compile your code without optimization if you plan to use gcov because the optimization, by combining some lines of code into one function, may not give you as much information as you need to look for 'hot spots' where the code is using a great deal of computer time. Likewise, because gcov accumulates statistics by line (at the lowest resolution), it works best with a programming style that places only one statement on each line. If you use complicated macros that expand to loops or to other control structures, the statistics are less helpful---they only report on the line where the macro call appears. If your complex macros behave like functions, you can replace them with inline functions to solve this problem. gcov creates a logfile called sourcefile.gcov which indicates how many times each line of a source file sourcefile.c has executed. You can use these logfiles along with gprof to aid in fine-tuning the performance of your programs. gprof gives timing information you can use along with the information you get from gcov. gcov works only on code compiled with GCC . It is not compatible with any other profiling or test coverage mechanism.","Process Name":"aarch64-linux-gnu-gcov","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-gcov"}},{"Process":{"Description":"\"gprof\" produces an execution profile of C, Pascal, or Fortran77 programs. The effect of called routines is incorporated in the profile of each caller. The profile data is taken from the call graph profile file (gmon.out default) which is created by programs that are compiled with the -pg option of \"cc\", \"pc\", and \"f77\". The -pg option also links in versions of the library routines that are compiled for profiling. \"Gprof\" reads the given object file (the default is \"a.out\") and establishes the relation between its symbol table and the call graph profile from gmon.out. If more than one profile file is specified, the \"gprof\" output shows the sum of the profile information in the given profile files. \"Gprof\" calculates the amount of time spent in each routine. Next, these times are propagated along the edges of the call graph. Cycles are discovered, and calls into a cycle are made to share the time of the cycle. Several forms of output are available from the analysis. The flat profile shows how much time your program spent in each function, and how many times that function was called. If you simply want to know which functions burn most of the cycles, it is stated concisely here. The call graph shows, for each function, which functions called it, which other functions it called, and how many times. There is also an estimate of how much time was spent in the subroutines of each function. This can suggest places where you might try to eliminate function calls that use a lot of time. The annotated source listing is a copy of the program's source code, labeled with the number of times each line of the program was executed.","Process Name":"aarch64-linux-gnu-gprof","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-gprof"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"aarch64-linux-gnu-ld","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-ld"}},{"Process":{"Description":"nlmconv converts the relocatable i386 object file infile into the NetWare Loadable Module outfile, optionally reading headerfile for NLM header information. For instructions on writing the NLM command file language used in header files, see the linkers section, NLMLINK in particular, of the NLM Development and Tools Overview, which is part of the NLM Software Developer's Kit (\" NLM SDK \"), available from Novell, Inc. nlmconv uses the GNU Binary File Descriptor library to read infile; nlmconv can perform a link step. In other words, you can list more than one object file for input if you list them in the definitions file (rather than simply specifying one input file on the command line). In this case, nlmconv calls the linker for you.","Process Name":"aarch64-linux-gnu-nlmconv","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-nlmconv"}},{"Process":{"Description":"GNU nm lists the symbols from object files objfile.... If no object files are listed as arguments, nm assumes the file a.out. For each symbol, nm shows: \u2022 The symbol value, in the radix selected by options (see below), or hexadecimal by default. \u2022 The symbol type. At least the following types are used; others are, as well, depending on the object file format. If lowercase, the symbol is usually local; if uppercase, the symbol is global (external). There are however a few lowercase symbols that are shown for special global symbols (\"u\", \"v\" and \"w\"). \"A\" The symbol's value is absolute, and will not be changed by further linking. \"B\" \"b\" The symbol is in the uninitialized data section (known as BSS ). \"C\" The symbol is common. Common symbols are uninitialized data. When linking, multiple common symbols may appear with the same name. If the symbol is defined anywhere, the common symbols are treated as undefined references. \"D\" \"d\" The symbol is in the initialized data section. \"G\" \"g\" The symbol is in an initialized data section for small objects. Some object file formats permit more efficient access to small data objects, such as a global int variable as opposed to a large global array. \"i\" For PE format files this indicates that the symbol is in a section specific to the implementation of DLLs. For ELF format files this indicates that the symbol is an indirect function. This is a GNU extension to the standard set of ELF symbol types. It indicates a symbol which if referenced by a relocation does not evaluate to its address, but instead must be invoked at runtime. The runtime execution will then return the value to be used in the relocation. \"N\" The symbol is a debugging symbol. \"p\" The symbols is in a stack unwind section. \"R\" \"r\" The symbol is in a read only data section. \"S\" \"s\" The symbol is in an uninitialized data section for small objects. \"T\" \"t\" The symbol is in the text (code) section. \"U\" The symbol is undefined. \"u\" The symbol is a unique global symbol. This is a GNU extension to the standard set of ELF symbol bindings. For such a symbol the dynamic linker will make sure that in the entire process there is just one symbol with this name and type in use. \"V\" \"v\" The symbol is a weak object. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the weak symbol becomes zero with no error. On some systems, uppercase indicates that a default value has been specified. \"W\" \"w\" The symbol is a weak symbol that has not been specifically tagged as a weak object symbol. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the symbol is determined in a system-specific manner without error. On some systems, uppercase indicates that a default value has been specified. \"-\" The symbol is a stabs symbol in an a.out object file. In this case, the next values printed are the stabs other field, the stabs desc field, and the stab type. Stabs symbols are used to hold debugging information. \"?\" The symbol type is unknown, or object file format specific. \u2022 The symbol name.","Process Name":"aarch64-linux-gnu-nm","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-nm"}},{"Process":{"Description":"The GNU objcopy utility copies the contents of an object file to another. objcopy uses the GNU BFD Library to read and write the object files. It can write the destination object file in a format different from that of the source object file. The exact behavior of objcopy is controlled by command-line options. Note that objcopy should be able to copy a fully linked file between any two formats. However, copying a relocatable object file between any two formats may not work as expected. objcopy creates temporary files to do its translations and deletes them afterward. objcopy uses BFD to do all its translation work; it has access to all the formats described in BFD and thus is able to recognize most formats without being told explicitly. objcopy can be used to generate S-records by using an output target of srec (e.g., use -O srec). objcopy can be used to generate a raw binary file by using an output target of binary (e.g., use -O binary). When objcopy generates a raw binary file, it will essentially produce a memory dump of the contents of the input object file. All symbols and relocation information will be discarded. The memory dump will start at the load address of the lowest section copied into the output file. When generating an S-record or a raw binary file, it may be helpful to use -S to remove sections containing debugging information. In some cases -R will be useful to remove sections which contain information that is not needed by the binary file. Note---objcopy is not able to change the endianness of its input files. If the input format has an endianness (some formats do not), objcopy can only copy the inputs into file formats that have the same endianness or which have no endianness (e.g., srec). (However, see the --reverse-bytes option.)","Process Name":"aarch64-linux-gnu-objcopy","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-objcopy"}},{"Process":{"Description":"objdump displays information about one or more object files. The options control what particular information to display. This information is mostly useful to programmers who are working on the compilation tools, as opposed to programmers who just want their program to compile and work. objfile... are the object files to be examined. When you specify archives, objdump shows information on each of the member object files.","Process Name":"aarch64-linux-gnu-objdump","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-objdump"}},{"Process":{"Description":"ranlib generates an index to the contents of an archive and stores it in the archive. The index lists each symbol defined by a member of an archive that is a relocatable object file. You may use nm -s or nm --print-armap to list this index. An archive with such an index speeds up linking to the library and allows routines in the library to call each other without regard to their placement in the archive. The GNU ranlib program is another form of GNU ar; running ranlib is completely equivalent to executing ar -s.","Process Name":"aarch64-linux-gnu-ranlib","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-ranlib"}},{"Process":{"Description":"readelf displays information about one or more ELF format object files. The options control what particular information to display. elffile... are the object files to be examined. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files. This program performs a similar function to objdump but it goes into more detail and it exists independently of the BFD library, so if there is a bug in BFD then readelf will not be affected.","Process Name":"aarch64-linux-gnu-readelf","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-readelf"}},{"Process":{"Description":"The GNU size utility lists the section sizes---and the total size---for each of the object or archive files objfile in its argument list. By default, one line of output is generated for each object file or each module in an archive. objfile... are the object files to be examined. If none are specified, the file \"a.out\" will be used.","Process Name":"aarch64-linux-gnu-size","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-size"}},{"Process":{"Description":"For each file given, GNU strings prints the printable character sequences that are at least 4 characters long (or the number given with the options below) and are followed by an unprintable character. By default, it only prints the strings from the initialized and loaded sections of object files; for other types of files, it prints the strings from the whole file. strings is mainly useful for determining the contents of non-text files.","Process Name":"aarch64-linux-gnu-strings","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-strings"}},{"Process":{"Description":"GNU strip discards all symbols from object files objfile. The list of object files may include archives. At least one object file must be given. strip modifies the files named in its argument, rather than writing modified copies under different names.","Process Name":"aarch64-linux-gnu-strip","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-strip"}},{"Process":{"Description":"windmc reads message definitions from an input file (.mc) and translate them into a set of output files. The output files may be of four kinds: \"h\" A C header file containing the message definitions. \"rc\" A resource file compilable by the windres tool. \"bin\" One or more binary files containing the resource data for a specific message language. \"dbg\" A C include file that maps message id's to their symbolic name. The exact description of these different formats is available in documentation from Microsoft. When windmc converts from the \"mc\" format to the \"bin\" format, \"rc\", \"h\", and optional \"dbg\" it is acting like the Windows Message Compiler.","Process Name":"aarch64-linux-gnu-windmc","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-windmc"}},{"Process":{"Description":"windres reads resources from an input file and copies them into an output file. Either file may be in one of three formats: \"rc\" A text format read by the Resource Compiler. \"res\" A binary format generated by the Resource Compiler. \"coff\" A COFF object or executable. The exact description of these different formats is available in documentation from Microsoft. When windres converts from the \"rc\" format to the \"res\" format, it is acting like the Windows Resource Compiler. When windres converts from the \"res\" format to the \"coff\" format, it is acting like the Windows \"CVTRES\" program. When windres generates an \"rc\" file, the output is similar but not identical to the format expected for the input. When an input \"rc\" file refers to an external filename, an output \"rc\" file will instead include the file contents. If the input or output format is not specified, windres will guess based on the file name, or, for the input file, the file contents. A file with an extension of .rc will be treated as an \"rc\" file, a file with an extension of .res will be treated as a \"res\" file, and a file with an extension of .o or .exe will be treated as a \"coff\" file. If no output file is specified, windres will print the resources in \"rc\" format to standard output. The normal use is for you to write an \"rc\" file, use windres to convert it to a COFF object file, and then link the COFF file into your application. This will make the resources described in the \"rc\" file available to Windows.","Process Name":"aarch64-linux-gnu-windres","Link":"https:\/\/linux.die.net\/man\/1\/aarch64-linux-gnu-windres"}},{"Process":{"Description":"This manual page explains the aaxine program. Aaxine, is an aalib based frontend for libxine, a versatile video\/multimedia player. aaxine is for those who don't have a high end video card, but just want to watch DVDs on their good old vt100 ;-) Like xine, aaxine plays MPEG system (audio and video) streams, mpeg elementary streams (e.g. .mp3 or .mpv files), avi files (using win32 codecs or ffmpeg), (S)VCDs, DVDs and many more. In short, anything that's supported by xine-lib.","Process Name":"aaxine","Link":"https:\/\/linux.die.net\/man\/1\/aaxine"}},{"Process":{"Description":"","Process Name":"ab","Link":"https:\/\/linux.die.net\/man\/1\/ab"}},{"Process":{"Description":"Ordinarily, the process of grabbing the data off a CD and encoding it, then tagging or commenting it, is very involved. abcde is designed to automate this. It will take an entire CD and convert it into a compressed audio format - Ogg\/Vorbis, MPEG Audio Layer III, Free Lossless Audio Codec (FLAC), Ogg\/Speex, MPP\/MP+(Musepack) and\/or M4A (AAC) format(s). With one command, it will: * Do a CDDB or Musicbrainz query over the Internet to look up your CD or use a locally stored CDDB entry * Grab an audio track (or all the audio CD tracks) from your CD * Normalize the volume of the individual file (or the album as a single unit) * Compress to Ogg\/Vorbis, MP3, FLAC, Ogg\/Speex, MPP\/MP+(Musepack) and\/or M4A format(s), all in one CD read * Comment or ID3\/ID3v2 tag * Give an intelligible filename * Calculate replaygain values for the individual file (or the album as a single unit) * Delete the intermediate WAV file (or save it for later use) * Repeat until finished Alternatively, abcde can also grab a CD and turn it into a single FLAC file with an embedded cuesheet which can be user later on as a source for other formats, and will be treated as if it was the original CD. In a way, abcde can take a compressed backup of your CD collection.","Process Name":"abcde","Link":"https:\/\/linux.die.net\/man\/1\/abcde"}},{"Process":{"Description":"AbiWord is a full-featured, efficient word processing application suitable for a wide range of word processing tasks, and extensible with a variety of plugins. It integrates with enchant(1) to provide spell-checking.","Process Name":"abiword","Link":"https:\/\/linux.die.net\/man\/1\/abiword"}},{"Process":{"Description":"libablmmm.a is a library that enables to represent a boolean function in a LISP-like form. An ABL is a prefixed internal representation for a boolean function having standard operators as OR,NOR,NAND,XOR,NOT and AND. An ABL is only made up of doublets. A doublet is composed of two fields wich are accessible by the functionnal #define CAR and CDR. A doublet is implemented with a MBK chain_list. Expression is the generic term for a boolean function represented by an ABL. An expression can be an atomic expression or an operator expression. The function f = a is represented by an atomic expression whereas f = (or a b) is represented by an operator expression. An atomic expression is made up of a single doublet having the NEXT pointer equal to NULL and DATA pointer equal to the identifier pointer. A constant atomic expression is an atomic expression having the string \"'0'\" or \"'1'\" as identifier. An operator expression is more complicated than an atomic expression. It's a list of items, the first item is the head operator of the expression and the following items are the arguments of the expression. It's possible to go trough the arguments by calling the functionnal #define CDR. Then each argument is accessible by the functionnal #define CAR. An argument can be recursively an atomic or an operator expression. The arity of an operator expression is the number of arguments of the first level. Functions are divided into two groups, the low level functions are written with #define and are used to manage the ABL internal form, the high level functions are used to manage the boolean expressions. All functions are defined in the file \"prefbib.c\" (#define in \"logmmm.h\"). Functionnal #define ATOM - checks the kind of an expression (atomic or operator expression). CAR - returns the DATA pointer of a doublet. CADR - returns the DATA pointer of the NEXT pointer of a doublet. CDR - returns the NEXT pointer of a doublet. OPER - returns the operator number of an operator expression. VALUE_ATOM - returns the associated char * of an atomic expression. Functions and procedures addHExpr - adds a new arguments at the head of an operator expression. addQExpr - adds a new arguments at the queue of an operator expression. anyExpr - returns the value of a logical OR applied on the results of the application of a function on the arguments of an operator expression. changeOperExpr - changes the operator of the head of an expression. charToOper - converts an operator string into an operator number. copyExpr - copies an expression. createAtom - creates an atomic expression. createBinExpr - creates a binary operator expression with an eventual merging of the operator. createExpr - creates the head of an operator expression. deleteNumExpr - removes the i-th argument in an operator expression. devXor2Expr - converts XOR 2 to OR-AND. devXorExpr - removes XOR in an expression. displayExpr - displays an expression in a prefixed notation. displayInfExpr - displays an expression in infixed notation. equalExpr - checks that two expressions are strictly equal. equalVarExpr - checks that two expressions are syntactically equal. everyExpr - returns the value of a logical AND applied on the results of the application of a function on the arguments of an operator expression. exprToChar - converts an expression into a string. charToExpr - converts a string into an expression. flatArityExpr - flattens the operators of an expression. flatPolarityExpr - translates the inverters of an expression to the level of atomic expressions. freeExpr - frees an expression. identExpr - gives an identifier from an operator expression. lengthExpr - returns the number of arguments in an expression. mapCarExpr - creates a new expression by applying a function to all arguments of an operator expression. mapExpr - applies a procedure to all the arguments of an operator expression. maxExpr - returns the highest argument of an operator expression. minExpr - returns the lowest argument of an operator expression. normExpr - normalizes an expression. notExpr - complements an expression and eventually does a simplification. numberAtomExpr - returns the number of atoms in an expression. numberOccExpr - returns the number of time an atom appears in an expression. numberOperBinExpr - returns the number of equivalent binary operators in an expression. operToChar - converts an operator number into an operator string. profExpr - returns the depth of an expression. profAOExpr - returns the depth of an expression without taking the inverters into account. searchExpr - searches for a specific atom in an expression. searchNumExpr - fetches the i-th argument in an operator expression. searchOperExpr - searches for an operator in an expression. simplif10Expr - makes simplifications on an expression including constant atomic expressions. simplifNotExpr - makes simplifications on an expression including inverters. sortExpr - sorts an expression. substExpr - copies an expression by substituting a given atom by an expression. substPhyExpr - substitutes an atomic expression by an expression within an expression. supportChain_listExpr - returns the support of an expression in a chain_list. supportPtype_listExpr - returns the support of an expression in a ptype_list. wPMExpr - returns 1 if the pattern matching is possible between two expressions.","Process Name":"abl","Link":"https:\/\/linux.die.net\/man\/1\/abl"}},{"Process":{"Description":"This manual page documents briefly the abook program. This manual page was written for the Debian GNU\/Linux distribution because the original program does not have a manual page. abook is a text-based address book program. It contains Name, Email, Address and various Phone fields. It is designed for use with mutt, but can be equally useful on its own.","Process Name":"abook","Link":"https:\/\/linux.die.net\/man\/1\/abook"}},{"Process":{"Description":"The tool reads a file named backtrace from problem directory, generates duplication hash, backtrace rating, and identifies crash function. Then it saves this data as new elements global_uuid, rating, crash_function in this problem directory. Integration with libreport events abrt-action-analyze-backtrace can be used as a secondary analyzer, after backtrace has been generated. The data generated by abrt-action-analyze-backtrace is useful for reporting the crash to bug databases: rating makes it possible to prevent reporting of bugs with low quality (non-informative) backtraces, duplication hash is used to find already filed bugs about similar crashes. Example usage in report_event.conf: EVENT=analyze analyzer=CCpp\n        abrt-action-generate-backtrace || exit $?\n        abrt-action-analyze-backtrace","Process Name":"abrt-action-analyze-backtrace","Link":"https:\/\/linux.die.net\/man\/1\/abrt-action-analyze-backtrace"}},{"Process":{"Description":"The tool reads the file named coredump from a problem data directory, processes it and generates a universally unique identifier (UUID). Then it saves this data as new element uuid. Integration with ABRT events abrt-action-analyze-c can be used to generate the UUID of a newly saved coredump. EVENT=post-create analyzer=CCpp   abrt-action-analyze-c","Process Name":"abrt-action-analyze-c","Link":"https:\/\/linux.die.net\/man\/1\/abrt-action-analyze-c"}},{"Process":{"Description":"The tool reads the file named backtrace from a problem data directory, parses it as a kernel oops record and generates a duplication hash and a universally unique identifier (UUID). Then it saves this data as new elements duphash and uuid. Integration with ABRT events abrt-action-analyze-oops can be used to generate the duplication hash of a newly discovered kernel oops. EVENT=post-create analyzer=Kerneloops   abrt-action-analyze-oops","Process Name":"abrt-action-analyze-oops","Link":"https:\/\/linux.die.net\/man\/1\/abrt-action-analyze-oops"}},{"Process":{"Description":"The tool reads the file named backtrace from a problem data directory, parses it as a Python exception and generates a duplication hash and a universally unique identifier (UUID). Then it saves this data as new elements duphash and uuid. Integration with ABRT events abrt-action-analyze-python can be used to generate the duplication hash of a newly discovered Python crash. EVENT=post-create analyzer=Python   abrt-action-analyze-python","Process Name":"abrt-action-analyze-python","Link":"https:\/\/linux.die.net\/man\/1\/abrt-action-analyze-python"}},{"Process":{"Description":"The tool reads the file named vmcore from a problem data directory, processes it and the oops message from it. Then it saves this data as new element backtrace. Integration with ABRT events abrt-action-analyze-vmcore can be used as an analyzer for critical kernel crashes which dump core. Example usage in report_event.conf: EVENT=analyze_VMcore analyzer=vmcore\n        abrt-action-analyze-vmcore","Process Name":"abrt-action-analyze-vmcore","Link":"https:\/\/linux.die.net\/man\/1\/abrt-action-analyze-vmcore"}},{"Process":{"Description":"This tool runs gdb(1) on a file named coredump in problem directory DIR. gdb(1) generates backtrace and other diagnostic information about the state of the application at the moment when coredump was generated. Then the tool saves it as new element backtrace in this problem directory. Integration with libreport events abrt-action-generate-backtrace can be used as an analyzer for application crashes which dump core. Example usage in report_event.conf: EVENT=analyze analyzer=CCpp\n        abrt-action-generate-backtrace","Process Name":"abrt-action-generate-backtrace","Link":"https:\/\/linux.die.net\/man\/1\/abrt-action-generate-backtrace"}},{"Process":{"Description":"This tool uses coredump from the file coredump and binary at the path stored in file executable in the problem directory to generate coredump-level backtrace. Coredump-level backtrace resembles ordinary backtrace in that it contains information about call frames present on the stack at the time of the crash. However, it only contains information that can be obtained from the coredump without debugging symbols available - mainly relative addresses of the stored instruction pointers. Such backtrace can still be useful for reporting and reproducing the bug and does not require debugging information files to be installed. See FILE FORMAT for the description of the generated file. The result is saved in the problem directory in a file named core_backtrace. Integration with libreport events abrt-action-generate-core-backtrace can be used as an analyzer for application crashes which dump core. Example usage in report_event.conf: EVENT=analyze analyzer=CCpp\n        abrt-action-generate-core-backtrace","Process Name":"abrt-action-generate-core-backtrace","Link":"https:\/\/linux.die.net\/man\/1\/abrt-action-generate-core-backtrace"}},{"Process":{"Description":"The tool reads a file containing the mapped memory regions. Output is printed to stdout or file. Output format: %p %n %v %i where %p path to library or binary file, which is mapped in memory region %n name, version, release, architecture of package, where %p belongs %v vendor of package %i installation time","Process Name":"abrt-action-list-dsos","Link":"https:\/\/linux.die.net\/man\/1\/abrt-action-list-dsos"}},{"Process":{"Description":"The tool reads problem directory DIR. It analyzes contents of analyzer, executable, cmdline' and remote elements, checks database of installed packages, and creates new elements package and component. This data is usually necessary if the problem will be reported to a bug tracking database. Integration with ABRT events This tool can be used as an ABRT reporter. Example fragment for \/etc\/libreport\/report_event.conf: # Determine in which package\/component the crash happened (if not yet done):\nEVENT=post-create component=\n        abrt-action-save-package-data","Process Name":"abrt-action-save-package-data","Link":"https:\/\/linux.die.net\/man\/1\/abrt-action-save-package-data"}},{"Process":{"Description":"","Process Name":"abrt-action-trim-files","Link":"https:\/\/linux.die.net\/man\/1\/abrt-action-trim-files"}},{"Process":{"Description":"","Process Name":"abrt-cli","Link":"https:\/\/linux.die.net\/man\/1\/abrt-cli"}},{"Process":{"Description":"The tool unpacks FILENAME located in UPLOAD_DIR and moves the problem data found in it to ABRT_DIR. It supports unpacking tarballs compressed by gzip, bzip2 or xz. It's called by abrtd when a new file is noticed in the upload directory configured by the WatchCrashdumpArchiveDir option.","Process Name":"abrt-handle-upload","Link":"https:\/\/linux.die.net\/man\/1\/abrt-handle-upload"}},{"Process":{"Description":"abrt-install-ccpp-hook registers ABRT coredump handler (which saves segfault data) into kernel.","Process Name":"abrt-install-ccpp-hook","Link":"https:\/\/linux.die.net\/man\/1\/abrt-install-ccpp-hook"}},{"Process":{"Description":"abrt-server is executed by abrtd daemon to handle socket connections. Every application in system is able to invoke creation of a new problem directory by following the communication protocol (described below in section PROTOCOL).","Process Name":"abrt-server","Link":"https:\/\/linux.die.net\/man\/1\/abrt-server"}},{"Process":{"Description":"ac prints out a report of connect time (in hours) based on the logins\/logouts in the current wtmp file. A total is also printed out. The accounting file wtmp is maintained by init(8) and login(1). Neither ac nor login creates the wtmp if it doesn't exist, no accounting is done. To begin accounting, create the file with a length of zero. NOTE: The wtmp file can get really big, really fast. You might want to trim it every once and a while. GNU ac works nearly the same u*x ac, though it's a little smarter in several ways. You should therefore expect differences in the output of GNU ac and the output of ac's on other systems. Use the command info accounting to get additional information.","Process Name":"ac","Link":"https:\/\/linux.die.net\/man\/1\/ac"}},{"Process":{"Description":"This manual page document describes the atool commands. These commands are used for managing file archives of various types, such as tar and Zip archives. Each command can be executed individually or by giving the appropriate options to atool (see OPTIONS below). aunpack extracts files from an archive. Often one wants to extract all files in an archive to a single subdirectory. However, some archives contain multiple files in their root directories. The aunpack program overcomes this problem by first extracting files to a unique (temporary) directory, and then moving its contents back if possible. This also prevents local files from being overwritten by mistake. apack creates archives (or compresses files). If no file arguments are specified, filenames to add are read from standard in. als lists files in an archive. acat extracts files in an archive to standard out. adiff generates a diff between two archives using diff(1). arepack repacks archives to a different format. It does this by first extracting all files of the old archive into a temporary directory, then packing all files extracted to that directory to the new archive. Use the --each (-e) option in combination with --format (-F) to repack multiple archives using a single invocation of atool. Note that arepack will not remove the old archive. Unless the --format (-F) option is provided, the archive format is determined by the archive file extension. I.e. an extension \".tar.gz\" or \".tgz\" means tar+gzip format. Note that the extensions are checked in the order listed in the section ARCHIVE TYPES below, which is why a file with extension \".tar.gz\" is considered to a be tar+gzip archive, not a gzip compressed file.","Process Name":"acat","Link":"https:\/\/linux.die.net\/man\/1\/acat"}},{"Process":{"Description":"Exit successfully if file can be accessed with the specified mode. mode is one or more letters of rwx, where r is for readable, w is for writable, and x is for executable. The difference between access and test is that the latter looks at the permission bits, while the former checks using the access(2) system call. This makes a difference when file systems have been mounted read-only.","Process Name":"access","Link":"https:\/\/linux.die.net\/man\/1\/access"}},{"Process":{"Description":"","Process Name":"ace.pl","Link":"https:\/\/linux.die.net\/man\/1\/ace.pl"}},{"Process":{"Description":"Ack is designed as a replacement for 99% of the uses of grep. Ack searches the named input FILEs (or standard input if no files are named, or the file name - is given) for lines containing a match to the given PATTERN . By default, ack prints the matching lines. Ack can also list files that would be searched, without actually searching them, to let you take advantage of ack's file-type filtering capabilities.","Process Name":"ack","Link":"https:\/\/linux.die.net\/man\/1\/ack"}},{"Process":{"Description":"Generate 'aclocal.m4' by scanning 'configure.ac' or 'configure.in'","Process Name":"aclocal","Link":"https:\/\/linux.die.net\/man\/1\/aclocal"}},{"Process":{"Description":"Generate 'aclocal.m4' by scanning 'configure.ac' or 'configure.in'","Process Name":"aclocal-1.11","Link":"https:\/\/linux.die.net\/man\/1\/aclocal-1.11"}},{"Process":{"Description":"aconnect is a utility to connect and disconnect two existing ports on ALSA sequencer system. The ports with the arbitrary subscription permission, such as created by aseqview(1) , can be connected to any (MIDI) device ports using aconnect. For example, to connect from port 64:0 to 65:0, run as follows: % aconnect 64:0 65:0 The connection is one-way, and the whole data to the sender port (64:0) is redirected to the receiver port (65:0). When another port (e.g. 65:1) is attached to the same sender port, the data is sent to both receiver ports. For disconnection, use -d option. % aconnect -d 64:0 65:0 The address can be given using the client's name. % aconnect External:0 Emu8000:1 Then the port 0 of the client matching with the string \"External\" is connected to the port 1 of the client matching with the \"Emu8000\". Another function of aconnect is to list the present ports on the given condition. The input ports, which may become sender ports, can be listed with -i option. % aconnect -i\nclient 0: 'System' [type=kernel]\n    0 'Timer           '\n    1 'Announce        '\nclient 64: 'External MIDI-0' [type=kernel]\n    0 'MIDI 0-0        ' Similarly, to see the output ports, use -o flag. You can remove all existing exported connections using -x option. This function is useful for terminating the ALSA drivers, because the modules with sequencer connections cannot be unloaded unless their connections are removed.","Process Name":"aconnect","Link":"https:\/\/linux.die.net\/man\/1\/aconnect"}},{"Process":{"Description":"acpitool is a Linux ACPI client. It simply reads \/proc\/acpi or \/sys\/class entries and presents the output in a meaningfull, human-readable format. It provides a.o. information on battery status, AC adapter presence, thermal reading, etc. This command is most useful on laptops with an ACPI compliant BIOS and a Linux kernel, preferably from the 2.6 series, with ACPI enabled. Acpitool also allows the machine to be put into standby, if your laptop supports it. If your laptop is a Toshiba , it allows you to set the LCD brighness level and toggle the fan on\/off. If you have an Asus laptop, it can also set the LCD brightness level, switch the LCD panel on or off, and control the mail led and wireless led. If you have an IBM Thinkpad laptop, it can once again set the LCD brightness level, and also eject the ultrabay device.","Process Name":"acpitool","Link":"https:\/\/linux.die.net\/man\/1\/acpitool"}},{"Process":{"Description":"acyclic is a filter that takes a directed graph as input and outputs a copy of the graph with sufficient edges reversed to make the graph acyclic. The reversed edge inherits all of the attributes of the original edge. The optional file argument specifies where the the input graph is stored; by default, the program reads from stdin.","Process Name":"acyclic","Link":"https:\/\/linux.die.net\/man\/1\/acyclic"}},{"Process":{"Description":"ad is a UNIX file utlity suite with Netatalk compatibity. AppleDouble files in .AppleDouble directories and the CNID databases are updated as appropiate.","Process Name":"ad","Link":"https:\/\/linux.die.net\/man\/1\/ad"}},{"Process":{"Description":"adcfw-log is a tool for analyzing firewall logs in order to extract meaningful information. It is designed to be a standalone script with very few requirements that can generate different kinds of reports, such as fully formatted reports of what had been logged, with summaries by source or destination host, the type of service, or protocol. There are also options to filter the input data by date, host, protocol, service, and so on. Only netfilter log format is supported at this time.","Process Name":"adcfw-log","Link":"https:\/\/linux.die.net\/man\/1\/adcfw-log"}},{"Process":{"Description":"","Process Name":"add_friends","Link":"https:\/\/linux.die.net\/man\/1\/add_friends"}},{"Process":{"Description":"The program runs as a filter, standard input is passed to standard output, a CTRL-d is appended. Some PS print jobs generated on UNIX systems contain pure PostScript without a finishing CTRL-d. Sending such a print job to a printer may cause problems as the end of print job is not recognized properly by the printer.","Process Name":"addctrld","Link":"https:\/\/linux.die.net\/man\/1\/addctrld"}},{"Process":{"Description":"adddebug modifies the Makefile in the current directory (and optionally in its subdirectories) to add debug info (-g3). It will also remove optimisations (-O[1-9]). This utility is part of the KDE Software Development Kit.","Process Name":"adddebug","Link":"https:\/\/linux.die.net\/man\/1\/adddebug"}},{"Process":{"Description":"addftinfo reads a troff font file and adds some additional font-metric information that is used by the groff system. The font file with the information added is written on the standard output. The information added is guessed using some parametric information about the font and assumptions about the traditional troff names for characters. The main information added is the heights and depths of characters. The res and unitwidth arguments should be the same as the corresponding parameters in the DESC file; font is the name of the file describing the font; if font ends with I the font will be assumed to be italic.","Process Name":"addftinfo","Link":"https:\/\/linux.die.net\/man\/1\/addftinfo"}},{"Process":{"Description":"addr2line translates addresses into file names and line numbers. Given an address in an executable or an offset in a section of a relocatable object, it uses the debugging information to figure out which file name and line number are associated with it. The executable or relocatable object to use is specified with the -e option. The default is the file a.out. The section in the relocatable object to use is specified with the -j option. addr2line has two modes of operation. In the first, hexadecimal addresses are specified on the command line, and addr2line displays the file name and line number for each address. In the second, addr2line reads hexadecimal addresses from standard input, and prints the file name and line number for each address on standard output. In this mode, addr2line may be used in a pipe to convert dynamically chosen addresses. The format of the output is FILENAME:LINENO . The file name and line number for each address is printed on a separate line. If the -f option is used, then each FILENAME:LINENO line is preceded by a FUNCTIONNAME line which is the name of the function containing the address. If the file name or function name can not be determined, addr2line will print two question marks in their place. If the line number can not be determined, addr2line will print 0.","Process Name":"addr2line","Link":"https:\/\/linux.die.net\/man\/1\/addr2line"}},{"Process":{"Description":"","Process Name":"addr2name","Link":"https:\/\/linux.die.net\/man\/1\/addr2name"}},{"Process":{"Description":"addresses will connect to your Palm handheld and dump all entries found in the AddressDB.pdb (Address Book) file to STDOUT (generally the screen) or, if redirected with the '>' (greater-than) character, it will redirect to a file of your choosing (called \"MyAddresses.txt in the above example, but may be called any other filename you wish).","Process Name":"addresses","Link":"https:\/\/linux.die.net\/man\/1\/addresses"}},{"Process":{"Description":"","Process Name":"addrinfo","Link":"https:\/\/linux.die.net\/man\/1\/addrinfo"}},{"Process":{"Description":"addwords adds words to specified dictionary according to the word information from the standard input. The standard input must be in a format of ''Text Dictionary format'' of Canna. dicname must be a ''Text Dictionary format'' which can be rewritten.","Process Name":"addwords","Link":"https:\/\/linux.die.net\/man\/1\/addwords"}},{"Process":{"Description":"","Process Name":"adie","Link":"https:\/\/linux.die.net\/man\/1\/adie"}},{"Process":{"Description":"This manual page document describes the atool commands. These commands are used for managing file archives of various types, such as tar and Zip archives. Each command can be executed individually or by giving the appropriate options to atool (see OPTIONS below). aunpack extracts files from an archive. Often one wants to extract all files in an archive to a single subdirectory. However, some archives contain multiple files in their root directories. The aunpack program overcomes this problem by first extracting files to a unique (temporary) directory, and then moving its contents back if possible. This also prevents local files from being overwritten by mistake. apack creates archives (or compresses files). If no file arguments are specified, filenames to add are read from standard in. als lists files in an archive. acat extracts files in an archive to standard out. adiff generates a diff between two archives using diff(1). arepack repacks archives to a different format. It does this by first extracting all files of the old archive into a temporary directory, then packing all files extracted to that directory to the new archive. Use the --each (-e) option in combination with --format (-F) to repack multiple archives using a single invocation of atool. Note that arepack will not remove the old archive. Unless the --format (-F) option is provided, the archive format is determined by the archive file extension. I.e. an extension \".tar.gz\" or \".tgz\" means tar+gzip format. Note that the extensions are checked in the order listed in the section ARCHIVE TYPES below, which is why a file with extension \".tar.gz\" is considered to a be tar+gzip archive, not a gzip compressed file.","Process Name":"adiff","Link":"https:\/\/linux.die.net\/man\/1\/adiff"}},{"Process":{"Description":"admsXml is a code generator that converts electrical compact device models specified in high-level description language into ready-to-compile c code for the API of spice simulators.","Process Name":"admsxml","Link":"https:\/\/linux.die.net\/man\/1\/admsxml"}},{"Process":{"Description":"The XML Bookmark Exchange Language, or XBEL, is an Internet \"bookmarks\" interchange format. adr_parse parses Opera bookmark files.","Process Name":"adr_parse","Link":"https:\/\/linux.die.net\/man\/1\/adr_parse"}},{"Process":{"Description":"The main purpose of this utility is to recompress the data present in the .png, .mng, .gz, .tgz and .svgz files. The internal structure of the files isn't changed. Only the compressed data is modified.","Process Name":"advdef","Link":"https:\/\/linux.die.net\/man\/1\/advdef"}},{"Process":{"Description":"This utility prints all the games contained in the DEST_INFO file and not contained in ORIG_INFO file. A game is considered contained if all his roms are present in a game with the same name. The DEST_INFO and ORIG_INFO files are the rom information files generated with the -listinfo\/-listxml or -gameinfo options of the emulators. You can use this utility to create differential romset. For example you can use this command: advdiff mame.lst raine.lst > raine_diff.lst to create the list of roms needed by Raine, which are not already included in the standard MAME set. You can also use this utility to create the list of roms, which need to be updated between two emulator releases. For example you can use this command: advdiff mame79.xml mame80.xml > mame79-80.xmlt to create the list of roms needed to upgrade the MAME romset from the version 0.79 version to the version 0.80.","Process Name":"advdiff","Link":"https:\/\/linux.die.net\/man\/1\/advdiff"}},{"Process":{"Description":"The main purpose of this utility is to recompress MNG files to get the smallest possible size. Please note that this utility is not able to read a generic file. It's granted to be able to read only the files generated by the AdvanceMAME emulator. To compress the files this utility uses the following strategies: \u2022 Remove all ancillary chunks. \u2022 Use the MNG Delta feature to compress video clips with only small changes between frames. \u2022 Use the MNG Move feature to compress video clips with a scrolling background (option -s). \u2022 Reduce the color depth to 8 bit (option -r). \u2022 Use the 7zip Deflate implementation.","Process Name":"advmng","Link":"https:\/\/linux.die.net\/man\/1\/advmng"}},{"Process":{"Description":"The main purpose of this utility is to recompress png files to get the smallest possible size. Please note that this utility is not able to read a generic file. It's granted to be able to read only the files generated by the AdvanceMAME emulator. To compress the files this utility uses the following strategies: \u2022 Remove all ancillary chunks. \u2022 Concatenate all the IDAT chunks. \u2022 Use the 7zip Deflate implementation.","Process Name":"advpng","Link":"https:\/\/linux.die.net\/man\/1\/advpng"}},{"Process":{"Description":"advscan is a command line utility for maintaining a zipped archive of roms and sample zip archive for the MAME, MESS, xmame, AdvanceMAME, AdvanceMESS and Raine emulators. The goal of advscan is to obtain a complete and perfect rom and sample zip archive with differential merging. Differential merging means that any game has its zip archive, which contains all the rom files, which are not already in the parent zip archive (if exists). advscan has these features: \u2022 Directly read, writes zip archives without decompressing and recompressing them for best performance. \u2022 Add, copy, move and rename files in the zip archives. Any rom that you have is placed automatically in the correct zip. \u2022 Recognizes the text files added by rom sites and delete them. \u2022 Recognizes the text files added by the rom dumpers and keep or delete them as your choice. \u2022 It's safe. On all the zip operations any file removed or overwritten is saved in the 'rom_unknown' 'sample_unknown' directories and keep for future uses. This will prevent any unwanted remove operation. but also has these misfeatures: \u2022 Supports only rom and sample archives zipped. \u2022 Supports only rom organized with differential merging. \u2022 Doesn't support .chd files in subdirectories.","Process Name":"advscan","Link":"https:\/\/linux.die.net\/man\/1\/advscan"}},{"Process":{"Description":"The main purpose of this utility is to recompress and test the zip archives to get the smallest possible size. For recompression the 7-Zip (www.7-zip.com) Deflate implementation is used. This implementation generally gives 5-10% more compression than the zLib Deflate implementation. For experimental purpose also the 7-Zip LZMA algorithm is available with the -N option. In this case, the generated zips WILL NOT BE USABLE by any other program. To make them usable you need to recompress them without the -N option. Generally this algorithm gives 10-20% more compression than the 7-Zip Deflate implementation.","Process Name":"advzip","Link":"https:\/\/linux.die.net\/man\/1\/advzip"}},{"Process":{"Description":"aee and xae are non-modal editors, that is, the user does not need to switch from one mode to another to insert and delete text. The editor is always in text mode. Control sequences and function keys are used to perform the editing functions. In the case of xae, the mouse can also be used to position the cursor, and perform text selection and pasting. rae and rxae are the same as aee and xae respectively, except that they are restricted to editing the file(s) named on the invoking command line. No other files may be opened or written, nor may shell operations be performed. The arrow keys (up, down, left, right) may be used to move the cursor. If the keyboard is so equipped, the prev and next keys will move the cursor to the previous and next pages, respectively. The gold key is a key that is used to provide alternate behavior for a key, much like a gold function key on a calculator keyboard. So, for example, function key number 7 (f7) may be used for initiating a search, while pressing the gold key and then f7 will produce a prompt for the string to be searched for. The gold function can be assigned to any assignable key. By default, the keys f1 and control-g (^g) are assigned the gold function. The rest of the keys by default will behave as described below.","Process Name":"aee","Link":"https:\/\/linux.die.net\/man\/1\/aee"}},{"Process":{"Description":"aespipe reads from standard input and writes to standard output. It can be used to create and restore encrypted tar or cpio archives. It can be used to encrypt and decrypt loop-AES compatible encrypted disk images. The AES cipher is used in CBC (cipher block chaining) mode. Data is encrypted and decrypted in 512 byte chains. aespipe supports three key setup modes; single-key, multi-key-v2 and multi-key-v3 modes. Single-key mode uses simple sector IV and one AES key to encrypt and decrypt all data sectors. Multi-key-v2 mode uses cryptographically more secure MD5 IV and 64 different AES keys to encrypt and decrypt data sectors. In multi-key mode first key is used for first sector, second key for second sector, and so on. Multi-key-v3 is same as multi-key-v2 except is uses one extra 65th key as additional input to MD5 IV computation. See -K option for more information about how to enable multi-key-v3 mode. Recommended key setup mode is multi-key-v3, which is based on gpg encrypted key files. In this mode, the passphrase is protected against optimized dictionary attacks via salting and key iteration of gpg. Passphrase length should be 20 characters or more. Single-key mode preserves input size at 16 byte granularity. Multi-key mode preserves input size at 512 byte granularity. If input size is not multiple of 16 or 512 bytes, input data is padded with null bytes so that both input and output sizes are multiples of 16 or 512 bytes.","Process Name":"aespipe","Link":"https:\/\/linux.die.net\/man\/1\/aespipe"}},{"Process":{"Description":"The goal of this program is to monitor what change on your host : new\/deleted\/modified files. So it can be used as an intrusion detection system ( by integrity checking ). It is designed to be a portable clone of aide (Advanced Intrusion Detection Environment), or Tripwire software. you should launch it regulary (by cron for example) and after any software change. this is a command-line program, you can use afick-tk.pl if you prefer a graphical interface.","Process Name":"afick","Link":"https:\/\/linux.die.net\/man\/1\/afick"}},{"Process":{"Description":"afick-tk is designed to help to use afick for people who prefer graphical interfaces. Graphical reports such \"tree-view\" may help to have a quick overview.","Process Name":"afick-tk","Link":"https:\/\/linux.die.net\/man\/1\/afick-tk"}},{"Process":{"Description":"Afio manipulates groups of files, copying them within the (collective) filesystem or between the filesystem and an afio archive. With -o, reads pathnames from the standard input and writes an archive. With -t, reads an archive and writes a table-of-contents to the standard output. With -i, installs the contents of an archive relative to the working directory. With -p, reads pathnames from the standard input and copies the files to each directory. Cannot be combined with the -Z option. With -r, reads archive and verifies it against the filesystem. This is useful for verifying tape archives. Creates missing directories as necessary, with permissions to match their parents. Removes leading slashes from pathnames, making all paths relative to the current directory. This is a safety feature to prevent inadvertent overwriting of system files when doing restores. To suppress this safety feature, the -A option must be used while writing an archive, but also when reading (installing), verifying, and cataloging an existing archive. Supports compression while archiving, with the -Z option. Will compress individual files in the archive, not the entire archive datastream, which makes afio compressed archives much more robust than 'tar zc' type archives. Supports multi-volume archives during interactive operation (i.e., when \/dev\/tty is accessible and SIGINT is not being ignored).","Process Name":"afio","Link":"https:\/\/linux.die.net\/man\/1\/afio"}},{"Process":{"Description":"PostScript fonts are (or should be) accompanied by font metric files such as Times-Roman.afm, which describes the characteristics of the font called Times-Roman. To use such fonts with TeX, we need TFM files that contain similar information. This program does that conversion. For more information, print out the dvips manual.","Process Name":"afm2tfm","Link":"https:\/\/linux.die.net\/man\/1\/afm2tfm"}},{"Process":{"Description":"afmtodit creates a font file for use with groff and grops. afmtodit is written in perl; you must have perl version 3 or newer installed in order to run afmtodit. afm_file is the AFM (Adobe Font Metric) file for the font. map_file is a file that says which groff character names map onto each PostScript character name; this file should contain a sequence of lines of the form ps_char groff_char where ps_char is the PostScript name of the character and groff_char is the groff name of the character (as used in the groff font file). The same ps_char can occur multiple times in the file; each groff_char must occur at most once. Lines starting with # and blank lines are ignored. font is the groff name of the font. If a PostScript character is in the encoding to be used for the font but is not mentioned in map_file then afmtodit will put it in the groff font file as an unnamed character, which can be accessed by the \\N escape sequence in troff. The groff font file will be output to a file called font. If there is a downloadable font file for the font, it may be listed in the file \/usr\/share\/groff\/1.18.1.4\/font\/devps\/download; see grops(1). If the -i option is used, afmtodit will automatically generate an italic correction, a left italic correction and a subscript correction for each character (the significance of these parameters is explained in groff_font(5)); these parameters may be specified for individual characters by adding to the afm_file lines of the form: italicCorrection ps_char n leftItalicCorrection ps_char n subscriptCorrection ps_char n where ps_char is the PostScript name of the character, and n is the desired value of the corresponding parameter in thousandths of an em. These parameters are normally needed only for italic (or oblique) fonts.","Process Name":"afmtodit","Link":"https:\/\/linux.die.net\/man\/1\/afmtodit"}},{"Process":{"Description":"afp_client command allows you to perform some basic functions to access AFP volumes, such as mount, unmount, get status, suspend and resume. Do not confuse this with afpcmd; afp_client is to be used only for the FUSE client, in conjunction with afpfsd(1). afpcmd is a batch-mode file transferring client. Both of them use the afpfs-ng libraries. afp_mount(1) is normally a symlink to afp_client. It only handles mounting commands with a fully formed AFP URL.","Process Name":"afp_client","Link":"https:\/\/linux.die.net\/man\/1\/afp_client"}},{"Process":{"Description":"afpcmd is a command-line tool to help transfer files to and from a server using AFP. This is typically either Netatalk or Mac OS or Mac OS X. This can be done as a non-root user. It offers either an interactive command-line (like a traditional FTP client) or for batch retrievals. Do not confuse this with the FUSE mounting tools (mount_afp, afpfsd, afp_client), which offer the ability to mount an entire filesystem.","Process Name":"afpcmd","Link":"https:\/\/linux.die.net\/man\/1\/afpcmd"}},{"Process":{"Description":"afpfsd is a daemon that manages AFP sessions. Functions (like mounting, getting status, etc) can be performed using the afp_client(1) tool. This client communicates with the daemon over a named pipe. afpfsd will not start if another instance is already running. There needs to be one copy of afpfsd running per user.","Process Name":"afpfsd","Link":"https:\/\/linux.die.net\/man\/1\/afpfsd"}},{"Process":{"Description":"afpcmd is a command-line tool that parses and prints the status information of an AFP server. It does this without having to login to a server. It is a response to the DSI GetStatus request (which is the same as the AFP FPGetSrvrInfo). It only handles IPv4 addresses.","Process Name":"afpgetstatus","Link":"https:\/\/linux.die.net\/man\/1\/afpgetstatus"}},{"Process":{"Description":"afpldaptest is a simple command to syntactically check \/etc\/netatalk\/\/afp_ldap.conf.","Process Name":"afpldaptest","Link":"https:\/\/linux.die.net\/man\/1\/afpldaptest"}},{"Process":{"Description":"afppasswd allows the maintenance of afppasswd files created by netatalk for use by the uams_randnum.so UAM (providing the \"Randnum exchange\" and \"2-Way Randnum exchange\" User Authentication Modules). afppasswd can either be called by root with parameters, or can be called by local system users with no parameters to change their AFP passwords. Note With this utility you can only change the passwords used by two specific UAMs. As they provide only weak password encryption, the use of the \"Randnum exchange\" and \"2-Way Randnum exchange\" UAMs is deprecated unless one has to support very old AFP clients, that can not deal with the more secure \"DHCAST128\" and \"DHX2\" UAM instead. Please compare with the Authentication chapter inside Netatalk's documentation.","Process Name":"afppasswd","Link":"https:\/\/linux.die.net\/man\/1\/afppasswd"}},{"Process":{"Description":"The afs5log command uses Kerberos to obtain AFS tokens for the named cells. If no cell is named, tokens are obtained for the local cell. If the principal name of the afs service for the cell is known, the cell name may also be given in the form cell= principal_name. Attempts are made to obtain Kerberos IV credentials first, and if that fails, afs5log will create Kerberos 5 ( 2b-style) tokens.","Process Name":"afs5log","Link":"https:\/\/linux.die.net\/man\/1\/afs5log"}},{"Process":{"Description":"The afslog command obtains AFS tokens, args are either a name of a cell or a pathnames of a file in the cell to get tokens for. If an argument is . or .. or contains a slash it is assumed to be a pathname. Otherwise it is assumed to be a name of a cell or a prefix thereof. The -c and -p flags can be used to resolve ambiguities. afslog might fail to guess the Kerberos realm to get tickets for (for instance if the volume location servers of the cell does not reside in the kerberos realm that holds the AFS service key, and the correct realm isn't the same as the cell name or the local realm (I didn't say this was a common problem)). Anyway, the -k can be used to give a hint. It should not be used unless there is a problem, since all tickets will be taken from the specified realm and this is not (usually) what you want. -createuser means that afslog should try to run pts to create a remote user principal in another cell. -d can be used for debugging. If the -unlog flag is given any tokens are removed and all other arguments are ignored.","Process Name":"afslog","Link":"https:\/\/linux.die.net\/man\/1\/afslog"}},{"Process":{"Description":"agedu scans a directory tree and produces reports about how much disk space is used in each directory and subdirectory, and also how that usage of disk space corresponds to files with last-access times a long time ago. In other words, agedu is a tool you might use to help you free up disk space. It lets you see which directories are taking up the most space, as du does; but unlike du, it also distinguishes between large collections of data which are still in use and ones which have not been accessed in months or years - for instance, large archives downloaded, unpacked, used once, and never cleaned up. Where du helps you find whatAqs using your disk space, agedu helps you find whatAqs wasting your disk space. agedu has several operating modes. In one mode, it scans your disk and builds an index file containing a data structure which allows it to efficiently retrieve any information it might need. Typically, you would use it in this mode first, and then run it in one of a number of 'query' modes to display a report of the disk space usage of a particular directory and its subdirectories. Those reports can be produced as plain text (much like du) or as HTML. agedu can even run as a miniature web server, presenting each directoryAqs HTML report with hyperlinks to let you navigate around the file system to similar reports for other directories. So you would typically start using agedu by telling it to do a scan of a directory tree and build an index. This is done with a command such as $ agedu -s \/home\/fred which will build a large data file called agedu.dat in your current directory. (If that current directory is inside \/home\/fred, donAqt worry - agedu is smart enough to discount its own index file.) Having built the index, you would now query it for reports of disk space usage. If you have a graphical web browser, the simplest and nicest way to query the index is by running agedu in web server mode: $ agedu -w which will print (among other messages) a URL on its standard output along the lines of URL: http:\/\/127.0.0.1:48638\/ (That URL will always begin with ' 127.', meaning that itAqs in the localhost address space. So only processes running on the same computer can even try to connect to that web server, and also there is access control to prevent other users from seeing it - see below for more detail.) Now paste that URL into your web browser, and you will be shown a graphical representation of the disk usage in \/home\/fred and its immediate subdirectories, with varying colours used to show the difference between disused and recently-accessed data. Click on any subdirectory to descend into it and see a report for its subdirectories in turn; click on parts of the pathname at the top of any page to return to higher-level directories. When youAqve finished browsing, you can just press Ctrl-D to send an end-of-file indication to agedu, and it will shut down. After that, you probably want to delete the data file agedu.dat, since itAqs pretty large. In fact, the command agedu -R will do this for you; and you can chain agedu commands on the same command line, so that instead of the above you could have done $ agedu -s \/home\/fred -w -R for a single self-contained run of agedu which builds its index, serves web pages from it, and cleans it up when finished. If you don't have a graphical web browser, you can do text-based queries as well. Having scanned \/home\/fred as above, you might run $ agedu -t \/home\/fred which again gives a summary of the disk usage in \/home\/fred and its immediate subdirectories; but this time agedu will print it on standard output, in much the same format as du. If you then want to find out how much old data is there, you can add the -a option to show only files last accessed a certain length of time ago. For example, to show only files which havenAqt been looked at in six months or more: $ agedu -t \/home\/fred -a 6m That's the essence of what agedu does. It has other modes of operation for more complex situations, and the usual array of configurable options. The following sections contain a complete reference for all its functionality.","Process Name":"agedu","Link":"https:\/\/linux.die.net\/man\/1\/agedu"}},{"Process":{"Description":"rancid is a perl(1) script which uses the login scripts (see clogin(1)) to login to a device, execute commands to display the configuration, etc, then filters the output for formatting, security, and so on. rancid's product is a file with the name of it's last argument plus the suffix .new. For example, hostname.new. There are complementary scripts for other platforms and\/or manufacturers that are supported by rancid(1). Briefly, these are: agmrancid Cisco Anomaly Guard Module (AGM) arancid Alteon WebOS switches arrancid Arista Networks devices brancid Bay Networks (nortel) cat5rancid Cisco catalyst switches cssrancid Cisco content services switches erancid ADC-kentrox EZ-T3 mux f10rancid Force10 f5rancid F5 BigIPs fnrancid Fortinet Firewalls francid Foundry and HP procurve OEMs of Foundry hrancid HP Procurve Switches htranicd Hitachi Routers jerancid Juniper Networks E-series jrancid Juniper Networks mrancid MRTd mrvrancid MRV optical switches nrancid Netscreen firewalls nsrancid Netscaler nxrancid Cisco Nexus boxes prancid Procket Networks rivrancid Riverstone rrancid Redback srancid SMC switch (some Dell OEMs) trancid Netopia sDSL\/T1 routers tntrancid Lucent TNT xrancid Extreme switches xrrancid Cisco IOS-XR boxes zrancid Zebra routing software The command-line options are as follows: -V Prints package name and version strings. -d Display debugging information. -l Display somewhat less debugging information. -f rancid should interpret the next argument as a filename which contains the output it would normally collect from the device ( hostname) with clogin(1).","Process Name":"agmrancid","Link":"https:\/\/linux.die.net\/man\/1\/agmrancid"}},{"Process":{"Description":"AmigaGuide Reader ( agr for short) is a command line program for viewing AmigaGuide files, the native documentation format commonly used on the Amiga platform and its clones. It supports the complete v39 specification and a significant part of the v40 specification. AmigaGuide files traditionally use the .guide filename suffix.","Process Name":"agr","Link":"https:\/\/linux.die.net\/man\/1\/agr"}},{"Process":{"Description":"Searches for approximate matches of PATTERN in each FILE or standard input. Example: 'agrep -2 optimize foo.txt' outputs all lines in file 'foo.txt' that match \"optimize\" within two errors. E.g. lines which contain \"optimise\", \"optmise\", and \"opitmize\" all match.","Process Name":"agrep","Link":"https:\/\/linux.die.net\/man\/1\/agrep"}},{"Process":{"Description":"aide is an intrusion detection system for checking the integrity of files.","Process Name":"aide","Link":"https:\/\/linux.die.net\/man\/1\/aide"}},{"Process":{"Description":"aimk is a wrapper program for make, used to portably select options to build PVM and PVM applications on various machines. Each port of PVM is assigned an architecture name. The name is used both during compilation (to conditionally switch in code) and at runtime (to select an executable or host). aimk uses the value of environment variable $PVM_ARCH if it is set, otherwise it calls $PVM_ROOT\/pvmgetarch to determine the architecture name. pvmgetarch is a script that sniffs at various parts of the system to determine the correct architecture name. It is updated as new PVM ports are defined, and can be augmented locally. aimk determines the machine architecture and execs make, passing it the architecture and a configuration file along with arguments supplied to aimk. It runs make in a subdirectory to prevent executables from becoming intermixed and to permit overlapping compiles. A different makefile can be placed in each subdirectory or a single makefile, Makefile.aimk, can be shared between architectures. Per-architecture definitions from the $PVM_ROOT\/conf directory are appended to the common makefile. aimk calls make is called in one of three ways, depending on what makefiles are present: i. If $PVM_ARCH\/Makefile or $PVM_ARCH\/makefile exists, change directory to $PVM_ARCH and exec make there: (cd $PVM_ARCH ; make PVM_ARCH=$PVM_ARCH < aimk args >) ii. Else if Makefile.aimk exists, create $PVM_ARCH directory if it doesn't exist, then: (cd $PVM_ARCH ; \\ make -f $PVM_ROOT\/conf\/$PVM_ARCH.def \\ -f ..\/Makefile.aimk PVM_ARCH=$PVM_ARCH < aimk args >) iii. Else just exec make in current directory: make PVM_ARCH=$PVM_ARCH < aimk args > If aimk succeeds in calling make, the exit status is that of make, otherwise it is 1.","Process Name":"aimk","Link":"https:\/\/linux.die.net\/man\/1\/aimk"}},{"Process":{"Description":"aircrack-ng is a 802.11 WEP \/ WPA-PSK key cracker. It implements the so-called Fluhrer - Mantin - Shamir (FMS) attack, along with some new attacks by a talented hacker named KoreK. When enough encrypted packets have been gathered, aircrack-ng can almost instantly recover the WEP key.","Process Name":"aircrack-ng","Link":"https:\/\/linux.die.net\/man\/1\/aircrack-ng"}},{"Process":{"Description":"airdaemon invokes a child process and ensures that it is restarted if it encounters any errors. The delay between child exit and restart can be chosen, and can either be constant or exponentially increasing up to a specified maximum. Use of two dashes (--) after all airdaemon command-line switches allows PROGRAM_ARGS to be interpreted by the PROGRAM rather than airdaemon itself. While they are not strictly required if you do not need to pass arguments to PROGRAM , they should be used for consistency.","Process Name":"airdaemon","Link":"https:\/\/linux.die.net\/man\/1\/airdaemon"}},{"Process":{"Description":"airdecap-ng decrypts a WEP\/WPA crypted pcap file to a uncrypted one by using the right WEP\/WPA keys.","Process Name":"airdecap-ng","Link":"https:\/\/linux.die.net\/man\/1\/airdecap-ng"}},{"Process":{"Description":"aireplay-ng injects specially generated ARP-request packets into an existing wireless network in order to generate traffic. By sending these ARP-request packets again and again, the target host will respond with encrypted replies, thus providing new and possibly weak IVs. aireplay-ng supports single-NIC injection\/monitor. This feature needs driver patching.","Process Name":"aireplay-ng","Link":"https:\/\/linux.die.net\/man\/1\/aireplay-ng"}},{"Process":{"Description":"airinv is a small program showing how to use the AirInv library. airinv accepts the following options: --prefix Show the AirInv installation prefix. -v, --version Print the currently installed version of AirInv on the standard output. -h, --help Produce that message and show usage. -b, --builtin The sample BOM tree can be either built-in or parsed from an input file. That latter must then be given with either the -i\/--inventory option or the set of -s\/--schedule, -o\/--ond options. -f, --for_schedule The BOM tree should be built from a schedule file (instead of from an inventory dump). -i, --inventory <path-to-input-file> Path (absolute or relative) of the (CSV) input file specifying the inventory (flight-date) details. -s, --schedule <path-to-input-file> Path (absolute or relative) of the (CSV) input file specifying the network\/schedule (flight-period) details. -o, --ond <path-to-input-file> Path (absolute or relative) of the (CSV) input file specifying the O&D (origin\/destination) details. -y, --yield <path-to-input-file> Path (absolute or relative) of the (CSV) input file specifying the yield rule details. -l, --log <path-to-output-log-file> Path (absolute or relative) of the output log file. See the output of the 'airinv --help' command for default options.","Process Name":"airinv","Link":"https:\/\/linux.die.net\/man\/1\/airinv"}},{"Process":{"Description":"airinv-config is a tool that is used by configure to determine the compiler and linker flags that should be used to compile and link programs that use AirInv. It is also used internally by the .m4 macros, that are included with AirInv, for GNU autoconf.","Process Name":"airinv-config","Link":"https:\/\/linux.die.net\/man\/1\/airinv-config"}},{"Process":{"Description":"airinv is a small program showing how to use the AirInv library. airinv accepts the following options: --prefix Show the AirInv installation prefix. -v, --version Print the currently installed version of AirInv on the standard output. -h, --help Produce that message and show usage. -b, --builtin The sample BOM tree can be either built-in or parsed from an input file. That latter must then be given with either the -i\/--inventory option or the set of -s\/--schedule, -o\/--ond options. -f, --for_schedule The BOM tree should be built from a schedule file (instead of from an inventory dump). -i, --inventory <path-to-input-file> Path (absolute or relative) of the (CSV) input file specifying the inventory (flight-date) details. -s, --schedule <path-to-input-file> Path (absolute or relative) of the (CSV) input file specifying the network\/schedule (flight-period) details. -o, --ond <path-to-input-file> Path (absolute or relative) of the (CSV) input file specifying the O&D (origin\/destination) details. -y, --yield <path-to-input-file> Path (absolute or relative) of the (CSV) input file specifying the yield rule details. -k, --segment_date_key Key of the segment on which the sell will be made. -c, --class_code Booking class code on which the sell will be made. -p, --party_size Size of the party booking on the given segment-date. -l, --log <path-to-output-log-file> Path (absolute or relative) of the output log file. See the output of the 'airinv --help' command for default options.","Process Name":"airinv_parseinventory","Link":"https:\/\/linux.die.net\/man\/1\/airinv_parseinventory"}},{"Process":{"Description":"AirInvClient is a small program sending and receiving simple JSON-formatted messages to the ZeroMQ-based server (AirInvServer). The aim of that client is to test that the AirInv server is up and running, and that it receives and sends JSON requests. AirInvClient does not accept any option for now. Do not hesitate to contribute!","Process Name":"airinvclient","Link":"https:\/\/linux.die.net\/man\/1\/airinvclient"}},{"Process":{"Description":"AirInvServer is a small program wrapping the AirInv library, so as to expose it as a ZeroMQ-based server. That server listens to and answers with JSON-formatted messages. Typically, a Django-based Web application will use those AirInv services, so as to expose to the end user a clean and light user interface. AirInvServer accepts the following options: --prefix Show the AirInv installation prefix. -v, --version Print the currently installed version of AirInv on the standard output. -h, --help Produce that message and show usage. -b, --builtin The sample BOM tree can be either built-in or parsed from an input file. That latter must then be given with either the -i\/--inventory option or the set of -s\/--schedule, -o\/--ond options. -f, --for_schedule The BOM tree should be built from a schedule file (instead of from an inventory dump). -i, --inventory <path-to-input-file> Path (absolute or relative) of the (CSV) input file specifying the inventory (flight-date) details. -s, --schedule <path-to-input-file> Path (absolute or relative) of the (CSV) input file specifying the network\/schedule (flight-period) details. -o, --ond <path-to-input-file> Path (absolute or relative) of the (CSV) input file specifying the O&D (origin\/destination) details. -y, --yield <path-to-input-file> Path (absolute or relative) of the (CSV) input file specifying the yield rule details. -t, --protocol <server network='' protocol>=''> Server protocol -a, --address <server host='' address>=''> Server address -p, --port <server host='' port>=''> Server port -l, --log <path-to-output-log-file> Path (absolute or relative) of the output log file. See the output of the 'AirInv --help' command for default options.","Process Name":"airinvserver","Link":"https:\/\/linux.die.net\/man\/1\/airinvserver"}},{"Process":{"Description":"The airmass command computes value of airmass coefficient (X) for given julian date, object's coordinates and observer's coordinates. It may also append the values to a set of measurements stored in a text file. When the -j option is present on the command line, the value of airmass coefficient is printed to the standard output stream. If one or more filenames are present on the command line, each source file given is processed line by line, the program expects the JD value in the first column, which must be divided at least one of common used dividers (semicolon, comma, space, tab char, ...). The JD value can be in full (2453xxx.x) or short (53xxx.x) form. Decimal places must be separated by point, not comma. The value of airmass coefficient is computed and appended to the end of the line. If the line starts with the text JD, it is considered to be a table header and the text AIRMASS is appended to the end of the line. All other lines which do not fit to any of previous rules are copied to the output file without modification.","Process Name":"airmass","Link":"https:\/\/linux.die.net\/man\/1\/airmass"}},{"Process":{"Description":"airmon-ng is a bash script designed to turn wireless cards into monitor mode. It autodetects which card you have and run the right commands. airmon-ng can set the right sources in \/etc\/kismet\/kismet.conf too.","Process Name":"airmon-ng","Link":"https:\/\/linux.die.net\/man\/1\/airmon-ng"}},{"Process":{"Description":"airodump-ng is a packet capture tool for aircrack-ng. It allows dumping packets directly from WLAN interface and saving them to a pcap or IVs file.","Process Name":"airodump-ng","Link":"https:\/\/linux.die.net\/man\/1\/airodump-ng"}},{"Process":{"Description":"airrac is a small program showing how to use the AirRAC library. airrac accepts the following options: --prefix Show the AirRAC installation prefix. -v, --version Print the currently installed version of AirRAC on the standard output. -h, --help Produce that message and show usage. -b, --builtin The sample BOM tree can be either built-in or parsed from an input file. When that option is selected, an input file must then be given with the -y\/--yield option. When that option is not selected, the airrac program builds a simple use case directly from internal hard-coded parameters (yield rules). -y, --yield <path-to-input-file> Path (absolute or relative) of the (CSV) input file specifying the yield rules. -l, --log <path-to-output-log-file> Path (absolute or relative) of the output log file. See the output of the 'airrac --help' command for default options.","Process Name":"airrac","Link":"https:\/\/linux.die.net\/man\/1\/airrac"}},{"Process":{"Description":"airrac-config is a tool that is used by configure to determine the compiler and linker flags that should be used to compile and link programs that use AirRAC. It is also used internally by the .m4 macros, that are included with AirRAC, for GNU autoconf.","Process Name":"airrac-config","Link":"https:\/\/linux.die.net\/man\/1\/airrac-config"}},{"Process":{"Description":"airsched is a small program showing how to use the Airsched library. airsched accepts the following options: --prefix Show the Airsched installation prefix. -v, --version Print the currently installed version of Airsched on the standard output. -h, --help Produce that message and show usage. -b, --builtin The sample BOM tree can be either built-in or parsed from input files. In that latter case, the -i\/--input option must be specified as well -i, --input <path-to-input-file> Path (absolute or relative) of the (CSV) input file specifying the schedule (flight-period) entries. -l, --log <path-to-output-log-file> Path (absolute or relative) of the output log file. -r, --read_booking_request Indicates that a booking request is given as a command-line option. That latter must then be given with the -b\/--bkg_req option. -q, --bkg_req <booking-request> Booking request word list (e.g. 'NCE BKK NCE 2007-04-21 2007-04-21 10:00:00 C 1 DF RO 5 NONE 10:0:0 2000.0 20.0'), which should be located at the end of the command line (otherwise, the other options would be interpreted as part of that booking request word list). See the output of the 'airsched --help' command for default options.","Process Name":"airsched","Link":"https:\/\/linux.die.net\/man\/1\/airsched"}},{"Process":{"Description":"airsched-config is a tool that is used by configure to determine the compiler and linker flags that should be used to compile and link programs that use AirSched. It is also used internally by the .m4 macros, that are included with AirSched, for GNU autoconf.","Process Name":"airsched-config","Link":"https:\/\/linux.die.net\/man\/1\/airsched-config"}},{"Process":{"Description":"airsnort is a WEP key cracking tool designed to exploit the RC4 scheduling weakness discussed by Fluhrer, Mantin, and Shamir (FMS) and first exploited by Stubblefield et al. - Running AirSnort Once launched, airsnort must be configured to work with your wireless nic and to make crack attempts according to your desires. In order to properly capture packets, first indicate the name of your wireless networking device in the \"Network device\" field. This will be something like \"wlanX\" for cards that use the wlan-ng drivers and \"ethX\" for other cards. Next select the type of card that you are using in the \"Card type\" drop down box. Available choices are Prism2, Orinoco, and other. Cisco cards fall into the other category. The purpose if this field is primarily to inform airsnort how to place your nic into monitor mode. In monitor mode a wireless nic gathers all packets indiscriminately, and no association with an access point is required. For wlan-ng and orinoco_cs based nics, monitor mode is entered automatically when the 'Start' button is clicked to initiate a capture session. Other card types must be put into monitor mode outside of airsnort, prior to clicking Start. Choose between \"scan\" mode to scan through all 11 802.11b channels at a regular interval, or \"channel mode to monitor a specific channel. Note that in either case it is quite possible to receive packets that bleed through from neighboring channels. - Capture Details Capture uses the pcap library to receive monitor mode packets. The packets go through two filters. First, non-encrypted packets are filtered out. Then, if they are encrypted, useless packets (those without a weak IV) are discarded. All non-data packets are discarded with the exception of 802.11b Beacon and probe response packets which are examined in order to obtain access point SSID data. To distinguish encrypted and non-encrypted packets, capture examines the first two bytes of the output. Since unencrypted IP packets have a first pair value of 0xAAAA (part of the SNAP), all of these packets get dropped. For a description of what constitutes an interesting packet please refer to the FMS paper and its discussion of \"weak IVs\" - Cracking Details Cracking attempts are made in parallel with packet capture. Currently, the cracker attempts to crack the captured packets for both a 40 bit and 128 bit key each time 10 new weak IVs are seen for a given access point. Airsnort uses a probabalistic attack, so, the best guess may not be the right one. With limited captured data and enough CPU power, you can perform more exaustive searches. The search for a key involves a depth first traversal of an n-ary tree. The depth of tree is 5 for 40 bit key attempts and 13 for 128 bit key attempts. The breadth of the trees is governed by the 40 and 128 bit crack depth fields in the airsnort gui. A breadth parameter of 'n' instructs airsnort to try the n most likely values at each key position using statistics derived from the IVs that have been collected. Large breadth setting can result in very slow processing time for crack attempts default values of 3 for 40 bit cracks and 2 for 128 bit cracks are recommended for starters. If a large number of weak IVs have been gathered (> 1500 if a 40 bit key is suspected, > 3000 if a 128 bit key is suspected), you may want to try increasing the breadth values. The number of interesting packets needed to perform a successful crack depends on two things; luck and key length. Assuming that luck is on your side, the key length is the only important factor. For a key length of 128 bits, this translates to about 1500 packets. For other key lengths, assume 115 packets per byte of the key. Some keys are more resistant to this technique than others and may require far more packets. If you have a lot of packets and no key, either wait for more packets or try a larger breadth. In any case, if the cracker believes it has a correct password, it checks the checksum of a random packet. If this is successful, the correct password is printed in ASCII and Hex, and the successful crack is indicated by an 'X' in the leftmost column of the display. When executing the cracking operation, crack operates with a partial key search from the given data. Since it is a probabalistic attack, The best guess may not be the right one, so, with limited captured data and enough CPU power, you can perform more exaustive searches. By setting the breadth parameter, you can specify to search \"worse\" guesses. It is not suggested that you specify a breadth of more than three or four. - Save and Restore Airsnort saves data in two formats. All packets captured by aisrnort can be saved in pcap dump file format by selecting the \"Log to file\" option from the File menu. This must be done before a capture session is initiated. Airsnort can also save a much smaller amount of data of data about a capture session in the form of \"crack\" files. These files represent the minimum amount of data that airsnort maintains for each access point that it discovers. Crack files contain summary data of those packets that airsnort has seen that actually use weak IVs. Airsnort will always ask the user to save data to a crack file whenever the program is terminated. By using save files, airsnort session can effectively be paused and resumed at a later time by first loading the save file, then starting a capture session. Restoration of data from a pcap dump file amounts to replaying the entire capture session from which the dump file was created, all statistics will reflect what was seen during the live capture session. Restoration of data from a crack file will only display statistics about packets that use weak IVs, thus packet counts are likely to be much smaller than seen during the live capture. It is possible to load a pcap dump file and create a corresponding crack file in order to reduce the amount of stored data.","Process Name":"airsnort","Link":"https:\/\/linux.die.net\/man\/1\/airsnort"}},{"Process":{"Description":"airtun-ng creates a virtual tunnel interface (atX) for sending arbitrary IP packets by using raw ieee802.11 packet injection.","Process Name":"airtun-ng","Link":"https:\/\/linux.die.net\/man\/1\/airtun-ng"}},{"Process":{"Description":"AL is the Mono assembly linkder. This linker is used to put together assemblies from a collection of modules (.netmodule files), assembly manifest files and resources. Do not confuse this with the monolinker, which is a tool to reduce the size of assemblies based on the code used. Use al for processing 1.0 assemblies, use al2 to process 2.0 assemblies.","Process Name":"al","Link":"https:\/\/linux.die.net\/man\/1\/al"}},{"Process":{"Description":"alc is a graphical utility to create an ED2k link to any file on your computer. This programm doesn't take any arguments.","Process Name":"alc","Link":"https:\/\/linux.die.net\/man\/1\/alc"}},{"Process":{"Description":"","Process Name":"alc_bug_report","Link":"https:\/\/linux.die.net\/man\/1\/alc_bug_report"}},{"Process":{"Description":"alcbanner display on stdout a standardized banner for the Alliance tools. This is a compiled version of the alliancebanner(2) function.","Process Name":"alcbanner","Link":"https:\/\/linux.die.net\/man\/1\/alcbanner"}},{"Process":{"Description":"Compute the ED2K links of all the input files given in the <inputfiles_list> (There can be one or more files). Usage: -h, --help Prints a short usage description. -v, --verbose Be verbose: print calculation steps. -p, --parthashes Compute and add part hashes to the computed ED2K links.","Process Name":"alcc","Link":"https:\/\/linux.die.net\/man\/1\/alcc"}},{"Process":{"Description":"Aldo is a morse code trainer mainly developed for GNU\/Linux and released under GPL. It is written from scratch in ISO C++. It uses GNU C++ Standard Library. At this moment Aldo provides four kinds of exercises: Classic exercise With this exercise you must guess some random strings of characters that Aldo plays in morse code. Read from file With this exercise you can write something in a text file and read this file with Aldo. Callsign exercise With this exercise you can training yourself reciving random generated callsigns. Koch method","Process Name":"aldo","Link":"https:\/\/linux.die.net\/man\/1\/aldo"}},{"Process":{"Description":"alevt is an X11 program for browsing and searching Teletext pages received by a compatible decoder (at the moment, bttv).","Process Name":"alevt","Link":"https:\/\/linux.die.net\/man\/1\/alevt"}},{"Process":{"Description":"alevt-cap is a simple program to capture teletext pages and write them to disk. You just give it a list of pages to fetch and it will save them. Nothing fancy like time-outs, page ranges, or channel name detection is supported. Though, it supports different file formats - at the moment ascii, ansi (ascii with color escape sequences), html, png, and ppm.","Process Name":"alevt-cap","Link":"https:\/\/linux.die.net\/man\/1\/alevt-cap"}},{"Process":{"Description":"alevt-date displays the time received from a Teletext source. It can be used to set the system time. The date is not interpreted (not even transmitted on most channels). So it allows only adjustment of +\/-12 hours. The default allowed adjustment is limited to +\/-2 hours (use -delta to change). Without the -set option it just displays the date in the format of the date(1) command.","Process Name":"alevt-date","Link":"https:\/\/linux.die.net\/man\/1\/alevt-date"}},{"Process":{"Description":"alevtd is http daemon which serves videotext pages as HTML. Tune in some station with a utility like v4lctl or some TV application. Then start it and point your browser to http:\/\/localhost:5654\/ Pages may be requested either in HTML format (http:\/\/localhost:5654\/<page>\/ or http:\/\/localhost:5654\/<page>\/<subpage>.html) or in ASCII text format (http:\/\/localhost:5654\/<page>\/<subpage>.txt). Subpage \"00\" can be used for pages without subpages.","Process Name":"alevtd","Link":"https:\/\/linux.die.net\/man\/1\/alevtd"}},{"Process":{"Description":"","Process Name":"alias","Link":"https:\/\/linux.die.net\/man\/1\/alias"}},{"Process":{"Description":"alink will one day be a the linker companion tool to asl. It is work in progress, and it won't be of much use for most users in its current state. Contact the author (see below) if you want to experiment with it.","Process Name":"alink","Link":"https:\/\/linux.die.net\/man\/1\/alink"}},{"Process":{"Description":"allcm forces a large number of Computer Modern Fonts to be calculated as pixel files. This is done through running dvips(1) over a certain test-file. Therefore, the fonts are created in the resolution needed by dvips. If the -r flag is specified, the command dvired(1) will be used instead of dvips. allcm does not recalculate existing fonts (as long as the Metafont mode does not change).","Process Name":"allcm","Link":"https:\/\/linux.die.net\/man\/1\/allcm"}},{"Process":{"Description":"allcm forces a large number of Computer Modern Fonts to be calculated as pixel files. This is done through running dvips(1) over a certain test-file. Therefore, the fonts are created in the resolution needed by dvips. If the -r flag is specified, the command dvired(1) will be used instead of dvips. allcm does not recalculate existing fonts (as long as the Metafont mode does not change).","Process Name":"allec","Link":"https:\/\/linux.die.net\/man\/1\/allec"}},{"Process":{"Description":"allneeded forces the calculation of all fonts that are needed to preview a set of dvi files. Just specify where the program should search for files on the commandline. The fonts generation is triggered by running dvips(1) over all accessible dvi-files. Therefore, the fonts are created in the resolution needed by dvips. If the -r flag is specified, the command dvired(1) will be used instead of dvips. allneeded does not recalculate existing fonts (as long as the Metafont mode does not change).","Process Name":"allneeded","Link":"https:\/\/linux.die.net\/man\/1\/allneeded"}},{"Process":{"Description":"With AllTray you can dock any application with no native tray icon(like Evolution, Thunderbird, Terminals) into the system tray. A high-light feature is that a click on the \"close\" button will minimize back to system tray.It works well with Gnome, KDE, XFCE 4*, Fluxbox* and WindowMaker*. Xmms is supported in particular. *no drag n drop support.","Process Name":"alltray","Link":"https:\/\/linux.die.net\/man\/1\/alltray"}},{"Process":{"Description":"clogin is an expect(1) script to automate the process of logging into a Cisco router, catalyst switch, Extreme switch, Juniper ERX\/E-series, Procket Networks, or Redback router. There are complementary scripts for Alteon, Avocent (Cyclades), Bay Networks (nortel), ADC-kentrox EZ-T3 mux, Foundry, HP Procurve Switches and Cisco AGMs, Hitachi Routers, Juniper Networks, MRV optical switch, Netscreen firewalls, Netscaler, Riverstone, Netopia, and Lucent TNT, named alogin, avologin, blogin, elogin, flogin, fnlogin, hlogin, htlogin, jlogin, mrvlogin, nlogin, nslogin, rivlogin, tlogin, and tntlogin, respectively. clogin reads the .cloginrc file for its configuration, then connects and logs into each of the routers specified on the command line in the order listed. Command-line options exist to override some of the directives found in the .cloginrc configuration file. The command-line options are as follows: -S Save the configuration on exit, if the device prompts at logout time. This only has affect when used with -s. -V Prints package name and version strings. -c Command to be run on each router list on the command-line. Multiple commands maybe listed by separating them with semi-colons (;). The argument should be quoted to avoid shell expansion. -d Enable expect debugging. -E Specifies a variable to pass through to scripts (-s). For example, the command-line option -Efoo=bar will produce a global variable by the name Efoo with the initial value \"bar\". -e Specify a password to be supplied when gaining enable privileges on the router(s). Also see the password directive of the .cloginrc file. -f Specifies an alternate configuration file. The default is $HOME\/.cloginrc. -p Specifies a password associated with the user specified by the -u option, user directive of the .cloginrc file, or the Unix username of the user. -s The filename of an expect(1) script which will be sourced after the login is successful and is expected to return control to clogin, with the connection to the router intact, when it is done. Note that clogin disables log_user of expect(1)when -s is used. Example script(s) can be found in share\/rancid\/*.exp. -t Alters the timeout interval; the period that clogin waits for an individual command to return a prompt or the login process to produce a prompt or failure. The argument is in seconds. -u Specifies the username used when prompted. The command-line option overrides any user directive found in .cloginrc. The default is the current Unix username. -v Specifies a vty password, that which is prompted for upon connection to the router. This overrides the vty password of the .cloginrc file's password directive. -w Specifies the username used if prompted when gaining enable privileges. The command-line option overrides any user or enauser directives found in .cloginrc. The default is the current Unix username. -x Similar to the -c option; -x specifies a file with commands to run on each of the routers. The commands must not expect additional input, such as 'copy rcp startup-config' does. For example: show version\nshow logging -y Specifies the encryption algorithm for use with the ssh(1) -c option. The default encryption type is often not supported. See the ssh(1) man page for details. The default is 3des.","Process Name":"alogin","Link":"https:\/\/linux.die.net\/man\/1\/alogin"}},{"Process":{"Description":"addr2line translates addresses into file names and line numbers. Given an address in an executable or an offset in a section of a relocatable object, it uses the debugging information to figure out which file name and line number are associated with it. The executable or relocatable object to use is specified with the -e option. The default is the file a.out. The section in the relocatable object to use is specified with the -j option. addr2line has two modes of operation. In the first, hexadecimal addresses are specified on the command line, and addr2line displays the file name and line number for each address. In the second, addr2line reads hexadecimal addresses from standard input, and prints the file name and line number for each address on standard output. In this mode, addr2line may be used in a pipe to convert dynamically chosen addresses. The format of the output is FILENAME:LINENO . The file name and line number for each input address is printed on separate lines. If the -f option is used, then each FILENAME:LINENO line is preceded by FUNCTIONNAME which is the name of the function containing the address. If the -i option is used and the code at the given address is present there because of inlining by the compiler then the { FUNCTIONNAME } FILENAME:LINENO information for the inlining function will be displayed afterwards. This continues recursively until there is no more inlining to report. If the -a option is used then the output is prefixed by the input address. If the -p option is used then the output for each input address is displayed on one, possibly quite long, line. If -p is not used then the output is broken up into multiple lines, based on the paragraphs above. If the file name or function name can not be determined, addr2line will print two question marks in their place. If the line number can not be determined, addr2line will print 0.","Process Name":"alpha-linux-gnu-addr2line","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-addr2line"}},{"Process":{"Description":"The GNU ar program creates, modifies, and extracts from archives. An archive is a single file holding a collection of other files in a structure that makes it possible to retrieve the original individual files (called members of the archive). The original files' contents, mode (permissions), timestamp, owner, and group are preserved in the archive, and can be restored on extraction. GNU ar can maintain archives whose members have names of any length; however, depending on how ar is configured on your system, a limit on member-name length may be imposed for compatibility with archive formats maintained with other tools. If it exists, the limit is often 15 characters (typical of formats related to a.out) or 16 characters (typical of formats related to coff). ar is considered a binary utility because archives of this sort are most often used as libraries holding commonly needed subroutines. ar creates an index to the symbols defined in relocatable object modules in the archive when you specify the modifier s. Once created, this index is updated in the archive whenever ar makes a change to its contents (save for the q update operation). An archive with such an index speeds up linking to the library, and allows routines in the library to call each other without regard to their placement in the archive. You may use nm -s or nm --print-armap to list this index table. If an archive lacks the table, another form of ar called ranlib can be used to add just the table. GNU ar can optionally create a thin archive, which contains a symbol index and references to the original copies of the member files of the archives. Such an archive is useful for building libraries for use within a local build, where the relocatable objects are expected to remain available, and copying the contents of each object would only waste time and space. Thin archives are also flattened, so that adding one or more archives to a thin archive will add the elements of the nested archive individually. The paths to the elements of the archive are stored relative to the archive itself. GNU ar is designed to be compatible with two different facilities. You can control its activity using command-line options, like the different varieties of ar on Unix systems; or, if you specify the single command-line option -M, you can control it with a script supplied via standard input, like the MRI \"librarian\" program.","Process Name":"alpha-linux-gnu-ar","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-ar"}},{"Process":{"Description":"GNU as is really a family of assemblers. If you use (or have used) the GNU assembler on one architecture, you should find a fairly similar environment when you use it on another architecture. Each version has much in common with the others, including object file formats, most assembler directives (often called pseudo-ops) and assembler syntax. as is primarily intended to assemble the output of the GNU C compiler \"gcc\" for use by the linker \"ld\". Nevertheless, we've tried to make as assemble correctly everything that other assemblers for the same machine would assemble. Any exceptions are documented explicitly. This doesn't mean as always uses the same syntax as another assembler for the same architecture; for example, we know of several incompatible versions of 680x0 assembly language syntax. Each time you run as it assembles exactly one source program. The source program is made up of one or more files. (The standard input is also a file.) You give as a command line that has zero or more input file names. The input files are read (from left file name to right). A command line argument (in any position) that has no special meaning is taken to be an input file name. If you give as no file names it attempts to read one input file from the as standard input, which is normally your terminal. You may have to type ctl-D to tell as there is no more program to assemble. Use -- if you need to explicitly name the standard input file in your command line. If the source is empty, as produces a small, empty object file. as may write warnings and error messages to the standard error file (usually your terminal). This should not happen when a compiler runs as automatically. Warnings report an assumption made so that as could keep assembling a flawed program; errors report a grave problem that stops the assembly. If you are invoking as via the GNU C compiler, you can use the -Wa option to pass arguments through to the assembler. The assembler arguments must be separated from each other (and the -Wa) by commas. For example: gcc -c -g -O -Wa,-alh,-L file.c This passes two options to the assembler: -alh (emit a listing to standard output with high-level and assembly source) and -L (retain local symbols in the symbol table). Usually you do not need to use this -Wa mechanism, since many compiler command-line options are automatically passed to the assembler by the compiler. (You can call the GNU compiler driver with the -v option to see precisely what options it passes to each compilation pass, including the assembler.)","Process Name":"alpha-linux-gnu-as","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-as"}},{"Process":{"Description":"The C ++ and Java languages provide function overloading, which means that you can write many functions with the same name, providing that each function takes parameters of different types. In order to be able to distinguish these similarly named functions C ++ and Java encode them into a low-level assembler name which uniquely identifies each different version. This process is known as mangling. The c++filt [1] program does the inverse mapping: it decodes (demangles) low-level names into user-level names so that they can be read. Every alphanumeric word (consisting of letters, digits, underscores, dollars, or periods) seen in the input is a potential mangled name. If the name decodes into a C ++ name, the C ++ name replaces the low-level name in the output, otherwise the original word is output. In this way you can pass an entire assembler source file, containing mangled names, through c++filt and see the same source file containing demangled names. You can also use c++filt to decipher individual symbols by passing them on the command line: c++filt <symbol> If no symbol arguments are given, c++filt reads symbol names from the standard input instead. All the results are printed on the standard output. The difference between reading names from the command line versus reading names from the standard input is that command line arguments are expected to be just mangled names and no checking is performed to separate them from surrounding text. Thus for example: c++filt -n _Z1fv will work and demangle the name to \"f()\" whereas: c++filt -n _Z1fv, will not work. (Note the extra comma at the end of the mangled name which makes it invalid). This command however will work: echo _Z1fv, | c++filt -n and will display \"f(),\", i.e., the demangled name followed by a trailing comma. This behaviour is because when the names are read from the standard input it is expected that they might be part of an assembler source file where there might be extra, extraneous characters trailing after a mangled name. For example: .type   _Z1fv, @function","Process Name":"alpha-linux-gnu-c++filt","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-c++filt"}},{"Process":{"Description":"The C preprocessor, often known as cpp, is a macro processor that is used automatically by the C compiler to transform your program before compilation. It is called a macro processor because it allows you to define macros, which are brief abbreviations for longer constructs. The C preprocessor is intended to be used only with C, C ++ , and Objective-C source code. In the past, it has been abused as a general text processor. It will choke on input which does not obey C's lexical rules. For example, apostrophes will be interpreted as the beginning of character constants, and cause errors. Also, you cannot rely on it preserving characteristics of the input which are not significant to C-family languages. If a Makefile is preprocessed, all the hard tabs will be removed, and the Makefile will not work. Having said that, you can often get away with using cpp on things which are not C. Other Algol-ish programming languages are often safe (Pascal, Ada, etc.) So is assembly, with caution. -traditional-cpp mode preserves more white space, and is otherwise more permissive. Many of the problems can be avoided by writing C or C ++ style comments instead of native language comments, and keeping macros simple. Wherever possible, you should use a preprocessor geared to the language you are writing in. Modern versions of the GNU assembler have macro facilities. Most high level programming languages have their own conditional compilation and inclusion mechanism. If all else fails, try a true general text processor, such as GNU M4. C preprocessors vary in some details. This manual discusses the GNU C preprocessor, which provides a small superset of the features of ISO Standard C. In its default mode, the GNU C preprocessor does not do a few things required by the standard. These are features which are rarely, if ever, used, and may cause surprising changes to the meaning of a program which does not expect them. To get strict ISO Standard C, you should use the -std=c90, -std=c99 or -std=c11 options, depending on which version of the standard you want. To get all the mandatory diagnostics, you must also use -pedantic. This manual describes the behavior of the ISO preprocessor. To minimize gratuitous differences, where the ISO preprocessor's behavior does not conflict with traditional semantics, the traditional preprocessor should behave the same way. The various differences that do exist are detailed in the section Traditional Mode. For clarity, unless noted otherwise, references to CPP in this manual refer to GNU CPP .","Process Name":"alpha-linux-gnu-cpp","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-cpp"}},{"Process":{"Description":"dlltool reads its inputs, which can come from the -d and -b options as well as object files specified on the command line. It then processes these inputs and if the -e option has been specified it creates a exports file. If the -l option has been specified it creates a library file and if the -z option has been specified it creates a def file. Any or all of the -e, -l and -z options can be present in one invocation of dlltool. When creating a DLL , along with the source for the DLL , it is necessary to have three other files. dlltool can help with the creation of these files. The first file is a .def file which specifies which functions are exported from the DLL , which functions the DLL imports, and so on. This is a text file and can be created by hand, or dlltool can be used to create it using the -z option. In this case dlltool will scan the object files specified on its command line looking for those functions which have been specially marked as being exported and put entries for them in the .def file it creates. In order to mark a function as being exported from a DLL , it needs to have an -export:<name_of_function> entry in the .drectve section of the object file. This can be done in C by using the asm() operator: asm (\".section .drectve\");\nasm (\".ascii \\\"-export:my_func\\\"\");\n\nint my_func (void) { ... } The second file needed for DLL creation is an exports file. This file is linked with the object files that make up the body of the DLL and it handles the interface between the DLL and the outside world. This is a binary file and it can be created by giving the -e option to dlltool when it is creating or reading in a .def file. The third file needed for DLL creation is the library file that programs will link with in order to access the functions in the DLL (an 'import library'). This file can be created by giving the -l option to dlltool when it is creating or reading in a .def file. If the -y option is specified, dlltool generates a delay-import library that can be used instead of the normal import library to allow a program to link to the dll only as soon as an imported function is called for the first time. The resulting executable will need to be linked to the static delayimp library containing __delayLoadHelper2(), which in turn will import LoadLibraryA and GetProcAddress from kernel32. dlltool builds the library file by hand, but it builds the exports file by creating temporary files containing assembler statements and then assembling these. The -S command line option can be used to specify the path to the assembler that dlltool will use, and the -f option can be used to pass specific flags to that assembler. The -n can be used to prevent dlltool from deleting these temporary assembler files when it is done, and if -n is specified twice then this will prevent dlltool from deleting the temporary object files it used to build the library. Here is an example of creating a DLL from a source file dll.c and also creating a program (from an object file called program.o) that uses that DLL: gcc -c dll.c\ndlltool -e exports.o -l dll.lib dll.o\ngcc dll.o exports.o -o dll.dll\ngcc program.o dll.lib -o program dlltool may also be used to query an existing import library to determine the name of the DLL to which it is associated. See the description of the -I or --identify option.","Process Name":"alpha-linux-gnu-dlltool","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-dlltool"}},{"Process":{"Description":"elfedit updates the ELF header of ELF files which have the matching ELF machine and file types. The options control how and which fields in the ELF header should be updated. elffile... are the ELF files to be updated. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files.","Process Name":"alpha-linux-gnu-elfedit","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-elfedit"}},{"Process":{"Description":"When you invoke GCC , it normally does preprocessing, compilation, assembly and linking. The \"overall options\" allow you to stop this process at an intermediate stage. For example, the -c option says not to run the linker. Then the output consists of object files output by the assembler. Other options are passed on to one stage of processing. Some options control the preprocessor and others the compiler itself. Yet other options control the assembler and linker; most of these are not documented here, since you rarely need to use any of them. Most of the command-line options that you can use with GCC are useful for C programs; when an option is only useful with another language (usually C ++ ), the explanation says so explicitly. If the description for a particular option does not mention a source language, you can use that option with all supported languages. The gcc program accepts options and file names as operands. Many options have multi-letter names; therefore multiple single-letter options may not be grouped: -dv is very different from -d -v. You can mix options and other arguments. For the most part, the order you use doesn't matter. Order does matter when you use several options of the same kind; for example, if you specify -L more than once, the directories are searched in the order specified. Also, the placement of the -l option is significant. Many options have long names starting with -f or with -W---for example, -fmove-loop-invariants, -Wformat and so on. Most of these have both positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"alpha-linux-gnu-gcc","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-gcc"}},{"Process":{"Description":"gcov is a test coverage program. Use it in concert with GCC to analyze your programs to help create more efficient, faster running code and to discover untested parts of your program. You can use gcov as a profiling tool to help discover where your optimization efforts will best affect your code. You can also use gcov along with the other profiling tool, gprof, to assess which parts of your code use the greatest amount of computing time. Profiling tools help you analyze your code's performance. Using a profiler such as gcov or gprof, you can find out some basic performance statistics, such as: \u2022 how often each line of code executes \u2022 what lines of code are actually executed \u2022 how much computing time each section of code uses Once you know these things about how your code works when compiled, you can look at each module to see which modules should be optimized. gcov helps you determine where to work on optimization. Software developers also use coverage testing in concert with testsuites, to make sure software is actually good enough for a release. Testsuites can verify that a program works as expected; a coverage program tests to see how much of the program is exercised by the testsuite. Developers can then determine what kinds of test cases need to be added to the testsuites to create both better testing and a better final product. You should compile your code without optimization if you plan to use gcov because the optimization, by combining some lines of code into one function, may not give you as much information as you need to look for 'hot spots' where the code is using a great deal of computer time. Likewise, because gcov accumulates statistics by line (at the lowest resolution), it works best with a programming style that places only one statement on each line. If you use complicated macros that expand to loops or to other control structures, the statistics are less helpful---they only report on the line where the macro call appears. If your complex macros behave like functions, you can replace them with inline functions to solve this problem. gcov creates a logfile called sourcefile.gcov which indicates how many times each line of a source file sourcefile.c has executed. You can use these logfiles along with gprof to aid in fine-tuning the performance of your programs. gprof gives timing information you can use along with the information you get from gcov. gcov works only on code compiled with GCC . It is not compatible with any other profiling or test coverage mechanism.","Process Name":"alpha-linux-gnu-gcov","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-gcov"}},{"Process":{"Description":"\"gprof\" produces an execution profile of C, Pascal, or Fortran77 programs. The effect of called routines is incorporated in the profile of each caller. The profile data is taken from the call graph profile file (gmon.out default) which is created by programs that are compiled with the -pg option of \"cc\", \"pc\", and \"f77\". The -pg option also links in versions of the library routines that are compiled for profiling. \"Gprof\" reads the given object file (the default is \"a.out\") and establishes the relation between its symbol table and the call graph profile from gmon.out. If more than one profile file is specified, the \"gprof\" output shows the sum of the profile information in the given profile files. \"Gprof\" calculates the amount of time spent in each routine. Next, these times are propagated along the edges of the call graph. Cycles are discovered, and calls into a cycle are made to share the time of the cycle. Several forms of output are available from the analysis. The flat profile shows how much time your program spent in each function, and how many times that function was called. If you simply want to know which functions burn most of the cycles, it is stated concisely here. The call graph shows, for each function, which functions called it, which other functions it called, and how many times. There is also an estimate of how much time was spent in the subroutines of each function. This can suggest places where you might try to eliminate function calls that use a lot of time. The annotated source listing is a copy of the program's source code, labeled with the number of times each line of the program was executed.","Process Name":"alpha-linux-gnu-gprof","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-gprof"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"alpha-linux-gnu-ld","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-ld"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"alpha-linux-gnu-ld.bfd","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-ld.bfd"}},{"Process":{"Description":"nlmconv converts the relocatable i386 object file infile into the NetWare Loadable Module outfile, optionally reading headerfile for NLM header information. For instructions on writing the NLM command file language used in header files, see the linkers section, NLMLINK in particular, of the NLM Development and Tools Overview, which is part of the NLM Software Developer's Kit (\" NLM SDK \"), available from Novell, Inc. nlmconv uses the GNU Binary File Descriptor library to read infile; nlmconv can perform a link step. In other words, you can list more than one object file for input if you list them in the definitions file (rather than simply specifying one input file on the command line). In this case, nlmconv calls the linker for you.","Process Name":"alpha-linux-gnu-nlmconv","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-nlmconv"}},{"Process":{"Description":"GNU nm lists the symbols from object files objfile.... If no object files are listed as arguments, nm assumes the file a.out. For each symbol, nm shows: \u2022 The symbol value, in the radix selected by options (see below), or hexadecimal by default. \u2022 The symbol type. At least the following types are used; others are, as well, depending on the object file format. If lowercase, the symbol is usually local; if uppercase, the symbol is global (external). There are however a few lowercase symbols that are shown for special global symbols (\"u\", \"v\" and \"w\"). \"A\" The symbol's value is absolute, and will not be changed by further linking. \"B\" \"b\" The symbol is in the uninitialized data section (known as BSS ). \"C\" The symbol is common. Common symbols are uninitialized data. When linking, multiple common symbols may appear with the same name. If the symbol is defined anywhere, the common symbols are treated as undefined references. \"D\" \"d\" The symbol is in the initialized data section. \"G\" \"g\" The symbol is in an initialized data section for small objects. Some object file formats permit more efficient access to small data objects, such as a global int variable as opposed to a large global array. \"i\" For PE format files this indicates that the symbol is in a section specific to the implementation of DLLs. For ELF format files this indicates that the symbol is an indirect function. This is a GNU extension to the standard set of ELF symbol types. It indicates a symbol which if referenced by a relocation does not evaluate to its address, but instead must be invoked at runtime. The runtime execution will then return the value to be used in the relocation. \"N\" The symbol is a debugging symbol. \"p\" The symbols is in a stack unwind section. \"R\" \"r\" The symbol is in a read only data section. \"S\" \"s\" The symbol is in an uninitialized data section for small objects. \"T\" \"t\" The symbol is in the text (code) section. \"U\" The symbol is undefined. \"u\" The symbol is a unique global symbol. This is a GNU extension to the standard set of ELF symbol bindings. For such a symbol the dynamic linker will make sure that in the entire process there is just one symbol with this name and type in use. \"V\" \"v\" The symbol is a weak object. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the weak symbol becomes zero with no error. On some systems, uppercase indicates that a default value has been specified. \"W\" \"w\" The symbol is a weak symbol that has not been specifically tagged as a weak object symbol. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the symbol is determined in a system-specific manner without error. On some systems, uppercase indicates that a default value has been specified. \"-\" The symbol is a stabs symbol in an a.out object file. In this case, the next values printed are the stabs other field, the stabs desc field, and the stab type. Stabs symbols are used to hold debugging information. \"?\" The symbol type is unknown, or object file format specific. \u2022 The symbol name.","Process Name":"alpha-linux-gnu-nm","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-nm"}},{"Process":{"Description":"The GNU objcopy utility copies the contents of an object file to another. objcopy uses the GNU BFD Library to read and write the object files. It can write the destination object file in a format different from that of the source object file. The exact behavior of objcopy is controlled by command-line options. Note that objcopy should be able to copy a fully linked file between any two formats. However, copying a relocatable object file between any two formats may not work as expected. objcopy creates temporary files to do its translations and deletes them afterward. objcopy uses BFD to do all its translation work; it has access to all the formats described in BFD and thus is able to recognize most formats without being told explicitly. objcopy can be used to generate S-records by using an output target of srec (e.g., use -O srec). objcopy can be used to generate a raw binary file by using an output target of binary (e.g., use -O binary). When objcopy generates a raw binary file, it will essentially produce a memory dump of the contents of the input object file. All symbols and relocation information will be discarded. The memory dump will start at the load address of the lowest section copied into the output file. When generating an S-record or a raw binary file, it may be helpful to use -S to remove sections containing debugging information. In some cases -R will be useful to remove sections which contain information that is not needed by the binary file. Note---objcopy is not able to change the endianness of its input files. If the input format has an endianness (some formats do not), objcopy can only copy the inputs into file formats that have the same endianness or which have no endianness (e.g., srec). (However, see the --reverse-bytes option.)","Process Name":"alpha-linux-gnu-objcopy","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-objcopy"}},{"Process":{"Description":"objdump displays information about one or more object files. The options control what particular information to display. This information is mostly useful to programmers who are working on the compilation tools, as opposed to programmers who just want their program to compile and work. objfile... are the object files to be examined. When you specify archives, objdump shows information on each of the member object files.","Process Name":"alpha-linux-gnu-objdump","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-objdump"}},{"Process":{"Description":"ranlib generates an index to the contents of an archive and stores it in the archive. The index lists each symbol defined by a member of an archive that is a relocatable object file. You may use nm -s or nm --print-armap to list this index. An archive with such an index speeds up linking to the library and allows routines in the library to call each other without regard to their placement in the archive. The GNU ranlib program is another form of GNU ar; running ranlib is completely equivalent to executing ar -s.","Process Name":"alpha-linux-gnu-ranlib","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-ranlib"}},{"Process":{"Description":"readelf displays information about one or more ELF format object files. The options control what particular information to display. elffile... are the object files to be examined. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files. This program performs a similar function to objdump but it goes into more detail and it exists independently of the BFD library, so if there is a bug in BFD then readelf will not be affected.","Process Name":"alpha-linux-gnu-readelf","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-readelf"}},{"Process":{"Description":"The GNU size utility lists the section sizes---and the total size---for each of the object or archive files objfile in its argument list. By default, one line of output is generated for each object file or each module in an archive. objfile... are the object files to be examined. If none are specified, the file \"a.out\" will be used.","Process Name":"alpha-linux-gnu-size","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-size"}},{"Process":{"Description":"For each file given, GNU strings prints the printable character sequences that are at least 4 characters long (or the number given with the options below) and are followed by an unprintable character. By default, it only prints the strings from the initialized and loaded sections of object files; for other types of files, it prints the strings from the whole file. strings is mainly useful for determining the contents of non-text files.","Process Name":"alpha-linux-gnu-strings","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-strings"}},{"Process":{"Description":"GNU strip discards all symbols from object files objfile. The list of object files may include archives. At least one object file must be given. strip modifies the files named in its argument, rather than writing modified copies under different names.","Process Name":"alpha-linux-gnu-strip","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-strip"}},{"Process":{"Description":"windmc reads message definitions from an input file (.mc) and translate them into a set of output files. The output files may be of four kinds: \"h\" A C header file containing the message definitions. \"rc\" A resource file compilable by the windres tool. \"bin\" One or more binary files containing the resource data for a specific message language. \"dbg\" A C include file that maps message id's to their symbolic name. The exact description of these different formats is available in documentation from Microsoft. When windmc converts from the \"mc\" format to the \"bin\" format, \"rc\", \"h\", and optional \"dbg\" it is acting like the Windows Message Compiler.","Process Name":"alpha-linux-gnu-windmc","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-windmc"}},{"Process":{"Description":"windres reads resources from an input file and copies them into an output file. Either file may be in one of three formats: \"rc\" A text format read by the Resource Compiler. \"res\" A binary format generated by the Resource Compiler. \"coff\" A COFF object or executable. The exact description of these different formats is available in documentation from Microsoft. When windres converts from the \"rc\" format to the \"res\" format, it is acting like the Windows Resource Compiler. When windres converts from the \"res\" format to the \"coff\" format, it is acting like the Windows \"CVTRES\" program. When windres generates an \"rc\" file, the output is similar but not identical to the format expected for the input. When an input \"rc\" file refers to an external filename, an output \"rc\" file will instead include the file contents. If the input or output format is not specified, windres will guess based on the file name, or, for the input file, the file contents. A file with an extension of .rc will be treated as an \"rc\" file, a file with an extension of .res will be treated as a \"res\" file, and a file with an extension of .o or .exe will be treated as a \"coff\" file. If no output file is specified, windres will print the resources in \"rc\" format to standard output. The normal use is for you to write an \"rc\" file, use windres to convert it to a COFF object file, and then link the COFF file into your application. This will make the resources described in the \"rc\" file available to Windows.","Process Name":"alpha-linux-gnu-windres","Link":"https:\/\/linux.die.net\/man\/1\/alpha-linux-gnu-windres"}},{"Process":{"Description":"Alpine is a screen-oriented message-handling tool. In its default configuration, Alpine offers an intentionally limited set of functions geared toward the novice user, but it also has a large list of optional \"power-user\" and personal-preference features. alpinef is a variant of Alpine that uses function keys rather than mnemonic single-letter commands. Alpine's basic feature set includes: View, Save, Export, Delete, Print, Reply and Forward messages. Compose messages in a simple editor (Pico) with word-wrap and a spelling checker. Messages may be postponed for later completion. Full-screen selection and management of message folders. Address book to keep a list of long or frequently-used addresses. Personal distribution lists may be defined. Addresses may be taken into the address book from incoming mail without retyping them. New mail checking and notification occurs automatically every 2.5 minutes and after certain commands, e.g. refresh-screen (Ctrl-L). On-line, context-sensitive help screens. Alpine supports MIME (Multipurpose Internet Mail Extensions), an Internet Standard for representing multipart and multimedia data in email. Alpine allows you to save MIME objects to files, and in some cases, can also initiate the correct program for viewing the object. It uses the system's mailcap configuration file to determine what program can process a particular MIME object type. Alpine's message composer does not have integral multimedia capability, but any type of data file --including multimedia-- can be attached to a text message and sent using MIME's encoding rules. This allows any group of individuals with MIME-capable mail software (e.g. Alpine, PC-Alpine, or many other programs) to exchange formatted documents, spread-sheets, image files, etc, via Internet email. Alpine uses the c-client messaging API to access local and remote mail folders. This library provides a variety of low-level message-handling functions, including drivers for a variety of different mail file formats, as well as routines to access remote mail and news servers, using IMAP (Internet Message Access Protocol) and NNTP (Network News Transport Protocol). Outgoing mail is usually posted directly via SMTP (Simple Mail Transfer Protocol).","Process Name":"alpine","Link":"https:\/\/linux.die.net\/man\/1\/alpine"}},{"Process":{"Description":"This manual page document describes the atool commands. These commands are used for managing file archives of various types, such as tar and Zip archives. Each command can be executed individually or by giving the appropriate options to atool (see OPTIONS below). aunpack extracts files from an archive. Often one wants to extract all files in an archive to a single subdirectory. However, some archives contain multiple files in their root directories. The aunpack program overcomes this problem by first extracting files to a unique (temporary) directory, and then moving its contents back if possible. This also prevents local files from being overwritten by mistake. apack creates archives (or compresses files). If no file arguments are specified, filenames to add are read from standard in. als lists files in an archive. acat extracts files in an archive to standard out. adiff generates a diff between two archives using diff(1). arepack repacks archives to a different format. It does this by first extracting all files of the old archive into a temporary directory, then packing all files extracted to that directory to the new archive. Use the --each (-e) option in combination with --format (-F) to repack multiple archives using a single invocation of atool. Note that arepack will not remove the old archive. Unless the --format (-F) option is provided, the archive format is determined by the archive file extension. I.e. an extension \".tar.gz\" or \".tgz\" means tar+gzip format. Note that the extensions are checked in the order listed in the section ARCHIVE TYPES below, which is why a file with extension \".tar.gz\" is considered to a be tar+gzip archive, not a gzip compressed file.","Process Name":"als","Link":"https:\/\/linux.die.net\/man\/1\/als"}},{"Process":{"Description":"alsactl is used to control advanced settings for the ALSA soundcard drivers. It supports multiple soundcards. If your card has features that you can't seem to control from a mixer application, you have come to the right place.","Process Name":"alsactl","Link":"https:\/\/linux.die.net\/man\/1\/alsactl"}},{"Process":{"Description":"alsaloop allows create a PCM loopback between a PCM capture device and a PCM playback device. alsaloop supports multiple soundcards, adaptive clock synchronization, adaptive rate resampling using the samplerate library (if available in the system). Also, mixer controls can be redirected from one card to another (for example Master and PCM).","Process Name":"alsaloop","Link":"https:\/\/linux.die.net\/man\/1\/alsaloop"}},{"Process":{"Description":"alsamixer is an ncurses mixer program for use with the ALSA soundcard drivers. It supports multiple soundcards with multiple devices.","Process Name":"alsamixer","Link":"https:\/\/linux.die.net\/man\/1\/alsamixer"}},{"Process":{"Description":"amidi is a command-line utility which allows to receive and send SysEx (system exclusive) data from\/to external MIDI devices. It can also send any other MIDI commands. amidi handles only files containing raw MIDI commands, without timing information. amidi does not support Standard MIDI (.mid) files, but aplaymidi(1) and arecordmidi(1) do.","Process Name":"amidi","Link":"https:\/\/linux.die.net\/man\/1\/amidi"}},{"Process":{"Description":"amixer allows command-line control of the mixer for the ALSA soundcard driver. amixer supports multiple soundcards. amixer with no arguments will display the current mixer settings for the default soundcard and device. This is a good way to see a list of the simple mixer controls you can use.","Process Name":"amixer","Link":"https:\/\/linux.die.net\/man\/1\/amixer"}},{"Process":{"Description":"amqp-consume consumes messages from a queue on an AMQP server. For each message that arrives, a receiving command is run, with the message body supplied to it on standard input. amqp-consume can consume from an existing queue, or it can create a new queue. It can optionally bind the queue to an existing exchange. By default, messages will be consumed with explicit acknowledgements. A message will only be acknowledged if the receiving command exits successfully (i.e. with an exit code of zero). The AMQP \"no ack\" mode (a.k.a. auto-ack mode) can be enable with the -A option.","Process Name":"amqp-consume","Link":"https:\/\/linux.die.net\/man\/1\/amqp-consume"}},{"Process":{"Description":"amqp-declare-queue attempts to create a queue on an AMQP server, and exits. If the empty-string is supplied as the queue name, a fresh queue name is generated by the server and returned. In all cases, if a queue was successfully declared, the (raw binary) name of the queue is printed to standard output, followed by a newline.","Process Name":"amqp-declare-queue","Link":"https:\/\/linux.die.net\/man\/1\/amqp-declare-queue"}},{"Process":{"Description":"amqp-delete-queue deletes a queue from an AMQP server, and exits after printing to standard output the number of messages that were in the queue at the time of its deletion.","Process Name":"amqp-delete-queue","Link":"https:\/\/linux.die.net\/man\/1\/amqp-delete-queue"}},{"Process":{"Description":"amqp-get attempts to consume a single message from a queue on an AMQP server, and exits. Unless the queue was empty, the body of the resulting message is sent to standard output.","Process Name":"amqp-get","Link":"https:\/\/linux.die.net\/man\/1\/amqp-get"}},{"Process":{"Description":"Publishes a message to an exchange on an AMQP server. Options allow the various properties of the message and parameters of the AMQP basic.publish method to be specified. By default, the message body is read from standard input. Alternatively, the -b option allows the message body to be provided as part of the command.","Process Name":"amqp-publish","Link":"https:\/\/linux.die.net\/man\/1\/amqp-publish"}},{"Process":{"Description":"This manual page is not meant to be exhaustive. The complete documentation for this version of TeX can be found in the info file or manual Web2C: A TeX implementation. The AmSTeX language is described in the book The Joy of TeX for nroff. AmSTeX is a TeX macro package, not a modification to the TeX source program, so all the capabilities described in tex(1) are present. The AmSTeX macros encourage writers to think about the content of their documents, rather than the form. The ideal, not always realized, is to have no formatting commands (like ''switch to italic'' or ''skip 2 picas'') in the document at all; instead, everything is done by specific markup instructions: ''emphasize'', ''start a section''. AmSTeX is the official typesetting system of the American Mathematical Society, and nearly all of its publications are typeset using AmSTeX. The online version of AMS Math Reviews uses AmSTeX input syntax for display of mathematical material. AMS authors can provide editors with computer-readable AmSTeX files. For authors more familiar with LaTeX, an AMS-supported package called amsmath.sty is is available.","Process Name":"amstex","Link":"https:\/\/linux.die.net\/man\/1\/amstex"}},{"Process":{"Description":"amtterm provides access to the serial-over-lan port of Intel AMT managed machines. host is the hostname or IP address of the machine amtterm should connect to. port is the tcp port to use and defaults to 16994 (standard AMT redirection port) if unspecified. For more inforamtions on Intel AMT check amt-howto(7).","Process Name":"amtterm","Link":"https:\/\/linux.die.net\/man\/1\/amtterm"}},{"Process":{"Description":"amttool is a perl script which speaks SOAP to Intel AMT managed machines. It can query informations about the machine in question and also send some commands for basic remote control. host is the hostname or IP address of the machine amttool should control. command is an optional command. You must set fill AMT_PASSWORD environment variable with the AMT password. For more inforamtions on Intel AMT check amt-howto(7).","Process Name":"amttool","Link":"https:\/\/linux.die.net\/man\/1\/amttool"}},{"Process":{"Description":"","Process Name":"amule","Link":"https:\/\/linux.die.net\/man\/1\/amule"}},{"Process":{"Description":"amulecmd is a console-based client to control aMule. -h, --host= <host> Host where aMule is running (default: localhost). <host> might be an IP address or a DNS name -p, --port= <port> aMule's port for External Connections, as set in Preferences->Remote Controls (default: 4712) -P, --password= <passwd> External Connections password -f, --config-file= <path> Use the given configuration file. Default configuration file is ~\/.aMule\/remote.conf -q, --quiet Do not print any output to stdout. This seems to be a very much useless option with amulecmd. -v, --verbose Be verbose - show also debug messages -l, --locale= <lang> Sets program locale (language). See section LANGUAGES for the description of the <lang> parameter. -w, --write-config Write command line options to config file and exit -c, --command= <command> Execute <command> as if it was entered at amulecmd's prompt and exit. --create-config-from= <path> Create config file based upon <path>, which must point to a valid aMule config file, and then exit --help Prints a short usage description. --version Displays the current version number.","Process Name":"amulecmd","Link":"https:\/\/linux.die.net\/man\/1\/amulecmd"}},{"Process":{"Description":"-v, --version Displays the current version number. -h, --help Prints a short usage description. -i, --enable-stdin Does not disable stdin. -f, --full-daemon Forks to background. -c, --config-dir= <dir> Reads config from <dir> instead of home. -e, --ec-config Configures EC (External Connections). -d, --disable-fatal Does not handle fatal exception. -o, --log-stdout Prints log messages to stdout. -r, --reset-config Resets config to default values.","Process Name":"amuled","Link":"https:\/\/linux.die.net\/man\/1\/amuled"}},{"Process":{"Description":"","Process Name":"amuleweb","Link":"https:\/\/linux.die.net\/man\/1\/amuleweb"}},{"Process":{"Description":"","Process Name":"and","Link":"https:\/\/linux.die.net\/man\/1\/and"}},{"Process":{"Description":"Wiggling tentacles.","Process Name":"anemone","Link":"https:\/\/linux.die.net\/man\/1\/anemone"}},{"Process":{"Description":"The program demonstrates a search algorithm designed for locating a source of odor in turbulent atmosphere. The odor is convected by wind which has a constant mean direction and fluctuations around it. The searcher is able to sense the odor and determine local instantaneous wind direction. The goal is to find the source in the shortest mean time. Some animals face this task to find mates, food, home etc. They exhibit very particular, zigzagging search trajectories. This is modeled as a search on a discrete two-dimensional lattice. The source releases particles that drift with constant velocity in one direction and walk randomly in the other direction. The searcher knows if it hit a particle, and if so, particle's position one time step earlier (local wind direction). The program paints sources and particles released by them as well as trajectories of serchers who are trying to capture the sources.","Process Name":"anemotaxis","Link":"https:\/\/linux.die.net\/man\/1\/anemotaxis"}},{"Process":{"Description":"Image Settings: -authenticate value decrypt image with this password -backdrop display image centered on a backdrop -channel type apply option to select image channels -colormap type Shared or Private -colorspace type alternate image colorspace -decipher filename convert cipher pixels to plain pixels -define format:option define one or more image format options -delay value display the next image after pausing -density geometry horizontal and vertical density of the image -depth value image depth -display server display image to this X server -dispose method layer disposal method -dither method apply error diffusion to image -format \"string\" output formatted image characteristics -gamma value level of gamma correction -geometry geometry preferred size and location of the Image window -gravity type horizontal and vertical backdrop placement -identify identify the format and characteristics of the image -immutable displayed image cannot be modified -interlace type type of image interlacing scheme -interpolate method pixel color interpolation method -limit type value pixel cache resource limit -loop iterations loop images then exit -matte store matte channel if the image has one -map type display image using this Standard Colormap -monitor monitor progress -pause seconds to pause before reanimating -page geometry size and location of an image canvas (setting) -quantize colorspace reduce colors in this colorspace -quiet suppress all warning messages -regard-warnings pay attention to warning messages -remote command execute a command in an remote display process -sampling-factor geometry horizontal and vertical sampling factor -scenes range image scene range -seed value seed a new sequence of pseudo-random numbers -set attribute value set an image attribute -size geometry width and height of image -support factor resize support: > 1.0 is blurry, < 1.0 is sharp -transparent-color color transparent color -treedepth value color tree depth -verbose print detailed information about the image -visual type display image using this visual type -virtual-pixel method virtual pixel access method -window id display image to background of this window Image Operators: -colors value preferred number of colors in the image -crop geometry preferred size and location of the cropped image -extract geometry extract area from image -monochrome transform image to black and white -repage geometry size and location of an image canvas (operator) -resample geometry change the resolution of an image -resize geometry resize the image -rotate degrees apply Paeth rotation to the image -strip strip image of all profiles and comments -trim trim image edges Image Sequence Operators: -coalesce merge a sequence of images -flatten flatten a sequence of images Miscellaneous Options: -debug events display copious debugging information -help print program options -log format format of debugging information -list type print a list of supported option arguments -version print version information In addition to those listed above, you can specify these standard X resources as command line options: -background, -bordercolor, -borderwidth, -font, -foreground, -iconGeometry, -iconic, -name, -mattecolor, -shared-memory, or -title. By default, the image format of 'file' is determined by its magic number. To specify a particular image format, precede the filename with an image format name and a colon (i.e. ps:image) or specify the image type as the filename suffix (i.e. image.ps). Specify 'file' as '-' for standard input or output. Buttons: 1 press to map or unmap the Command widget","Process Name":"animate","Link":"https:\/\/linux.die.net\/man\/1\/animate"}},{"Process":{"Description":"annotate-output will execute the specified program, while prepending every line with the current time and O for stdout and E for stderr.","Process Name":"annotate-output","Link":"https:\/\/linux.die.net\/man\/1\/annotate-output"}},{"Process":{"Description":"Ansible is an extra-simple tool\/framework\/API for doing 'remote things' over SSH.","Process Name":"ansible","Link":"https:\/\/linux.die.net\/man\/1\/ansible"}},{"Process":{"Description":"ansible-doc displays information on modules installed in Ansible libraries. It displays a terse listing of modules and their short descriptions, provides a printout of their DOCUMENTATION strings, and it can create a short \"snippet\" which can be pasted into a playbook.","Process Name":"ansible-doc","Link":"https:\/\/linux.die.net\/man\/1\/ansible-doc"}},{"Process":{"Description":"Ansible playbooks are a configuration and multinode deployment system. Ansible-playbook is the tool used to run them. See the project home page (link below) for more information.","Process Name":"ansible-playbook","Link":"https:\/\/linux.die.net\/man\/1\/ansible-playbook"}},{"Process":{"Description":"Ansible is an extra-simple tool\/framework\/API for doing 'remote things' over SSH. Use ansible-pull to set up a remote copy of ansible on each managed node, each set to run via cron and update playbook source via git. This inverts the default push architecture of ansible into a pull architecture, which has near-limitless scaling potential. The setup playbook can be tuned to change the cron frequency, logging locations, and parameters to ansible-pull. This is useful both for extreme scale-out as well as periodic remediation. Usage of the fetch module to retrieve logs from ansible-pull runs would be an excellent way to gather and analyze remote logs from ansible-pull.","Process Name":"ansible-pull","Link":"https:\/\/linux.die.net\/man\/1\/ansible-pull"}},{"Process":{"Description":"A cellular automaton that is really a two-dimensional Turing machine: as the heads (\"ants\") walk along the screen, they change pixel values in their path. Then, as they pass over changed pixels, their behavior is influenced.","Process Name":"ant","Link":"https:\/\/linux.die.net\/man\/1\/ant"}},{"Process":{"Description":"This program provides a powerful, configurable and flexible SOCKS server.","Process Name":"antinat","Link":"https:\/\/linux.die.net\/man\/1\/antinat"}},{"Process":{"Description":"The antinspect code displays three ant-powered balls churning in a circle.","Process Name":"antinspect","Link":"https:\/\/linux.die.net\/man\/1\/antinspect"}},{"Process":{"Description":"Antiword is an application that displays the text and the images of Microsoft Word documents. A wordfile named - stands for a Word document read from the standard input. Only documents made by MS Word version 2 and version 6 or later are supported.","Process Name":"antiword","Link":"https:\/\/linux.die.net\/man\/1\/antiword"}},{"Process":{"Description":"The antspotlight code displays a single ant spotting out a screenshot.","Process Name":"antspotlight","Link":"https:\/\/linux.die.net\/man\/1\/antspotlight"}},{"Process":{"Description":"Converts files from .ps\/.ps.gz\/.pdf to .djvu by running them through a web server willing to perform this task. Invoke with -h switch for usage information.","Process Name":"any2djvu","Link":"https:\/\/linux.die.net\/man\/1\/any2djvu"}},{"Process":{"Description":"The overall goal of this project is to provide remote control service on Linux through Bluetooth, InfraRed, Wi-Fi or just TCP\/IP connection. anyRemote supports wide range of modern cell phones like Nokia, SonyEricsson, Motorola and others. It was developed as thin \"communication\" layer between Bluetooth (IR, Wi-Fi)-capabled phone and Linux, and in theory could be configured to manage almost any software. anyRemote is console application, but in addition there are GUI front-ends for Gnome and KDE.","Process Name":"anyremote","Link":"https:\/\/linux.die.net\/man\/1\/anyremote"}},{"Process":{"Description":"Anytooff reads OOGL geometry data (from the given files, if any, otherwise from standard input) and outputs the new polylist (in OFF format) to standard output. Offcombine is a synonym for anytooff.","Process Name":"anytooff","Link":"https:\/\/linux.die.net\/man\/1\/anytooff"}},{"Process":{"Description":"This program is part of Netpbm(1). anytopnm converts the input image, which may be in any of about 100 graphics formats, to PBM, PGM, or PPM format, depending on that nature of the input image, and outputs it to Standard Output. To determine the format of the input, anytopnm uses the file program (possibly assisted by the magic numbers file fragment included with Netpbm). If that fails (very few image formats have magic numbers), anytopnm looks at the filename extension. If that fails, anytopnm punts. The type of the output file depends on the input image. anytopnm uses the converters for particular graphics formats that are in the Netpbm package, so it can't convert any format that you couldn't convert with some other Netpbm program. What anytopnm adds is the ability to recognize the format and choose the appropriate Netpbm program to convert it. For example, if you invoke anytopnm on a GIF file, anytopnm will recognize that it is a GIF file and therefore giftopnm knows how to convert it to PNM, so anytopnm invokes giftopnm. anytopnm cannot recognize every possible input format, so you may still be able to convert an image with a specific Netpbm program when anytopnm fails to convert it. If file indicates that the input file is compressed (either via Unix compress, gzip, or bzip compression), anytopnm uncompresses it and proceeds as above with the uncompressed result. If file indicates that the input file is encoded by uuencode or btoa, anytopnm decodes it and proceeds as above with the decoded result. If file is - or not given, anytopnm takes its input from Standard Input.","Process Name":"anytopnm","Link":"https:\/\/linux.die.net\/man\/1\/anytopnm"}},{"Process":{"Description":"Anytoucd reads a geom from standard in and outputs the equivalent ucd stream to standard out. Anytoucd calls anytopl(1) before converting to ucd. Full documentation for ucd format is available from AVS documentation in an appendix in the Developer's Guide.","Process Name":"anytoucd","Link":"https:\/\/linux.die.net\/man\/1\/anytoucd"}},{"Process":{"Description":"ApacheTop watches a logfile generated by Apache (in standard common or combined logformat, and generates human-parsable output in realtime. -f logfile Select which file to watch. Specify this option multiple times to watch multiple files. -H hits | -T time These options are mutually exclusive. Specify only one, if any at all. They work as follows. ApacheTop maintains a table of information internally containing all the relevant information about the hits it's seen. This table can only be a finite size, so you need to decide how big it's going to be. You have two options. You can either: Use -H to say \"remember <this many> hits\" or Use -T to say \"remember all hits in <this many> seconds\" The default (at the moment) is to remember hits for 30 seconds. Setting this too large (whichever option you choose) will cause ApacheTop to use more memory and more CPU time. My experimentation finds that remembering no more than around 5000 requests works well. -q Instructs ApacheTop to keep the querystrings, not remove them. -l Instructs ApacheTop to lowercase all URLs, thus \/FOO and \/foo are treated as the same and accumulate the same statistics. -s segments Instructs ApacheTop to only keep the first <segments> parts of the path. Trailing slashes are kept if present. Statistics are then merged for each truncated url. -p Instructs ApacheTop to keep the protocol (http:\/\/ usually) at the front of its' referrer strings. Normal behaviour is to remove them to give more room to more useful information. -r secs Set default refresh delay, in seconds.","Process Name":"apachetop","Link":"https:\/\/linux.die.net\/man\/1\/apachetop"}},{"Process":{"Description":"This manual page document describes the atool commands. These commands are used for managing file archives of various types, such as tar and Zip archives. Each command can be executed individually or by giving the appropriate options to atool (see OPTIONS below). aunpack extracts files from an archive. Often one wants to extract all files in an archive to a single subdirectory. However, some archives contain multiple files in their root directories. The aunpack program overcomes this problem by first extracting files to a unique (temporary) directory, and then moving its contents back if possible. This also prevents local files from being overwritten by mistake. apack creates archives (or compresses files). If no file arguments are specified, filenames to add are read from standard in. als lists files in an archive. acat extracts files in an archive to standard out. adiff generates a diff between two archives using diff(1). arepack repacks archives to a different format. It does this by first extracting all files of the old archive into a temporary directory, then packing all files extracted to that directory to the new archive. Use the --each (-e) option in combination with --format (-F) to repack multiple archives using a single invocation of atool. Note that arepack will not remove the old archive. Unless the --format (-F) option is provided, the archive format is determined by the archive file extension. I.e. an extension \".tar.gz\" or \".tgz\" means tar+gzip format. Note that the extensions are checked in the order listed in the section ARCHIVE TYPES below, which is why a file with extension \".tar.gz\" is considered to a be tar+gzip archive, not a gzip compressed file.","Process Name":"apack","Link":"https:\/\/linux.die.net\/man\/1\/apack"}},{"Process":{"Description":"apg generates several random passwords. It uses several password generation algorithms (currently two) and a built-in pseudo random number generator. Default algorithm is pronounceable password generation algorithm designed by Morrie Gasser and described in A Random Word Generator For Pronounceable Passwords National Technical Information Service (NTIS) AD-A-017676. The original paper is very old and had never been put online, so I have to use NIST implementation described in FIPS-181. Another algorithm is simple random character generation algorithm, but it uses four user-defined symbol sets to produce random password. It means that user can choose type of symbols that should appear in password. Symbol sets are: numeric symbol set (0,...,9) , capital letters symbol set (A,...,Z) , small letters symbol set (a,...,z) and special symbols symbol set (#,@,!,...). Built-in pseudo random number generator is an implementation of algorithm described in Appendix C of ANSI X9.17 or RFC1750 with exception that it uses CAST or SHA-1 instead of Triple DES. It uses local time with precision of microseconds (see gettimeofday(2)) and \/dev\/random (if available) to produce initial random seed. apg also have the ability to check generated password quality using dictionary. You can use this ability if you specify command-line options -r dictfile or -b filtername where dictfile is the dictionary file name and filtername is the name of Bloom filter file. In that dictionary you may place words (one per line) that should not appear as generated passwords. For example: user names, common words, etc. You even can use one of the dictionaries that come with dictionary password crackers. Bloom filter file should be created with apgbfm(1) utility included in apg distribution. In future releases I plan to implement some other techniques to check passwords (like pattern check) just to make life easier.","Process Name":"apg","Link":"https:\/\/linux.die.net\/man\/1\/apg"}},{"Process":{"Description":"apgbfm is used to manage Bloom filter that is used to restrict password generation in APG pasword generation software. Usage of the Bloom filter allows to speed up password check for large dictionaries and has some other benefits. The idea to use Bloom filter for that purpose is came from the description of the OPUS project OPUS: Preventing Weak Password Choices Purdue Technical Report CSD-TR 92-028 writen by Eugene H. Spafford. You can obtain this article from: http:\/\/www.cerias.purdue.edu\/homes\/spaf\/tech-reps\/9128.ps It has very nice description of Bloom filter and it's advantages for password checking systems. In simple words, apgbfm generates n hash values for every word and sets corresponding bits in filter file to 1. To check the word apgbfm generates the same hash functions for that word and if all n corresponding bits in filter file are set to 1 then it suppose that word exists in dicionary. apgbfm uses SHA-1 as a hash function. apgbfm can be used as standalone utility, not only with apg, or apgd. WARNING !!! Filter file format can be changed in the future. I'll try to make file formats compatible but i can not guaranty this. WARNING !!! apgbfm may slow down your computer during filter creation.","Process Name":"apgbfm","Link":"https:\/\/linux.die.net\/man\/1\/apgbfm"}},{"Process":{"Description":"arecord is a command-line soundfile recorder for the ALSA soundcard driver. It supports several file formats and multiple soundcards with multiple devices. If recording with interleaved mode samples the file is automatically split before the 2GB filesize. aplay is much the same, only it plays instead of recording. For supported soundfile formats, the sampling rate, bit depth, and so forth can be automatically determined from the soundfile header. If filename is not specified, the standard output or input is used. The aplay utility accepts multiple filenames.","Process Name":"aplay","Link":"https:\/\/linux.die.net\/man\/1\/aplay"}},{"Process":{"Description":"aplaymidi is a command-line utility that plays the specified MIDI file(s) to one or more ALSA sequencer ports.","Process Name":"aplaymidi","Link":"https:\/\/linux.die.net\/man\/1\/aplaymidi"}},{"Process":{"Description":"apm reads \/proc\/apm and presents the output in a human-readable format. Since primarily battery status information is provided, this command is most useful on laptops with a compliant APM BIOS. apm also allows the machine to be put into standby or suspend mode.","Process Name":"apm","Link":"https:\/\/linux.die.net\/man\/1\/apm"}},{"Process":{"Description":"Some computers, especially laptops, can wake-up from a low-power suspend to DRAM mode using the Real-time-clock (RTC) chip. Apmsleep can be used to set the alarm time in the RTC and to go into suspend or standby mode. An interrupt from the RTC causes the computer to wake-up. The program detects this event, by waiting for a leap in the kernel time and terminates successfully. If no time leap occurs within one minute, or something goes wrong, the exit value will be non-zero. The wake-up time can be specified in two formats: +hh:mm specifies a relative offset to the current time. The computer will suspend for exactly hh hours and mm minutes plus a few seconds to wake up. On some laptops, the timing is not completely accurate so it may be a few minutes (or more?) late. hh:mm specifies absolute local time in 24-hour format. The time stored in the RTC is not important. You may change the time zone used, with the TZ environment variable as usual. Daylight saving time is not obeyed in this version, but might be in a future release. WARNING: Do not close cover of laptop after suspending the laptop with apmsleep. Most laptops overheat when running with closed cover. Energy conservation with APM is little for a desktop. Turning of the screen will save 1\/2, going into standby with drives turned off will save another 1\/6th of the current. -V, --version Print the apmsleep program version and exit immediately. -s, --suspend Put the machine into suspend mode if possible (default). On my laptop, suspend mode turns off everything except the memory. -S, --standby Put the machine into standby mode if possible. On my laptop, standby mode turns off screen, hard disk, and CPU. -w, --wait Wait indefinitely for the time leap. -p, --precise Wait for alarm time to match actual time. Do not wait for time leap. This might be useful even without APM. -n, --noapm Do not call apm bios to suspend computer, just set the alarm clock and wait for time leap indefinitely. -d, --debug Print some information about what is going on.","Process Name":"apmsleep","Link":"https:\/\/linux.die.net\/man\/1\/apmsleep"}},{"Process":{"Description":"apol is a graphical tool that allows the user to inspect aspects of a SELinux policy. The tool allows the user to browse policy components (types, classes, roles, users, etc.), rules (TE, RBAC, MLS), and file system contexts. The tool also provides in depth analyses of domain transitions, information flows, and relabeling permissions.","Process Name":"apol","Link":"https:\/\/linux.die.net\/man\/1\/apol"}},{"Process":{"Description":"Packs a large circle with smaller circles, demonstrating the Descartes Circle Theorem.","Process Name":"apollonian","Link":"https:\/\/linux.die.net\/man\/1\/apollonian"}},{"Process":{"Description":"\"yaf\" can examine packet payloads and determine the application protocol in use within a flow, and export a 16-bit application label with each flow if \"yaf\" is built with application labeler support (using the --enable-applabel option to .\/configure when yaf is built). The exported application label uses the common port number for the protocol. For example, HTTP Traffic, independent of what port the traffic is detected on, will be labeled with a value of 80, the default HTTP port. Labels and rules are taken from a configuration file read by \"yaf\" at startup time. This rule file can be given on the command line with the --applabel-rules option or will try to be read from the default location of \/usr\/etc\/yafApplabelRules.conf. Application labeling requires payload capture to be enabled with the --max-payload option. A minimum payload capture length of 384 bytes is recommended for best results. Application labeling is presently experimental, and not guaranteed to be 100% accurate. However, application labels are supported in \"yafscii\" and SiLK via \"rwflowpack\", \"flowcap\", and \"rwipfix2silk\".","Process Name":"applabel","Link":"https:\/\/linux.die.net\/man\/1\/applabel"}},{"Process":{"Description":"The apple2 program simulates an original Apple ][ Plus computer in all its 1979 glory. It also reproduces the appearance of display on a color television set of the period. There are 3 modes: basic, slideshow, and text. Normally it chooses a mode randomly, but you can override with the -basic, -slideshow, or -text options. In basic mode a simulated user types in a Basic program and runs it. In slideshow mode it chooses a number of images from the image source you configured into XScreenSaver and displays them within the limitations of the Apple ][ display hardware. With only 6 available colors, you can only make out the general shape of the pictures. In text mode it displays the output of a command (by default your system's fortune program, but can be overridden with -program). In text mode, it is also a fully functional (if anachronistic) vt100 terminal emulator.","Process Name":"apple2","Link":"https:\/\/linux.die.net\/man\/1\/apple2"}},{"Process":{"Description":"apple_dump dump AppleSingle\/AppleDouble format file.","Process Name":"apple_dump","Link":"https:\/\/linux.die.net\/man\/1\/apple_dump"}},{"Process":{"Description":"The appletviewer command connects to the documents or resources designated by urls and displays each applet referenced by the documents in its own window. Note: if the documents referred to by urls do not reference any applets with the OBJECT, EMBED, or APPLET tag, then appletviewer does nothing. For details on the HTML tags that appletviewer supports, see AppletViewer Tags @ http:\/\/java.sun.com\/javase\/6\/docs\/technotes\/tools\/appletviewertags.html. Note: The appletviewer requires encoded URLs according to the escaping mechanism defined in RFC2396. Only encoded URLs are supported. However, file names must be unencoded, as specified in RFC2396.","Process Name":"appletviewer-java-1.6.0-openjdk","Link":"https:\/\/linux.die.net\/man\/1\/appletviewer-java-1.6.0-openjdk"}},{"Process":{"Description":"The appletviewer command connects to the documents or resources designated by urls and displays each applet referenced by the documents in its own window. Note: if the documents referred to by urls do not reference any applets with the OBJECT, EMBED, or APPLET tag, then appletviewer does nothing. For details on the HTML tags that appletviewer supports, see AppletViewer Tags @ http:\/\/docs.oracle.com\/javase\/7\/docs\/technotes\/tools\/appletviewertags.html. Note: The appletviewer is intended for development purposes only. See About Sample \/ Test Applications and Code for more information. Note: The appletviewer requires encoded URLs according to the escaping mechanism defined in RFC2396. Only encoded URLs are supported. However, file names must be unencoded, as specified in RFC2396.","Process Name":"appletviewer-java-1.7.0-openjdk","Link":"https:\/\/linux.die.net\/man\/1\/appletviewer-java-1.7.0-openjdk"}},{"Process":{"Description":"The appres program prints the resources seen by an application (or subhierarchy of an application) with the specified class and instance names. It can be used to determine which resources a particular program will load. For example, % appres  XTerm will list the resources that any xterm program will load. If no application class is specified, the class -AppResTest- is used. To match a particular instance name, specify an instance name explicitly after the class name, or use the normal Xt toolkit option. For example, % appres  XTerm  myxterm or % appres  XTerm  -name  myxterm To list resources that match a subhierarchy of an application, specify hierarchical class and instance names. The number of class and instance components must be equal, and the instance name should not be specified with a toolkit option. For example, % appres  Xman.TopLevelShell.Form  xman.topBox.form will list the resources of widgets of xman topBox hierarchy. To list just the resources matching a specific level in the hierarchy, use the -1 option. For example, % appres  XTerm.VT100  xterm.vt100  -1 will list the resources matching the xterm vt100 widget.","Process Name":"appres","Link":"https:\/\/linux.die.net\/man\/1\/appres"}},{"Process":{"Description":"","Process Name":"approve_friends","Link":"https:\/\/linux.die.net\/man\/1\/approve_friends"}},{"Process":{"Description":"apropos searches a set of database files containing short descriptions of system commands for keywords and displays the result on the standard output.","Process Name":"apropos","Link":"https:\/\/linux.die.net\/man\/1\/apropos"}},{"Process":{"Description":"The tool apt, annotation processing tool, includes a set of new reflective APIs and supporting infrastructure to process program annotations. The apt reflective APIs provide a build-time, source-based, read-only view of program structure. These reflective APIs are designed to cleanly model the Java(TM) programming language's type system after the addition of generics. First, apt runs annotation processors that can produce new source code and other files. Next, apt can cause compilation of both original and generated source files, easing development. The reflective APIs and other APIs used to interact with the tool are subpackages of com.sun.mirror. A fuller discussion of how the tool operates as well as instructions for developing with apt are in Getting Started with apt. @ http:\/\/java.sun.com\/javase\/6\/docs\/technotes\/guides\/apt\/GettingStarted.html Note: The functionality of apt has been subsumed by the annotation-processing infrastructure that is now part of the javac tool [ Solaris and Linux @ http:\/\/java.sun.com\/javase\/6\/docs\/technotes\/tools\/solaris\/javac.html] [Windows] and standardized for use by all Java compilers. This new infrastructure relies on the language model and annotation-processing APIs that are now part of the Java Platform. It is recommended that new annotation processor development be based on the new APIs and the javac tool.","Process Name":"apt-java-1.6.0-openjdk","Link":"https:\/\/linux.die.net\/man\/1\/apt-java-1.6.0-openjdk"}},{"Process":{"Description":"Note: The apt tool and its associated API contained in the package com.sun.mirror have been deprecated since JDK 7 and are planned to be removed in the next major JDK release. Use the options available in the javac(1) tool and the APIs contained in the packages javax.annotation.processing and javax.lang.model to process annotations. The tool apt, annotation processing tool, includes reflective APIs and supporting infrastructure to process program annotations. The apt reflective APIs provide a build-time, source-based, read-only view of program structure. These reflective APIs are designed to cleanly model the Java(TM) programming language's type system after the addition of generics. First, apt runs annotation processors that can produce new source code and other files. Next, apt can cause compilation of both original and generated source files, easing development. The reflective APIs and other APIs used to interact with the tool are subpackages of com.sun.mirror. A fuller discussion of how the tool operates as well as instructions for developing with apt are in Getting Started with apt. @ http:\/\/docs.oracle.com\/javase\/7\/docs\/technotes\/guides\/apt\/GettingStarted.html","Process Name":"apt-java-1.7.0-openjdk","Link":"https:\/\/linux.die.net\/man\/1\/apt-java-1.7.0-openjdk"}},{"Process":{"Description":"The GNU ar program creates, modifies, and extracts from archives. An archive is a single file holding a collection of other files in a structure that makes it possible to retrieve the original individual files (called members of the archive). The original files' contents, mode (permissions), timestamp, owner, and group are preserved in the archive, and can be restored on extraction. GNU ar can maintain archives whose members have names of any length; however, depending on how ar is configured on your system, a limit on member-name length may be imposed for compatibility with archive formats maintained with other tools. If it exists, the limit is often 15 characters (typical of formats related to a.out) or 16 characters (typical of formats related to coff). ar is considered a binary utility because archives of this sort are most often used as libraries holding commonly needed subroutines. ar creates an index to the symbols defined in relocatable object modules in the archive when you specify the modifier s. Once created, this index is updated in the archive whenever ar makes a change to its contents (save for the q update operation). An archive with such an index speeds up linking to the library, and allows routines in the library to call each other without regard to their placement in the archive. You may use nm -s or nm --print-armap to list this index table. If an archive lacks the table, another form of ar called ranlib can be used to add just the table. GNU ar can optionally create a thin archive, which contains a symbol index and references to the original copies of the member files of the archives. Such an archive is useful for building libraries for use within a local build, where the relocatable objects are expected to remain available, and copying the contents of each object would only waste time and space. Thin archives are also flattened, so that adding one or more archives to a thin archive will add the elements of the nested archive individually. The paths to the elements of the archive are stored relative to the archive itself. GNU ar is designed to be compatible with two different facilities. You can control its activity using command-line options, like the different varieties of ar on Unix systems; or, if you specify the single command-line option -M, you can control it with a script supplied via standard input, like the MRI \"librarian\" program.","Process Name":"ar","Link":"https:\/\/linux.die.net\/man\/1\/ar"}},{"Process":{"Description":"rancid is a perl(1) script which uses the login scripts (see clogin(1)) to login to a device, execute commands to display the configuration, etc, then filters the output for formatting, security, and so on. rancid's product is a file with the name of it's last argument plus the suffix .new. For example, hostname.new. There are complementary scripts for other platforms and\/or manufacturers that are supported by rancid(1). Briefly, these are: agmrancid Cisco Anomaly Guard Module (AGM) arancid Alteon WebOS switches arrancid Arista Networks devices brancid Bay Networks (nortel) cat5rancid Cisco catalyst switches cssrancid Cisco content services switches erancid ADC-kentrox EZ-T3 mux f10rancid Force10 f5rancid F5 BigIPs fnrancid Fortinet Firewalls francid Foundry and HP procurve OEMs of Foundry hrancid HP Procurve Switches htranicd Hitachi Routers jerancid Juniper Networks E-series jrancid Juniper Networks mrancid MRTd mrvrancid MRV optical switches nrancid Netscreen firewalls nsrancid Netscaler nxrancid Cisco Nexus boxes prancid Procket Networks rivrancid Riverstone rrancid Redback srancid SMC switch (some Dell OEMs) trancid Netopia sDSL\/T1 routers tntrancid Lucent TNT xrancid Extreme switches xrrancid Cisco IOS-XR boxes zrancid Zebra routing software The command-line options are as follows: -V Prints package name and version strings. -d Display debugging information. -l Display somewhat less debugging information. -f rancid should interpret the next argument as a filename which contains the output it would normally collect from the device ( hostname) with clogin(1).","Process Name":"arancid","Link":"https:\/\/linux.die.net\/man\/1\/arancid"}},{"Process":{"Description":"Arc is a general archive and file compression utility, used to maintain a compressed archive of files. An archive is a single file that combines many files, reducing storage space and allowing multiple files to be handled as one. Arc uses one of several compression methods for each file within the archive, based on whichever method yields the smallest result.","Process Name":"arc","Link":"https:\/\/linux.die.net\/man\/1\/arc"}},{"Process":{"Description":"The arcacl command retrieves\/sets permisions (ACL) of data or computing object.","Process Name":"arcacl","Link":"https:\/\/linux.die.net\/man\/1\/arcacl"}},{"Process":{"Description":"The arccat command can be used to view the stdout or stderr of a running job. It can also be used to show the grid manager's error log of a job.","Process Name":"arccat","Link":"https:\/\/linux.die.net\/man\/1\/arccat"}},{"Process":{"Description":"The arcclean command removes a job from the remote cluster.","Process Name":"arcclean","Link":"https:\/\/linux.die.net\/man\/1\/arcclean"}},{"Process":{"Description":"The arccp command copies files to, from and between grid storage elements.","Process Name":"arccp","Link":"https:\/\/linux.die.net\/man\/1\/arccp"}},{"Process":{"Description":"The arcecho command is a client for the ARC echo service.","Process Name":"arcecho","Link":"https:\/\/linux.die.net\/man\/1\/arcecho"}},{"Process":{"Description":"The arcget command is used for retrieving the results from a job.","Process Name":"arcget","Link":"https:\/\/linux.die.net\/man\/1\/arcget"}},{"Process":{"Description":"Print machine architecture. --help display this help and exit --version output version information and exit","Process Name":"arch","Link":"https:\/\/linux.die.net\/man\/1\/arch"}},{"Process":{"Description":"archivemail is a tool for archiving and compressing old email in mailboxes. By default it will read the mailbox MAILBOX, moving messages that are older that the specified number of days (180 by default) to a mbox(5)-format mailbox in the same directory that is compressed with gzip(1). It can also just delete old email rather than archive it. archivemail supports reading IMAP, Maildir, MH and mbox-format mailboxes, but always writes mbox-format archives. Messages that are flagged important are not archived or deleted unless explicitely requested with the --include-flagged option. Also, archivemail can be configured not to archive unread mail, or to only archive messages larger than a specified size. To archive an IMAP-format mailbox, use the format imap:\/\/username:password@server\/mailbox to specify the mailbox. You can omit the password from the URL; use the --pwfile option to make archivemail read the password from a file, or alternatively just enter it upon request. If the --pwfile option is set, archivemail does not look for a password in the URL, and the colon is not considered a delimiter. Substitute 'imap' with 'imaps', and archivemail will establish a secure SSL connection. See below for more IMAP peculiarities. archivemail has some support for being run as the root user on user mailboxes. When running as root, it will seteuid(2) to the owner of the mailbox it is reading, creating any archive files as that user. Warning: this automatic seteuid feature is insecure and deprecated. It will be removed from later versions of archivemail.","Process Name":"archivemail","Link":"https:\/\/linux.die.net\/man\/1\/archivemail"}},{"Process":{"Description":"The archivemount command mounts the file tree contained in the archive archivepath on the directory mountpoint. The archive's contents can subsequently be accessed inside mountpoint as a file system. The umount(8) command performs the corresponding unmount operation. archivemount is known to work under both linux and Mac OS X (via MacFUSE).","Process Name":"archivemount","Link":"https:\/\/linux.die.net\/man\/1\/archivemount"}},{"Process":{"Description":"The arcinfo command is used for obtaining status and information of clusters on the grid.","Process Name":"arcinfo","Link":"https:\/\/linux.die.net\/man\/1\/arcinfo"}},{"Process":{"Description":"The arckill command is used to kill running jobs.","Process Name":"arckill","Link":"https:\/\/linux.die.net\/man\/1\/arckill"}},{"Process":{"Description":"The arcls command is used for listing files in grid storage elements and file index catalogues.","Process Name":"arcls","Link":"https:\/\/linux.die.net\/man\/1\/arcls"}},{"Process":{"Description":"The arcmigrate command is used for migrating queued jobs from one cluster to another cluster. Currently migration only works between clusters running A-REX.","Process Name":"arcmigrate","Link":"https:\/\/linux.die.net\/man\/1\/arcmigrate"}},{"Process":{"Description":"The arcmkdir command creates directories on grid storage elements and indexing services.","Process Name":"arcmkdir","Link":"https:\/\/linux.die.net\/man\/1\/arcmkdir"}},{"Process":{"Description":"The arcplugin command prints description of ARC plugin or creates Arc Plugin Descriptor (APD) file.","Process Name":"arcplugin","Link":"https:\/\/linux.die.net\/man\/1\/arcplugin"}},{"Process":{"Description":"arcproxy generates proxy credentials (general proxy certificate, or proxy certificate with voms AC extenstion) from private key and certificate of user.","Process Name":"arcproxy","Link":"https:\/\/linux.die.net\/man\/1\/arcproxy"}},{"Process":{"Description":"The arcrenew command is used for renewing the proxy of jobs that have been submitted to grid enabled resources.","Process Name":"arcrenew","Link":"https:\/\/linux.die.net\/man\/1\/arcrenew"}},{"Process":{"Description":"The arcresub command is used for resubmitting jobs to grid enabled computing resources.","Process Name":"arcresub","Link":"https:\/\/linux.die.net\/man\/1\/arcresub"}},{"Process":{"Description":"The arcresume command is used for resuming a job that have been submitted to grid enabled resources. The job will be resumed at the last ok state reported by the cluster.","Process Name":"arcresume","Link":"https:\/\/linux.die.net\/man\/1\/arcresume"}},{"Process":{"Description":"The arcrm command deletes files on grid storage elements and indexing services.","Process Name":"arcrm","Link":"https:\/\/linux.die.net\/man\/1\/arcrm"}},{"Process":{"Description":"arcslcs generates short-lived credential based on the credential to IdP in SAML2SSO profile (normally the username\/password to Shibboleth IdP).","Process Name":"arcslcs","Link":"https:\/\/linux.die.net\/man\/1\/arcslcs"}},{"Process":{"Description":"The arcsrmping command is a ping client for the SRM service.","Process Name":"arcsrmping","Link":"https:\/\/linux.die.net\/man\/1\/arcsrmping"}},{"Process":{"Description":"The arcstat command is used for obtaining the status of jobs that have been submitted to grid enabled resources and the status of the clusters in the grid.","Process Name":"arcstat","Link":"https:\/\/linux.die.net\/man\/1\/arcstat"}},{"Process":{"Description":"The arcsub command is used for submitting jobs to grid enabled computing resources.","Process Name":"arcsub","Link":"https:\/\/linux.die.net\/man\/1\/arcsub"}},{"Process":{"Description":"The arcsync command synchronizes your local jobs list with the information at a given cluster or index server.","Process Name":"arcsync","Link":"https:\/\/linux.die.net\/man\/1\/arcsync"}},{"Process":{"Description":"The arctest command tests basic client and server setup.","Process Name":"arctest","Link":"https:\/\/linux.die.net\/man\/1\/arctest"}},{"Process":{"Description":"The arcwsrf command issues WSRF resource property query to service specified by its URL.","Process Name":"arcwsrf","Link":"https:\/\/linux.die.net\/man\/1\/arcwsrf"}},{"Process":{"Description":"arecord is a command-line soundfile recorder for the ALSA soundcard driver. It supports several file formats and multiple soundcards with multiple devices. If recording with interleaved mode samples the file is automatically split before the 2GB filesize. aplay is much the same, only it plays instead of recording. For supported soundfile formats, the sampling rate, bit depth, and so forth can be automatically determined from the soundfile header. If filename is not specified, the standard output or input is used. The aplay utility accepts multiple filenames.","Process Name":"arecord","Link":"https:\/\/linux.die.net\/man\/1\/arecord"}},{"Process":{"Description":"arecordmidi is a command-line utility that records a Standard MIDI File from one or more ALSA sequencer ports. To stop recording, press Ctrl+C.","Process Name":"arecordmidi","Link":"https:\/\/linux.die.net\/man\/1\/arecordmidi"}},{"Process":{"Description":"This manual page document describes the atool commands. These commands are used for managing file archives of various types, such as tar and Zip archives. Each command can be executed individually or by giving the appropriate options to atool (see OPTIONS below). aunpack extracts files from an archive. Often one wants to extract all files in an archive to a single subdirectory. However, some archives contain multiple files in their root directories. The aunpack program overcomes this problem by first extracting files to a unique (temporary) directory, and then moving its contents back if possible. This also prevents local files from being overwritten by mistake. apack creates archives (or compresses files). If no file arguments are specified, filenames to add are read from standard in. als lists files in an archive. acat extracts files in an archive to standard out. adiff generates a diff between two archives using diff(1). arepack repacks archives to a different format. It does this by first extracting all files of the old archive into a temporary directory, then packing all files extracted to that directory to the new archive. Use the --each (-e) option in combination with --format (-F) to repack multiple archives using a single invocation of atool. Note that arepack will not remove the old archive. Unless the --format (-F) option is provided, the archive format is determined by the archive file extension. I.e. an extension \".tar.gz\" or \".tgz\" means tar+gzip format. Note that the extensions are checked in the order listed in the section ARCHIVE TYPES below, which is why a file with extension \".tar.gz\" is considered to a be tar+gzip archive, not a gzip compressed file.","Process Name":"arepack","Link":"https:\/\/linux.die.net\/man\/1\/arepack"}},{"Process":{"Description":"aria2 is a utility for downloading files. The supported protocols are HTTP(S), FTP, BitTorrent, and Metalink. aria2 can download a file from multiple sources\/protocols and tries to utilize your maximum download bandwidth. It supports downloading a file from HTTP(S)\/FTP and BitTorrent at the same time, while the data downloaded from HTTP(S)\/FTP is uploaded to the BitTorrent swarm. Using Metalink's chunk checksums, aria2 automatically validates chunks of data while downloading a file like BitTorrent.","Process Name":"aria2c","Link":"https:\/\/linux.die.net\/man\/1\/aria2c"}},{"Process":{"Description":"","Process Name":"arj","Link":"https:\/\/linux.die.net\/man\/1\/arj"}},{"Process":{"Description":"The arjdisp command is a graphical interface for arj.","Process Name":"arjdisp","Link":"https:\/\/linux.die.net\/man\/1\/arjdisp"}},{"Process":{"Description":"Ark is a program for managing various archive formats within the KDE environment. Archives can be viewed, extracted, created and modified from within Ark. The program can handle various formats such as tar, gzip, bzip2, zip, rar and lha (if appropriate command-line programs are installed). Ark can work closely with Konqueror in the KDE environment to handle archives, if you install the Konqueror Integration plugin available in the kdeaddons package.","Process Name":"ark","Link":"https:\/\/linux.die.net\/man\/1\/ark"}},{"Process":{"Description":"addr2line translates addresses into file names and line numbers. Given an address in an executable or an offset in a section of a relocatable object, it uses the debugging information to figure out which file name and line number are associated with it. The executable or relocatable object to use is specified with the -e option. The default is the file a.out. The section in the relocatable object to use is specified with the -j option. addr2line has two modes of operation. In the first, hexadecimal addresses are specified on the command line, and addr2line displays the file name and line number for each address. In the second, addr2line reads hexadecimal addresses from standard input, and prints the file name and line number for each address on standard output. In this mode, addr2line may be used in a pipe to convert dynamically chosen addresses. The format of the output is FILENAME:LINENO . The file name and line number for each input address is printed on separate lines. If the -f option is used, then each FILENAME:LINENO line is preceded by FUNCTIONNAME which is the name of the function containing the address. If the -i option is used and the code at the given address is present there because of inlining by the compiler then the { FUNCTIONNAME } FILENAME:LINENO information for the inlining function will be displayed afterwards. This continues recursively until there is no more inlining to report. If the -a option is used then the output is prefixed by the input address. If the -p option is used then the output for each input address is displayed on one, possibly quite long, line. If -p is not used then the output is broken up into multiple lines, based on the paragraphs above. If the file name or function name can not be determined, addr2line will print two question marks in their place. If the line number can not be determined, addr2line will print 0.","Process Name":"arm-linux-gnu-addr2line","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-addr2line"}},{"Process":{"Description":"The GNU ar program creates, modifies, and extracts from archives. An archive is a single file holding a collection of other files in a structure that makes it possible to retrieve the original individual files (called members of the archive). The original files' contents, mode (permissions), timestamp, owner, and group are preserved in the archive, and can be restored on extraction. GNU ar can maintain archives whose members have names of any length; however, depending on how ar is configured on your system, a limit on member-name length may be imposed for compatibility with archive formats maintained with other tools. If it exists, the limit is often 15 characters (typical of formats related to a.out) or 16 characters (typical of formats related to coff). ar is considered a binary utility because archives of this sort are most often used as libraries holding commonly needed subroutines. ar creates an index to the symbols defined in relocatable object modules in the archive when you specify the modifier s. Once created, this index is updated in the archive whenever ar makes a change to its contents (save for the q update operation). An archive with such an index speeds up linking to the library, and allows routines in the library to call each other without regard to their placement in the archive. You may use nm -s or nm --print-armap to list this index table. If an archive lacks the table, another form of ar called ranlib can be used to add just the table. GNU ar can optionally create a thin archive, which contains a symbol index and references to the original copies of the member files of the archives. Such an archive is useful for building libraries for use within a local build, where the relocatable objects are expected to remain available, and copying the contents of each object would only waste time and space. Thin archives are also flattened, so that adding one or more archives to a thin archive will add the elements of the nested archive individually. The paths to the elements of the archive are stored relative to the archive itself. GNU ar is designed to be compatible with two different facilities. You can control its activity using command-line options, like the different varieties of ar on Unix systems; or, if you specify the single command-line option -M, you can control it with a script supplied via standard input, like the MRI \"librarian\" program.","Process Name":"arm-linux-gnu-ar","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-ar"}},{"Process":{"Description":"GNU as is really a family of assemblers. If you use (or have used) the GNU assembler on one architecture, you should find a fairly similar environment when you use it on another architecture. Each version has much in common with the others, including object file formats, most assembler directives (often called pseudo-ops) and assembler syntax. as is primarily intended to assemble the output of the GNU C compiler \"gcc\" for use by the linker \"ld\". Nevertheless, we've tried to make as assemble correctly everything that other assemblers for the same machine would assemble. Any exceptions are documented explicitly. This doesn't mean as always uses the same syntax as another assembler for the same architecture; for example, we know of several incompatible versions of 680x0 assembly language syntax. Each time you run as it assembles exactly one source program. The source program is made up of one or more files. (The standard input is also a file.) You give as a command line that has zero or more input file names. The input files are read (from left file name to right). A command line argument (in any position) that has no special meaning is taken to be an input file name. If you give as no file names it attempts to read one input file from the as standard input, which is normally your terminal. You may have to type ctl-D to tell as there is no more program to assemble. Use -- if you need to explicitly name the standard input file in your command line. If the source is empty, as produces a small, empty object file. as may write warnings and error messages to the standard error file (usually your terminal). This should not happen when a compiler runs as automatically. Warnings report an assumption made so that as could keep assembling a flawed program; errors report a grave problem that stops the assembly. If you are invoking as via the GNU C compiler, you can use the -Wa option to pass arguments through to the assembler. The assembler arguments must be separated from each other (and the -Wa) by commas. For example: gcc -c -g -O -Wa,-alh,-L file.c This passes two options to the assembler: -alh (emit a listing to standard output with high-level and assembly source) and -L (retain local symbols in the symbol table). Usually you do not need to use this -Wa mechanism, since many compiler command-line options are automatically passed to the assembler by the compiler. (You can call the GNU compiler driver with the -v option to see precisely what options it passes to each compilation pass, including the assembler.)","Process Name":"arm-linux-gnu-as","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-as"}},{"Process":{"Description":"The C ++ and Java languages provide function overloading, which means that you can write many functions with the same name, providing that each function takes parameters of different types. In order to be able to distinguish these similarly named functions C ++ and Java encode them into a low-level assembler name which uniquely identifies each different version. This process is known as mangling. The c++filt [1] program does the inverse mapping: it decodes (demangles) low-level names into user-level names so that they can be read. Every alphanumeric word (consisting of letters, digits, underscores, dollars, or periods) seen in the input is a potential mangled name. If the name decodes into a C ++ name, the C ++ name replaces the low-level name in the output, otherwise the original word is output. In this way you can pass an entire assembler source file, containing mangled names, through c++filt and see the same source file containing demangled names. You can also use c++filt to decipher individual symbols by passing them on the command line: c++filt <symbol> If no symbol arguments are given, c++filt reads symbol names from the standard input instead. All the results are printed on the standard output. The difference between reading names from the command line versus reading names from the standard input is that command line arguments are expected to be just mangled names and no checking is performed to separate them from surrounding text. Thus for example: c++filt -n _Z1fv will work and demangle the name to \"f()\" whereas: c++filt -n _Z1fv, will not work. (Note the extra comma at the end of the mangled name which makes it invalid). This command however will work: echo _Z1fv, | c++filt -n and will display \"f(),\", i.e., the demangled name followed by a trailing comma. This behaviour is because when the names are read from the standard input it is expected that they might be part of an assembler source file where there might be extra, extraneous characters trailing after a mangled name. For example: .type   _Z1fv, @function","Process Name":"arm-linux-gnu-c++filt","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-c++filt"}},{"Process":{"Description":"The C preprocessor, often known as cpp, is a macro processor that is used automatically by the C compiler to transform your program before compilation. It is called a macro processor because it allows you to define macros, which are brief abbreviations for longer constructs. The C preprocessor is intended to be used only with C, C ++ , and Objective-C source code. In the past, it has been abused as a general text processor. It will choke on input which does not obey C's lexical rules. For example, apostrophes will be interpreted as the beginning of character constants, and cause errors. Also, you cannot rely on it preserving characteristics of the input which are not significant to C-family languages. If a Makefile is preprocessed, all the hard tabs will be removed, and the Makefile will not work. Having said that, you can often get away with using cpp on things which are not C. Other Algol-ish programming languages are often safe (Pascal, Ada, etc.) So is assembly, with caution. -traditional-cpp mode preserves more white space, and is otherwise more permissive. Many of the problems can be avoided by writing C or C ++ style comments instead of native language comments, and keeping macros simple. Wherever possible, you should use a preprocessor geared to the language you are writing in. Modern versions of the GNU assembler have macro facilities. Most high level programming languages have their own conditional compilation and inclusion mechanism. If all else fails, try a true general text processor, such as GNU M4. C preprocessors vary in some details. This manual discusses the GNU C preprocessor, which provides a small superset of the features of ISO Standard C. In its default mode, the GNU C preprocessor does not do a few things required by the standard. These are features which are rarely, if ever, used, and may cause surprising changes to the meaning of a program which does not expect them. To get strict ISO Standard C, you should use the -std=c90, -std=c99 or -std=c11 options, depending on which version of the standard you want. To get all the mandatory diagnostics, you must also use -pedantic. This manual describes the behavior of the ISO preprocessor. To minimize gratuitous differences, where the ISO preprocessor's behavior does not conflict with traditional semantics, the traditional preprocessor should behave the same way. The various differences that do exist are detailed in the section Traditional Mode. For clarity, unless noted otherwise, references to CPP in this manual refer to GNU CPP .","Process Name":"arm-linux-gnu-cpp","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-cpp"}},{"Process":{"Description":"dlltool reads its inputs, which can come from the -d and -b options as well as object files specified on the command line. It then processes these inputs and if the -e option has been specified it creates a exports file. If the -l option has been specified it creates a library file and if the -z option has been specified it creates a def file. Any or all of the -e, -l and -z options can be present in one invocation of dlltool. When creating a DLL , along with the source for the DLL , it is necessary to have three other files. dlltool can help with the creation of these files. The first file is a .def file which specifies which functions are exported from the DLL , which functions the DLL imports, and so on. This is a text file and can be created by hand, or dlltool can be used to create it using the -z option. In this case dlltool will scan the object files specified on its command line looking for those functions which have been specially marked as being exported and put entries for them in the .def file it creates. In order to mark a function as being exported from a DLL , it needs to have an -export:<name_of_function> entry in the .drectve section of the object file. This can be done in C by using the asm() operator: asm (\".section .drectve\");\nasm (\".ascii \\\"-export:my_func\\\"\");\n\nint my_func (void) { ... } The second file needed for DLL creation is an exports file. This file is linked with the object files that make up the body of the DLL and it handles the interface between the DLL and the outside world. This is a binary file and it can be created by giving the -e option to dlltool when it is creating or reading in a .def file. The third file needed for DLL creation is the library file that programs will link with in order to access the functions in the DLL (an 'import library'). This file can be created by giving the -l option to dlltool when it is creating or reading in a .def file. If the -y option is specified, dlltool generates a delay-import library that can be used instead of the normal import library to allow a program to link to the dll only as soon as an imported function is called for the first time. The resulting executable will need to be linked to the static delayimp library containing __delayLoadHelper2(), which in turn will import LoadLibraryA and GetProcAddress from kernel32. dlltool builds the library file by hand, but it builds the exports file by creating temporary files containing assembler statements and then assembling these. The -S command line option can be used to specify the path to the assembler that dlltool will use, and the -f option can be used to pass specific flags to that assembler. The -n can be used to prevent dlltool from deleting these temporary assembler files when it is done, and if -n is specified twice then this will prevent dlltool from deleting the temporary object files it used to build the library. Here is an example of creating a DLL from a source file dll.c and also creating a program (from an object file called program.o) that uses that DLL: gcc -c dll.c\ndlltool -e exports.o -l dll.lib dll.o\ngcc dll.o exports.o -o dll.dll\ngcc program.o dll.lib -o program dlltool may also be used to query an existing import library to determine the name of the DLL to which it is associated. See the description of the -I or --identify option.","Process Name":"arm-linux-gnu-dlltool","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-dlltool"}},{"Process":{"Description":"elfedit updates the ELF header of ELF files which have the matching ELF machine and file types. The options control how and which fields in the ELF header should be updated. elffile... are the ELF files to be updated. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files.","Process Name":"arm-linux-gnu-elfedit","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-elfedit"}},{"Process":{"Description":"When you invoke GCC , it normally does preprocessing, compilation, assembly and linking. The \"overall options\" allow you to stop this process at an intermediate stage. For example, the -c option says not to run the linker. Then the output consists of object files output by the assembler. Other options are passed on to one stage of processing. Some options control the preprocessor and others the compiler itself. Yet other options control the assembler and linker; most of these are not documented here, since you rarely need to use any of them. Most of the command-line options that you can use with GCC are useful for C programs; when an option is only useful with another language (usually C ++ ), the explanation says so explicitly. If the description for a particular option does not mention a source language, you can use that option with all supported languages. The gcc program accepts options and file names as operands. Many options have multi-letter names; therefore multiple single-letter options may not be grouped: -dv is very different from -d -v. You can mix options and other arguments. For the most part, the order you use doesn't matter. Order does matter when you use several options of the same kind; for example, if you specify -L more than once, the directories are searched in the order specified. Also, the placement of the -l option is significant. Many options have long names starting with -f or with -W---for example, -fmove-loop-invariants, -Wformat and so on. Most of these have both positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"arm-linux-gnu-gcc","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-gcc"}},{"Process":{"Description":"gcov is a test coverage program. Use it in concert with GCC to analyze your programs to help create more efficient, faster running code and to discover untested parts of your program. You can use gcov as a profiling tool to help discover where your optimization efforts will best affect your code. You can also use gcov along with the other profiling tool, gprof, to assess which parts of your code use the greatest amount of computing time. Profiling tools help you analyze your code's performance. Using a profiler such as gcov or gprof, you can find out some basic performance statistics, such as: \u2022 how often each line of code executes \u2022 what lines of code are actually executed \u2022 how much computing time each section of code uses Once you know these things about how your code works when compiled, you can look at each module to see which modules should be optimized. gcov helps you determine where to work on optimization. Software developers also use coverage testing in concert with testsuites, to make sure software is actually good enough for a release. Testsuites can verify that a program works as expected; a coverage program tests to see how much of the program is exercised by the testsuite. Developers can then determine what kinds of test cases need to be added to the testsuites to create both better testing and a better final product. You should compile your code without optimization if you plan to use gcov because the optimization, by combining some lines of code into one function, may not give you as much information as you need to look for 'hot spots' where the code is using a great deal of computer time. Likewise, because gcov accumulates statistics by line (at the lowest resolution), it works best with a programming style that places only one statement on each line. If you use complicated macros that expand to loops or to other control structures, the statistics are less helpful---they only report on the line where the macro call appears. If your complex macros behave like functions, you can replace them with inline functions to solve this problem. gcov creates a logfile called sourcefile.gcov which indicates how many times each line of a source file sourcefile.c has executed. You can use these logfiles along with gprof to aid in fine-tuning the performance of your programs. gprof gives timing information you can use along with the information you get from gcov. gcov works only on code compiled with GCC . It is not compatible with any other profiling or test coverage mechanism.","Process Name":"arm-linux-gnu-gcov","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-gcov"}},{"Process":{"Description":"\"gprof\" produces an execution profile of C, Pascal, or Fortran77 programs. The effect of called routines is incorporated in the profile of each caller. The profile data is taken from the call graph profile file (gmon.out default) which is created by programs that are compiled with the -pg option of \"cc\", \"pc\", and \"f77\". The -pg option also links in versions of the library routines that are compiled for profiling. \"Gprof\" reads the given object file (the default is \"a.out\") and establishes the relation between its symbol table and the call graph profile from gmon.out. If more than one profile file is specified, the \"gprof\" output shows the sum of the profile information in the given profile files. \"Gprof\" calculates the amount of time spent in each routine. Next, these times are propagated along the edges of the call graph. Cycles are discovered, and calls into a cycle are made to share the time of the cycle. Several forms of output are available from the analysis. The flat profile shows how much time your program spent in each function, and how many times that function was called. If you simply want to know which functions burn most of the cycles, it is stated concisely here. The call graph shows, for each function, which functions called it, which other functions it called, and how many times. There is also an estimate of how much time was spent in the subroutines of each function. This can suggest places where you might try to eliminate function calls that use a lot of time. The annotated source listing is a copy of the program's source code, labeled with the number of times each line of the program was executed.","Process Name":"arm-linux-gnu-gprof","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-gprof"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"arm-linux-gnu-ld","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-ld"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"arm-linux-gnu-ld.bfd","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-ld.bfd"}},{"Process":{"Description":"nlmconv converts the relocatable i386 object file infile into the NetWare Loadable Module outfile, optionally reading headerfile for NLM header information. For instructions on writing the NLM command file language used in header files, see the linkers section, NLMLINK in particular, of the NLM Development and Tools Overview, which is part of the NLM Software Developer's Kit (\" NLM SDK \"), available from Novell, Inc. nlmconv uses the GNU Binary File Descriptor library to read infile; nlmconv can perform a link step. In other words, you can list more than one object file for input if you list them in the definitions file (rather than simply specifying one input file on the command line). In this case, nlmconv calls the linker for you.","Process Name":"arm-linux-gnu-nlmconv","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-nlmconv"}},{"Process":{"Description":"GNU nm lists the symbols from object files objfile.... If no object files are listed as arguments, nm assumes the file a.out. For each symbol, nm shows: \u2022 The symbol value, in the radix selected by options (see below), or hexadecimal by default. \u2022 The symbol type. At least the following types are used; others are, as well, depending on the object file format. If lowercase, the symbol is usually local; if uppercase, the symbol is global (external). There are however a few lowercase symbols that are shown for special global symbols (\"u\", \"v\" and \"w\"). \"A\" The symbol's value is absolute, and will not be changed by further linking. \"B\" \"b\" The symbol is in the uninitialized data section (known as BSS ). \"C\" The symbol is common. Common symbols are uninitialized data. When linking, multiple common symbols may appear with the same name. If the symbol is defined anywhere, the common symbols are treated as undefined references. \"D\" \"d\" The symbol is in the initialized data section. \"G\" \"g\" The symbol is in an initialized data section for small objects. Some object file formats permit more efficient access to small data objects, such as a global int variable as opposed to a large global array. \"i\" For PE format files this indicates that the symbol is in a section specific to the implementation of DLLs. For ELF format files this indicates that the symbol is an indirect function. This is a GNU extension to the standard set of ELF symbol types. It indicates a symbol which if referenced by a relocation does not evaluate to its address, but instead must be invoked at runtime. The runtime execution will then return the value to be used in the relocation. \"N\" The symbol is a debugging symbol. \"p\" The symbols is in a stack unwind section. \"R\" \"r\" The symbol is in a read only data section. \"S\" \"s\" The symbol is in an uninitialized data section for small objects. \"T\" \"t\" The symbol is in the text (code) section. \"U\" The symbol is undefined. \"u\" The symbol is a unique global symbol. This is a GNU extension to the standard set of ELF symbol bindings. For such a symbol the dynamic linker will make sure that in the entire process there is just one symbol with this name and type in use. \"V\" \"v\" The symbol is a weak object. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the weak symbol becomes zero with no error. On some systems, uppercase indicates that a default value has been specified. \"W\" \"w\" The symbol is a weak symbol that has not been specifically tagged as a weak object symbol. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the symbol is determined in a system-specific manner without error. On some systems, uppercase indicates that a default value has been specified. \"-\" The symbol is a stabs symbol in an a.out object file. In this case, the next values printed are the stabs other field, the stabs desc field, and the stab type. Stabs symbols are used to hold debugging information. \"?\" The symbol type is unknown, or object file format specific. \u2022 The symbol name.","Process Name":"arm-linux-gnu-nm","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-nm"}},{"Process":{"Description":"The GNU objcopy utility copies the contents of an object file to another. objcopy uses the GNU BFD Library to read and write the object files. It can write the destination object file in a format different from that of the source object file. The exact behavior of objcopy is controlled by command-line options. Note that objcopy should be able to copy a fully linked file between any two formats. However, copying a relocatable object file between any two formats may not work as expected. objcopy creates temporary files to do its translations and deletes them afterward. objcopy uses BFD to do all its translation work; it has access to all the formats described in BFD and thus is able to recognize most formats without being told explicitly. objcopy can be used to generate S-records by using an output target of srec (e.g., use -O srec). objcopy can be used to generate a raw binary file by using an output target of binary (e.g., use -O binary). When objcopy generates a raw binary file, it will essentially produce a memory dump of the contents of the input object file. All symbols and relocation information will be discarded. The memory dump will start at the load address of the lowest section copied into the output file. When generating an S-record or a raw binary file, it may be helpful to use -S to remove sections containing debugging information. In some cases -R will be useful to remove sections which contain information that is not needed by the binary file. Note---objcopy is not able to change the endianness of its input files. If the input format has an endianness (some formats do not), objcopy can only copy the inputs into file formats that have the same endianness or which have no endianness (e.g., srec). (However, see the --reverse-bytes option.)","Process Name":"arm-linux-gnu-objcopy","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-objcopy"}},{"Process":{"Description":"objdump displays information about one or more object files. The options control what particular information to display. This information is mostly useful to programmers who are working on the compilation tools, as opposed to programmers who just want their program to compile and work. objfile... are the object files to be examined. When you specify archives, objdump shows information on each of the member object files.","Process Name":"arm-linux-gnu-objdump","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-objdump"}},{"Process":{"Description":"ranlib generates an index to the contents of an archive and stores it in the archive. The index lists each symbol defined by a member of an archive that is a relocatable object file. You may use nm -s or nm --print-armap to list this index. An archive with such an index speeds up linking to the library and allows routines in the library to call each other without regard to their placement in the archive. The GNU ranlib program is another form of GNU ar; running ranlib is completely equivalent to executing ar -s.","Process Name":"arm-linux-gnu-ranlib","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-ranlib"}},{"Process":{"Description":"readelf displays information about one or more ELF format object files. The options control what particular information to display. elffile... are the object files to be examined. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files. This program performs a similar function to objdump but it goes into more detail and it exists independently of the BFD library, so if there is a bug in BFD then readelf will not be affected.","Process Name":"arm-linux-gnu-readelf","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-readelf"}},{"Process":{"Description":"The GNU size utility lists the section sizes---and the total size---for each of the object or archive files objfile in its argument list. By default, one line of output is generated for each object file or each module in an archive. objfile... are the object files to be examined. If none are specified, the file \"a.out\" will be used.","Process Name":"arm-linux-gnu-size","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-size"}},{"Process":{"Description":"For each file given, GNU strings prints the printable character sequences that are at least 4 characters long (or the number given with the options below) and are followed by an unprintable character. By default, it only prints the strings from the initialized and loaded sections of object files; for other types of files, it prints the strings from the whole file. strings is mainly useful for determining the contents of non-text files.","Process Name":"arm-linux-gnu-strings","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-strings"}},{"Process":{"Description":"GNU strip discards all symbols from object files objfile. The list of object files may include archives. At least one object file must be given. strip modifies the files named in its argument, rather than writing modified copies under different names.","Process Name":"arm-linux-gnu-strip","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-strip"}},{"Process":{"Description":"windmc reads message definitions from an input file (.mc) and translate them into a set of output files. The output files may be of four kinds: \"h\" A C header file containing the message definitions. \"rc\" A resource file compilable by the windres tool. \"bin\" One or more binary files containing the resource data for a specific message language. \"dbg\" A C include file that maps message id's to their symbolic name. The exact description of these different formats is available in documentation from Microsoft. When windmc converts from the \"mc\" format to the \"bin\" format, \"rc\", \"h\", and optional \"dbg\" it is acting like the Windows Message Compiler.","Process Name":"arm-linux-gnu-windmc","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-windmc"}},{"Process":{"Description":"windres reads resources from an input file and copies them into an output file. Either file may be in one of three formats: \"rc\" A text format read by the Resource Compiler. \"res\" A binary format generated by the Resource Compiler. \"coff\" A COFF object or executable. The exact description of these different formats is available in documentation from Microsoft. When windres converts from the \"rc\" format to the \"res\" format, it is acting like the Windows Resource Compiler. When windres converts from the \"res\" format to the \"coff\" format, it is acting like the Windows \"CVTRES\" program. When windres generates an \"rc\" file, the output is similar but not identical to the format expected for the input. When an input \"rc\" file refers to an external filename, an output \"rc\" file will instead include the file contents. If the input or output format is not specified, windres will guess based on the file name, or, for the input file, the file contents. A file with an extension of .rc will be treated as an \"rc\" file, a file with an extension of .res will be treated as a \"res\" file, and a file with an extension of .o or .exe will be treated as a \"coff\" file. If no output file is specified, windres will print the resources in \"rc\" format to standard output. The normal use is for you to write an \"rc\" file, use windres to convert it to a COFF object file, and then link the COFF file into your application. This will make the resources described in the \"rc\" file available to Windows.","Process Name":"arm-linux-gnu-windres","Link":"https:\/\/linux.die.net\/man\/1\/arm-linux-gnu-windres"}},{"Process":{"Description":"arp-fingerprint fingerprints the specified target host using the ARP protocol. It sends various different types of ARP request to the target, and records which types it responds to. From this, it constructs a fingerprint string consisting of \"1\" where the target responded and \"0\" where it did not. An example of a fingerprint string is 01000100000. This fingerprint string is then used to lookup the likely target operating system. Many of the fingerprint strings are shared by several operating systems, so there is not always a one-to-one mapping between fingerprint strings and operating systems. Also the fact that a system's fingerprint matches a certain operating system (or list of operating systems) does not necessarily mean that the system being fingerprinted is that operating system, although it is quite likely. This is because the list of operating systems is not exhaustive; it is just what I have discovered to date, and there are bound to be operating systems that are not listed. The ARP fingerprint of a system is generally a function of that system's kernel (although it is possible for the ARP function to be implemented in user space, it almost never is). Sometimes, an operating system can give different fingerprints depending on the configuration. An example is Linux, which will respond to a non-local source IP address if that IP is routed through the interface being tested. This is both good and bad: on one hand it makes the fingerprinting task more complex; but on the other, it can allow some aspects of the system configuration to be determined. Sometimes the fact that two different operating systems share a common ARP fingerprint string points to a re-use of networking code. One example of this is Windows NT and FreeBSD. arp-fingerprint uses arp-scan to send the ARP requests and receive the replies. There are other methods that can be used to fingerprint a system using arp-scan which can be used in addition to arp-fingerprint. These additional methods are not included in arp-fingerprint either because they are likely to cause disruption to the target system, or because they require knowledge of the target's configuration that may not always be available. arp-fingerprint is still being developed, and the results should not be relied on. As most of the ARP requests that it sends are non-standard, it is possible that it may disrupt some systems, so caution is advised. If you find a system that arp-fingerprint reports as UNKNOWN, and you know what operating system it is running, could you please send details of the operating system and fingerprint to arp-scan@nta-monitor.com so I can include it in future versions. Please include the exact version of the operating system if you know it, as fingerprints sometimes change between versions.","Process Name":"arp-fingerprint","Link":"https:\/\/linux.die.net\/man\/1\/arp-fingerprint"}},{"Process":{"Description":"arp-scan sends ARP packets to hosts on the local network and displays any responses that are received. The network interface to use can be specified with the --interface option. If this option is not present, arp-scan will search the system interface list for the lowest numbered, configured up interface (excluding loopback). By default, the ARP packets are sent to the Ethernet broadcast address, ff:ff:ff:ff:ff:ff, but that can be changed with the --destaddr option. The target hosts to scan may be specified in one of three ways: by specifying the targets on the command line; by specifying a file containing the targets with the --file option; or by specifying the --localnet option which causes all possible hosts on the network attached to the interface (as defined by the interface address and mask) to be scanned. For hosts specified on the command line, or with the --file For hosts specified on the command line, or with the --file option, you can use either IP addresses or hostnames. You can also use network specifications IPnetwork\/bits, IPstart-IPend, or IPnetwork:NetMask. The list of target hosts is stored in memory. Each host in this list uses 28 bytes of memory, so scanning a Class-B network (65,536 hosts) requires about 1.75MB of memory for the list, and scanning a Class-A (16,777,216 hosts) requires about 448MB. arp-scan supports Ethernet and 802.11 wireless networks. It could also support token ring and FDDI, but they have not been tested. It does not support serial links such as PPP or SLIP, because ARP is not supported on them. The ARP protocol is a layer-2 (datalink layer) protocol that is used to determine a host's layer-2 address given its layer-3 (network layer) address. ARP was designed to work with any layer-2 and layer-3 address format, but the most common use is to map IP addresses to Ethernet hardware addresses, and this is what arp-scan supports. ARP only operates on the local network, and cannot be routed. Although the ARP protocol makes use of IP addresses, it is not an IP-based protocol and arp-scan can be used on an interface that is not configured for IP. ARP is only used by IPv4 hosts. IPv6 uses NDP (neighbour discovery protocol) instead, which is a different protocol and is not supported by arp-scan. One ARP packet is sent for each for each target host, with the target protocol address (the ar$tpa field) set to the IP address of this host. If a host does not respond, then the ARP packet will be re-sent once more. The maximum number of retries can be changed with the --retry option. Reducing the number of retries will reduce the scanning time at the possible risk of missing some results due to packet loss. You can specify the bandwidth that arp-scan will use for the outgoing ARP packets with the --bandwidth option. By default, it uses a bandwidth of 256000 bits per second. Increasing the bandwidth will reduce the scanning time, but setting the bandwidth too high may result in an ARP storm which can disrupt network operation. Also, setting the bandwidth too high can send packets faster than the network interface can transmit them, which will eventually fill the kernel's transmit buffer resulting in the error message: No buffer space available. Another way to specify the outgoing ARP packet rate is with the --interval option, which is an alternative way to modify the same underlying parameter. The time taken to perform a single-pass scan (i.e. with --retry=1) is given by: time = n*i + t + o Where n is the number of hosts in the list, i is the time interval between packets (specified with --interval, or calculated from --bandwidth), t is the timeout value (specified with --timeout) and o is the overhead time taken to load the targets into the list and read the MAC\/Vendor mapping files. For small lists of hosts, the timeout value will dominate, but for large lists the packet interval is the most important value. With 65,536 hosts, the default bandwidth of 256,000 bits\/second (which results in a packet interval of 2ms), the default timeout of 100ms, and a single pass ( --retry=1), and assuming an overhead of 1 second, the scan would take 65536*0.002 + 0.1 + 1 = 132.172 seconds, or about 2 minutes 12 seconds. Any part of the outgoing ARP packet may be modified through the use of the various --arpXXX options. The use of some of these options may make the outgoing ARP packet non RFC compliant. Different operating systems handle the various non standard ARP packets in different ways, and this may be used to fingerprint these systems. See arp-fingerprint(1) for information about a script which uses these options to fingerprint the target operating system. The table below summarises the options that change the outgoing ARP packet. In this table, the Field column gives the ARP packet field name from RFC 826, Bits specifies the number of bits in the field, Option shows the arp-scan option to modify this field, and Notes gives the default value and any other notes. Outgoing ARP Packet Options Field Bits Option Notes ar$hrd 16 --arphrd Default is 1 (ARPHRD_ETHER) ar$pro 16 --arppro Default is 0x0800 ar$hln 8 --arphln Default is 6 (ETH_ALEN) ar$pln 8 --arppln Default is 4 (IPv4) ar$op 16 --arpop Default is 1 (ARPOP_REQUEST) ar$sha 48 --arpsha Default is interface h\/w address ar$spa 32 --arpspa Default is interface IP address ar$tha 48 --arptha Default is zero (00:00:00:00:00:00) ar$tpa 32 None Set to the target host IP address The most commonly used outgoing ARP packet option is --arpspa, which sets the source IP address in the ARP packet. This option allows the outgoing ARP packet to use a different source IP address from the outgoing interface address. With this option it is possible to use arp-scan on an interface with no IP address configured, which can be useful if you want to ensure that the testing host does not interact with the network being tested. It is also possible to change the values in the Ethernet frame header that proceeds the ARP packet in the outgoing packets. The table below summarises the options that change values in the Ethernet frame header. Outgoing Ethernet Frame Options Field Bits Option Notes Dest Address 48 --destaddr Default is ff:ff:ff:ff:ff:ff Source Address 48 --srcaddr Default is interface address Protocol Type 16 --prototype Default is 0x0806 The most commonly used outgoing Ethernet frame option is --destaddr, which sets the destination Ethernet address for the ARP packet. --prototype is not often used, because it will cause the packet to be interpreted as a different Ethernet protocol. Any ARP responses that are received are displayed in the following format: <IP Address> <Hardware Address> <Vendor Details> Where IP Address is the IP address of the responding target, Hardware Address is its Ethernet hardware address (also known as the MAC address) and Vendor Details are the vendor details, decoded from the hardware address. The output fields are separated by a single tab character. The responses are displayed in the order that they are received, which is not always the same order as the requests were sent because some hosts may respond faster than others. The vendor decoding uses the files ieee-oui.txt, ieee-iab.txt and mac-vendor.txt which are supplied with arp-scan. The ieee-oui.txt and ieee-iab.txt files are generated from the OUI and IAB data on the IEEE website at http:\/\/standards.ieee.org\/regauth\/oui\/ieee-oui.txt and http:\/\/standards.ieee.org\/regauth\/oui\/iab.txt. The Perl scripts get-oui and get-iab, which are included in the arp-scan package, can be used to update these files with the latest data from the IEEE website. The mac-vendor.txt file contains other MAC to Vendor mappings that are not covered by the IEEE OUI and IAB files. Almost all hosts that support IP will respond to arp-scan if they receive an ARP packet with the target protocol address (ar$tpa) set to their IP address. This includes firewalls and other hosts with IP filtering that drop all IP traffic from the testing system. For this reason, arp-scan is a useful tool to quickly determine all the active IP hosts on a given Ethernet network segment.","Process Name":"arp-scan","Link":"https:\/\/linux.die.net\/man\/1\/arp-scan"}},{"Process":{"Description":"arpaname translates IP addresses (IPv4 and IPv6) to the corresponding IN-ADDR.ARPA or IP6.ARPA names.","Process Name":"arpaname","Link":"https:\/\/linux.die.net\/man\/1\/arpaname"}},{"Process":{"Description":"rancid is a perl(1) script which uses the login scripts (see clogin(1)) to login to a device, execute commands to display the configuration, etc, then filters the output for formatting, security, and so on. rancid's product is a file with the name of it's last argument plus the suffix .new. For example, hostname.new. There are complementary scripts for other platforms and\/or manufacturers that are supported by rancid(1). Briefly, these are: agmrancid Cisco Anomaly Guard Module (AGM) arancid Alteon WebOS switches arrancid Arista Networks devices brancid Bay Networks (nortel) cat5rancid Cisco catalyst switches cssrancid Cisco content services switches erancid ADC-kentrox EZ-T3 mux f10rancid Force10 f5rancid F5 BigIPs fnrancid Fortinet Firewalls francid Foundry and HP procurve OEMs of Foundry hrancid HP Procurve Switches htranicd Hitachi Routers jerancid Juniper Networks E-series jrancid Juniper Networks mrancid MRTd mrvrancid MRV optical switches nrancid Netscreen firewalls nsrancid Netscaler nxrancid Cisco Nexus boxes prancid Procket Networks rivrancid Riverstone rrancid Redback srancid SMC switch (some Dell OEMs) trancid Netopia sDSL\/T1 routers tntrancid Lucent TNT xrancid Extreme switches xrrancid Cisco IOS-XR boxes zrancid Zebra routing software The command-line options are as follows: -V Prints package name and version strings. -d Display debugging information. -l Display somewhat less debugging information. -f rancid should interpret the next argument as a filename which contains the output it would normally collect from the device ( hostname) with clogin(1).","Process Name":"arrancid","Link":"https:\/\/linux.die.net\/man\/1\/arrancid"}},{"Process":{"Description":"","Process Name":"arx","Link":"https:\/\/linux.die.net\/man\/1\/arx"}},{"Process":{"Description":"GNU as is really a family of assemblers. If you use (or have used) the GNU assembler on one architecture, you should find a fairly similar environment when you use it on another architecture. Each version has much in common with the others, including object file formats, most assembler directives (often called pseudo-ops) and assembler syntax. as is primarily intended to assemble the output of the GNU C compiler \"gcc\" for use by the linker \"ld\". Nevertheless, we've tried to make as assemble correctly everything that other assemblers for the same machine would assemble. Any exceptions are documented explicitly. This doesn't mean as always uses the same syntax as another assembler for the same architecture; for example, we know of several incompatible versions of 680x0 assembly language syntax. Each time you run as it assembles exactly one source program. The source program is made up of one or more files. (The standard input is also a file.) You give as a command line that has zero or more input file names. The input files are read (from left file name to right). A command line argument (in any position) that has no special meaning is taken to be an input file name. If you give as no file names it attempts to read one input file from the as standard input, which is normally your terminal. You may have to type ctl-D to tell as there is no more program to assemble. Use -- if you need to explicitly name the standard input file in your command line. If the source is empty, as produces a small, empty object file. as may write warnings and error messages to the standard error file (usually your terminal). This should not happen when a compiler runs as automatically. Warnings report an assumption made so that as could keep assembling a flawed program; errors report a grave problem that stops the assembly. If you are invoking as via the GNU C compiler, you can use the -Wa option to pass arguments through to the assembler. The assembler arguments must be separated from each other (and the -Wa) by commas. For example: gcc -c -g -O -Wa,-alh,-L file.c This passes two options to the assembler: -alh (emit a listing to standard output with high-level and assembly source) and -L (retain local symbols in the symbol table). Usually you do not need to use this -Wa mechanism, since many compiler command-line options are automatically passed to the assembler by the compiler. (You can call the GNU compiler driver with the -v option to see precisely what options it passes to each compilation pass, including the assembler.)","Process Name":"as","Link":"https:\/\/linux.die.net\/man\/1\/as"}},{"Process":{"Description":"Compiles a file written in ActionScript 3.0 to a SWF file.","Process Name":"as3compile","Link":"https:\/\/linux.die.net\/man\/1\/as3compile"}},{"Process":{"Description":"as86 is an assembler for the 8086..80386 processors, it's syntax is closer to the intel\/microsoft form rather than the more normal generic form of the unix system assembler. The src file can be '-' to assemble the standard input. This assembler can be compiled to support the 6809 cpu and may even work. as86_encap is a shell script to call as86 and convert the created binary into a C file prog.v to be included in or linked with programs like boot block installers. The prefix_ argument is a prefix to be added to all variables defined by the source, it defaults to the name of the source file. The variables defined include prefix_start prefix_size and prefix_data to define and contain the code, plus integers containing the values of all exported labels. Either or both the prog.s and prog.v arguments can be '-' for standard in\/out.","Process Name":"as86","Link":"https:\/\/linux.die.net\/man\/1\/as86"}},{"Process":{"Description":"The asa utility shall write its input files to standard output, mapping carriage-control characters from the text files to line-printer control sequences in an implementation-defined manner. The first character of every line shall be removed from the input, and the following actions are performed. If the character removed is: <space> The rest of the line is output without change. A <newline> is output, then the rest of the input line. One or more implementation-defined characters that causes an advance to the next page shall be output, followed by the rest of the input line. + The <newline> of the previous line shall be replaced with one or more implementation-defined characters that causes printing to return to column position 1, followed by the rest of the input line. If the '+' is the first character in the input, it shall be equivalent to <space>. The action of the asa utility is unspecified upon encountering any character other than those listed above as the first character in a line.","Process Name":"asa","Link":"https:\/\/linux.die.net\/man\/1\/asa"}},{"Process":{"Description":"Ascii-xfr Transfers files in ASCII mode. This means no flow control, no checksumming and no file-name negotiation. It should only be used if the remote system doesn't understand anything else. The ASCII protocol transfers files line-by-line. The EOL (End-Of-Line) character is transmitted as CRLF. When receiving, the CR character is stripped from the incoming file. The Control-Z (ASCII 26) character signals End-Of-File, if option -e is specified (unless you change it to Cotrol-D (ASCII 4) with -d). Ascii-xfr reads from stdin when receiving, and sends data on stdout when sending. Some form of input or output redirection to the modem device is thus needed when downloading or uploading, respectively.","Process Name":"ascii-xfr","Link":"https:\/\/linux.die.net\/man\/1\/ascii-xfr"}},{"Process":{"Description":"ascii2uni converts various 7-bit ASCII representations to UTF-8. It reads from the standard input and writes to the standard output. The representations understood are listed below under the command line options. If no format is specified, standard hexadecimal format (e.g. 0x00e9) is assumed.","Process Name":"ascii2uni","Link":"https:\/\/linux.die.net\/man\/1\/ascii2uni"}},{"Process":{"Description":"Ascii85 (also called \"Base85\") is a form of binary-to-text encoding, wich is mainly used in Adobe's PostScript and PDF format. ascii85 is a command line utility, modeled after base64(1) from the GNU coreutils, can encode text files to Ascii85 or decode Ascii85 files to text, and then print the result to the standard output. If no file is given on the command line, ascii85 reads from the standard input.","Process Name":"ascii85","Link":"https:\/\/linux.die.net\/man\/1\/ascii85"}},{"Process":{"Description":"The asciidoc(1) command translates the AsciiDoc text file FILE to a DocBook, HTML or LinuxDoc file. If FILE is - then the standard input is used.","Process Name":"asciidoc","Link":"https:\/\/linux.die.net\/man\/1\/asciidoc"}},{"Process":{"Description":"This program is part of Netpbm(1). asciitopgm reads ASCII data as input and produces a PGM image with pixel values which are an approximation of the 'brightness' of the ASCII characters, assuming black-on-white printing. In other words, a capital M is very dark, a period is very light, and a space is white. Obviously, asciitopgm assumes a certain font in assigning a brightness value to a character. asciitopgm considers ASCII control characters to be all white. It assigns special brightnesses to lower case letters which have nothing to do with what they look like printed. asciitopgm takes the ASCII character code from the lower 7 bits of each input byte. But it warns you if the most signficant bit of any input byte is not zero. Input lines which are fewer than width characters are automatically padded with spaces. The divisor value is an integer (decimal) by which the blackness of an input character is divided; the default value is 1. You can use this to adjust the brightness of the output: for example, if the image is too bright, increase the divisor. In keeping with (I believe) Fortran line-printer conventions, input lines beginning with a + (plus) character are assumed to 'overstrike' the previous line, allowing a larger range of gray values. If you're looking for something that creates an image of text, with that text specified in ASCII, that is something quite different. Use pbmtext for that.","Process Name":"asciitopgm","Link":"https:\/\/linux.die.net\/man\/1\/asciitopgm"}},{"Process":{"Description":"aseqdump is a command-line utility that prints the sequencer events it receives as text. To stop receiving, press Ctrl+C.","Process Name":"aseqdump","Link":"https:\/\/linux.die.net\/man\/1\/aseqdump"}},{"Process":{"Description":"aseqnet is an ALSA sequencer client which sends and receives event packets over network. Suppose two hosts connected by network, hostA as a server and hostB as a client. The ALSA sequencer system must be running on both hosts. For creating the server port, run the following on hostA: hostA% aseqnet sequencer opened: 128:0 Then a user client 128 with port 0 was opened on hostA. (The client number may vary.) For creating the (network-)client port, run aseqnet with the hostname of the server: hostB% aseqnet hostA sequencer opened: 132:0 Now all events sent to hostA:128:0 are transferred to hostB:132:0, and vice versa. The ports created by aseqnet can be connected arbitrary to other sequencer ports via aconnect(1). For example, to connect hostB:132:0 to a MIDI output device 65:0: hostB% aconnect 132:0 65:0 Then events to hostA:128:0 will be delivered to hostB:65:0. The following command plays MIDI on hostB. hostA% pmidi -p 128:0 foo.mid The multiple clients may exist simultaneously. If hostC is connected as a client to hostA, events from from hostA are sent to all connected network clients, i.e. hostB and hostC. However, only one connection is allowed from a client to a server. To disconnect network, stop all clients before server by ctrl-C or sending signal to them. The server will automatically quit.","Process Name":"aseqnet","Link":"https:\/\/linux.die.net\/man\/1\/aseqnet"}},{"Process":{"Description":"Sh is the standard command interpreter for the system. The current version of sh is in the process of being changed to conform with the POSIX 1003.2 and 1003.2a specifications for the shell. This version has many features which make it appear similar in some respects to the Korn shell, but it is not a Korn shell clone (see ksh(1)). Only features designated by POSIX, plus a few Berkeley extensions, are being incorporated into this shell. We expect POSIX conformance by the time 4.4 BSD is released. This man page is not intended to be a tutorial or a complete specification of the shell. Overview The shell is a command that reads lines from either a file or the terminal, interprets them, and generally executes other commands. It is the program that is running when a user logs into the system (although a user can select a different shell with the chsh(1) command). The shell implements a language that has flow control constructs, a macro facility that provides a variety of features in addition to data storage. It incorporates many features to aid interactive use and has the advantage that the interpretative language is common to both interactive and non-interactive use (shell scripts). That is, commands can be typed directly to the running shell or can be put into a file and the file can be executed directly by the shell. Invocation If no args are present and if the standard input of the shell is connected to a terminal (or if the -i flag is set), and the -c option is not present, the shell is considered an interactive shell. An interactive shell generally prompts before each command and handles programming and command errors differently (as described below). When first starting, the shell inspects argument 0, and if it begins with a dash '-', the shell is also considered a login shell. This is normally done automatically by the system when the user first logs in. A login shell first reads commands from the files \/etc\/profile and .profile if they exist. If the environment variable ENV is set on entry to a shell, or is set in the .profile of a login shell, the shell next reads commands from the file named in ENV. Therefore, a user should place commands that are to be executed only at login time in the .profile file, and commands that are executed for every shell inside the ENV file. To set the ENV variable to some file, place the following line in your .profile of your home directory ENV=$HOME\/.shinit; export ENV substituting for ''.shinit'' any filename you wish. Since the ENV file is read for every invocation of the shell, including shell scripts and non-interactive shells, the following paradigm is useful for restricting commands in the ENV file to interactive invocations. Place commands within the ''case'' and ''esac'' below (these commands are described later): case $- in *i*) # commands for interactive use only ... esac If command line arguments besides the options have been specified, then the shell treats the first argument as the name of a file from which to read commands (a shell script), and the remaining arguments are set as the positional parameters of the shell ($1, $2, etc). Otherwise, the shell reads commands from its standard input. Argument List Processing All of the single letter options have a corresponding name that can be used as an argument to the -o option. The set -o name is provided next to the single letter option in the description below. Specifying a dash ''-'' turns the option on, while using a plus ''+'' disables the option. The following options can be set from the command line or with the set(1) builtin (described later).             -a allexport'                 Export all variables assigned to.(UNIMPLEMENTED for 4.4alpha) -c' Read commands from the command line. No commands will be read from the standard input. -C noclobber' Don't overwrite existing files with ''>''. (UNIMPLEMENTED for 4.4alpha) -e errexit' If not interactive, exit immediately if any untested command fails. The exit status of a command is considered to be explicitly tested if the command is used to control an if, elif, while, or until; or if the command is the left hand operand of an ''&&'' or ''||'' operator. -f noglob' Disable pathname expansion. -n noexec' If not interactive, read commands but do not execute them. This is useful for checking the syntax of shell scripts. -u nounset' Write a message to standard error when attempting to expand a variable that is not set, and if the shell is not interactive, exit immediately. (UNIMPLEMENTED for 4.4alpha) -v verbose' The shell writes its input to standard error as it is read. Useful for debugging. -x xtrace' Write each command to standard error (preceded by a '+ ') before it is executed. Useful for debugging. -q quietprofile If the -v or -x options have been set, do not apply them when reading initialization files, these being \/etc\/profile, .profile, and the file specified by the ENV environment variable. -I ignoreeof' Ignore EOF's from input when interactive. -i interactive Force the shell to behave interactively. -m monitor' Turn on job control (set automatically when interactive). -s stdin' Read commands from standard input (set automatically if no file arguments are present). This option has no effect when set after the shell has already started running (i.e. with set(1)). -V vi' Enable the built-in vi(1) command line editor (disables -E if it has been set). -E emacs' Enable the built-in emacs(1) command line editor (disables -V if it has been set). -b notify' Enable asynchronous notification of background job completion. (UNIMPLEMENTED for 4.4alpha) Lexical Structure The shell reads input in terms of lines from a file and breaks it up into words at whitespace (blanks and tabs), and at certain sequences of characters that are special to the shell called ''operators''. There are two types of operators: control operators and redirection operators (their meaning is discussed later). Following is a list of operators: Control operators: & && (); ;; | || <newline> Redirection operator: < > >| << >> <& >& <<- <> Quoting Quoting is used to remove the special meaning of certain characters or words to the shell, such as operators, whitespace, or keywords. There are three types of quoting: matched single quotes, matched double quotes, and backslash. Backslash A backslash preserves the literal meaning of the following character, with the exception of <newline>. A backslash preceding a <newline> is treated as a line continuation. Single Quotes Enclosing characters in single quotes preserves the literal meaning of all the characters (except single quotes, making it impossible to put single-quotes in a single-quoted string). Double Quotes Enclosing characters within double quotes preserves the literal meaning of all characters except dollarsign ($), backquote ('), and backslash (\\). The backslash inside double quotes is historically weird, and serves to quote only the following characters: $ ' \" \\ <newline>. Otherwise it remains literal. Reserved Words Reserved words are words that have special meaning to the shell and are recognized at the beginning of a line and after a control operator. The following are reserved words: ! elif fi while case else for then { } do done until if esac Their meaning is discussed later. Aliases An alias is a name and corresponding value set using the alias(1) builtin command. Whenever a reserved word may occur (see above), and after checking for reserved words, the shell checks the word to see if it matches an alias. If it does, it replaces it in the input stream with its value. For example, if there is an alias called ''lf'' with the value ''ls -F'', then the input: lf foobar <return> would become ls -F foobar <return> Aliases provide a convenient way for naive users to create shorthands for commands without having to learn how to create functions with arguments. They can also be used to create lexically obscure code. This use is discouraged. Commands The shell interprets the words it reads according to a language, the specification of which is outside the scope of this man page (refer to the BNF in the POSIX 1003.2 document). Essentially though, a line is read and if the first word of the line (or after a control operator) is not a reserved word, then the shell has recognized a simple command. Otherwise, a complex command or some other special construct may have been recognized. Simple Commands If a simple command has been recognized, the shell performs the following actions: 1. Leading words of the form ''name=value'' are stripped off andassigned to the environment of the simple command.Redirection operators and their arguments (as described below)are stripped off and saved for processing. 2. The remaining words are expanded as described in the section called ''Expansions'', and the first remaining word is considered the command name and the command is located. The remaining words are considered the arguments of the command. If no command name resulted, then the ''name=value'' variable assignments recognized in item 1 affect the current shell. 3. Redirections are performed as described in the next section. Redirections Redirections are used to change where a command reads its input or sends its output. In general, redirections open, close, or duplicate an existing reference to a file. The overall format used for redirection is: [n] redir-op file where redir-op is one of the redirection operators mentioned previously. Following is a list of the possible redirections. The [n] is an optional number, as in '3' (not '[3]', that refers to a file descriptor. [n]> file Redirect standard output (or n) to file. [n]>| file Same, but override the -C option. [n]>> file Append standard output (or n) to file. [n]< file Redirect standard input (or n) from file. [n1]<&n2' Duplicate standard input (or n1) from file descriptor n2. [n]<&-' Close standard input (or n). [n1]>&n2' Duplicate standard output (or n1) from n2. [n]>&-' Close standard output (or n). [n]<> file Open file for reading and writing on standard input (or n). The following redirection is often called a ''here-document''. [n]<< delimiter here-doc-text... delimiter All the text on successive lines up to the delimiter is saved away and made available to the command on standard input, or file descriptor n if it is specified. If the delimiter as specified on the initial line is quoted, then the here-doc-text is treated literally, otherwise the text is subjected to parameter expansion, command substitution, and arithmetic expansion (as described in the section on ''Expansions''). If the operator is ''<<-'' instead of ''<<'', then leading tabs in the here-doc-text are stripped. Search and Execution There are three types of commands: shell functions, builtin commands, and normal programs -- and the command is searched for (by name) in that order. They each are executed in a different way. When a shell function is executed, all of the shell positional parameters (except $0, which remains unchanged) are set to the arguments of the shell function. The variables which are explicitly placed in the environment of the command (by placing assignments to them before the function name) are made local to the function and are set to the values given. Then the command given in the function definition is executed. The positional parameters are restored to their original values when the command completes. This all occurs within the current shell. Shell builtins are executed internally to the shell, without spawning a new process. Otherwise, if the command name doesn't match a function or builtin, the command is searched for as a normal program in the filesystem (as described in the next section). When a normal program is executed, the shell runs the program, passing the arguments and the environment to the program. If the program is not a normal executable file (i.e., if it does not begin with the \"magic number\" whose ASCII representation is \"#!\", so execve(2) returns ENOEXEC then) the shell will interpret the program in a subshell. The child shell will reinitialize itself in this case, so that the effect will be as if a new shell had been invoked to handle the ad-hoc shell script, except that the location of hashed commands located in the parent shell will be remembered by the child. Note that previous versions of this document and the source code itself misleadingly and sporadically refer to a shell script without a magic number as a \"shell procedure\". Path Search When locating a command, the shell first looks to see if it has a shell function by that name. Then it looks for a builtin command by that name. If a builtin command is not found, one of two things happen: 1. Command names containing a slash are simply executed without performing any searches. 2. The shell searches each entry in PATH in turn for the command. The value of the PATH variable should be a series of entries separated by colons. Each entry consists of a directory name. The current directory may be indicated implicitly by an empty directory name, or explicitly by a single period. Command Exit Status Each command has an exit status that can influence the behavior of other shell commands. The paradigm is that a command exits with zero for normal or success, and non-zero for failure, error, or a false indication. The man page for each command should indicate the various exit codes and what they mean. Additionally, the builtin commands return exit codes, as does an executed shell function. Complex Commands Complex commands are combinations of simple commands with control operators or reserved words, together creating a larger complex command. More generally, a command is one of the following: \u2022 simple command \u2022 pipeline \u2022 list or compound-list \u2022 compound command \u2022 function definition Unless otherwise stated, the exit status of a command is that of the last simple command executed by the command. Pipelines A pipeline is a sequence of one or more commands separated by the control operator |. The standard output of all but the last command is connected to the standard input of the next command. The standard output of the last command is inherited from the shell, as usual. The format for a pipeline is: [!] command1 [| command2 ...] The standard output of command1 is connected to the standard input of command2. The standard input, standard output, or both of a command is considered to be assigned by the pipeline before any redirection specified by redirection operators that are part of the command. If the pipeline is not in the background (discussed later), the shell waits for all commands to complete. If the reserved word ! does not precede the pipeline, the exit status is the exit status of the last command specified in the pipeline. Otherwise, the exit status is the logical NOT of the exit status of the last command. That is, if the last command returns zero, the exit status is 1; if the last command returns greater than zero, the exit status is zero. Because pipeline assignment of standard input or standard output or both takes place before redirection, it can be modified by redirection. For example: $ command1 2>&1 | command2 sends both the standard output and standard error of command1 to the standard input of command2. A ; or <newline> terminator causes the preceding AND-OR-list (described next) to be executed sequentially; a & causes asynchronous execution of the preceding AND-OR-list. Note that unlike some other shells, each process in the pipeline is a child of the invoking shell (unless it is a shell builtin, in which case it executes in the current shell -- but any effect it has on the environment is wiped). Background Commands -- & If a command is terminated by the control operator ampersand (&), the shell executes the command asynchronously -- that is, the shell does not wait for the command to finish before executing the next command. The format for running a command in background is: command1 & [command2 & ...] If the shell is not interactive, the standard input of an asynchronous command is set to \/dev\/null. Lists -- Generally Speaking A list is a sequence of zero or more commands separated by newlines, semicolons, or ampersands, and optionally terminated by one of these three characters. The commands in a list are executed in the order they are written. If command is followed by an ampersand, the shell starts the command and immediately proceed onto the next command; otherwise it waits for the command to terminate before proceeding to the next one. Short-Circuit List Operators ''&&'' and ''||'' are AND-OR list operators. ''&&'' executes the first command, and then executes the second command iff the exit status of the first command is zero. ''||'' is similar, but executes the second command iff the exit status of the first command is nonzero. ''&&'' and ''||'' both have the same priority. Flow-Control Constructs -- if, while, for, case The syntax of the if command is if list then list [ elif list then list ] ... [ else list ] fi The syntax of the while command is while list\ndo   list\ndone The two lists are executed repeatedly while the exit status of the first list is zero. The until command is similar, but has the word until in place of while, which causes it to repeat until the exit status of the first list is zero. The syntax of the for command is for variable in word...\ndo   list\ndone The words are expanded, and then the list is executed repeatedly with the variable set to each word in turn. do and done may be replaced with ''{'' and ''}''. The syntax of the break and continue command is break [ num ]\ncontinue [ num ] Break terminates the num innermost for or while loops. Continue continues with the next iteration of the innermost loop. These are implemented as builtin commands. The syntax of the case command is case word in\npattern) list ;;\n...\nesac The pattern can actually be one or more patterns (see Shell Patterns described later), separated by ''|'' characters. Grouping Commands Together Commands may be grouped by writing either (list) or { list; } The first of these executes the commands in a subshell. Builtin commands grouped into a (list) will not affect the current shell. The second form does not fork another shell so is slightly more efficient. Grouping commands together this way allows you to redirect their output as though they were one program: { printf \" hello \" ; printf \" world\\n\" ; } > greeting Functions The syntax of a function definition is name () command A function definition is an executable statement; when executed it installs a function named name and returns an exit status of zero. The command is normally a list enclosed between ''{'' and ''}''. Variables may be declared to be local to a function by using a local command. This should appear as the first statement of a function, and the syntax is local [variable | -] ... Local is implemented as a builtin command. When a variable is made local, it inherits the initial value and exported and readonly flags from the variable with the same name in the surrounding scope, if there is one. Otherwise, the variable is initially unset. The shell uses dynamic scoping, so that if you make the variable x local to function f, which then calls function g, references to the variable x made inside g will refer to the variable x declared inside f, not to the global variable named x. The only special parameter than can be made local is ''-''. Making ''-'' local any shell options that are changed via the set command inside the function to be restored to their original values when the function returns. The syntax of the return command is return [exitstatus] It terminates the currently executing function. Return is implemented as a builtin command. Variables and Parameters The shell maintains a set of parameters. A parameter denoted by a name is called a variable. When starting up, the shell turns all the environment variables into shell variables. New variables can be set using the form name=value Variables set by the user must have a name consisting solely of alphabetics, numerics, and underscores - the first of which must not be numeric. A parameter can also be denoted by a number or a special character as explained below. Positional Parameters A positional parameter is a parameter denoted by a number (n > 0). The shell sets these initially to the values of its command line arguments that follow the name of the shell script. The set(1) builtin can also be used to set or reset them. Special Parameters A special parameter is a parameter denoted by one of the following special characters. The value of the parameter is listed next to its character. *'             Expands to the positional parameters, starting from one.When the expansion occurs within a double-quoted string itexpands to a single field with the value of each parameterseparated by the first character of the IFS variable, or bya <space> if IFS is unset. @' Expands to the positional parameters, starting from one. When the expansion occurs within double-quotes, each positional parameter expands as a separate argument. If there are no positional parameters, the expansion of @ generates zero arguments, even when @ is double-quoted. What this basically means, for example, is if $1 is ''abc'' and $2 is ''def ghi'', then \"$@\" expands to the two arguments: \"abc\" \"def ghi\" #' Expands to the number of positional parameters. ?' Expands to the exit status of the most recent pipeline. - (Hyphen.) Expands to the current option flags (the single-letter option names concatenated into a string) as specified on invocation, by the set builtin command, or implicitly by the shell. $' Expands to the process ID of the invoked shell. A subshell retains the same value of $ as its parent. !' Expands to the process ID of the most recent background command executed from the current shell. For a pipeline, the process ID is that of the last command in the pipeline. 0 (Zero.)' Expands to the name of the shell or shell script. Word Expansions This clause describes the various expansions that are performed on words. Not all expansions are performed on every word, as explained later. Tilde expansions, parameter expansions, command substitutions, arithmetic expansions, and quote removals that occur within a single word expand to a single field. It is only field splitting or pathname expansion that can create multiple fields from a single word. The single exception to this rule is the expansion of the special parameter @ within double-quotes, as was described above. The order of word expansion is: 1. Tilde Expansion, Parameter Expansion, Command Substitution, Arithmetic Expansion (these all occur at the same time). 2. Field Splitting is performed on fields generated by step (1) unless the IFS variable is null. 3. Pathname Expansion (unless set -f is in effect). 4. Quote Removal. The $ character is used to introduce parameter expansion, command substitution, or arithmetic evaluation. Tilde Expansion (substituting a user's home directory) A word beginning with an unquoted tilde character (~) is subjected to tilde expansion. All the characters up to a slash (\/) or the end of the word are treated as a username and are replaced with the user's home directory. If the username is missing (as in ~\/foobar), the tilde is replaced with the value of the HOME variable (the current user's home directory). Parameter Expansion The format for parameter expansion is as follows: ${expression} where expression consists of all characters until the matching ''}''. Any ''}'' escaped by a backslash or within a quoted string, and characters in embedded arithmetic expansions, command substitutions, and variable expansions, are not examined in determining the matching ''}''. The simplest form for parameter expansion is: ${parameter} The value, if any, of parameter is substituted. The parameter name or symbol can be enclosed in braces, which are optional except for positional parameters with more than one digit or when parameter is followed by a character that could be interpreted as part of the name. If a parameter expansion occurs inside double-quotes: 1. Pathname expansion is not performed on the results of the expansion. 2. Field splitting is not performed on the results of the expansion, with the exception of @. In addition, a parameter expansion can be modified by using one of the following formats. ${parameter:-word}' Use Default Values. If parameter is unset or null, the expansion of word is substituted; otherwise, the value of parameter is substituted. ${parameter:=word}' Assign Default Values. If parameter is unset or null, the expansion of word is assigned to parameter. In all cases, the final value of parameter is substituted. Only variables, not positional parameters or special parameters, can be assigned in this way. ${parameter:?[word]} Indicate Error if Null or Unset. If parameter is unset or null, the expansion of word (or a message indicating it is unset if word is omitted) is written to standard error and the shell exits with a nonzero exit status. Otherwise, the value of parameter is substituted. An interactive shell need not exit. ${parameter:+word}' Use Alternative Value. If parameter is unset or null, null is substituted; otherwise, the expansion of word is substituted. In the parameter expansions shown previously, use of the colon in the format results in a test for a parameter that is unset or null; omission of the colon results in a test for a parameter that is only unset. ${#parameter}' String Length. The length in characters of the value of parameter. The following four varieties of parameter expansion provide for substring processing. In each case, pattern matching notation (see Shell Patterns), rather than regular expression notation, is used to evaluate the patterns. If parameter is * or @, the result of the expansion is unspecified. Enclosing the full parameter expansion string in double-quotes does not cause the following four varieties of pattern characters to be quoted, whereas quoting characters within the braces has this effect. ${parameter%word}' Remove Smallest Suffix Pattern. The word is expanded to produce a pattern. The parameter expansion then results in parameter, with the smallest portion of the suffix matched by the pattern deleted. ${parameter%%word}' Remove Largest Suffix Pattern. The word is expanded to produce a pattern. The parameter expansion then results in parameter, with the largest portion of the suffix matched by the pattern deleted. ${parameter#word}' Remove Smallest Prefix Pattern. The word is expanded to produce a pattern. The parameter expansion then results in parameter, with the smallest portion of the prefix matched by the pattern deleted. ${parameter##word}' Remove Largest Prefix Pattern. The word is expanded to produce a pattern. The parameter expansion then results in parameter, with the largest portion of the prefix matched by the pattern deleted. Command Substitution Command substitution allows the output of a command to be substituted in place of the command name itself. Command substitution occurs when the command is enclosed as follows: $(command) or ( ''backquoted'' version): 'command' The shell expands the command substitution by executing command in a subshell environment and replacing the command substitution with the standard output of the command, removing sequences of one or more <newline>s at the end of the substitution. (Embedded <newline>s before the end of the output are not removed; however, during field splitting, they may be translated into <space>s, depending on the value of IFS and quoting that is in effect.) Arithmetic Expansion Arithmetic expansion provides a mechanism for evaluating an arithmetic expression and substituting its value. The format for arithmetic expansion is as follows: $((expression)) The expression is treated as if it were in double-quotes, except that a double-quote inside the expression is not treated specially. The shell expands all tokens in the expression for parameter expansion, command substitution, and quote removal. Next, the shell treats this as an arithmetic expression and substitutes the value of the expression. White Space Splitting (Field Splitting) After parameter expansion, command substitution, and arithmetic expansion the shell scans the results of expansions and substitutions that did not occur in double-quotes for field splitting and multiple fields can result. The shell treats each character of the IFS as a delimiter and use the delimiters to split the results of parameter expansion and command substitution into fields. Pathname Expansion (File Name Generation) Unless the -f flag is set, file name generation is performed after word splitting is complete. Each word is viewed as a series of patterns, separated by slashes. The process of expansion replaces the word with the names of all existing files whose names can be formed by replacing each pattern with a string that matches the specified pattern. There are two restrictions on this: first, a pattern cannot match a string containing a slash, and second, a pattern cannot match a string starting with a period unless the first character of the pattern is a period. The next section describes the patterns used for both Pathname Expansion and the case(1) command. Shell Patterns A pattern consists of normal characters, which match themselves, and meta-characters. The meta-characters are ''!'', ''*'', ''?'', and ''[''. These characters lose their special meanings if they are quoted. When command or variable substitution is performed and the dollar sign or back quotes are not double quoted, the value of the variable or the output of the command is scanned for these characters and they are turned into meta-characters. An asterisk (''*'') matches any string of characters. A question mark matches any single character. A left bracket (''['') introduces a character class. The end of the character class is indicated by a ('']''); if the '']'' is missing then the ''['' matches a ''['' rather than introducing a character class. A character class matches any of the characters between the square brackets. A range of characters may be specified using a minus sign. The character class may be complemented by making an exclamation point the first character of the character class. To include a '']'' in a character class, make it the first character listed (after the ''!'', if any). To include a minus sign, make it the first or last character listed Builtins This section lists the builtin commands which are builtin because they need to perform some operation that can't be performed by a separate process. In addition to these, there are several other commands that may be builtin for efficiency (e.g. printf(1), echo(1), test(1), etc). :' A null command that returns a 0 (true) exit value. . file The commands in the specified file are read and executed by the shell. alias [name[=string ...]] If name=string is specified, the shell defines the alias name with value string. If just name is specified, the value of the alias name is printed. With no arguments, the alias builtin prints the names and values of all defined aliases (see unalias). bg [job] ... Continue the specified jobs (or the current job if no jobs are given) in the background. command command arg... Execute the specified builtin command. (This is useful when you have a shell function with the same name as a builtin command.) cd [directory] Switch to the specified directory (default $HOME). If an entry for CDPATH appears in the environment of the cd command or the shell variable CDPATH is set and the directory name does not begin with a slash, then the directories listed in CDPATH will be searched for the specified directory. The format of CDPATH is the same as that of PATH. In an interactive shell, the cd command will print out the name of the directory that it actually switched to if this is different from the name that the user gave. These may be different either because the CDPATH mechanism was used or because a symbolic link was crossed. eval string... Concatenate all the arguments with spaces. Then re-parse and execute the command. exec [command arg...] Unless command is omitted, the shell process is replaced with the specified program (which must be a real program, not a shell builtin or function). Any redirections on the exec command are marked as permanent, so that they are not undone when the exec command finishes. exit [exitstatus] Terminate the shell process. If exitstatus is given it is used as the exit status of the shell; otherwise the exit status of the preceding command is used. export name... export -p The specified names are exported so that they will appear in the environment of subsequent commands. The only way to un-export a variable is to unset it. The shell allows the value of a variable to be set at the same time it is exported by writing export name=value With no arguments the export command lists the names of all exported variables. With the -p option specified the output will be formatted suitably for non-interactive use. fg [job] Move the specified job or the current job to the foreground. getopts optstring var The POSIX getopts command, not to be confused with the Bell Labs -derived getopt(1). The first argument should be a series of letters, each of which may be optionally followed by a colon to indicate that the option requires an argument. The variable specified is set to the parsed option. The getopts command deprecates the older getopt(1) utility due to its handling of arguments containing whitespace. The getopts builtin may be used to obtain options and their arguments from a list of parameters. When invoked, getopts places the value of the next option from the option string in the list in the shell variable specified by var and it's index in the shell variable OPTIND. When the shell is invoked, OPTIND is initialized to 1. For each option that requires an argument, the getopts builtin will place it in the shell variable OPTARG. If an option is not allowed for in the optstring, then OPTARG will be unset. optstring is a string of recognized option letters (see getopt(3)). If a letter is followed by a colon, the option is expected to have an argument which may or may not be separated from it by white space. If an option character is not found where expected, getopts will set the variable var to a ''?''; getopts will then unset OPTARG and write output to standard error. By specifying a colon as the first character of optstring all errors will be ignored. A nonzero value is returned when the last option is reached. If there are no remaining arguments, getopts will set var to the special option, ''--'', otherwise, it will set var to ''?''. The following code fragment shows how one might process the arguments for a command that can take the options [a] and [b], and the option [c], which requires an argument. while getopts abc: f do case $f in a | b) flag=$f;; c) carg=$OPTARG;; \\?) echo $USAGE; exit 1;; esac done shift 'expr $OPTIND - 1' This code will accept any of the following as equivalent: cmd -acarg file file\ncmd -a -c arg file file\ncmd -carg -a file file\ncmd -a -carg -- file file hash -rv command... The shell maintains a hash table which remembers the locations of commands. With no arguments whatsoever, the hash command prints out the contents of this table. Entries which have not been looked at since the last cd command are marked with an asterisk; it is possible for these entries to be invalid. With arguments, the hash command removes the specified commands from the hash table (unless they are functions) and then locates them. With the -v option, hash prints the locations of the commands as it finds them. The -r option causes the hash command to delete all the entries in the hash table except for functions. jobs This command lists out all the background processes which are children of the current shell process. pwd' Print the current directory. The builtin command may differ from the program of the same name because the builtin command remembers what the current directory is rather than recomputing it each time. This makes it faster. However, if the current directory is renamed, the builtin version of pwd will continue to print the old name for the directory. read [-p prompt] [-r] variable... The prompt is printed if the -p option is specified and the standard input is a terminal. Then a line is read from the standard input. The trailing newline is deleted from the line and the line is split as described in the section on word splitting above, and the pieces are assigned to the variables in order. At least one variable must be specified. If there are more pieces than variables, the remaining pieces (along with the characters in IFS that separated them) are assigned to the last variable. If there are more variables than pieces, the remaining variables are assigned the null string. The read builtin will indicate success unless EOF is encountered on input, in which case failure is returned. By default, unless the -r option is specified, the backslash ''\\'' acts as an escape character, causing the following character to be treated literally. If a backslash is followed by a newline, the backslash and the newline will be deleted. readonly name... readonly -p The specified names are marked as read only, so that they cannot be subsequently modified or unset. The shell allows the value of a variable to be set at the same time it is marked read only by writing readonly name=value With no arguments the readonly command lists the names of all read only variables. With the -p option specified the output will be formatted suitably for non-interactive use. set [ { -options | +options | -- }] arg... The set command performs three different functions. With no arguments, it lists the values of all shell variables. If options are given, it sets the specified option flags, or clears them as described in the section called Argument List Processing. The third use of the set command is to set the values of the shell's positional parameters to the specified args. To change the positional parameters without changing any options, use ''--'' as the first argument to set. If no args are present, the set command will clear all the positional parameters (equivalent to executing ''shift $#''.) setvar variable value Assigns value to variable. (In general it is better to write variable=value rather than using setvar. setvar is intended to be used in functions that assign values to variables whose names are passed as parameters.) shift [n] Shift the positional parameters n times. A shift sets the value of $1 to the value of $2, the value of $2 to the value of $3, and so on, decreasing the value of $# by one. If n is greater than the number of positional parameters, shift will issue an error message, and exit with return status 2. times Print the accumulated user and system times for the shell and for processes run from the shell. The return status is 0. trap action signal... Cause the shell to parse and execute action when any of the specified signals are received. The signals are specified by signal number. If signal is 0, the action is executed when the shell exits. action may be null or ''-''; the former causes the specified signal to be ignored and the latter causes the default action to be taken. When the shell forks off a subshell, it resets trapped (but not ignored) signals to the default action. The trap command has no effect on signals that were ignored on entry to the shell. type [name ...] Interpret each name as a command and print the resolution of the command search. Possible resolutions are: shell keyword, alias, shell builtin, command, tracked alias and not found. For aliases the alias expansion is printed; for commands and tracked aliases the complete pathname of the command is printed. ulimit [-H | -S] [-a | -tfdscmlpn [value]] Inquire about or set the hard or soft limits on processes or set new limits. The choice between hard limit (which no process is allowed to violate, and which may not be raised once it has been lowered) and soft limit (which causes processes to be signaled but not necessarily killed, and which may be raised) is made with these flags: -H' set or inquire about hard limits -S' set or inquire about soft limits. If neither -H nor -S is specified, the soft limit is displayed or both limits are set. If both are specified, the last one wins. The limit to be interrogated or set, then, is chosen by specifying any one of these flags: -a' show all the current limits -t' show or set the limit on CPU time (in seconds) -f' show or set the limit on the largest file that can be created (in 512-byte blocks) -d' show or set the limit on the data segment size of a process (in kilobytes) -s' show or set the limit on the stack size of a process (in kilobytes) -c' show or set the limit on the largest core dump size that can be produced (in 512-byte blocks) -m' show or set the limit on the total physical memory that can be in use by a process (in kilobytes) -l' show or set the limit on how much memory a process can lock with mlock(2) (in kilobytes) -p' show or set the limit on the number of processes this user can have at one time -n' show or set the limit on the number files a process can have open at once If none of these is specified, it is the limit on file size that is shown or set. If value is specified, the limit is set to that number; otherwise the current limit is displayed. Limits of an arbitrary process can be displayed or set using the sysctl(8) utility. umask [mask] Set the value of umask (see umask(2)) to the specified octal value. If the argument is omitted, the umask value is printed. unalias [-a] [name] If name is specified, the shell removes that alias. If -a is specified, all aliases are removed. unset name... The specified variables and functions are unset and unexported. If a given name corresponds to both a variable and a function, both the variable and the function are unset. wait [job] Wait for the specified job to complete and return the exit status of the last process in the job. If the argument is omitted, wait for all jobs to complete and the return an exit status of zero.","Process Name":"ash","Link":"https:\/\/linux.die.net\/man\/1\/ash"}},{"Process":{"Description":"asimut is a logical simulation tool for hardware descriptions. It compiles and loads a complete hardware description written in VHDL (Very high speed integrated circuits Hardware Description Language). The hardware description may be structural (a hierarchy of instances) or behavioural. Only a subset of VHDL is supported. Descriptions that do not match this subset cause a syntax error during compilation. See vhdl(5) for detailed information about the supported subset of VHDL. Once a hardware description is loaded, asimut looks for a simulation pattern description file. This file is to be written in pat format. The file is compiled, loaded and linked with the hardware description. Then, the simulation is started. When patterns are processed, a result file in pat format is produced. If a save action has been requested in the pattern description file (see pat (5)), asimut creates also a save file representing the state of the description at the end of the simulation of the last pattern. The save file is named root_file.sav, where root_file is the name of the description. The save file can be used in a later simulation sequence to initialize the state of the (same) hardware description before the simulation begins. Using this mechanism, a large sequence of patterns can be breaked onto several small sequences, each one initializing the hardware description with the save file resulted from the previous sequence. asimut reads several parameters from the environment variables : MBK_CATA_LIB list of directories containing description and pattern files (using $PATH syntax). The default path is the current directory (see mbk(1)). MBK_WORK_LIB specifies the current working directory. The working directory idicates the place where all output files are written. MBK_CATAL_NAME Indicates the file where the behavioral description files are listed. This file is used to leaf cells of a structural description.(see mbk(1)) MBK_IN_LO file extension for structural entity. (see mbk(1)) VH_BEHSFX list of file extensions for behavioural entities (using $PATH syntax). The default file extension is vbe. VH_PATSFX list of file extensions for pattern description entities (using $PATH syntax). The default file extension is pat. VH_DLYSFX list of file extensions for delays description entities (using $PATH syntax). The default file extension is dly. VH_MAXERR maximum number of errors allowed during simulation phase. If the number of errors occured during simulation reaches VH_MAXERR, asimut stops the simulation at the end of processing the current pattern. Patterns following the current pattern remain unprocessed and are reproduced in the result file. The default value of VH_MAXERR is 10. root_file is the name of the description. By default asimut looks for a structural description. It uses the MBK_IN_LO environment variables to identify both the format and the extension of structural description files. To load structural VHDL files MBK_IN_LO must be set to vst. To load a pure behavioural description -b option must be specified. In such a case asimut loads a data flow VHDL description file. The VH_BEHSFX environment variable gives the extensions to be used. pattern_file is the entity name of the pattern description. The file containing this entity must be named pattern_file.ext , where ext is one of the extension specified in VH_PATSFX. result_file is the result file produced by asimut. The result file is a pattern description file with the extension specified by VH_PATSFX.","Process Name":"asimut","Link":"https:\/\/linux.die.net\/man\/1\/asimut"}},{"Process":{"Description":"asip-status.pl is a perl script that sends a FPGetSrvrInfo request to an AFP server at HOSTNAME:PORT and displays the results, namely \"Machine type\", the server's name, supported AFP versions, UAMs and AFP flags, the \"server signature\" and the network addresses, the server provides AFP services on. When you don't supply :PORT, then the default AFP port, 548, will be used.","Process Name":"asip-status.pl","Link":"https:\/\/linux.die.net\/man\/1\/asip-status.pl"}},{"Process":{"Description":"AS is a cross assembler that can be used to write assembler programs for a variety of different microprocessors and -controllers. asl is the UNIX\/C implementation of AS. A complete description of AS is far beyond the scope of this manual page, which is only intended as a quick reference for AS's command line interface. For a more detailed introduction into the usage of AS, see the user's manual.","Process Name":"asl","Link":"https:\/\/linux.die.net\/man\/1\/asl"}},{"Process":{"Description":"Generates a DER encoding of ASN.1 DEFINITIONS file and ASSIGNMENTS file with value assignments. Mandatory arguments to long options are mandatory for short options too. -c, --check checks the syntax only -o, --output FILE output file -h, --help display this help and exit -v, --version output version information and exit","Process Name":"asn1coding","Link":"https:\/\/linux.die.net\/man\/1\/asn1coding"}},{"Process":{"Description":"Decodes DER data in ENCODED file, for the ASN1TYPE element described in ASN.1 DEFINITIONS file, and print decoded structures. -c, --check checks the syntax only -h, --help display this help and exit -v, --version output version information and exit","Process Name":"asn1decoding","Link":"https:\/\/linux.die.net\/man\/1\/asn1decoding"}},{"Process":{"Description":"The asn1parse command is a diagnostic utility that can parse ASN .1 structures. It can also be used to extract data from ASN .1 formatted data.","Process Name":"asn1parse","Link":"https:\/\/linux.die.net\/man\/1\/asn1parse"}},{"Process":{"Description":"Read FILE with ASN.1 definitions and generate a C array that is used with libtasn1 functions. Mandatory arguments to long options are mandatory for short options too. -c, --check checks the syntax only -o, --output FILE output file -n, --name NAME array name -h, --help display this help and exit -v, --version output version information and exit","Process Name":"asn1parser","Link":"https:\/\/linux.die.net\/man\/1\/asn1parser"}},{"Process":{"Description":"This program will run Apache::ASP scripts from the command line. Each file that is specified will be run, and the $Request->QueryString() and $Request->Form() data will be initialized by the @arguments following the script file name. The @arguments will be written as space separated words, and will be initialized as an associate array where %arguments = @arguments. As an example: asp-perl file.asp key1 value1 key2 value2 would be similar to calling the file.asp in a web environment like \/file.asp?key1=value1&key2=value2 The asp.conf script will be read from the current directory for parameters that would be set with PerlSetVar normally under mod_perl. For more information on how to configure the asp.conf file, please see < http:\/\/www.apache-asp.org\/cgi.html >","Process Name":"asp-perl","Link":"https:\/\/linux.die.net\/man\/1\/asp-perl"}},{"Process":{"Description":"aspell is a utility program that connects to the Aspell library so that it can function as an ispell -a replacement, as an independent spell checker, as a test utility to test out Aspell library features, and as a utility for managing dictionaries used by the library. The Aspell library contains an interface allowing other programs direct access to it's functions and therefore reducing the complex task of spell checking to simple library calls. The default library does not contain dictionary word lists. To add language dictionaries, please check your distro first for modified dictionaries, otherwise look here for base language dictionaries <http:\/\/aspell.net>. The following information describes the commands and options used by the Aspell Utility. This manual page is maintained separately from the official documentation so it may be out of date or incomplete. The official documentation is maintained as a Texinfo manual. See the 'aspell' entry in info for more complete documentation.","Process Name":"aspell","Link":"https:\/\/linux.die.net\/man\/1\/aspell"}},{"Process":{"Description":"aspell-import is a command that will search for old personal dictionaries and will import them into GNU Aspell. It will look for both Ispell and Aspell dictionaries. To use it just run it from the command prompt.","Process Name":"aspell-import","Link":"https:\/\/linux.die.net\/man\/1\/aspell-import"}},{"Process":{"Description":"at and batch read commands from standard input or a specified file which are to be executed at a later time. at executes commands at a specified time. atq lists the user's pending jobs, unless the user is the superuser; in that case, everybody's jobs are listed. The format of the output lines (one for each job) is: Job number, date, hour, queue, and username. atrm deletes jobs, identified by their job number. batch executes commands when system load levels permit; in other words, when the load average drops below 0.8, or the value specified in the invocation of atd. At allows fairly complex time specifications, extending the POSIX.2 standard. It accepts times of the form HH:MM to run a job at a specific time of day. (If that time is already past, the next day is assumed.) You may also specify midnight, noon, or teatime (4pm) and you can have a time-of-day suffixed with AM or PM for running in the morning or the evening. You can also say what day the job will be run, by giving a date in the form month-name day with an optional year, or giving a date of the form MMDDYY or MM\/DD\/YY or DD.MM.YY or YYYY-MM-DD. The specification of a date must follow the specification of the time of day. You can also give times like now + count time-units, where the time-units can be minutes, hours, days, or weeks and you can tell at to run the job today by suffixing the time with today and to run the job tomorrow by suffixing the time with tomorrow. For example, to run a job at 4pm three days from now, you would do at 4pm + 3 days, to run a job at 10:00am on July 31, you would do at 10am Jul 31 and to run a job at 1am tomorrow, you would do at 1am tomorrow. The exact definition of the time specification can be found in \/usr\/share\/doc\/at-3.1.10\/timespec. For both at and batch, commands are read from standard input or the file specified with the -f option and executed. The working directory, the environment (except for the variables TERM, DISPLAY and _) and the umask are retained from the time of invocation. An at - or batch - command invoked from a su(1) shell will retain the current userid. The user will be mailed standard error and standard output from his commands, if any. Mail will be sent using the command \/usr\/sbin\/sendmail. If at is executed from a su(1) shell, the owner of the login shell will receive the mail. The superuser may use these commands in any case. For other users, permission to use at is determined by the files \/etc\/at.allow and \/etc\/at.deny. If the file \/etc\/at.allow exists, only usernames mentioned in it are allowed to use at. If \/etc\/at.allow does not exist, \/etc\/at.deny is checked, every username not mentioned in it is then allowed to use at. If neither exists, only the superuser is allowed use of at. An empty \/etc\/at.deny means that every user is allowed use these commands, this is the default configuration.","Process Name":"at","Link":"https:\/\/linux.die.net\/man\/1\/at"}},{"Process":{"Description":"atasm assembles 6502 code to an Atari DOS executable, an Atari XFD disk image, or a raw binary file. The syntax used is compatible with the Atari 8-bit assembler MAC\/65, with a few extensions (notably 6502 illegal opcode support)","Process Name":"atasm","Link":"https:\/\/linux.die.net\/man\/1\/atasm"}},{"Process":{"Description":"aterm, version 1.0.0, is a colour vt102 terminal emulator, based on rxvt 2.4.8 with Alfredo Kojima's additions of fast transparency, intended as an xterm(1) replacement for users who do not require features such as Tektronix 4014 emulation and toolkit-style configurability. As a result, aterm uses much less swap space -- a significant advantage on a machine serving many X sessions. It was created with AfterStep Window Manger users in mind, but is not tied to any libraries, and can be used anywhere.","Process Name":"aterm","Link":"https:\/\/linux.die.net\/man\/1\/aterm"}},{"Process":{"Description":"atftp can be used interactively or in batch mode to retrieve files from TFTP servers. When used interactively, a summary of the commands can be printed by typing 'help'. This TFTP client support all basic feature from RFC1350, RFC2347, RFC2348 and RFC2349. It also support multicast implementation of RFC2090 and mtftp as defined in the PXE specification.","Process Name":"atftp","Link":"https:\/\/linux.die.net\/man\/1\/atftp"}},{"Process":{"Description":"This program is part of Netpbm(1). atktopbm reads an Andrew Toolkit raster object as input. and produces a PBM image as output.","Process Name":"atktopbm","Link":"https:\/\/linux.die.net\/man\/1\/atktopbm"}},{"Process":{"Description":"This is xfishtank writ large: a GL animation of a number of sharks, dolphins, and whales. The swimming motions are great.","Process Name":"atlantis","Link":"https:\/\/linux.die.net\/man\/1\/atlantis"}},{"Process":{"Description":"The bitmap program is a rudimentary tool for creating or editing rectangular images made up of 1's and 0's. Bitmaps are used in X for defining clipping regions, cursor shapes, icon shapes, and tile and stipple patterns. The bmtoa and atobm filters convert bitmap files (FILE FORMAT) to and from ASCII strings. They are most commonly used to quickly print out bitmaps and to generate versions for including in text.","Process Name":"atobm","Link":"https:\/\/linux.die.net\/man\/1\/atobm"}},{"Process":{"Description":"This manual page document describes the atool commands. These commands are used for managing file archives of various types, such as tar and Zip archives. Each command can be executed individually or by giving the appropriate options to atool (see OPTIONS below). aunpack extracts files from an archive. Often one wants to extract all files in an archive to a single subdirectory. However, some archives contain multiple files in their root directories. The aunpack program overcomes this problem by first extracting files to a unique (temporary) directory, and then moving its contents back if possible. This also prevents local files from being overwritten by mistake. apack creates archives (or compresses files). If no file arguments are specified, filenames to add are read from standard in. als lists files in an archive. acat extracts files in an archive to standard out. adiff generates a diff between two archives using diff(1). arepack repacks archives to a different format. It does this by first extracting all files of the old archive into a temporary directory, then packing all files extracted to that directory to the new archive. Use the --each (-e) option in combination with --format (-F) to repack multiple archives using a single invocation of atool. Note that arepack will not remove the old archive. Unless the --format (-F) option is provided, the archive format is determined by the archive file extension. I.e. an extension \".tar.gz\" or \".tgz\" means tar+gzip format. Note that the extensions are checked in the order listed in the section ARCHIVE TYPES below, which is why a file with extension \".tar.gz\" is considered to a be tar+gzip archive, not a gzip compressed file.","Process Name":"atool","Link":"https:\/\/linux.die.net\/man\/1\/atool"}},{"Process":{"Description":"The program atop is an interactive monitor to view the load on a Linux system. It shows the occupation of the most critical hardware resources (from a performance point of view) on system level, i.e. cpu, memory, disk and network. It also shows which processes are responsible for the indicated load with respect to cpu- and memory load on process level. Disk load is shown if per process \"storage accounting\" is active in the kernel or if the kernel patch 'cnt' has been installed. Network load is only shown per process if the kernel patch 'cnt' has been installed. Every interval (default: 10 seconds) information is shown about the resource occupation on system level (cpu, memory, disks and network layers), followed by a list of processes which have been active during the last interval (note that all processes that were unchanged during the last interval are not shown, unless the key 'a' has been pressed). If the list of active processes does not entirely fit on the screen, only the top of the list is shown (sorted in order of activity). The intervals are repeated till the number of samples (specified as command argument) is reached, or till the key 'q' is pressed in interactive mode. When atop is started, it checks whether the standard output channel is connected to a screen, or to a file\/pipe. In the first case it produces screen control codes (via the ncurses library) and behaves interactively; in the second case it produces flat ASCII-output. In interactive mode, the output of atop scales dynamically to the current dimensions of the screen\/window. If the window is resized horizontally, columns will be added or removed automatically. For this purpose, every column has a particular weight. The columns with the highest weigths that fit within the current width will be shown. If the window is resized vertically, lines of the process-list will be added or removed automatically. Furthermore in interactive mode the output of atop can be controlled by pressing particular keys. However it is also possible to specify such key as flag on the command line. In the latter case atop will switch to the indicated mode on beforehand; this mode can be modified again interactively. Specifying such key as flag is especially useful when running atop with output to a pipe or file (non-interactively). The flags used are the same as the keys which can be pressed in interactive mode (see section INTERACTIVE COMMANDS). Additional flags are available to support storage of atop-data in raw format (see section RAW DATA STORAGE).","Process Name":"atop","Link":"https:\/\/linux.die.net\/man\/1\/atop"}},{"Process":{"Description":"The program atopsar can be used to report statistics on system level. In the first synopsis line (no sampling interval specified), atopsar extracts data from a raw logfile that has been recorded previously by the program atop (option -w of the atop program). You can specify the name of the logfile with the -r option of the atopsar program. When a daily logfile of atop is used, named \/var\/log\/atop\/atop_YYYYMMDD (where YYYYMMDD reflects the date), the required date of the form YYYYMMDD can be specified with the -r option instead of the filename, or the symbolic name 'y' can be used for yesterday's daily logfile (this can be repeated so 'yyyy' indicates the logfile of four days ago). If the -r option is not specified at all, today's daily logfile is used by default. The starting and ending times of the report can be defined using the options -b and -e followed by a time argument of the form hh:mm. In the second synopsis line, atopsar reads actual activity counters from the kernel with the specified interval (in seconds) and the specified number of samples (optionally). When atopsar is activated in this way it immediately sends the output for every requested report to standard output. If only one type of report is requested, the header is printed once and after every interval seconds the statistical counters are shown for that period. If several reports are requested, a header is printed per sample followed by the statistical counters for that period. Some generic flags can be specified to influence the behaviour of the atopsar program: -S By default the timestamp at the beginning of a line is suppressed if more lines are shown for one interval. With this flag a timestamp is given for every output-line (easier for post-processing). -a By default certain resources as disks and network interfaces are only shown when they were active during the interval. With this flag all resources of a given type are shown, even if they were inactive during the interval. -x By default atopsar only uses colors if output is directed to a terminal (window). These colors might indicate that a critical occupation percentage has been reached (red) or has been almost reached (cyan) for a particular resource. See the man-page of atop for a detailed description of this feature (section COLORS). With the flag -x the use of colors is suppressed unconditionally. -C By default atopsar only uses colors if output is directed to a terminal (window). These colors might indicate that a critical occupation percentage has been reached (red) or has been almost reached (cyan) for a particular resource. See the man-page of atop for a detailed description of this feature (section COLORS). With the flag -C colors will always be used, even if output is not directed to a terminal. -M Use markers at the end of a line to indicate that a critical occupation percentage has been reached ('*') or has been almost reached ('+') for particular resources. The marker '*' is similar to the color red and the marker '+' to the color cyan. See the man-page of atop for a detailed description of these colors (section COLORS). -H Repeat the header line within a report for every N detail lines. The value of N is determined dynamically in case of output to a tty\/window (depending on the number of lines); for output to a file or pipe this value is 23. -R Summarize cnt samples into one sample. When the logfile contains e.g. samples of 10 minutes, the use of the flag '-R 6' shows a report with one sample for every hour. Other flags are used to define which reports are required: -A Show all possible reports. -c Report about CPU utilization (in total and per cpu). -p Report about processor-related matters, like load-averages and hardware interrupts. -P Report about processes. -m Current memory- and swap-occupation. -s Report about paging- and swapping-activity, and overcommitment. -l Report about utilization of logical volumes. -f Report about utilization of multiple devices. -d Report about utilization of disks. -i Report about the network interfaces. -I Report about errors for network-interfaces. -w Report about IP version 4 network traffic. -W Report about errors for IP version 4 traffic. -y General report about ICMP version 4 layer activity. -Y Per-type report about ICMP version 4 layer activity. -u Report about UDP version 4 network traffic. -z Report about IP version 6 network traffic. -Z Report about errors for IP version 6 traffic. -k General report about ICMP version 6 layer activity. -K Per-type report about ICMP version 6 layer activity. -U Report about UDP version 6 network traffic. -t Report about TCP network traffic. -T Report about errors for TCP-traffic. -O Report about top-3 processes consuming most processor capacity. -G Report about top-3 processes consuming most resident memory. -D Report about top-3 processes issueing most disk transfers. -N Report about top-3 processes issueing most IPv4\/IPv6 socket transfers.","Process Name":"atopsar","Link":"https:\/\/linux.die.net\/man\/1\/atopsar"}},{"Process":{"Description":"at and batch read commands from standard input or a specified file which are to be executed at a later time. at executes commands at a specified time. atq lists the user's pending jobs, unless the user is the superuser; in that case, everybody's jobs are listed. The format of the output lines (one for each job) is: Job number, date, hour, queue, and username. atrm deletes jobs, identified by their job number. batch executes commands when system load levels permit; in other words, when the load average drops below 0.8, or the value specified in the invocation of atd. At allows fairly complex time specifications, extending the POSIX.2 standard. It accepts times of the form HH:MM to run a job at a specific time of day. (If that time is already past, the next day is assumed.) You may also specify midnight, noon, or teatime (4pm) and you can have a time-of-day suffixed with AM or PM for running in the morning or the evening. You can also say what day the job will be run, by giving a date in the form month-name day with an optional year, or giving a date of the form MMDDYY or MM\/DD\/YY or DD.MM.YY or YYYY-MM-DD. The specification of a date must follow the specification of the time of day. You can also give times like now + count time-units, where the time-units can be minutes, hours, days, or weeks and you can tell at to run the job today by suffixing the time with today and to run the job tomorrow by suffixing the time with tomorrow. For example, to run a job at 4pm three days from now, you would do at 4pm + 3 days, to run a job at 10:00am on July 31, you would do at 10am Jul 31 and to run a job at 1am tomorrow, you would do at 1am tomorrow. The exact definition of the time specification can be found in \/usr\/share\/doc\/at-3.1.10\/timespec. For both at and batch, commands are read from standard input or the file specified with the -f option and executed. The working directory, the environment (except for the variables TERM, DISPLAY and _) and the umask are retained from the time of invocation. An at - or batch - command invoked from a su(1) shell will retain the current userid. The user will be mailed standard error and standard output from his commands, if any. Mail will be sent using the command \/usr\/sbin\/sendmail. If at is executed from a su(1) shell, the owner of the login shell will receive the mail. The superuser may use these commands in any case. For other users, permission to use at is determined by the files \/etc\/at.allow and \/etc\/at.deny. If the file \/etc\/at.allow exists, only usernames mentioned in it are allowed to use at. If \/etc\/at.allow does not exist, \/etc\/at.deny is checked, every username not mentioned in it is then allowed to use at. If neither exists, only the superuser is allowed use of at. An empty \/etc\/at.deny means that every user is allowed use these commands, this is the default configuration.","Process Name":"atq","Link":"https:\/\/linux.die.net\/man\/1\/atq"}},{"Process":{"Description":"at and batch read commands from standard input or a specified file which are to be executed at a later time. at executes commands at a specified time. atq lists the user's pending jobs, unless the user is the superuser; in that case, everybody's jobs are listed. The format of the output lines (one for each job) is: Job number, date, hour, queue, and username. atrm deletes jobs, identified by their job number. batch executes commands when system load levels permit; in other words, when the load average drops below 0.8, or the value specified in the invocation of atd. At allows fairly complex time specifications, extending the POSIX.2 standard. It accepts times of the form HH:MM to run a job at a specific time of day. (If that time is already past, the next day is assumed.) You may also specify midnight, noon, or teatime (4pm) and you can have a time-of-day suffixed with AM or PM for running in the morning or the evening. You can also say what day the job will be run, by giving a date in the form month-name day with an optional year, or giving a date of the form MMDDYY or MM\/DD\/YY or DD.MM.YY or YYYY-MM-DD. The specification of a date must follow the specification of the time of day. You can also give times like now + count time-units, where the time-units can be minutes, hours, days, or weeks and you can tell at to run the job today by suffixing the time with today and to run the job tomorrow by suffixing the time with tomorrow. For example, to run a job at 4pm three days from now, you would do at 4pm + 3 days, to run a job at 10:00am on July 31, you would do at 10am Jul 31 and to run a job at 1am tomorrow, you would do at 1am tomorrow. The exact definition of the time specification can be found in \/usr\/share\/doc\/at-3.1.10\/timespec. For both at and batch, commands are read from standard input or the file specified with the -f option and executed. The working directory, the environment (except for the variables TERM, DISPLAY and _) and the umask are retained from the time of invocation. An at - or batch - command invoked from a su(1) shell will retain the current userid. The user will be mailed standard error and standard output from his commands, if any. Mail will be sent using the command \/usr\/sbin\/sendmail. If at is executed from a su(1) shell, the owner of the login shell will receive the mail. The superuser may use these commands in any case. For other users, permission to use at is determined by the files \/etc\/at.allow and \/etc\/at.deny. If the file \/etc\/at.allow exists, only usernames mentioned in it are allowed to use at. If \/etc\/at.allow does not exist, \/etc\/at.deny is checked, every username not mentioned in it is then allowed to use at. If neither exists, only the superuser is allowed use of at. An empty \/etc\/at.deny means that every user is allowed use these commands, this is the default configuration.","Process Name":"atrm","Link":"https:\/\/linux.die.net\/man\/1\/atrm"}},{"Process":{"Description":"attila automates the process of compiling and installing one or more Alliance tools. The tool can be installed either in the user's account (during the devellopment stage) or in the Alliance system wide tree (for instance \/asim\/alliance) when a new version is made avalaible to all. attila proceed with the following steps : 1. Checks if the sources of tools are present in the user's account. If not, check them out from the Alliance CVS tree. Note that you must have access to it. 2. In case of --asim or --full installations, attila will fork itself on one Linux computer (currently bip) and on one Solaris computer (beny). As to connect on thoses computer it will uses rsh so you must setup your ~\/.rhosts to access them whithout passwords. You also can uses ssh (but the procedure to allow automatic login is more complicated). 3. Run autostuff for the tool in the ~\/alliance\/src directory. 4. Run configure in the build directory (see below). 5. Install the tool in the local install directory (see below) or in the system-wide Alliance directory rooted under \/asim\/alliance. After an --asim install: the build directory tree of the tool will be removed to avoid messing with further local installations. DIRECTORY STRUCTURE attila relies on the following tree structure : (all paths below are given relative to the user's home directory) \u2022 ~\/alliance\/src where the tools sources are to be found. \u2022 ~\/alliance\/Linux\/build\/$TOOL : the top directory under which the tool will be compiled for Linux. This is where the configure script will be run. \u2022 ~\/alliance\/Linux\/install : the top of the install tree when the tool is compiled locally for Linux. Under this directory you will found (at least) : .\/bin, .\/lib and .\/include. \u2022 ~\/alliance\/Solaris\/build\/$TOOL : the tool's build directory for Solaris. \u2022 ~\/alliance\/Solaris\/install : top of the local install tree for Solaris. CVS CHECKOUT If the sources of the requested tool(s) are not found under ~\/alliance\/src\/ attila will try to check them out. So, as says above, you must have access rights to the Alliance CVS tree. In addition to the tool(s) sources, it will also checks for the minimal set of files needed for configure to run. As for now : \u2022 autostuff \u2022 alliance.m4 \u2022 motif.m4 \u2022 Makefile.am GUESSING CVSROOT The root of the CVS tree will be set according to the following rules : 1. Uses the user's environment variable CVSROOT if sets. 2. Uses the attila default value sets in attila.conf (variable ATTILA_CVSROOT).","Process Name":"attila","Link":"https:\/\/linux.die.net\/man\/1\/attila"}},{"Process":{"Description":"The attr utility allows the manipulation of extended attributes associated with filesystem objects from within shell scripts. There are four main operations that attr can perform: GET The -g attrname option tells attr to search the named object and print (to stdout) the value associated with that attribute name. With the -q flag, stdout will be exactly and only the value of the attribute, suitable for storage directly into a file or processing via a piped command. LIST The -l option tells attr to list the names of all the attributes that are associated with the object, and the number of bytes in the value of each of those attributes. With the -q flag, stdout will be a simple list of only the attribute names, one per line, suitable for input into a script. REMOVE The -r attrname option tells attr to remove an attribute with the given name from the object if the attribute exists. There is no output on sucessful completion. SET\/CREATE The -s attrname option tells attr to set the named attribute of the object to the value read from stdin. If an attribute with that name already exists, its value will be replaced with this one. If an attribute with that name does not already exist, one will be created with this value. With the -V attrvalue flag, the attribute will be set to have a value of attrvalue and stdin will not be read. With the -q flag, stdout will not be used. Without the -q flag, a message showing the attribute name and the entire value will be printed. When the -L option is given and the named object is a symbolic link, operate on the attributes of the object referenced by the symbolic link. Without this option, operate on the attributes of the symbolic link itself. When the -R option is given and the process has appropriate privileges, operate in the root attribute namespace rather that the USER attribute namespace. The -S option is similar, except it specifies use of the security attribute namespace. When the -q option is given attr will try to keep quiet. It will output error messages (to stderr) but will not print status messages (to stdout).","Process Name":"attr","Link":"https:\/\/linux.die.net\/man\/1\/attr"}},{"Process":{"Description":"The attraction program has several visually different modes of operation, all of which are based on the interactions of a set of control points which attract each other up to a certain distance, and then begin to repel each other. The attraction\/repulsion is proportional to the distance between any two particles.","Process Name":"attraction","Link":"https:\/\/linux.die.net\/man\/1\/attraction"}},{"Process":{"Description":"The atunnel program draws an animation of a journey in a GL tunnel.","Process Name":"atunnel","Link":"https:\/\/linux.die.net\/man\/1\/atunnel"}},{"Process":{"Description":"The auconvert program can be used to convert from one sound file format, and\/or data format to another. It can also change the comment, sampling rate, and volume of a sound file. Finally, auconvert can be used to convert raw audio data into a sound file. If no input file name is given, standard input will be used. If no output file name is given, the original file will be replaced by the converted file or standard out will be used if the input is coming from standard input.","Process Name":"auconvert","Link":"https:\/\/linux.die.net\/man\/1\/auconvert"}},{"Process":{"Description":"The auctl program can be used to control various audio server parameters. It is typically used when connecting up new devices, or to set user preferences. If no command is specified on the program command line, auctl reads commands from the standard output. If the standard input and standard output are both attached to a terminal, an interactive prompt is provided.","Process Name":"auctl","Link":"https:\/\/linux.die.net\/man\/1\/auctl"}},{"Process":{"Description":"Audacious is a media player, based on Beep Media Player, which is in turn based on the X Multimedia System. It is used to play audio and other kinds of media files. By default Audacious can play MPEG audio, Ogg Vorbis, RIFF wav, most module formats, and a few other formats. Audacious can be extended through plugins to play a number of other audio and video formats.","Process Name":"audacious","Link":"https:\/\/linux.die.net\/man\/1\/audacious"}},{"Process":{"Description":"Audacity is a graphical audio editor. This man page does not describe all of the features of Audacity or how to use it; for this, see the html documentation that came with the program, which should be accessible from the Help menu. This man page describes the Unix-specific features, including special files and environment variables. Audacity currently uses libsndfile to open many uncompressed audio formats such as WAV, AIFF, and AU, and it can also be linked to libmad, libvorbis, and libflac, to provide support for opening MP2\/3, Ogg Vorbis, and FLAC files, respectively. LAME, libvorbis, libflac and libtwolame provide facilities to export files to all these formats as well. Audacity is primarily an interactive, graphical editor, not a batch-processing tool. Whilst there is a basic batch processing tool it is experimental and incomplete. If you need to batch-process audio or do simple edits from the command line, using sox or ecasound driven by a bash script will be much more powerful than audacity.","Process Name":"audacity","Link":"https:\/\/linux.die.net\/man\/1\/audacity"}},{"Process":{"Description":"audemo provides an X-based window-oriented user interface to the Network Audio System service. It allows the user to play pre-recorded sound files, record new sound files and to manipulate Network Audio System buckets.","Process Name":"audemo","Link":"https:\/\/linux.die.net\/man\/1\/audemo"}},{"Process":{"Description":"The audial program generates touch tones suitable for dialing a North American telephone. audial can also recognize touch tones and produce the corresponding string.","Process Name":"audial","Link":"https:\/\/linux.die.net\/man\/1\/audial"}},{"Process":{"Description":"The script takes a list of files (or, with -R option, directories) and renames the given files (or audio files in the directories) according to the rules specified through the command line options. File extensions are preserved (by default). Some \"companion\" files (i.e., files with the same basename, and with an extension from a certain list) may be renamed together with audio files. A lot of care is taken to make the resulting file names as portable as possible: e.g., \"funny\" characters in file names are dumbed down (unless requested otherwise), long filename components may be shortened to certain limits. A care is taken so that renaming will not overwrite existing files; however, on OSes which allow rename() to overwrite files, race conditions can ruin the best intentions. E.g., do not run several \"overlapping\" rename procedures simultaneously!","Process Name":"audio_rename","Link":"https:\/\/linux.die.net\/man\/1\/audio_rename"}},{"Process":{"Description":"This utility scans the logs for messages logged when the system denied permission for operations, and generates a snippet of policy rules which, if loaded into policy, might have allowed those operations to succeed. However, this utility only generates Type Enforcement (TE) allow rules. Certain permission denials may require other kinds of policy changes, e.g. adding an attribute to a type declaration to satisfy an existing constraint, adding a role allow rule, or modifying a constraint. The audit2why(8) utility may be used to diagnose the reason when it is unclear. Care must be exercised while acting on the output of this utility to ensure that the operations being permitted do not pose a security threat. Often it is better to define new domains and\/or types, or make other structural changes to narrowly allow an optimal set of operations to succeed, as opposed to blindly implementing the sometimes broad changes recommended by this utility. Certain permission denials are not fatal to the application, in which case it may be preferable to simply suppress logging of the denial via a 'dontaudit' rule rather than an 'allow' rule.","Process Name":"audit2allow","Link":"https:\/\/linux.die.net\/man\/1\/audit2allow"}},{"Process":{"Description":"This utility scans the logs for messages logged when the system denied permission for operations, and generates a snippet of policy rules which, if loaded into policy, might have allowed those operations to succeed. However, this utility only generates Type Enforcement (TE) allow rules. Certain permission denials may require other kinds of policy changes, e.g. adding an attribute to a type declaration to satisfy an existing constraint, adding a role allow rule, or modifying a constraint. The audit2why(8) utility may be used to diagnose the reason when it is unclear. Care must be exercised while acting on the output of this utility to ensure that the operations being permitted do not pose a security threat. Often it is better to define new domains and\/or types, or make other structural changes to narrowly allow an optimal set of operations to succeed, as opposed to blindly implementing the sometimes broad changes recommended by this utility. Certain permission denials are not fatal to the application, in which case it may be preferable to simply suppress logging of the denial via a 'dontaudit' rule rather than an 'allow' rule.","Process Name":"audit2why","Link":"https:\/\/linux.die.net\/man\/1\/audit2why"}},{"Process":{"Description":"audtool is designed to send commands to a running audacious. It can handle various things like jumping to the next track in playlist or returning the current playing track title, as well as other status information.","Process Name":"audtool","Link":"https:\/\/linux.die.net\/man\/1\/audtool"}},{"Process":{"Description":"auedit provides an X-based window-oriented interface allowing the user to record and edit audio files. The editing features available include: o Cutting, copying, pasting, and mixing within a single auedit instance or between multiple auedit instances. o Changing the amplitude of an interval. o Reversing an interval. o Fading in or out an interval.","Process Name":"auedit","Link":"https:\/\/linux.die.net\/man\/1\/auedit"}},{"Process":{"Description":"Execute an Augeas module, most commonly to evaluate the tests it contains.","Process Name":"augparse","Link":"https:\/\/linux.die.net\/man\/1\/augparse"}},{"Process":{"Description":"Augeas is a configuration editing tool. It parses configuration files in their native formats and transforms them into a tree. Configuration changes are made by manipulating this tree and saving it back into native config files. augtool provides a command line interface to the generated tree. COMMAND can be a single command as described under \" COMMANDS \". When called with no COMMAND , it reads commands from standard input until an end-of-file is encountered.","Process Name":"augtool","Link":"https:\/\/linux.die.net\/man\/1\/augtool"}},{"Process":{"Description":"The auinfo program provides information describing a Network Audio System server. It lists the capabilities of the server, the devices that are attached to it, the data formats that are accepted, and any predefined sounds that have been stored in the server.","Process Name":"auinfo","Link":"https:\/\/linux.die.net\/man\/1\/auinfo"}},{"Process":{"Description":"This program adjusts the settings of an audio mixing device. It can be used from the command line, in scripts, or interactively with the keyboard or mouse.","Process Name":"aumix","Link":"https:\/\/linux.die.net\/man\/1\/aumix"}},{"Process":{"Description":"This manual page document describes the atool commands. These commands are used for managing file archives of various types, such as tar and Zip archives. Each command can be executed individually or by giving the appropriate options to atool (see OPTIONS below). aunpack extracts files from an archive. Often one wants to extract all files in an archive to a single subdirectory. However, some archives contain multiple files in their root directories. The aunpack program overcomes this problem by first extracting files to a unique (temporary) directory, and then moving its contents back if possible. This also prevents local files from being overwritten by mistake. apack creates archives (or compresses files). If no file arguments are specified, filenames to add are read from standard in. als lists files in an archive. acat extracts files in an archive to standard out. adiff generates a diff between two archives using diff(1). arepack repacks archives to a different format. It does this by first extracting all files of the old archive into a temporary directory, then packing all files extracted to that directory to the new archive. Use the --each (-e) option in combination with --format (-F) to repack multiple archives using a single invocation of atool. Note that arepack will not remove the old archive. Unless the --format (-F) option is provided, the archive format is determined by the archive file extension. I.e. an extension \".tar.gz\" or \".tgz\" means tar+gzip format. Note that the extensions are checked in the order listed in the section ARCHIVE TYPES below, which is why a file with extension \".tar.gz\" is considered to a be tar+gzip archive, not a gzip compressed file.","Process Name":"aunpack","Link":"https:\/\/linux.die.net\/man\/1\/aunpack"}},{"Process":{"Description":"aupanel provides an X-based window-oriented interface allowing the user to adjust the attributes of the devices provided by the Network Audio System service.","Process Name":"aupanel","Link":"https:\/\/linux.die.net\/man\/1\/aupanel"}},{"Process":{"Description":"auphone allows two-way real time voice communication between two audio servers.","Process Name":"auphone","Link":"https:\/\/linux.die.net\/man\/1\/auphone"}},{"Process":{"Description":"The auplay program can be used to play audio data stored in the .SND, .AU, or .WAV formats common on Sun workstations and PCs. It is typically used from shell scripts or command line procedures. If no filenames are given on the command line, audio data will be read fron stdin, unless the -l option is given.","Process Name":"auplay","Link":"https:\/\/linux.die.net\/man\/1\/auplay"}},{"Process":{"Description":"The aurecord program can be used to record audio data from a Network Audio System server into a file. The recording will continue until either the specified time has elapsed or aurecord receives a SIGINT or SIGTERM signal.","Process Name":"aurecord","Link":"https:\/\/linux.die.net\/man\/1\/aurecord"}},{"Process":{"Description":"auscope is an audio protocol filter that can be used to view the network packets being sent between an audio application and an audio server. auscope is written in Perl, so you must have Perl installed on your machine in order to run auscope. If your Perl executable is not installed as \/usr\/local\/bin\/perl, you should modify the first line of the auscope script to reflect the Perl executable's location. Or, you can invoke auscope as perl auscope [ option ] ... assuming the Perl executable is in your path. To operate, auscope must know the port on which it should listen for audio clients, the name of the desktop machine on which the audio server is running and the port to use to connect to the audio server. Both the output port (server) and input port (client) are automatically biased by 8000. The output port defaults to 0 and the input port defaults to 1.","Process Name":"auscope","Link":"https:\/\/linux.die.net\/man\/1\/auscope"}},{"Process":{"Description":"aut is a set of utilities functions and types that may be useful. Types : authelem - Hash table element type. authtable - Hash table type. auth2elem - Hash table element type. auth2table - Hash table type. Functions : autallocblock - memory allocator autallocheap - heap memory allocator autresizeblock - resizes a memory block autfreeblock - releases a memory block autfreeheap - releases an heap memory block. autexit - encapsulates exit function. createauthtable - creates a simple hash table. destroyauthtable - destroys a simple hash table. resetauthtable - resets a simple hash table. addauthelem - adds an element in the hash table. delauthelem - deletes an element in the hash table. searchauthelem - searches an element in the hash table. viewauthelem - displays an hash table element. viewauthtable - displays an hash table. createauth2table - creates an hash table with two keys. destroyauth2table - destroys an hash table with two keys. resetauth2table - resets an hash table with two keys. addauth2elem - adds an element in the hash table. delauth2elem - deletes an element in the hash table. searchauth2elem - searches an element in the hash table. viewauth2elem - displays an hash table element. viewauth2table - displays an hash table with two keys. sortautcompare - default heap sort comparison function. sortautarray - heap sort. libAut101.a : autallocblock, autallocblock, autresizeblock, autfreeblock, autfreeheap, autexit, setauthfunc, getauthsize, getauthkey, getauthindex, checkauthkey, createauthtable, destroyauthtable, resetauthtable, stretchauthtable, addauthelem, delauthelem, searchauthelem, viewauthelem, viewauthtable, setauth2func, getauth2size, getauth2key, getauth2index, checkauth2key, createauth2table, destroyauth2table, resetauth2table, stretchauth2table, addauth2elem, delauth2elem, searchauth2elem, viewauth2elem, viewauth2table, sortautcompare, sortautarray,","Process Name":"aut","Link":"https:\/\/linux.die.net\/man\/1\/aut"}},{"Process":{"Description":"This is to manage a users.dat type file for user logins into scripts via the web the main argument is the path to where the user.dat file is or you want it to be at.","Process Name":"authman","Link":"https:\/\/linux.die.net\/man\/1\/authman"}},{"Process":{"Description":"","Process Name":"auto-build","Link":"https:\/\/linux.die.net\/man\/1\/auto-build"}},{"Process":{"Description":"The build engine root contains a number of directories in which state is stored, or work performed. In unusual circumstances, some of this state can become malformed\/corrupt which may cause the build engine to fail. The \"auto-build-clean-root\" script provides a convenience for removing all state from the build root","Process Name":"auto-build-clean-root","Link":"https:\/\/linux.die.net\/man\/1\/auto-build-clean-root"}},{"Process":{"Description":"The build engine expects a number of stub directories to be created within the build root. Since this set of directories may change over time, the \"auto-build-make-root\" script provides a convenience for populating all pre-requisite directories.","Process Name":"auto-build-make-root","Link":"https:\/\/linux.die.net\/man\/1\/auto-build-make-root"}},{"Process":{"Description":"Generate a configuration script from a TEMPLATE-FILE if given, or 'configure.ac' if present, or else 'configure.in'. Output is sent to the standard output if TEMPLATE-FILE is given, else into 'configure'. Operation modes: -h, --help print this help, then exit -V, --version print version number, then exit -v, --verbose verbosely report processing -d, --debug don't remove temporary files -f, --force consider all files obsolete -o, --output= FILE save output in FILE (stdout is the default) -W, --warnings= CATEGORY report the warnings falling in CATEGORY [syntax] Warning categories include: 'cross' cross compilation issues 'obsolete' obsolete constructs 'syntax' dubious syntactic constructs 'all' all the warnings 'no-CATEGORY' turn off the warnings on CATEGORY 'none' turn off all the warnings 'error' warnings are error The environment variables 'M4' and 'WARNINGS' are honored. Library directories: -B, --prepend-include= DIR prepend directory DIR to search path -I, --include= DIR append directory DIR to search path Tracing: -t, --trace= MACRO[:FORMAT] report the list of calls to MACRO -i, --initialization also trace Autoconf's initialization process In tracing mode, no configuration script is created. FORMAT defaults to '$f:$l:$n:$%'; see 'autom4te --help' for information about FORMAT.","Process Name":"autoconf","Link":"https:\/\/linux.die.net\/man\/1\/autoconf"}},{"Process":{"Description":"","Process Name":"autoexpect","Link":"https:\/\/linux.die.net\/man\/1\/autoexpect"}},{"Process":{"Description":"The autoflat command compose a set of flar frames and makes one flat frame called 'master-flat'. Applying this function, you can achieve high quality correction frame and thus reducing the noise of a result. All source frames must be in the FITS format and of same dimensions. Frames of the same exposition duration and color filter should be used, avoid eventual camera rotation on its mount. The output file is written in the FITS format too.","Process Name":"autoflat","Link":"https:\/\/linux.die.net\/man\/1\/autoflat"}},{"Process":{"Description":"autofsd-probe will check the status of the autofsd(1) daemon on the specified host. Unless directed to another host by the -h option, autofsd-probe will contact the AutoFS daemon on the local host. The AutoFS file system is built on the Remote Procedure Call (rpc(3)) library routines. The -t option allows the total timeout and retry timeout intervals to be set for all remote procedure call operations used with autofsd-probe. This option accepts an interval argument in the form described in the pcpintro(1) manual page. autofsd-probe is typically used in an automated fashion from within pmdashping(1) and in conjunction with pmie(1), for monitoring response time and service failure. By default autofsd-probe will not produce any output, unless there is an error in which case a diagnostic message will be displayed and the exit status will indicate the reason for failure.","Process Name":"autofsd-probe","Link":"https:\/\/linux.die.net\/man\/1\/autofsd-probe"}},{"Process":{"Description":"The autogsdoc tool is a command-line utility that helps developers produce reference documentation for GNUstep APIs. It also enables developers to write and maintain other documentation in XML and have it converted to HTML. In detail, autogsdoc will: - Extract special comments describing the public interfaces of classes, categories, protocols, functions, and macros from Objective C source code (header files and optionally source files) into GSDoc XML files. - Convert GSDoc XML files, whether generated from source code or written manually by developers, into HTML. - Construct indices based on GSDoc XML file sets, and convert those to HTML as well. The most common usage this is to run the command with one or more header file names as arguments ... the tool will automatically parse corresponding source files in the same directory as the headers (or the current directory, or the directory specified using the DocumentationDirectory default), and produce GSDoc and HTML files as output. For best results this mode should be run from the directory containing the source files. (Note that since C is a subset of Objective C, this tool can operate to document functions and other C structures in plain C source.) GSDoc files may also be given directly in addition or by themselves, and will be converted to HTML. See the GSDoc HTML documentation or the gsdoc(7) man page for information on the GSDoc format. Finally, HTML files may be given on the command line. Cross-references to other parts of code documentation found within them will be rewritten based on what is found in the project currently.","Process Name":"autogsdoc","Link":"https:\/\/linux.die.net\/man\/1\/autogsdoc"}},{"Process":{"Description":"Create a template file of C '#define' statements for 'configure' to use. To this end, scan TEMPLATE-FILE, or 'configure.ac' if present, or else 'configure.in'. -h, --help print this help, then exit -V, --version print version number, then exit -v, --verbose verbosely report processing -d, --debug don't remove temporary files -f, --force consider all files obsolete -W, --warnings= CATEGORY report the warnings falling in CATEGORY Warning categories include: 'cross' cross compilation issues 'gnu' GNU coding standards (default in gnu and gnits modes) 'obsolete' obsolete features or constructions 'override' user redefinitions of Automake rules or variables 'portability' portability issues (default in gnu and gnits modes) 'syntax' dubious syntactic constructs (default) 'unsupported' unsupported or incomplete features (default) 'all' all the warnings 'no-CATEGORY' turn off warnings in CATEGORY 'none' turn off all the warnings 'error' treat warnings as errors Library directories: -B, --prepend-include= DIR prepend directory DIR to search path -I, --include= DIR append directory DIR to search path","Process Name":"autoheader","Link":"https:\/\/linux.die.net\/man\/1\/autoheader"}},{"Process":{"Description":"One of the most used shell commands is \"cd\". A quick survey among my friends revealed that between 10 and 20% of all commands they type are actually cd commands! Unfortunately, jumping from one part of your system to another with cd requires to enter almost the full path, which isn't very practical and requires a lot of keystrokes. autojump is a faster way to navigate your filesystem. It works by maintaining a database of the directories you use the most from the command line. The jumpstat command shows you the current contents of the database. You need to work a little bit before the database becomes useable. Once your database is reasonably complete, you can \"jump\" to a directory by typing: j dirspec where dirspec is a few characters of the directory you want to jump to. It will jump to the most used directory whose name matches the pattern given in dirspec. Note that autojump isn't meant to be a drop-in replacement for cd, but rather a complement. Cd is fine when staying in the same area of the filesystem; autojump is there to help when you need to jump far away from your current location. Autojump supports autocompletion. Try it!","Process Name":"autojump","Link":"https:\/\/linux.die.net\/man\/1\/autojump"}},{"Process":{"Description":"Run GNU M4 on the FILES, avoiding useless runs. Output the traces if tracing, the frozen file if freezing, otherwise the expansion of the FILES. If some of the FILES are named 'FILE.m4f' they are considered to be M4 frozen files of all the previous files (which are therefore not loaded). If 'FILE.m4f' is not found, then 'FILE.m4' will be used, together with all the previous files. Some files may be optional, i.e., will only be processed if found in the include path, but then must end in '.m4?'; the question mark is not part of the actual file name. Operation modes: -h, --help print this help, then exit -V, --version print version number, then exit -v, --verbose verbosely report processing -d, --debug don't remove temporary files -o, --output= FILE save output in FILE (defaults to '-', stdout) -f, --force don't rely on cached values -W, --warnings= CATEGORY report the warnings falling in CATEGORY -l, --language= LANG specify the set of M4 macros to use -C, --cache= DIRECTORY preserve results for future runs in DIRECTORY --no-cache disable the cache -m, --mode= OCTAL change the non trace output file mode (0666) -M, --melt don't use M4 frozen files Languages include: 'Autoconf' create Autoconf configure scripts 'Autotest' create Autotest test suites 'M4sh' create M4sh shell scripts 'M4sugar' create M4sugar output Warning categories include: 'cross' cross compilation issues 'gnu' GNU coding standards (default in gnu and gnits modes) 'obsolete' obsolete features or constructions 'override' user redefinitions of Automake rules or variables 'portability' portability issues (default in gnu and gnits modes) 'syntax' dubious syntactic constructs (default) 'unsupported' unsupported or incomplete features (default) 'all' all the warnings 'no-CATEGORY' turn off warnings in CATEGORY 'none' turn off all the warnings 'error' treat warnings as errors The environment variables 'M4' and 'WARNINGS' are honored. Library directories: -B, --prepend-include= DIR prepend directory DIR to search path -I, --include= DIR append directory DIR to search path Tracing: -t, --trace= MACRO[:FORMAT] report the MACRO invocations -p, --preselect= MACRO prepare to trace MACRO in a future run Freezing: -F, --freeze produce an M4 frozen state file for FILES FORMAT defaults to '$f:$l:$n:$%', and can use the following escapes: $$ literal $ $f file where macro was called $l line where macro was called $d nesting depth of macro call $n name of the macro $NUM argument NUM, unquoted and with newlines $SEP@ all arguments, with newlines, quoted, and separated by SEP $SEP* all arguments, with newlines, unquoted, and separated by SEP $SEP% all arguments, without newlines, unquoted, and separated by SEP SEP can be empty for the default (comma for @ and *, colon for %), a single character for that character, or {STRING} to use a string.","Process Name":"autom4te","Link":"https:\/\/linux.die.net\/man\/1\/autom4te"}},{"Process":{"Description":"Generate Makefile.in for configure from Makefile.am. Operation modes: --help print this help, then exit --version print version number, then exit -v, --verbose verbosely list files processed --no-force only update Makefile.in's that are out of date -W, --warnings= CATEGORY report the warnings falling in CATEGORY Dependency tracking: -i, --ignore-deps disable dependency tracking code --include-deps enable dependency tracking code Flavors: --cygnus assume program is part of Cygnus-style tree --foreign set strictness to foreign --gnits set strictness to gnits --gnu set strictness to gnu Library files: -a, --add-missing add missing standard files to package --libdir= DIR directory storing library files -c, --copy with -a, copy missing files (default is symlink) -f, --force-missing force update of standard files Warning categories include: 'gnu' GNU coding standards (default in gnu and gnits modes) 'obsolete' obsolete features or constructions 'override' user redefinitions of Automake rules or variables 'portability' portability issues (default in gnu and gnits modes) 'syntax' dubious syntactic constructs (default) 'unsupported' unsupported or incomplete features (default) 'all' all the warnings 'no-CATEGORY' turn off warnings in CATEGORY 'none' turn off all the warnings 'error' treat warnings as errors Files which are automatically distributed, if found: ABOUT-GNU README config.rpath ltcf-gcj.sh ABOUT-NLS THANKS config.sub ltconfig AUTHORS TODO configure ltmain.sh BACKLOG acconfig.h configure.ac mdate-sh COPYING aclocal.m4 configure.in missing COPYING.DOC ansi2knr.1 depcomp mkinstalldirs COPYING.LESSER ansi2knr.c elisp-comp py-compile COPYING.LIB compile install-sh stamp-vti ChangeLog config.guess libversion.in texinfo.tex INSTALL config.h.bot ltcf-c.sh ylwrap NEWS config.h.top ltcf-cxx.sh","Process Name":"automake","Link":"https:\/\/linux.die.net\/man\/1\/automake"}},{"Process":{"Description":"Generate Makefile.in for configure from Makefile.am. Operation modes: --help print this help, then exit --version print version number, then exit -v, --verbose verbosely list files processed --no-force only update Makefile.in's that are out of date -W, --warnings= CATEGORY report the warnings falling in CATEGORY Dependency tracking: -i, --ignore-deps disable dependency tracking code --include-deps enable dependency tracking code Flavors: --cygnus assume program is part of Cygnus-style tree --foreign set strictness to foreign --gnits set strictness to gnits --gnu set strictness to gnu Library files: -a, --add-missing add missing standard files to package --libdir= DIR directory storing library files -c, --copy with -a, copy missing files (default is symlink) -f, --force-missing force update of standard files Warning categories include: 'gnu' GNU coding standards (default in gnu and gnits modes) 'obsolete' obsolete features or constructions 'override' user redefinitions of Automake rules or variables 'portability' portability issues (default in gnu and gnits modes) 'syntax' dubious syntactic constructs (default) 'unsupported' unsupported or incomplete features (default) 'all' all the warnings 'no-CATEGORY' turn off warnings in CATEGORY 'none' turn off all the warnings 'error' treat warnings as errors Files which are automatically distributed, if found: ABOUT-GNU README config.rpath ltcf-gcj.sh ABOUT-NLS THANKS config.sub ltconfig AUTHORS TODO configure ltmain.sh BACKLOG acconfig.h configure.ac mdate-sh COPYING aclocal.m4 configure.in missing COPYING.DOC ansi2knr.1 depcomp mkinstalldirs COPYING.LESSER ansi2knr.c elisp-comp py-compile COPYING.LIB compile install-sh stamp-vti ChangeLog config.guess libversion.in texinfo.tex INSTALL config.h.bot ltcf-c.sh ylwrap NEWS config.h.top ltcf-cxx.sh","Process Name":"automake-1.11","Link":"https:\/\/linux.die.net\/man\/1\/automake-1.11"}},{"Process":{"Description":"The autool program is a replacement for the Sun audiotool program used to send audio files to Network Audio System servers. It can be used either by making a symbolic link name audiotool that points to autool and is before the OpenWindows version or by changing the OpenWindows deskset bindings database with the binder(1) command.","Process Name":"autool","Link":"https:\/\/linux.die.net\/man\/1\/autool"}},{"Process":{"Description":"Copies standard gettext infrastructure files into a source package.","Process Name":"autopoint","Link":"https:\/\/linux.die.net\/man\/1\/autopoint"}},{"Process":{"Description":"The pvf tools are a collection of tools to convert vgetty modem data to and from the 'raw modem data' format, and from that to and from various audio file formats (like .au or .wav). In addition, there are some tools to manipulate pvf files, like speed up files or cut off trailing noise. A list of commands is below in the \"see also\" section. You can run those commands with the -h switch for available options. Please also look at the individual contributed man pages.","Process Name":"autopvf","Link":"https:\/\/linux.die.net\/man\/1\/autopvf"}},{"Process":{"Description":"Run 'autoconf' (and 'autoheader', 'aclocal', 'automake', 'autopoint' (formerly 'gettextize'), and 'libtoolize' where appropriate) repeatedly to remake the GNU Build System files in specified DIRECTORIES and their subdirectories (defaulting to '.'). By default, it only remakes those files that are older than their sources. If you install new versions of the GNU Build System, you can make 'autoreconf' remake all of the files by giving it the '--force' option. Operation modes: -h, --help print this help, then exit -V, --version print version number, then exit -v, --verbose verbosely report processing -d, --debug don't remove temporary files -f, --force consider all files obsolete -i, --install copy missing auxiliary files --no-recursive don't rebuild sub-packages -s, --symlink with -i, install symbolic links instead of copies -m, --make when applicable, re-run .\/configure && make -W, --warnings= CATEGORY report the warnings falling in CATEGORY [syntax] Warning categories include: 'cross' cross compilation issues 'gnu' GNU coding standards (default in gnu and gnits modes) 'obsolete' obsolete features or constructions 'override' user redefinitions of Automake rules or variables 'portability' portability issues (default in gnu and gnits modes) 'syntax' dubious syntactic constructs (default) 'unsupported' unsupported or incomplete features (default) 'all' all the warnings 'no-CATEGORY' turn off warnings in CATEGORY 'none' turn off all the warnings 'error' treat warnings as errors The environment variable 'WARNINGS' is honored. Some subtools might support other warning types, using 'all' is encouraged. Library directories: -B, --prepend-include= DIR prepend directory DIR to search path -I, --include= DIR append directory DIR to search path The environment variables AUTOCONF, AUTOHEADER, AUTOMAKE, ACLOCAL, AUTOPOINT, LIBTOOLIZE, M4, and MAKE are honored.","Process Name":"autoreconf","Link":"https:\/\/linux.die.net\/man\/1\/autoreconf"}},{"Process":{"Description":"autorun automagically recognises all available CDROMs in your system, mounts them upon insertion and executes a possible 'autorun' executable on the CD. To allow an ordinary user to do this you have to add the options user,exec in \/etc\/fstab to the CDROMs you want to use. You may also use Autorun.desktop for KDE. Just place it in your Autostart folder. Command strings are parsed and %P% is replaced by the mountpoint path. %D% is replaced by the device path. After that the command string will be executed using \/bin\/sh -c \"command string\";","Process Name":"autorun","Link":"https:\/\/linux.die.net\/man\/1\/autorun"}},{"Process":{"Description":"Examine source files in the directory tree rooted at SRCDIR, or the current directory if none is given. Search the source files for common portability problems, check for incompleteness of 'configure.ac', and create a file 'configure.scan' which is a preliminary 'configure.ac' for that package. -h, --help print this help, then exit -V, --version print version number, then exit -v, --verbose verbosely report processing -d, --debug don't remove temporary files Library directories: -B, --prepend-include= DIR prepend directory DIR to search path -I, --include= DIR append directory DIR to search path","Process Name":"autoscan","Link":"https:\/\/linux.die.net\/man\/1\/autoscan"}},{"Process":{"Description":"AutoSearch performs a web-based search and puts the results set in qid\/index.html. Subsequent searches (i.e., the second form above) AutoSearch determine what changes (if any) occured to the results sent since the last run. These incremental changes are recorded in qid\/YYYYMMDD.html. AutoSearch is amenable to be run as a cron job because all the input parameters are saved in the web pages. AutoSearch can act as a automated query agent for a particular search. The output files are designed to be a set of web pages to easily display the results set with a web browser. Example: AutoSearch -n 'LSAM Replication'\n    -s '\"lsam replication\"'\n    -e AltaVista\n    replication_query This query (which should be all on one line) creates a directory replication_query and fills it with the fascinating output of the AltaVista query on \"lsam replication\", with pages titled '' LSAM Replication''. (Note the quoting: the single quotes in '\"lsam replication\"' are for the shell, the double quotes are for AltaVista to search for the phrase rather than the separate words.) A more complicated example: AutoSearch -n 'External Links to LSAM'\n    -s '(link:www.isi.edu\/lsam or link:www.isi.edu\/~lsam) -url:isi.edu'\n    -e AltaVista::AdvancedWeb\n    -o coolness=hot This query does an advanced AltaVista search and specifies the (hypothetical) ''coolness'' option to the search engine.","Process Name":"autosearch","Link":"https:\/\/linux.die.net\/man\/1\/autosearch"}},{"Process":{"Description":"autossh is a program to start a copy of ssh and monitor it, restarting it as necessary should it die or stop passing traffic. The original idea and the mechanism were from rstunnel (Reliable SSH Tunnel). With version 1.2 of autossh the method changed: autossh uses ssh to construct a loop of ssh forwardings (one from local to remote, one from remote to local), and then sends test data that it expects to get back. (The idea is thanks to Terrence Martin.) With version 1.3, a new method is added (thanks to Ron Yorston): a port may be specified for a remote echo service that will echo back the test data. This avoids the congestion and the aggravation of making sure all the port numbers on the remote machine do not collide. The loop-of-forwardings method remains available for situations where using an echo service may not be possible.","Process Name":"autossh","Link":"https:\/\/linux.die.net\/man\/1\/autossh"}},{"Process":{"Description":"The autotrace program accepts bitmap graphics from the file inputfile specified on the command line, and as output produces a collection of splines approximating the original image, the converting the image from bitmap to vector format. It behaves in a manner similar to the commercial software known as *tream*ine or *orel*race. The result is sent to standard output unless the -output-file option is active.","Process Name":"autotrace","Link":"https:\/\/linux.die.net\/man\/1\/autotrace"}},{"Process":{"Description":"Update each TEMPLATE-FILE if given, or 'configure.ac' if present, or else 'configure.in', to the syntax of the current version of Autoconf. The original files are backed up. Operation modes: -h, --help print this help, then exit -V, --version print version number, then exit -v, --verbose verbosely report processing -d, --debug don't remove temporary files -f, --force consider all files obsolete Library directories: -B, --prepend-include= DIR prepend directory DIR to search path -I, --include= DIR append directory DIR to search path","Process Name":"autoupdate","Link":"https:\/\/linux.die.net\/man\/1\/autoupdate"}},{"Process":{"Description":"Convert .au files (Sun audio format) on standard input into vbox files (vbox audio header) on standard output. autovbox is a link to vboxcnvt(1).","Process Name":"autovbox","Link":"https:\/\/linux.die.net\/man\/1\/autovbox"}},{"Process":{"Description":"auwave demonstrates the use of waveform elements. The user can interactively select any waveform supported by the server and control its frequency and volume. Additionally, the gain of the output device can be modified.","Process Name":"auwave","Link":"https:\/\/linux.die.net\/man\/1\/auwave"}},{"Process":{"Description":"Browse for mDNS\/DNS-SD network services and browsing domains using the Avahi daemon.","Process Name":"avahi-browse","Link":"https:\/\/linux.die.net\/man\/1\/avahi-browse"}},{"Process":{"Description":"Browse for mDNS\/DNS-SD network services and browsing domains using the Avahi daemon.","Process Name":"avahi-browse-domains","Link":"https:\/\/linux.die.net\/man\/1\/avahi-browse-domains"}},{"Process":{"Description":"Show a real-time graphical browse list for mDNS\/DNS-SD network services running on the local LAN using the Avahi daemon.","Process Name":"avahi-discover","Link":"https:\/\/linux.die.net\/man\/1\/avahi-discover"}},{"Process":{"Description":"Register an mDNS\/DNS-SD service or host name\/address mapping using the Avahi daemon.","Process Name":"avahi-publish","Link":"https:\/\/linux.die.net\/man\/1\/avahi-publish"}},{"Process":{"Description":"Register an mDNS\/DNS-SD service or host name\/address mapping using the Avahi daemon.","Process Name":"avahi-publish-address","Link":"https:\/\/linux.die.net\/man\/1\/avahi-publish-address"}},{"Process":{"Description":"Register an mDNS\/DNS-SD service or host name\/address mapping using the Avahi daemon.","Process Name":"avahi-publish-service","Link":"https:\/\/linux.die.net\/man\/1\/avahi-publish-service"}},{"Process":{"Description":"Resolve one or more mDNS\/DNS host name(s) to IP address(es) (and vice versa) using the Avahi daemon.","Process Name":"avahi-resolve","Link":"https:\/\/linux.die.net\/man\/1\/avahi-resolve"}},{"Process":{"Description":"Resolve one or more mDNS\/DNS host name(s) to IP address(es) (and vice versa) using the Avahi daemon.","Process Name":"avahi-resolve-address","Link":"https:\/\/linux.die.net\/man\/1\/avahi-resolve-address"}},{"Process":{"Description":"Resolve one or more mDNS\/DNS host name(s) to IP address(es) (and vice versa) using the Avahi daemon.","Process Name":"avahi-resolve-host-name","Link":"https:\/\/linux.die.net\/man\/1\/avahi-resolve-host-name"}},{"Process":{"Description":"Set the mDNS host name of a currently running Avahi daemon. The effect of this operation is not persistent across daemon restarts. This operation is usually privileged.","Process Name":"avahi-set-host-name","Link":"https:\/\/linux.die.net\/man\/1\/avahi-set-host-name"}},{"Process":{"Description":"AVaRICE runs on a POSIX machine and connects to gdb via a TCP socket and communicates via gdb's \"serial debug protocol\". This protocol allows gdb to send commands like \"set\/remove breakpoint\" and \"read\/write memory\". AVaRICE translates these commands into the Atmel protocol used to control the AVR JTAG ICE. Connection to the AVR JTAG ICE is via a serial port on the POSIX machine. Because the GDB <---> AVaRICE connection is via a TCP socket, the two programs do not need to run on the same machine. In an office environment, this allows a developer to debug a target in the lab from the comfort of their cube (or even better, their home!) NOTE: Even though you can run avarice and avr-gdb on different systems, it is not recommended because of the security risk involved. avarice was not designed to be a secure server. There is no authentication performed when a client connects to avarice when it is running in gdb server mode. Supported Devices avarice currently has support for the following devices: at90can128 at90can32 ( o) at90can64 ( o) at90pwm2 ( o) ( +) at90pwm216 ( o) ( +) at90pwm2b ( o) ( +) at90pwm3 ( o) ( +) at90pwm316 ( o) ( +) at90pwm3b ( o) ( +) at90usb1287 ( *) at90usb162 ( o) ( +) at90usb646 ( *) at90usb647 ( *) atmega128 atmega1280 ( *) atmega1281 ( *) atmega1284p ( *) atmega16 atmega162 atmega164p ( o) atmega165 ( o) atmega165p ( o) atmega168 ( o) ( +) atmega168p ( o) ( +) atmega169 atmega16hva ( o) atmega2560 ( *) atmega2561 ( *) atmega32 atmega323 atmega324p ( o) atmega325 ( o) atmega3250 ( o) atmega3250p ( o) atmega325p ( o) atmega328p ( o) ( +) atmega329 ( o) atmega3290 ( o) atmega3290p ( o) atmega329p ( o) atmega32c1 ( o) ( +) atmega32hvb ( o) ( +) atmega32m1 ( o) ( +) atmega32u4 ( o) atmega406 ( *) atmega48 ( o) ( +) atmega48p ( o) ( +) atmega64 atmega640 ( *) atmega644 ( *) atmega644p ( *) atmega645 ( *) atmega6450 ( *) atmega649 ( *) atmega6490 ( *) atmega88 ( o) ( +) atmega88p ( o) ( +) attiny13 ( o) ( +) attiny167 ( o) ( +) attiny2313 ( o) ( +) attiny24 ( o) ( +) attiny25 ( o) ( +) attiny261 ( o) ( +) attiny43u ( o) ( +) attiny44 ( o) ( +) attiny45 ( o) ( +) attiny461 ( o) ( +) attiny48 ( o) ( +) attiny84 ( o) ( +) attiny85 ( o) ( +) attiny861 ( o) ( +) attiny88 ( o) ( +) atxmega128a1 ( *) * - Only supported by the JTAG ICE mkII device. o - Only supported by the JTAG ICE mkII and AVR Dragon device. + - debugWire, see below Supported File Formats avarice uses libbfd for reading input files. As such, it can handle any file format that libbfd knowns about. This includes the Intel Hex, Motorola SRecord and ELF formats, among others. If you tell avarice to read an ELF file, it will automatically handle programming all of the sections contained in the file (e.g. flash, eeprom, etc.).","Process Name":"avarice","Link":"https:\/\/linux.die.net\/man\/1\/avarice"}},{"Process":{"Description":"avifix fixes the header of an AVI-file with the supplied parameters.","Process Name":"avifix","Link":"https:\/\/linux.die.net\/man\/1\/avifix"}},{"Process":{"Description":"aviindex writes a text file describing the index of an AVI file. It analyses the content or index if available of the AVI file and prints this information in a human readable form. An AVI file can have an optional chunk called \"idx1\" which contains information about keyframes (syncpoints) and locations of video frames resp. audio chunks. Though larger AVI files (>2-4GB), so-called OpenDML AVI or also AVI 2 files, have a more complicated indexing system, which consists of a superindex referring to (possibly) several \"standard\" indexes, the \"indexing principle\" is the same. Movie players use such indexes to seek in files. aviindex reads the AVI file ifile and writes the index into ofile. This can either happen in \"dumb\" mode where aviindex looks for an existing index (and trusts this index!) in the file and dumps this index into a human readable form. The \"dumb\" mode is used, when -n is NOT specified or when the filesize of the input file is smaller than 2 GB. In \"smart\" mode, aviindex scans through the complete AVI file and searches for chunks (may that video or audio) and reconstructs the index based on the information found. If an index chunk is found accidently, aviindex will use the information in this index to recover the keyframe information, which is important. aviindex will use smart mode, if given the -n option OR if the AVI file is larger than 2 GB. If the file is large, the index chunk cannot be found the usual way so one must use -n but it is possible that there is an index chunk in this file. Cross fingers. Also in smart mode, aviindex analyzes the content of the video frame and tries to detect keyframes by looking at the data depending on the video codec. The generated index file serves different purposes. * The library which handles AVI files in transcode(1) can read such index files and use this file to rebuild the index instead of scanning through the whole AVI file over and over again. Reading the index from the index file is much faster than scanning through the AVI. * It can be used as a seeking file. When given to transcode via the --nav_seek switch, transcode will use the file to seek directly to the position you specified via -c. This also works for multiple -c ranges. * Its nice to have for debugging.","Process Name":"aviindex","Link":"https:\/\/linux.die.net\/man\/1\/aviindex"}},{"Process":{"Description":"avimerge is a versatile tool. It can contatenate several AVI files into one. It can also be used to fix an index of a broken file and can also replace audio tracks or muxes new ones. It can read raw AC3 and MP3 files for multplexing.","Process Name":"avimerge","Link":"https:\/\/linux.die.net\/man\/1\/avimerge"}},{"Process":{"Description":"avisplit splits a single AVI-file into chunks of size size. Each of the created chunks will be an independent file, i.e. it can be played without needing any other of the chunk.","Process Name":"avisplit","Link":"https:\/\/linux.die.net\/man\/1\/avisplit"}},{"Process":{"Description":"avisync shift audio on frame basis.","Process Name":"avisync","Link":"https:\/\/linux.die.net\/man\/1\/avisync"}},{"Process":{"Description":"clogin is an expect(1) script to automate the process of logging into a Cisco router, catalyst switch, Extreme switch, Juniper ERX\/E-series, Procket Networks, or Redback router. There are complementary scripts for Alteon, Avocent (Cyclades), Bay Networks (nortel), ADC-kentrox EZ-T3 mux, Foundry, HP Procurve Switches and Cisco AGMs, Hitachi Routers, Juniper Networks, MRV optical switch, Netscreen firewalls, Netscaler, Riverstone, Netopia, and Lucent TNT, named alogin, avologin, blogin, elogin, flogin, fnlogin, hlogin, htlogin, jlogin, mrvlogin, nlogin, nslogin, rivlogin, tlogin, and tntlogin, respectively. clogin reads the .cloginrc file for its configuration, then connects and logs into each of the routers specified on the command line in the order listed. Command-line options exist to override some of the directives found in the .cloginrc configuration file. The command-line options are as follows: -S Save the configuration on exit, if the device prompts at logout time. This only has affect when used with -s. -V Prints package name and version strings. -c Command to be run on each router list on the command-line. Multiple commands maybe listed by separating them with semi-colons (;). The argument should be quoted to avoid shell expansion. -d Enable expect debugging. -E Specifies a variable to pass through to scripts (-s). For example, the command-line option -Efoo=bar will produce a global variable by the name Efoo with the initial value \"bar\". -e Specify a password to be supplied when gaining enable privileges on the router(s). Also see the password directive of the .cloginrc file. -f Specifies an alternate configuration file. The default is $HOME\/.cloginrc. -p Specifies a password associated with the user specified by the -u option, user directive of the .cloginrc file, or the Unix username of the user. -s The filename of an expect(1) script which will be sourced after the login is successful and is expected to return control to clogin, with the connection to the router intact, when it is done. Note that clogin disables log_user of expect(1)when -s is used. Example script(s) can be found in share\/rancid\/*.exp. -t Alters the timeout interval; the period that clogin waits for an individual command to return a prompt or the login process to produce a prompt or failure. The argument is in seconds. -u Specifies the username used when prompted. The command-line option overrides any user directive found in .cloginrc. The default is the current Unix username. -v Specifies a vty password, that which is prompted for upon connection to the router. This overrides the vty password of the .cloginrc file's password directive. -w Specifies the username used if prompted when gaining enable privileges. The command-line option overrides any user or enauser directives found in .cloginrc. The default is the current Unix username. -x Similar to the -c option; -x specifies a file with commands to run on each of the routers. The commands must not expect additional input, such as 'copy rcp startup-config' does. For example: show version\nshow logging -y Specifies the encryption algorithm for use with the ssh(1) -c option. The default encryption type is often not supported. See the ssh(1) man page for details. The default is 3des.","Process Name":"avologin","Link":"https:\/\/linux.die.net\/man\/1\/avologin"}},{"Process":{"Description":"rancid is a perl(1) script which uses the login scripts (see clogin(1)) to login to a device, execute commands to display the configuration, etc, then filters the output for formatting, security, and so on. rancid's product is a file with the name of it's last argument plus the suffix .new. For example, hostname.new. There are complementary scripts for other platforms and\/or manufacturers that are supported by rancid(1). Briefly, these are: agmrancid Cisco Anomaly Guard Module (AGM) arancid Alteon WebOS switches arrancid Arista Networks devices brancid Bay Networks (nortel) cat5rancid Cisco catalyst switches cssrancid Cisco content services switches erancid ADC-kentrox EZ-T3 mux f10rancid Force10 f5rancid F5 BigIPs fnrancid Fortinet Firewalls francid Foundry and HP procurve OEMs of Foundry hrancid HP Procurve Switches htranicd Hitachi Routers jerancid Juniper Networks E-series jrancid Juniper Networks mrancid MRTd mrvrancid MRV optical switches nrancid Netscreen firewalls nsrancid Netscaler nxrancid Cisco Nexus boxes prancid Procket Networks rivrancid Riverstone rrancid Redback srancid SMC switch (some Dell OEMs) trancid Netopia sDSL\/T1 routers tntrancid Lucent TNT xrancid Extreme switches xrrancid Cisco IOS-XR boxes zrancid Zebra routing software The command-line options are as follows: -V Prints package name and version strings. -d Display debugging information. -l Display somewhat less debugging information. -f rancid should interpret the next argument as a filename which contains the output it would normally collect from the device ( hostname) with clogin(1).","Process Name":"avorancid","Link":"https:\/\/linux.die.net\/man\/1\/avorancid"}},{"Process":{"Description":"addr2line translates addresses into file names and line numbers. Given an address in an executable or an offset in a section of a relocatable object, it uses the debugging information to figure out which file name and line number are associated with it. The executable or relocatable object to use is specified with the -e option. The default is the file a.out. The section in the relocatable object to use is specified with the -j option. addr2line has two modes of operation. In the first, hexadecimal addresses are specified on the command line, and addr2line displays the file name and line number for each address. In the second, addr2line reads hexadecimal addresses from standard input, and prints the file name and line number for each address on standard output. In this mode, addr2line may be used in a pipe to convert dynamically chosen addresses. The format of the output is FILENAME:LINENO . The file name and line number for each address is printed on a separate line. If the -f option is used, then each FILENAME:LINENO line is preceded by a FUNCTIONNAME line which is the name of the function containing the address. If the file name or function name can not be determined, addr2line will print two question marks in their place. If the line number can not be determined, addr2line will print 0.","Process Name":"avr-addr2line","Link":"https:\/\/linux.die.net\/man\/1\/avr-addr2line"}},{"Process":{"Description":"The GNU ar program creates, modifies, and extracts from archives. An archive is a single file holding a collection of other files in a structure that makes it possible to retrieve the original individual files (called members of the archive). The original files' contents, mode (permissions), timestamp, owner, and group are preserved in the archive, and can be restored on extraction. GNU ar can maintain archives whose members have names of any length; however, depending on how ar is configured on your system, a limit on member-name length may be imposed for compatibility with archive formats maintained with other tools. If it exists, the limit is often 15 characters (typical of formats related to a.out) or 16 characters (typical of formats related to coff). ar is considered a binary utility because archives of this sort are most often used as libraries holding commonly needed subroutines. ar creates an index to the symbols defined in relocatable object modules in the archive when you specify the modifier s. Once created, this index is updated in the archive whenever ar makes a change to its contents (save for the q update operation). An archive with such an index speeds up linking to the library, and allows routines in the library to call each other without regard to their placement in the archive. You may use nm -s or nm --print-armap to list this index table. If an archive lacks the table, another form of ar called ranlib can be used to add just the table. GNU ar can optionally create a thin archive, which contains a symbol index and references to the original copies of the member files of the archives. Such an archive is useful for building libraries for use within a local build, where the relocatable objects are expected to remain available, and copying the contents of each object would only waste time and space. Thin archives are also flattened, so that adding one or more archives to a thin archive will add the elements of the nested archive individually. The paths to the elements of the archive are stored relative to the archive itself. GNU ar is designed to be compatible with two different facilities. You can control its activity using command-line options, like the different varieties of ar on Unix systems; or, if you specify the single command-line option -M, you can control it with a script supplied via standard input, like the MRI \"librarian\" program.","Process Name":"avr-ar","Link":"https:\/\/linux.die.net\/man\/1\/avr-ar"}},{"Process":{"Description":"GNU as is really a family of assemblers. If you use (or have used) the GNU assembler on one architecture, you should find a fairly similar environment when you use it on another architecture. Each version has much in common with the others, including object file formats, most assembler directives (often called pseudo-ops) and assembler syntax. as is primarily intended to assemble the output of the GNU C compiler \"gcc\" for use by the linker \"ld\". Nevertheless, we've tried to make as assemble correctly everything that other assemblers for the same machine would assemble. Any exceptions are documented explicitly. This doesn't mean as always uses the same syntax as another assembler for the same architecture; for example, we know of several incompatible versions of 680x0 assembly language syntax. Each time you run as it assembles exactly one source program. The source program is made up of one or more files. (The standard input is also a file.) You give as a command line that has zero or more input file names. The input files are read (from left file name to right). A command line argument (in any position) that has no special meaning is taken to be an input file name. If you give as no file names it attempts to read one input file from the as standard input, which is normally your terminal. You may have to type ctl-D to tell as there is no more program to assemble. Use -- if you need to explicitly name the standard input file in your command line. If the source is empty, as produces a small, empty object file. as may write warnings and error messages to the standard error file (usually your terminal). This should not happen when a compiler runs as automatically. Warnings report an assumption made so that as could keep assembling a flawed program; errors report a grave problem that stops the assembly. If you are invoking as via the GNU C compiler, you can use the -Wa option to pass arguments through to the assembler. The assembler arguments must be separated from each other (and the -Wa) by commas. For example: gcc -c -g -O -Wa,-alh,-L file.c This passes two options to the assembler: -alh (emit a listing to standard output with high-level and assembly source) and -L (retain local symbols in the symbol table). Usually you do not need to use this -Wa mechanism, since many compiler command-line options are automatically passed to the assembler by the compiler. (You can call the GNU compiler driver with the -v option to see precisely what options it passes to each compilation pass, including the assembler.)","Process Name":"avr-as","Link":"https:\/\/linux.die.net\/man\/1\/avr-as"}},{"Process":{"Description":"The C ++ and Java languages provide function overloading, which means that you can write many functions with the same name, providing that each function takes parameters of different types. In order to be able to distinguish these similarly named functions C ++ and Java encode them into a low-level assembler name which uniquely identifies each different version. This process is known as mangling. The c++filt [1] program does the inverse mapping: it decodes (demangles) low-level names into user-level names so that they can be read. Every alphanumeric word (consisting of letters, digits, underscores, dollars, or periods) seen in the input is a potential mangled name. If the name decodes into a C ++ name, the C ++ name replaces the low-level name in the output, otherwise the original word is output. In this way you can pass an entire assembler source file, containing mangled names, through c++filt and see the same source file containing demangled names. You can also use c++filt to decipher individual symbols by passing them on the command line: c++filt <symbol> If no symbol arguments are given, c++filt reads symbol names from the standard input instead. All the results are printed on the standard output. The difference between reading names from the command line versus reading names from the standard input is that command line arguments are expected to be just mangled names and no checking is performed to separate them from surrounding text. Thus for example: c++filt -n _Z1fv will work and demangle the name to \"f()\" whereas: c++filt -n _Z1fv, will not work. (Note the extra comma at the end of the mangled name which makes it invalid). This command however will work: echo _Z1fv, | c++filt -n and will display \"f(),\", i.e., the demangled name followed by a trailing comma. This behaviour is because when the names are read from the standard input it is expected that they might be part of an assembler source file where there might be extra, extraneous characters trailing after a mangled name. For example: .type   _Z1fv, @function","Process Name":"avr-c++filt","Link":"https:\/\/linux.die.net\/man\/1\/avr-c++filt"}},{"Process":{"Description":"The C preprocessor, often known as cpp, is a macro processor that is used automatically by the C compiler to transform your program before compilation. It is called a macro processor because it allows you to define macros, which are brief abbreviations for longer constructs. The C preprocessor is intended to be used only with C, C ++ , and Objective-C source code. In the past, it has been abused as a general text processor. It will choke on input which does not obey C's lexical rules. For example, apostrophes will be interpreted as the beginning of character constants, and cause errors. Also, you cannot rely on it preserving characteristics of the input which are not significant to C-family languages. If a Makefile is preprocessed, all the hard tabs will be removed, and the Makefile will not work. Having said that, you can often get away with using cpp on things which are not C. Other Algol-ish programming languages are often safe (Pascal, Ada, etc.) So is assembly, with caution. -traditional-cpp mode preserves more white space, and is otherwise more permissive. Many of the problems can be avoided by writing C or C ++ style comments instead of native language comments, and keeping macros simple. Wherever possible, you should use a preprocessor geared to the language you are writing in. Modern versions of the GNU assembler have macro facilities. Most high level programming languages have their own conditional compilation and inclusion mechanism. If all else fails, try a true general text processor, such as GNU M4. C preprocessors vary in some details. This manual discusses the GNU C preprocessor, which provides a small superset of the features of ISO Standard C. In its default mode, the GNU C preprocessor does not do a few things required by the standard. These are features which are rarely, if ever, used, and may cause surprising changes to the meaning of a program which does not expect them. To get strict ISO Standard C, you should use the -std=c90 or -std=c99 options, depending on which version of the standard you want. To get all the mandatory diagnostics, you must also use -pedantic. This manual describes the behavior of the ISO preprocessor. To minimize gratuitous differences, where the ISO preprocessor's behavior does not conflict with traditional semantics, the traditional preprocessor should behave the same way. The various differences that do exist are detailed in the section Traditional Mode. For clarity, unless noted otherwise, references to CPP in this manual refer to GNU CPP .","Process Name":"avr-cpp","Link":"https:\/\/linux.die.net\/man\/1\/avr-cpp"}},{"Process":{"Description":"When you invoke GCC , it normally does preprocessing, compilation, assembly and linking. The \"overall options\" allow you to stop this process at an intermediate stage. For example, the -c option says not to run the linker. Then the output consists of object files output by the assembler. Other options are passed on to one stage of processing. Some options control the preprocessor and others the compiler itself. Yet other options control the assembler and linker; most of these are not documented here, since you rarely need to use any of them. Most of the command line options that you can use with GCC are useful for C programs; when an option is only useful with another language (usually C ++ ), the explanation says so explicitly. If the description for a particular option does not mention a source language, you can use that option with all supported languages. The gcc program accepts options and file names as operands. Many options have multi-letter names; therefore multiple single-letter options may not be grouped: -dv is very different from -d -v. You can mix options and other arguments. For the most part, the order you use doesn't matter. Order does matter when you use several options of the same kind; for example, if you specify -L more than once, the directories are searched in the order specified. Also, the placement of the -l option is significant. Many options have long names starting with -f or with -W---for example, -fmove-loop-invariants, -Wformat and so on. Most of these have both positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"avr-g++","Link":"https:\/\/linux.die.net\/man\/1\/avr-g++"}},{"Process":{"Description":"When you invoke GCC , it normally does preprocessing, compilation, assembly and linking. The \"overall options\" allow you to stop this process at an intermediate stage. For example, the -c option says not to run the linker. Then the output consists of object files output by the assembler. Other options are passed on to one stage of processing. Some options control the preprocessor and others the compiler itself. Yet other options control the assembler and linker; most of these are not documented here, since you rarely need to use any of them. Most of the command line options that you can use with GCC are useful for C programs; when an option is only useful with another language (usually C ++ ), the explanation says so explicitly. If the description for a particular option does not mention a source language, you can use that option with all supported languages. The gcc program accepts options and file names as operands. Many options have multi-letter names; therefore multiple single-letter options may not be grouped: -dv is very different from -d -v. You can mix options and other arguments. For the most part, the order you use doesn't matter. Order does matter when you use several options of the same kind; for example, if you specify -L more than once, the directories are searched in the order specified. Also, the placement of the -l option is significant. Many options have long names starting with -f or with -W---for example, -fmove-loop-invariants, -Wformat and so on. Most of these have both positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"avr-gcc","Link":"https:\/\/linux.die.net\/man\/1\/avr-gcc"}},{"Process":{"Description":"gcov is a test coverage program. Use it in concert with GCC to analyze your programs to help create more efficient, faster running code and to discover untested parts of your program. You can use gcov as a profiling tool to help discover where your optimization efforts will best affect your code. You can also use gcov along with the other profiling tool, gprof, to assess which parts of your code use the greatest amount of computing time. Profiling tools help you analyze your code's performance. Using a profiler such as gcov or gprof, you can find out some basic performance statistics, such as: \u2022 how often each line of code executes \u2022 what lines of code are actually executed \u2022 how much computing time each section of code uses Once you know these things about how your code works when compiled, you can look at each module to see which modules should be optimized. gcov helps you determine where to work on optimization. Software developers also use coverage testing in concert with testsuites, to make sure software is actually good enough for a release. Testsuites can verify that a program works as expected; a coverage program tests to see how much of the program is exercised by the testsuite. Developers can then determine what kinds of test cases need to be added to the testsuites to create both better testing and a better final product. You should compile your code without optimization if you plan to use gcov because the optimization, by combining some lines of code into one function, may not give you as much information as you need to look for 'hot spots' where the code is using a great deal of computer time. Likewise, because gcov accumulates statistics by line (at the lowest resolution), it works best with a programming style that places only one statement on each line. If you use complicated macros that expand to loops or to other control structures, the statistics are less helpful---they only report on the line where the macro call appears. If your complex macros behave like functions, you can replace them with inline functions to solve this problem. gcov creates a logfile called sourcefile.gcov which indicates how many times each line of a source file sourcefile.c has executed. You can use these logfiles along with gprof to aid in fine-tuning the performance of your programs. gprof gives timing information you can use along with the information you get from gcov. gcov works only on code compiled with GCC . It is not compatible with any other profiling or test coverage mechanism.","Process Name":"avr-gcov","Link":"https:\/\/linux.die.net\/man\/1\/avr-gcov"}},{"Process":{"Description":"The purpose of a debugger such as GDB is to allow you to see what is going on ''inside'' another program while it executes-or what another program was doing at the moment it crashed. GDB can do four main kinds of things (plus other things in support of these) to help you catch bugs in the act: \u2022 Start your program, specifying anything that might affect its behavior. \u2022 Make your program stop on specified conditions. \u2022 Examine what has happened, when your program has stopped. \u2022 Change things in your program, so you can experiment with correcting the effects of one bug and go on to learn about another. You can use GDB to debug programs written in C, C++, and Modula-2. Fortran support will be added when a GNU Fortran compiler is ready. GDB is invoked with the shell command gdb. Once started, it reads commands from the terminal until you tell it to exit with the GDB command quit. You can get online help from gdb itself by using the command help. You can run gdb with no arguments or options; but the most usual way to start GDB is with one argument or two, specifying an executable program as the argument: gdb program You can also start with both an executable program and a core file specified: gdb program core You can, instead, specify a process ID as a second argument, if you want to debug a running process: gdb program 1234 would attach GDB to process 1234 (unless you also have a file named '1234'; GDB does check for a core file first). Here are some of the most frequently needed GDB commands: break [ file :] function Set a breakpoint at function (in file). run [ arglist] Start your program (with arglist, if specified). bt Backtrace: display the program stack. print expr Display the value of an expression. c Continue running your program (after stopping, e.g. at a breakpoint). next Execute next program line (after stopping); step over any function calls in the line. edit [ file :] function look at the program line where it is presently stopped. list [ file :] function type the text of the program in the vicinity of where it is presently stopped. step Execute next program line (after stopping); step into any function calls in the line. help [ name] Show information about GDB command name, or general information about using GDB. quit Exit from GDB. For full details on GDB, see Using GDB: A Guide to the GNU Source-Level Debugger, by Richard M. Stallman and Roland H. Pesch. The same text is available online as the gdb entry in the info program.","Process Name":"avr-gdb","Link":"https:\/\/linux.die.net\/man\/1\/avr-gdb"}},{"Process":{"Description":"The purpose of a debugger such as GDB is to allow you to see what is going on ''inside'' another program while it executes-or what another program was doing at the moment it crashed. GDB can do four main kinds of things (plus other things in support of these) to help you catch bugs in the act: \u2022 Start your program, specifying anything that might affect its behavior. \u2022 Make your program stop on specified conditions. \u2022 Examine what has happened, when your program has stopped. \u2022 Change things in your program, so you can experiment with correcting the effects of one bug and go on to learn about another. You can use GDB to debug programs written in C, C++, and Modula-2. Fortran support will be added when a GNU Fortran compiler is ready. GDB is invoked with the shell command gdb. Once started, it reads commands from the terminal until you tell it to exit with the GDB command quit. You can get online help from gdb itself by using the command help. You can run gdb with no arguments or options; but the most usual way to start GDB is with one argument or two, specifying an executable program as the argument: gdb program You can also start with both an executable program and a core file specified: gdb program core You can, instead, specify a process ID as a second argument, if you want to debug a running process: gdb program 1234 would attach GDB to process 1234 (unless you also have a file named '1234'; GDB does check for a core file first). Here are some of the most frequently needed GDB commands: break [ file :] function Set a breakpoint at function (in file). run [ arglist] Start your program (with arglist, if specified). bt Backtrace: display the program stack. print expr Display the value of an expression. c Continue running your program (after stopping, e.g. at a breakpoint). next Execute next program line (after stopping); step over any function calls in the line. edit [ file :] function look at the program line where it is presently stopped. list [ file :] function type the text of the program in the vicinity of where it is presently stopped. step Execute next program line (after stopping); step into any function calls in the line. help [ name] Show information about GDB command name, or general information about using GDB. quit Exit from GDB. For full details on GDB, see Using GDB: A Guide to the GNU Source-Level Debugger, by Richard M. Stallman and Roland H. Pesch. The same text is available online as the gdb entry in the info program.","Process Name":"avr-gdbtui","Link":"https:\/\/linux.die.net\/man\/1\/avr-gdbtui"}},{"Process":{"Description":"\"gprof\" produces an execution profile of C, Pascal, or Fortran77 programs. The effect of called routines is incorporated in the profile of each caller. The profile data is taken from the call graph profile file (gmon.out default) which is created by programs that are compiled with the -pg option of \"cc\", \"pc\", and \"f77\". The -pg option also links in versions of the library routines that are compiled for profiling. \"Gprof\" reads the given object file (the default is \"a.out\") and establishes the relation between its symbol table and the call graph profile from gmon.out. If more than one profile file is specified, the \"gprof\" output shows the sum of the profile information in the given profile files. \"Gprof\" calculates the amount of time spent in each routine. Next, these times are propagated along the edges of the call graph. Cycles are discovered, and calls into a cycle are made to share the time of the cycle. Several forms of output are available from the analysis. The flat profile shows how much time your program spent in each function, and how many times that function was called. If you simply want to know which functions burn most of the cycles, it is stated concisely here. The call graph shows, for each function, which functions called it, which other functions it called, and how many times. There is also an estimate of how much time was spent in the subroutines of each function. This can suggest places where you might try to eliminate function calls that use a lot of time. The annotated source listing is a copy of the program's source code, labeled with the number of times each line of the program was executed.","Process Name":"avr-gprof","Link":"https:\/\/linux.die.net\/man\/1\/avr-gprof"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"avr-ld","Link":"https:\/\/linux.die.net\/man\/1\/avr-ld"}},{"Process":{"Description":"GNU nm lists the symbols from object files objfile.... If no object files are listed as arguments, nm assumes the file a.out. For each symbol, nm shows: \u2022 The symbol value, in the radix selected by options (see below), or hexadecimal by default. \u2022 The symbol type. At least the following types are used; others are, as well, depending on the object file format. If lowercase, the symbol is local; if uppercase, the symbol is global (external). \"A\" The symbol's value is absolute, and will not be changed by further linking. \"B\" \"b\" The symbol is in the uninitialized data section (known as BSS ). \"C\" The symbol is common. Common symbols are uninitialized data. When linking, multiple common symbols may appear with the same name. If the symbol is defined anywhere, the common symbols are treated as undefined references. \"D\" \"d\" The symbol is in the initialized data section. \"G\" \"g\" The symbol is in an initialized data section for small objects. Some object file formats permit more efficient access to small data objects, such as a global int variable as opposed to a large global array. \"i\" For PE format files this indicates that the symbol is in a section specific to the implementation of DLLs. For ELF format files this indicates that the symbol is an indirect function. This is a GNU extension to the standard set of ELF symbol types. It indicates a symbol which if referenced by a relocation does not evaluate to its address, but instead must be invoked at runtime. The runtime execution will then return the value to be used in the relocation. \"N\" The symbol is a debugging symbol. \"p\" The symbols is in a stack unwind section. \"R\" \"r\" The symbol is in a read only data section. \"S\" \"s\" The symbol is in an uninitialized data section for small objects. \"T\" \"t\" The symbol is in the text (code) section. \"U\" The symbol is undefined. \"u\" The symbol is a unique global symbol. This is a GNU extension to the standard set of ELF symbol bindings. For such a symbol the dynamic linker will make sure that in the entire process there is just one symbol with this name and type in use. \"V\" \"v\" The symbol is a weak object. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the weak symbol becomes zero with no error. On some systems, uppercase indicates that a default value has been specified. \"W\" \"w\" The symbol is a weak symbol that has not been specifically tagged as a weak object symbol. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the symbol is determined in a system-specific manner without error. On some systems, uppercase indicates that a default value has been specified. \"-\" The symbol is a stabs symbol in an a.out object file. In this case, the next values printed are the stabs other field, the stabs desc field, and the stab type. Stabs symbols are used to hold debugging information. \"?\" The symbol type is unknown, or object file format specific. \u2022 The symbol name.","Process Name":"avr-nm","Link":"https:\/\/linux.die.net\/man\/1\/avr-nm"}},{"Process":{"Description":"The GNU objcopy utility copies the contents of an object file to another. objcopy uses the GNU BFD Library to read and write the object files. It can write the destination object file in a format different from that of the source object file. The exact behavior of objcopy is controlled by command-line options. Note that objcopy should be able to copy a fully linked file between any two formats. However, copying a relocatable object file between any two formats may not work as expected. objcopy creates temporary files to do its translations and deletes them afterward. objcopy uses BFD to do all its translation work; it has access to all the formats described in BFD and thus is able to recognize most formats without being told explicitly. objcopy can be used to generate S-records by using an output target of srec (e.g., use -O srec). objcopy can be used to generate a raw binary file by using an output target of binary (e.g., use -O binary). When objcopy generates a raw binary file, it will essentially produce a memory dump of the contents of the input object file. All symbols and relocation information will be discarded. The memory dump will start at the load address of the lowest section copied into the output file. When generating an S-record or a raw binary file, it may be helpful to use -S to remove sections containing debugging information. In some cases -R will be useful to remove sections which contain information that is not needed by the binary file. Note---objcopy is not able to change the endianness of its input files. If the input format has an endianness (some formats do not), objcopy can only copy the inputs into file formats that have the same endianness or which have no endianness (e.g., srec). (However, see the --reverse-bytes option.)","Process Name":"avr-objcopy","Link":"https:\/\/linux.die.net\/man\/1\/avr-objcopy"}},{"Process":{"Description":"objdump displays information about one or more object files. The options control what particular information to display. This information is mostly useful to programmers who are working on the compilation tools, as opposed to programmers who just want their program to compile and work. objfile... are the object files to be examined. When you specify archives, objdump shows information on each of the member object files.","Process Name":"avr-objdump","Link":"https:\/\/linux.die.net\/man\/1\/avr-objdump"}},{"Process":{"Description":"ranlib generates an index to the contents of an archive and stores it in the archive. The index lists each symbol defined by a member of an archive that is a relocatable object file. You may use nm -s or nm --print-armap to list this index. An archive with such an index speeds up linking to the library and allows routines in the library to call each other without regard to their placement in the archive. The GNU ranlib program is another form of GNU ar; running ranlib is completely equivalent to executing ar -s.","Process Name":"avr-ranlib","Link":"https:\/\/linux.die.net\/man\/1\/avr-ranlib"}},{"Process":{"Description":"readelf displays information about one or more ELF format object files. The options control what particular information to display. elffile... are the object files to be examined. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files. This program performs a similar function to objdump but it goes into more detail and it exists independently of the BFD library, so if there is a bug in BFD then readelf will not be affected.","Process Name":"avr-readelf","Link":"https:\/\/linux.die.net\/man\/1\/avr-readelf"}},{"Process":{"Description":"Use 'run program' to execute a binary by interpreting machine instructions on your host computer. run is the same emulator used by GDB's 'target sim' command. You can run it directly by executing run if you just want to see your program execute, and do not need any debugger functionality. You can also use run to generate profiling information for analysis with gprof.","Process Name":"avr-run","Link":"https:\/\/linux.die.net\/man\/1\/avr-run"}},{"Process":{"Description":"The GNU size utility lists the section sizes---and the total size---for each of the object or archive files objfile in its argument list. By default, one line of output is generated for each object file or each module in an archive. objfile... are the object files to be examined. If none are specified, the file \"a.out\" will be used.","Process Name":"avr-size","Link":"https:\/\/linux.die.net\/man\/1\/avr-size"}},{"Process":{"Description":"For each file given, GNU strings prints the printable character sequences that are at least 4 characters long (or the number given with the options below) and are followed by an unprintable character. By default, it only prints the strings from the initialized and loaded sections of object files; for other types of files, it prints the strings from the whole file. strings is mainly useful for determining the contents of non-text files.","Process Name":"avr-strings","Link":"https:\/\/linux.die.net\/man\/1\/avr-strings"}},{"Process":{"Description":"GNU strip discards all symbols from object files objfile. The list of object files may include archives. At least one object file must be given. strip modifies the files named in its argument, rather than writing modified copies under different names.","Process Name":"avr-strip","Link":"https:\/\/linux.die.net\/man\/1\/avr-strip"}},{"Process":{"Description":"windmc reads message definitions from an input file (.mc) and translate them into a set of output files. The output files may be of four kinds: \"h\" A C header file containing the message definitions. \"rc\" A resource file compilable by the windres tool. \"bin\" One or more binary files containing the resource data for a specific message language. \"dbg\" A C include file that maps message id's to their symbolic name. The exact description of these different formats is available in documentation from Microsoft. When windmc converts from the \"mc\" format to the \"bin\" format, \"rc\", \"h\", and optional \"dbg\" it is acting like the Windows Message Compiler.","Process Name":"avr-windmc","Link":"https:\/\/linux.die.net\/man\/1\/avr-windmc"}},{"Process":{"Description":"addr2line translates addresses into file names and line numbers. Given an address in an executable or an offset in a section of a relocatable object, it uses the debugging information to figure out which file name and line number are associated with it. The executable or relocatable object to use is specified with the -e option. The default is the file a.out. The section in the relocatable object to use is specified with the -j option. addr2line has two modes of operation. In the first, hexadecimal addresses are specified on the command line, and addr2line displays the file name and line number for each address. In the second, addr2line reads hexadecimal addresses from standard input, and prints the file name and line number for each address on standard output. In this mode, addr2line may be used in a pipe to convert dynamically chosen addresses. The format of the output is FILENAME:LINENO . The file name and line number for each input address is printed on separate lines. If the -f option is used, then each FILENAME:LINENO line is preceded by FUNCTIONNAME which is the name of the function containing the address. If the -i option is used and the code at the given address is present there because of inlining by the compiler then the { FUNCTIONNAME } FILENAME:LINENO information for the inlining function will be displayed afterwards. This continues recursively until there is no more inlining to report. If the -a option is used then the output is prefixed by the input address. If the -p option is used then the output for each input address is displayed on one, possibly quite long, line. If -p is not used then the output is broken up into multiple lines, based on the paragraphs above. If the file name or function name can not be determined, addr2line will print two question marks in their place. If the line number can not be determined, addr2line will print 0.","Process Name":"avr32-linux-gnu-addr2line","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-addr2line"}},{"Process":{"Description":"The GNU ar program creates, modifies, and extracts from archives. An archive is a single file holding a collection of other files in a structure that makes it possible to retrieve the original individual files (called members of the archive). The original files' contents, mode (permissions), timestamp, owner, and group are preserved in the archive, and can be restored on extraction. GNU ar can maintain archives whose members have names of any length; however, depending on how ar is configured on your system, a limit on member-name length may be imposed for compatibility with archive formats maintained with other tools. If it exists, the limit is often 15 characters (typical of formats related to a.out) or 16 characters (typical of formats related to coff). ar is considered a binary utility because archives of this sort are most often used as libraries holding commonly needed subroutines. ar creates an index to the symbols defined in relocatable object modules in the archive when you specify the modifier s. Once created, this index is updated in the archive whenever ar makes a change to its contents (save for the q update operation). An archive with such an index speeds up linking to the library, and allows routines in the library to call each other without regard to their placement in the archive. You may use nm -s or nm --print-armap to list this index table. If an archive lacks the table, another form of ar called ranlib can be used to add just the table. GNU ar can optionally create a thin archive, which contains a symbol index and references to the original copies of the member files of the archives. Such an archive is useful for building libraries for use within a local build, where the relocatable objects are expected to remain available, and copying the contents of each object would only waste time and space. Thin archives are also flattened, so that adding one or more archives to a thin archive will add the elements of the nested archive individually. The paths to the elements of the archive are stored relative to the archive itself. GNU ar is designed to be compatible with two different facilities. You can control its activity using command-line options, like the different varieties of ar on Unix systems; or, if you specify the single command-line option -M, you can control it with a script supplied via standard input, like the MRI \"librarian\" program.","Process Name":"avr32-linux-gnu-ar","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-ar"}},{"Process":{"Description":"GNU as is really a family of assemblers. If you use (or have used) the GNU assembler on one architecture, you should find a fairly similar environment when you use it on another architecture. Each version has much in common with the others, including object file formats, most assembler directives (often called pseudo-ops) and assembler syntax. as is primarily intended to assemble the output of the GNU C compiler \"gcc\" for use by the linker \"ld\". Nevertheless, we've tried to make as assemble correctly everything that other assemblers for the same machine would assemble. Any exceptions are documented explicitly. This doesn't mean as always uses the same syntax as another assembler for the same architecture; for example, we know of several incompatible versions of 680x0 assembly language syntax. Each time you run as it assembles exactly one source program. The source program is made up of one or more files. (The standard input is also a file.) You give as a command line that has zero or more input file names. The input files are read (from left file name to right). A command line argument (in any position) that has no special meaning is taken to be an input file name. If you give as no file names it attempts to read one input file from the as standard input, which is normally your terminal. You may have to type ctl-D to tell as there is no more program to assemble. Use -- if you need to explicitly name the standard input file in your command line. If the source is empty, as produces a small, empty object file. as may write warnings and error messages to the standard error file (usually your terminal). This should not happen when a compiler runs as automatically. Warnings report an assumption made so that as could keep assembling a flawed program; errors report a grave problem that stops the assembly. If you are invoking as via the GNU C compiler, you can use the -Wa option to pass arguments through to the assembler. The assembler arguments must be separated from each other (and the -Wa) by commas. For example: gcc -c -g -O -Wa,-alh,-L file.c This passes two options to the assembler: -alh (emit a listing to standard output with high-level and assembly source) and -L (retain local symbols in the symbol table). Usually you do not need to use this -Wa mechanism, since many compiler command-line options are automatically passed to the assembler by the compiler. (You can call the GNU compiler driver with the -v option to see precisely what options it passes to each compilation pass, including the assembler.)","Process Name":"avr32-linux-gnu-as","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-as"}},{"Process":{"Description":"The C ++ and Java languages provide function overloading, which means that you can write many functions with the same name, providing that each function takes parameters of different types. In order to be able to distinguish these similarly named functions C ++ and Java encode them into a low-level assembler name which uniquely identifies each different version. This process is known as mangling. The c++filt [1] program does the inverse mapping: it decodes (demangles) low-level names into user-level names so that they can be read. Every alphanumeric word (consisting of letters, digits, underscores, dollars, or periods) seen in the input is a potential mangled name. If the name decodes into a C ++ name, the C ++ name replaces the low-level name in the output, otherwise the original word is output. In this way you can pass an entire assembler source file, containing mangled names, through c++filt and see the same source file containing demangled names. You can also use c++filt to decipher individual symbols by passing them on the command line: c++filt <symbol> If no symbol arguments are given, c++filt reads symbol names from the standard input instead. All the results are printed on the standard output. The difference between reading names from the command line versus reading names from the standard input is that command line arguments are expected to be just mangled names and no checking is performed to separate them from surrounding text. Thus for example: c++filt -n _Z1fv will work and demangle the name to \"f()\" whereas: c++filt -n _Z1fv, will not work. (Note the extra comma at the end of the mangled name which makes it invalid). This command however will work: echo _Z1fv, | c++filt -n and will display \"f(),\", i.e., the demangled name followed by a trailing comma. This behaviour is because when the names are read from the standard input it is expected that they might be part of an assembler source file where there might be extra, extraneous characters trailing after a mangled name. For example: .type   _Z1fv, @function","Process Name":"avr32-linux-gnu-c++filt","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-c++filt"}},{"Process":{"Description":"The C preprocessor, often known as cpp, is a macro processor that is used automatically by the C compiler to transform your program before compilation. It is called a macro processor because it allows you to define macros, which are brief abbreviations for longer constructs. The C preprocessor is intended to be used only with C, C ++ , and Objective-C source code. In the past, it has been abused as a general text processor. It will choke on input which does not obey C's lexical rules. For example, apostrophes will be interpreted as the beginning of character constants, and cause errors. Also, you cannot rely on it preserving characteristics of the input which are not significant to C-family languages. If a Makefile is preprocessed, all the hard tabs will be removed, and the Makefile will not work. Having said that, you can often get away with using cpp on things which are not C. Other Algol-ish programming languages are often safe (Pascal, Ada, etc.) So is assembly, with caution. -traditional-cpp mode preserves more white space, and is otherwise more permissive. Many of the problems can be avoided by writing C or C ++ style comments instead of native language comments, and keeping macros simple. Wherever possible, you should use a preprocessor geared to the language you are writing in. Modern versions of the GNU assembler have macro facilities. Most high level programming languages have their own conditional compilation and inclusion mechanism. If all else fails, try a true general text processor, such as GNU M4. C preprocessors vary in some details. This manual discusses the GNU C preprocessor, which provides a small superset of the features of ISO Standard C. In its default mode, the GNU C preprocessor does not do a few things required by the standard. These are features which are rarely, if ever, used, and may cause surprising changes to the meaning of a program which does not expect them. To get strict ISO Standard C, you should use the -std=c90, -std=c99 or -std=c11 options, depending on which version of the standard you want. To get all the mandatory diagnostics, you must also use -pedantic. This manual describes the behavior of the ISO preprocessor. To minimize gratuitous differences, where the ISO preprocessor's behavior does not conflict with traditional semantics, the traditional preprocessor should behave the same way. The various differences that do exist are detailed in the section Traditional Mode. For clarity, unless noted otherwise, references to CPP in this manual refer to GNU CPP .","Process Name":"avr32-linux-gnu-cpp","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-cpp"}},{"Process":{"Description":"dlltool reads its inputs, which can come from the -d and -b options as well as object files specified on the command line. It then processes these inputs and if the -e option has been specified it creates a exports file. If the -l option has been specified it creates a library file and if the -z option has been specified it creates a def file. Any or all of the -e, -l and -z options can be present in one invocation of dlltool. When creating a DLL , along with the source for the DLL , it is necessary to have three other files. dlltool can help with the creation of these files. The first file is a .def file which specifies which functions are exported from the DLL , which functions the DLL imports, and so on. This is a text file and can be created by hand, or dlltool can be used to create it using the -z option. In this case dlltool will scan the object files specified on its command line looking for those functions which have been specially marked as being exported and put entries for them in the .def file it creates. In order to mark a function as being exported from a DLL , it needs to have an -export:<name_of_function> entry in the .drectve section of the object file. This can be done in C by using the asm() operator: asm (\".section .drectve\");\nasm (\".ascii \\\"-export:my_func\\\"\");\n\nint my_func (void) { ... } The second file needed for DLL creation is an exports file. This file is linked with the object files that make up the body of the DLL and it handles the interface between the DLL and the outside world. This is a binary file and it can be created by giving the -e option to dlltool when it is creating or reading in a .def file. The third file needed for DLL creation is the library file that programs will link with in order to access the functions in the DLL (an 'import library'). This file can be created by giving the -l option to dlltool when it is creating or reading in a .def file. If the -y option is specified, dlltool generates a delay-import library that can be used instead of the normal import library to allow a program to link to the dll only as soon as an imported function is called for the first time. The resulting executable will need to be linked to the static delayimp library containing __delayLoadHelper2(), which in turn will import LoadLibraryA and GetProcAddress from kernel32. dlltool builds the library file by hand, but it builds the exports file by creating temporary files containing assembler statements and then assembling these. The -S command line option can be used to specify the path to the assembler that dlltool will use, and the -f option can be used to pass specific flags to that assembler. The -n can be used to prevent dlltool from deleting these temporary assembler files when it is done, and if -n is specified twice then this will prevent dlltool from deleting the temporary object files it used to build the library. Here is an example of creating a DLL from a source file dll.c and also creating a program (from an object file called program.o) that uses that DLL: gcc -c dll.c\ndlltool -e exports.o -l dll.lib dll.o\ngcc dll.o exports.o -o dll.dll\ngcc program.o dll.lib -o program dlltool may also be used to query an existing import library to determine the name of the DLL to which it is associated. See the description of the -I or --identify option.","Process Name":"avr32-linux-gnu-dlltool","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-dlltool"}},{"Process":{"Description":"elfedit updates the ELF header of ELF files which have the matching ELF machine and file types. The options control how and which fields in the ELF header should be updated. elffile... are the ELF files to be updated. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files.","Process Name":"avr32-linux-gnu-elfedit","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-elfedit"}},{"Process":{"Description":"When you invoke GCC , it normally does preprocessing, compilation, assembly and linking. The \"overall options\" allow you to stop this process at an intermediate stage. For example, the -c option says not to run the linker. Then the output consists of object files output by the assembler. Other options are passed on to one stage of processing. Some options control the preprocessor and others the compiler itself. Yet other options control the assembler and linker; most of these are not documented here, since you rarely need to use any of them. Most of the command-line options that you can use with GCC are useful for C programs; when an option is only useful with another language (usually C ++ ), the explanation says so explicitly. If the description for a particular option does not mention a source language, you can use that option with all supported languages. The gcc program accepts options and file names as operands. Many options have multi-letter names; therefore multiple single-letter options may not be grouped: -dv is very different from -d -v. You can mix options and other arguments. For the most part, the order you use doesn't matter. Order does matter when you use several options of the same kind; for example, if you specify -L more than once, the directories are searched in the order specified. Also, the placement of the -l option is significant. Many options have long names starting with -f or with -W---for example, -fmove-loop-invariants, -Wformat and so on. Most of these have both positive and negative forms; the negative form of -ffoo would be -fno-foo. This manual documents only one of these two forms, whichever one is not the default.","Process Name":"avr32-linux-gnu-gcc","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-gcc"}},{"Process":{"Description":"gcov is a test coverage program. Use it in concert with GCC to analyze your programs to help create more efficient, faster running code and to discover untested parts of your program. You can use gcov as a profiling tool to help discover where your optimization efforts will best affect your code. You can also use gcov along with the other profiling tool, gprof, to assess which parts of your code use the greatest amount of computing time. Profiling tools help you analyze your code's performance. Using a profiler such as gcov or gprof, you can find out some basic performance statistics, such as: \u2022 how often each line of code executes \u2022 what lines of code are actually executed \u2022 how much computing time each section of code uses Once you know these things about how your code works when compiled, you can look at each module to see which modules should be optimized. gcov helps you determine where to work on optimization. Software developers also use coverage testing in concert with testsuites, to make sure software is actually good enough for a release. Testsuites can verify that a program works as expected; a coverage program tests to see how much of the program is exercised by the testsuite. Developers can then determine what kinds of test cases need to be added to the testsuites to create both better testing and a better final product. You should compile your code without optimization if you plan to use gcov because the optimization, by combining some lines of code into one function, may not give you as much information as you need to look for 'hot spots' where the code is using a great deal of computer time. Likewise, because gcov accumulates statistics by line (at the lowest resolution), it works best with a programming style that places only one statement on each line. If you use complicated macros that expand to loops or to other control structures, the statistics are less helpful---they only report on the line where the macro call appears. If your complex macros behave like functions, you can replace them with inline functions to solve this problem. gcov creates a logfile called sourcefile.gcov which indicates how many times each line of a source file sourcefile.c has executed. You can use these logfiles along with gprof to aid in fine-tuning the performance of your programs. gprof gives timing information you can use along with the information you get from gcov. gcov works only on code compiled with GCC . It is not compatible with any other profiling or test coverage mechanism.","Process Name":"avr32-linux-gnu-gcov","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-gcov"}},{"Process":{"Description":"\"gprof\" produces an execution profile of C, Pascal, or Fortran77 programs. The effect of called routines is incorporated in the profile of each caller. The profile data is taken from the call graph profile file (gmon.out default) which is created by programs that are compiled with the -pg option of \"cc\", \"pc\", and \"f77\". The -pg option also links in versions of the library routines that are compiled for profiling. \"Gprof\" reads the given object file (the default is \"a.out\") and establishes the relation between its symbol table and the call graph profile from gmon.out. If more than one profile file is specified, the \"gprof\" output shows the sum of the profile information in the given profile files. \"Gprof\" calculates the amount of time spent in each routine. Next, these times are propagated along the edges of the call graph. Cycles are discovered, and calls into a cycle are made to share the time of the cycle. Several forms of output are available from the analysis. The flat profile shows how much time your program spent in each function, and how many times that function was called. If you simply want to know which functions burn most of the cycles, it is stated concisely here. The call graph shows, for each function, which functions called it, which other functions it called, and how many times. There is also an estimate of how much time was spent in the subroutines of each function. This can suggest places where you might try to eliminate function calls that use a lot of time. The annotated source listing is a copy of the program's source code, labeled with the number of times each line of the program was executed.","Process Name":"avr32-linux-gnu-gprof","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-gprof"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"avr32-linux-gnu-ld","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-ld"}},{"Process":{"Description":"ld combines a number of object and archive files, relocates their data and ties up symbol references. Usually the last step in compiling a program is to run ld. ld accepts Linker Command Language files written in a superset of AT&T 's Link Editor Command Language syntax, to provide explicit and total control over the linking process. This man page does not describe the command language; see the ld entry in \"info\" for full details on the command language and on other aspects of the GNU linker. This version of ld uses the general purpose BFD libraries to operate on object files. This allows ld to read, combine, and write object files in many different formats---for example, COFF or \"a.out\". Different formats may be linked together to produce any available kind of object file. Aside from its flexibility, the GNU linker is more helpful than other linkers in providing diagnostic information. Many linkers abandon execution immediately upon encountering an error; whenever possible, ld continues executing, allowing you to identify other errors (or, in some cases, to get an output file in spite of the error). The GNU linker ld is meant to cover a broad range of situations, and to be as compatible as possible with other linkers. As a result, you have many choices to control its behavior.","Process Name":"avr32-linux-gnu-ld.bfd","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-ld.bfd"}},{"Process":{"Description":"nlmconv converts the relocatable i386 object file infile into the NetWare Loadable Module outfile, optionally reading headerfile for NLM header information. For instructions on writing the NLM command file language used in header files, see the linkers section, NLMLINK in particular, of the NLM Development and Tools Overview, which is part of the NLM Software Developer's Kit (\" NLM SDK \"), available from Novell, Inc. nlmconv uses the GNU Binary File Descriptor library to read infile; nlmconv can perform a link step. In other words, you can list more than one object file for input if you list them in the definitions file (rather than simply specifying one input file on the command line). In this case, nlmconv calls the linker for you.","Process Name":"avr32-linux-gnu-nlmconv","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-nlmconv"}},{"Process":{"Description":"GNU nm lists the symbols from object files objfile.... If no object files are listed as arguments, nm assumes the file a.out. For each symbol, nm shows: \u2022 The symbol value, in the radix selected by options (see below), or hexadecimal by default. \u2022 The symbol type. At least the following types are used; others are, as well, depending on the object file format. If lowercase, the symbol is usually local; if uppercase, the symbol is global (external). There are however a few lowercase symbols that are shown for special global symbols (\"u\", \"v\" and \"w\"). \"A\" The symbol's value is absolute, and will not be changed by further linking. \"B\" \"b\" The symbol is in the uninitialized data section (known as BSS ). \"C\" The symbol is common. Common symbols are uninitialized data. When linking, multiple common symbols may appear with the same name. If the symbol is defined anywhere, the common symbols are treated as undefined references. \"D\" \"d\" The symbol is in the initialized data section. \"G\" \"g\" The symbol is in an initialized data section for small objects. Some object file formats permit more efficient access to small data objects, such as a global int variable as opposed to a large global array. \"i\" For PE format files this indicates that the symbol is in a section specific to the implementation of DLLs. For ELF format files this indicates that the symbol is an indirect function. This is a GNU extension to the standard set of ELF symbol types. It indicates a symbol which if referenced by a relocation does not evaluate to its address, but instead must be invoked at runtime. The runtime execution will then return the value to be used in the relocation. \"N\" The symbol is a debugging symbol. \"p\" The symbols is in a stack unwind section. \"R\" \"r\" The symbol is in a read only data section. \"S\" \"s\" The symbol is in an uninitialized data section for small objects. \"T\" \"t\" The symbol is in the text (code) section. \"U\" The symbol is undefined. \"u\" The symbol is a unique global symbol. This is a GNU extension to the standard set of ELF symbol bindings. For such a symbol the dynamic linker will make sure that in the entire process there is just one symbol with this name and type in use. \"V\" \"v\" The symbol is a weak object. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the weak symbol becomes zero with no error. On some systems, uppercase indicates that a default value has been specified. \"W\" \"w\" The symbol is a weak symbol that has not been specifically tagged as a weak object symbol. When a weak defined symbol is linked with a normal defined symbol, the normal defined symbol is used with no error. When a weak undefined symbol is linked and the symbol is not defined, the value of the symbol is determined in a system-specific manner without error. On some systems, uppercase indicates that a default value has been specified. \"-\" The symbol is a stabs symbol in an a.out object file. In this case, the next values printed are the stabs other field, the stabs desc field, and the stab type. Stabs symbols are used to hold debugging information. \"?\" The symbol type is unknown, or object file format specific. \u2022 The symbol name.","Process Name":"avr32-linux-gnu-nm","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-nm"}},{"Process":{"Description":"The GNU objcopy utility copies the contents of an object file to another. objcopy uses the GNU BFD Library to read and write the object files. It can write the destination object file in a format different from that of the source object file. The exact behavior of objcopy is controlled by command-line options. Note that objcopy should be able to copy a fully linked file between any two formats. However, copying a relocatable object file between any two formats may not work as expected. objcopy creates temporary files to do its translations and deletes them afterward. objcopy uses BFD to do all its translation work; it has access to all the formats described in BFD and thus is able to recognize most formats without being told explicitly. objcopy can be used to generate S-records by using an output target of srec (e.g., use -O srec). objcopy can be used to generate a raw binary file by using an output target of binary (e.g., use -O binary). When objcopy generates a raw binary file, it will essentially produce a memory dump of the contents of the input object file. All symbols and relocation information will be discarded. The memory dump will start at the load address of the lowest section copied into the output file. When generating an S-record or a raw binary file, it may be helpful to use -S to remove sections containing debugging information. In some cases -R will be useful to remove sections which contain information that is not needed by the binary file. Note---objcopy is not able to change the endianness of its input files. If the input format has an endianness (some formats do not), objcopy can only copy the inputs into file formats that have the same endianness or which have no endianness (e.g., srec). (However, see the --reverse-bytes option.)","Process Name":"avr32-linux-gnu-objcopy","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-objcopy"}},{"Process":{"Description":"objdump displays information about one or more object files. The options control what particular information to display. This information is mostly useful to programmers who are working on the compilation tools, as opposed to programmers who just want their program to compile and work. objfile... are the object files to be examined. When you specify archives, objdump shows information on each of the member object files.","Process Name":"avr32-linux-gnu-objdump","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-objdump"}},{"Process":{"Description":"ranlib generates an index to the contents of an archive and stores it in the archive. The index lists each symbol defined by a member of an archive that is a relocatable object file. You may use nm -s or nm --print-armap to list this index. An archive with such an index speeds up linking to the library and allows routines in the library to call each other without regard to their placement in the archive. The GNU ranlib program is another form of GNU ar; running ranlib is completely equivalent to executing ar -s.","Process Name":"avr32-linux-gnu-ranlib","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-ranlib"}},{"Process":{"Description":"readelf displays information about one or more ELF format object files. The options control what particular information to display. elffile... are the object files to be examined. 32-bit and 64-bit ELF files are supported, as are archives containing ELF files. This program performs a similar function to objdump but it goes into more detail and it exists independently of the BFD library, so if there is a bug in BFD then readelf will not be affected.","Process Name":"avr32-linux-gnu-readelf","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-readelf"}},{"Process":{"Description":"The GNU size utility lists the section sizes---and the total size---for each of the object or archive files objfile in its argument list. By default, one line of output is generated for each object file or each module in an archive. objfile... are the object files to be examined. If none are specified, the file \"a.out\" will be used.","Process Name":"avr32-linux-gnu-size","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-size"}},{"Process":{"Description":"For each file given, GNU strings prints the printable character sequences that are at least 4 characters long (or the number given with the options below) and are followed by an unprintable character. By default, it only prints the strings from the initialized and loaded sections of object files; for other types of files, it prints the strings from the whole file. strings is mainly useful for determining the contents of non-text files.","Process Name":"avr32-linux-gnu-strings","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-strings"}},{"Process":{"Description":"GNU strip discards all symbols from object files objfile. The list of object files may include archives. At least one object file must be given. strip modifies the files named in its argument, rather than writing modified copies under different names.","Process Name":"avr32-linux-gnu-strip","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-strip"}},{"Process":{"Description":"windmc reads message definitions from an input file (.mc) and translate them into a set of output files. The output files may be of four kinds: \"h\" A C header file containing the message definitions. \"rc\" A resource file compilable by the windres tool. \"bin\" One or more binary files containing the resource data for a specific message language. \"dbg\" A C include file that maps message id's to their symbolic name. The exact description of these different formats is available in documentation from Microsoft. When windmc converts from the \"mc\" format to the \"bin\" format, \"rc\", \"h\", and optional \"dbg\" it is acting like the Windows Message Compiler.","Process Name":"avr32-linux-gnu-windmc","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-windmc"}},{"Process":{"Description":"windres reads resources from an input file and copies them into an output file. Either file may be in one of three formats: \"rc\" A text format read by the Resource Compiler. \"res\" A binary format generated by the Resource Compiler. \"coff\" A COFF object or executable. The exact description of these different formats is available in documentation from Microsoft. When windres converts from the \"rc\" format to the \"res\" format, it is acting like the Windows Resource Compiler. When windres converts from the \"res\" format to the \"coff\" format, it is acting like the Windows \"CVTRES\" program. When windres generates an \"rc\" file, the output is similar but not identical to the format expected for the input. When an input \"rc\" file refers to an external filename, an output \"rc\" file will instead include the file contents. If the input or output format is not specified, windres will guess based on the file name, or, for the input file, the file contents. A file with an extension of .rc will be treated as an \"rc\" file, a file with an extension of .res will be treated as a \"res\" file, and a file with an extension of .o or .exe will be treated as a \"coff\" file. If no output file is specified, windres will print the resources in \"rc\" format to standard output. The normal use is for you to write an \"rc\" file, use windres to convert it to a COFF object file, and then link the COFF file into your application. This will make the resources described in the \"rc\" file available to Windows.","Process Name":"avr32-linux-gnu-windres","Link":"https:\/\/linux.die.net\/man\/1\/avr32-linux-gnu-windres"}},{"Process":{"Description":"Avrdude is a program for downloading code and data to Atmel AVR microcontrollers. Avrdude supports Atmel's STK500 programmer, Atmel's AVRISP and AVRISP mkII devices, Atmel's STK600, Atmel's JTAG ICE (both mkI and mkII, the latter also in ISP mode), programmers complying to AppNote AVR910 and AVR109 (including the Butterfly), as well as a simple hard-wired programmer connected directly to a ppi(4) or parport(4) parallel port, or to a standard serial port. In the simplest case, the hardware consists just of a cable connecting the respective AVR signal lines to the parallel port. The MCU is programmed in serial programming mode, so, for the ppi(4) based programmer, the MCU signals '\/RESET', 'SCK', 'MISO' and 'MOSI' need to be connected to the parallel port. Optionally, some otherwise unused output pins of the parallel port can be used to supply power for the MCU part, so it is also possible to construct a passive stand-alone programming device. Some status LEDs indicating the current operating state of the programmer can be connected, and a signal is available to control a buffer\/driver IC 74LS367 (or 74HCT367). The latter can be useful to decouple the parallel port from the MCU when in-system programming is used. A number of equally simple bit-bang programming adapters that connect to a serial port are supported as well, among them the popular Ponyprog serial adapter, and the DASA and DASA3 adapters that used to be supported by uisp(1). Note that these adapters are meant to be attached to a physical serial port. Connecting to a serial port emulated on top of USB is likely to not work at all, or to work abysmally slow. Atmel's STK500 programmer is also supported and connects to a serial port. Both, firmware versions 1.x and 2.x can be handled, but require a different programmer type specification (by now). Using firmware version 2, high-voltage programming is also supported, both parallel and serial (programmer types stk500pp and stk500hvsp). The Arduino (which is very similar to the STK500 1.x) is supported via its own programmer type specification ''arduino''. The BusPirate is a versatile tool that can also be used as an AVR programmer. A single BusPirate can be connected to up to 3 independent AVRs. See the section on extended parameters below for details. Atmel's STK600 programmer is supported in ISP and high-voltage programming modes, and connects through the USB. For ATxmega devices, the STK600 is supported in PDI mode. For ATtiny4\/5\/9\/10 devices, the STK600 and AVRISP mkII are supported in TPI mode. The simple serial programmer described in Atmel's application note AVR910, and the bootloader described in Atmel's application note AVR109 (which is also used by the AVR Butterfly evaluation board), are supported on a serial port. Atmel's JTAG ICE (both mkI and mkII) is supported as well to up- or download memory areas from\/to an AVR target (no support for on-chip debugging). For the JTAG ICE mkII, JTAG, debugWire and ISP mode are supported, provided it has a firmware revision of at least 4.14 (decimal). See below for the limitations of debugWire. For ATxmega devices, the JTAG ICE mkII is supported in PDI mode, provided it has a revision 1 hardware and firmware version of at least 5.37 (decimal). The AVR Dragon is supported in all modes (ISP, JTAG, HVSP, PP, debugWire). When used in JTAG and debugWire mode, the AVR Dragon behaves similar to a JTAG ICE mkII, so all device-specific comments for that device will apply as well. When used in ISP mode, the AVR Dragon behaves similar to an AVRISP mkII (or JTAG ICE mkII in ISP mode), so all device-specific comments will apply there. In particular, the Dragon starts out with a rather fast ISP clock frequency, so the -B bitclock option might be required to achieve a stable ISP communication. For ATxmega devices, the AVR Dragon is supported in PDI mode, provided it has a firmware version of at least 6.11 (decimal). The USBasp ISP and USBtinyISP adapters are also supported, provided avrdude has been compiled with libusb support. They both feature simple firmware-only USB implementations, running on an ATmega8 (or ATmega88), or ATtiny2313, respectively. Input files can be provided, and output files can be written in different file formats, such as raw binary files containing the data to download to the chip, Intel hex format, or Motorola S-record format. There are a number of tools available to produce those files, like asl(1) as a standalone assembler, or avr-objcopy(1) for the final stage of the GNU toolchain for the AVR microcontroller. Avrdude can program the EEPROM and flash ROM memory cells of supported AVR parts. Where supported by the serial instruction set, fuse bits and lock bits can be programmed as well. These are implemented within avrdude as separate memory types and can be programmed using data from a file (see the -m option) or from terminal mode (see the dump and write commands). It is also possible to read the chip (provided it has not been code-protected previously, of course) and store the data in a file. Finally, a ''terminal'' mode is available that allows one to interactively communicate with the MCU, and to display or program individual memory cells. On the STK500 and STK600 programmer, several operational parameters (target supply voltage, target Aref voltage, master clock) can be examined and changed from within terminal mode as well. Options In order to control all the different operation modi, a number of options need to be specified to avrdude.              -p partno This is the only option that is mandatory for every invocation of avrdude. It specifies the type of the MCU connected to the programmer. These are read from the config file. If avrdude does not know about a part that you have, simply add it to the config file (be sure and submit a patch back to the author so that it can be incorporated for the next version). See the sample config file for the format. Currently, the following MCU types are understood: HTML-IMAGEll. Option tag Official part name 1200 AT90S1200 2313 AT90S2313 2333 AT90S2333 2343 AT90S2343 (*) 4414 AT90S4414 4433 AT90S4433 4434 AT90S4434 8515 AT90S8515 8535 AT90S8535 c128 AT90CAN128 c32 AT90CAN32 c64 AT90CAN64 m103 ATmega103 m128 ATmega128 m1280 ATmega1280 m1281 ATmega1281 m1284p ATmega1284P m128rfa1 ATmega128RFA1 m16 ATmega16 m161 ATmega161 m162 ATmega162 m163 ATmega163 m164 ATmega164 m164p ATmega164P m168 ATmega168 m169 ATmega169 m2560 ATmega2560 (**) m2561 ATmega2561 (**) m32 ATmega32 m324p ATmega324P m325 ATmega325 m3250 ATmega3250 m328p ATmega328P m329 ATmega329 m3290 ATmega3290 m329p ATmega329P m3290p ATmega3290P m32u4 ATmega32U4 m48 ATmega48 m64 ATmega64 m640 ATmega640 m644p ATmega644P m644 ATmega644 m645 ATmega645 m6450 ATmega6450 m649 ATmega649 m6490 ATmega6490 m8 ATmega8 m8515 ATmega8515 m8535 ATmega8535 m88 ATmega88 pwm2 AT90PWM2 pwm2b AT90PWM2B pwm3 AT90PWM3 pwm3b AT90PWM3B t10 ATtiny10 t12 ATtiny12 (***) t13 ATtiny13 t15 ATtiny15 t2313 ATtiny2313 t25 ATtiny25 t26 ATtiny26 t261 ATtiny261 t4 ATtiny4 t44 ATtiny44 t45 ATtiny45 t461 ATtiny461 t5 ATtiny5 t84 ATtiny84 t85 ATtiny85 t861 ATtiny861 t88 ATtiny88 t9 ATtiny9 ucr2 AT32uca0512 usb1286 ATmega1286 usb1287 ATmega1287 usb162 ATmega162 usb646 ATmega647 usb647 ATmega647 usb82 ATmega82 x128a1 ATxmega128A1 x128a1d ATxmega128A1revD x128a3 ATxmega128A3 x128a4 ATxmega128A4 x16a4 ATxmega16A4 x192a1 ATxmega192A1 x192a3 ATxmega192A3 x256a1 ATxmega256A1 x256a3 ATxmega256A3 x256a3b ATxmega256A3B x32a4 ATxmega32A4 x64a1 ATxmega64A1 x64a3 ATxmega64A3 x64a4 ATxmega64A4 HTML-IMAGE-END.Bl -tag -width \"(**) \" (*)' The AT90S2323 and ATtiny22 use the same algorithm. (**)' Flash addressing above 128 KB is not supported by all programming hardware. Known to work are jtag2, stk500v2, and bit-bang programmers. (***) The ATtiny11 uses the same algorithm, but can only be programmed in high-voltage serial mode. Override the RS-232 connection baud rate specified in the respective programmer's entry of the configuration file. Specify the bit clock period for the JTAG interface or the ISP clock (JTAG ICE only). The value is a floating-point number in microseconds. The default value of the JTAG ICE results in about 1 microsecond bit clock period, suitable for target MCUs running at 4 MHz clock and above. Unlike certain parameters in the STK500, the JTAG ICE resets all its parameters to default values when the programming software signs off from the ICE, so for MCUs running at lower clock speeds, this parameter must be specified on the command-line. Use the pin configuration specified by the argument. Pin configurations are read from the config file (see the -C option). New pin configurations can be easily added or modified through the use of a config file to make avrdude work with different programmers as long as the programmer supports the Atmel AVR serial program method. You can use the 'default_programmer' keyword in your ${HOME}\/.avrduderc file to assign a default programmer to keep from having to specify this option on every invocation. Use the specified config file to load configuration data. This file contains all programmer and part definitions that avrdude knows about. If you have a programmer or part that avrdude does not know about, you can add it to the config file (be sure and submit a patch back to the author so that it can be incorporated for the next version). See the config file, located at ${PREFIX}\/etc\/avrdude\/avrdude.conf, which contains a description of the format. Disable auto erase for flash. When the -U option with flash memory is specified, avrdude will perform a chip erase before starting any of the programming operations, since it generally is a mistake to program the flash without performing an erase first. This option disables that. Auto erase is not used for ATxmega devices as these devices can use page erase before writing each page so no explicit chip erase is required. Note however that any page not affected by the current operation will retain its previous contents. Causes a chip erase to be executed. This will reset the contents of the flash ROM and EEPROM to the value '0xff', and clear all lock bits. Except for ATxmega devices which can use page erase, it is basically a prerequisite command before the flash ROM can be reprogrammed again. The only exception would be if the new contents would exclusively cause bits to be programmed from the value '1' to '0'. Note that in order to reprogram EERPOM cells, no explicit prior chip erase is required since the MCU provides an auto-erase cycle in that case before programming the cell. [,exitspec] By default, avrdude leaves the parallel port in the same state at exit as it has been found at startup. This option modifies the state of the '\/RESET' and 'Vcc' lines the parallel port is left at, according to the exitspec arguments provided, as follows: reset' The '\/RESET' signal will be left activated at program exit, that is it will be held low, in order to keep the MCU in reset state afterwards. Note in particular that the programming algorithm for the AT90S1200 device mandates that the '\/RESET' signal is active before powering up the MCU, so in case an external power supply is used for this MCU type, a previous invocation of avrdude with this option specified is one of the possible ways to guarantee this condition. noreset The '\/RESET' line will be deactivated at program exit, thus allowing the MCU target program to run while the programming hardware remains connected. vcc' This option will leave those parallel port pins active (i. e. high) that can be used to supply 'Vcc' power to the MCU. novcc' This option will pull the 'Vcc' pins of the parallel port down at program exit. Multiple exitspec arguments can be separated with commas. Normally, avrdude tries to verify that the device signature read from the part is reasonable before continuing. Since it can happen from time to time that a device has a broken (erased or overwritten) device signature but is otherwise operating normally, this options is provided to override the check. Also, for programmers like the Atmel STK500 and STK600 which can adjust parameters local to the programming tool (independent of an actual connection to a target controller), this option can be used together with -t to continue in terminal mode. For bitbang-type programmers, delay for approximately delay microseconds between each bit state change. If the host system is very fast, or the target runs off a slow clock (like a 32 kHz crystal, or the 128 kHz internal RC oscillator), this can become necessary to satisfy the requirement that the ISP clock frequency must not be higher than 1\/4 of the CPU clock frequency. This is implemented as a spin-loop delay to allow even for very short delays. On Unix-style operating systems, the spin loop is initially calibrated against a system timer, so the number of microseconds might be rather realistic, assuming a constant system load while avrdude is running. On Win32 operating systems, a preconfigured number of cycles per microsecond is assumed that might be off a bit for very fast or very slow machines. No-write - disables actually writing data to the MCU (useful for debugging avrdude ). Perform a RC oscillator run-time calibration according to Atmel application note AVR053. This is only supported on the STK500v2, AVRISP mkII, and JTAG ICE mkII hardware. Note that the result will be stored in the EEPROM cell at address 0. Use port to identify the device to which the programmer is attached. By default the \/dev\/ppi0 port is used, but if the programmer type normally connects to the serial port, the \/dev\/cuaa0 port is the default. If you need to use a different parallel or serial port, use this option to specify the alternate port name. On Win32 operating systems, the parallel ports are referred to as lpt1 through lpt3, referring to the addresses 0x378, 0x278, and 0x3BC, respectively. If the parallel port can be accessed through a different address, this address can be specified directly, using the common C language notation (i. e., hexadecimal values are prefixed by '0x' ). For the JTAG ICE mkII, if avrdude has been configured with libusb support, port can alternatively be specified as usb[:serialno]. This will cause avrdude to search a JTAG ICE mkII on USB. If serialno is also specified, it will be matched against the serial number read from any JTAG ICE mkII found on USB. The match is done after stripping any existing colons from the given serial number, and right-to-left, so only the least significant bytes from the serial number need to be given. As the AVRISP mkII device can only be talked to over USB, the very same method of specifying the port is required there. For the USB programmer \"AVR-Doper\" running in HID mode, the port must be specified as avrdoper. Libusb support is required on Unix but not on Windows. For more information about AVR-Doper see http:\/\/www.obdev.at\/avrusb\/avrdoper.html. For programmers that attach to a serial port using some kind of higher level protocol (as opposed to bit-bang style programmers), port can be specified as net:host:port. In this case, instead of trying to open a local device, a TCP network connection to (TCP) port on host is established. The remote endpoint is assumed to be a terminal or console server that connects the network stream to a local serial port where the actual programmer has been attached to. The port is assumed to be properly configured, for example using a transparent 8-bit data connection without parity at 115200 Baud for a STK500. This feature is currently not implemented for Win32 systems. Disable (or quell) output of the progress bar while reading or writing to the device. Specify it a second time for even quieter operation. Disable safemode prompting. When safemode discovers that one or more fuse bits have unintentionally changed, it will prompt for confirmation regarding whether or not it should attempt to recover the fuse bit(s). Specifying this flag disables the prompt and assumes that the fuse bit(s) should be recovered without asking for confirmation first. Tells avrdude to enter the interactive ''terminal'' mode instead of up- or downloading files. See below for a detailed description of the terminal mode. Disable the safemode fuse bit checks. Safemode is enabled by default and is intended to prevent unintentional fuse bit changes. When enabled, safemode will issue a warning if the any fuse bits are found to be different at program exit than they were when avrdude was invoked. Safemode won't alter fuse bits itself, but rather will prompt for instructions, unless the terminal is non-interactive, in which case safemode is disabled. See the -s option to disable safemode prompting. :op:filename[:format] Perform a memory operation as indicated. The memtype field specifies the memory type to operate on. The available memory types are device-dependent, the actual configuration can be viewed with the part command in terminal mode. Typically, a device's memory configuration at least contains the memory types flash and eeprom. All memory types currently known are: calibration One or more bytes of RC oscillator calibration data. eeprom' The EEPROM of the device. efuse' The extended fuse byte. flash' The flash ROM of the device. fuse' The fuse byte in devices that have only a single fuse byte. hfuse' The high fuse byte. lfuse' The low fuse byte. lock' The lock byte. signature' The three device signature bytes (device ID). fuseN' The fuse bytes of ATxmega devices, N is an integer number for each fuse supported by the device. application The application flash area of ATxmega devices. apptable' The application table flash area of ATxmega devices. boot' The boot flash area of ATxmega devices. prodsig' The production signature (calibration) area of ATxmega devices. usersig' The user signature area of ATxmega devices. The op field specifies what operation to perform: r' read device memory and write to the specified file w' read data from the specified file and write to the device memory v' read data from both the device and the specified file and perform a verify The filename field indicates the name of the file to read or write. The format field is optional and contains the format of the file to read or write. Format can be one of: i' Intel Hex s' Motorola S-record r' raw binary; little-endian byte order, in the case of the flash ROM data m' immediate; actual byte values specified on the command line, separated by commas or spaces. This is good for programming fuse bytes without having to create a single-byte file or enter terminal mode. a' auto detect; valid for input only, and only if the input is not provided at stdin. d' decimal; this and the following formats are only valid on output. They generate one line of output for the respective memory section, forming a comma-separated list of the values. This can be particularly useful for subsequent processing, like for fuse bit settings. h' hexadecimal; each value will get the string 0x prepended. o' octal; each value will get a 0 prepended unless it is less than 8 in which case it gets no prefix. b' binary; each value will get the string 0b prepended. The default is to use auto detection for input files, and raw binary format for output files. Note that if filename contains a colon, the format field is no longer optional since the filename part following the colon would otherwise be misinterpreted as format. As an abbreviation, the form -U filename is equivalent to specifying -U flash:w:filename:a. This will only work if filename does not have a colon in it. Enable verbose output. Disable automatic verify check when uploading data. Pass extended_param to the chosen programmer implementation as an extended parameter. The interpretation of the extended parameter depends on the programmer itself. See below for a list of programmers accepting extended parameters. Tells avrdude to use the last four bytes of the connected parts' EEPROM memory to track the number of times the device has been erased. When this option is used and the -e flag is specified to generate a chip erase, the previous counter will be saved before the chip erase, it is then incremented, and written back after the erase cycle completes. Presumably, the device would only be erased just before being programmed, and thus, this can be utilized to give an indication of how many erase-rewrite cycles the part has undergone. Since the FLASH memory can only endure a finite number of erase-rewrite cycles, one can use this option to track when a part is nearing the limit. The typical limit for Atmel AVR FLASH is 1000 cycles. Of course, if the application needs the last four bytes of EEPROM memory, this option should not be used. Instructs avrdude to initialize the erase-rewrite cycle counter residing at the last four bytes of EEPROM memory to the specified value. If the application needs the last four bytes of EEPROM memory, this option should not be used. Terminal mode In this mode, avrdude only initializes communication with the MCU, and then awaits user commands on standard input. Commands and parameters may be abbreviated to the shortest unambiguous form. Terminal mode provides a command history using readline(3), so previously entered command lines can be recalled and edited. The following commands are currently implemented: dump memtype addr nbytes Read nbytes bytes from the specified memory area, and display them in the usual hexadecimal and ASCII form. dump' Continue dumping the memory contents for another nbytes where the previous dump command left off. write memtype addr byte1 ... byteN Manually program the respective memory cells, starting at address addr, using the values byte1 through byteN. This feature is not implemented for bank-addressed memories such as the flash memory of ATMega devices. erase Perform a chip erase. send b1 b2 b3 b4 Send raw instruction codes to the AVR device. If you need access to a feature of an AVR part that is not directly supported by avrdude, this command allows you to use it, even though avrdude does not implement the command. When using direct SPI mode, up to 3 bytes can be omitted. sig' Display the device signature bytes. spi' Enter direct SPI mode. The pgmled pin acts as slave select. Only supported on parallel bitbang programmers. part' Display the current part settings and parameters. Includes chip specific information including all memory types supported by the device, read\/write timing, etc. pgm' Return to programming mode (from direct SPI mode). vtarg voltage Set the target's supply voltage to voltage Volts. Only supported on the STK500 and STK600 programmer. varef [ channel] voltage Set the adjustable voltage source to voltage Volts. This voltage is normally used to drive the target's Aref input on the STK500. On the Atmel STK600, two reference voltages are available, which can be selected by the optional channel argument (either 0 or 1). Only supported on the STK500 and STK600 programmer. fosc freq[M|k] Set the master oscillator to freq Hz. An optional trailing letter M multiplies by 1E6, a trailing letter k by 1E3. Only supported on the STK500 and STK600 programmer. fosc off Turn the master oscillator off. Only supported on the STK500 and STK600 programmer. sck period STK500 and STK600 programmer only: Set the SCK clock period to period microseconds. JTAG ICE only: Set the JTAG ICE bit clock period to period microseconds. Note that unlike STK500 settings, this setting will be reverted to its default value (approximately 1 microsecond) when the programming software signs off from the JTAG ICE. This parameter can also be used on the JTAG ICE mkII to specify the ISP clock period when operating the ICE in ISP mode. parms STK500 and STK600 programmer only: Display the current voltage and master oscillator parameters. JTAG ICE only: Display the current target supply voltage and JTAG bit clock rate\/period. ?' help' Give a short on-line summary of the available commands. quit' Leave terminal mode and thus avrdude. Default Parallel port pin connections (these can be changed, see the -c option) HTML-IMAGEll. Pin number Function 2-5 Vcc (optional power supply to MCU) 7 \/RESET (to MCU) 8 SCK (to MCU) 9 MOSI (to MCU) 10 MISO (from MCU) 18-25 GND HTML-IMAGE-END.Ss debugWire limitations The debugWire protocol is Atmel's proprietary one-wire (plus ground) protocol to allow an in-circuit emulation of the smaller AVR devices, using the '\/RESET' line. DebugWire mode is initiated by activating the 'DWEN' fuse, and then power-cycling the target. While this mode is mainly intended for debugging\/emulation, it also offers limited programming capabilities. Effectively, the only memory areas that can be read or programmed in this mode are flash ROM and EEPROM. It is also possible to read out the signature. All other memory areas cannot be accessed. There is no chip erase functionality in debugWire mode; instead, while reprogramming the flash ROM, each flash ROM page is erased right before updating it. This is done transparently by the JTAG ICE mkII (or AVR Dragon). The only way back from debugWire mode is to initiate a special sequence of commands to the JTAG ICE mkII (or AVR Dragon), so the debugWire mode will be temporarily disabled, and the target can be accessed using normal ISP programming. This sequence is automatically initiated by using the JTAG ICE mkII or AVR Dragon in ISP mode, when they detect that ISP mode cannot be entered. Programmers accepting extended parameters JTAG ICE mkII AVR Dragon When using the JTAG ICE mkII or AVR Dragon in JTAG mode, the following extended parameter is accepted: jtagchain=UB,UA,BB,BA Setup the JTAG scan chain for UB units before, UA units after, BB bits before, and BA bits after the target AVR, respectively. Each AVR unit within the chain shifts by 4 bits. Other JTAG units might require a different bit shift count. AVR910 devcode=VALUE Override the device code selection by using VALUE as the device code. The programmer is not queried for the list of supported device codes, and the specified VALUE is not verified but used directly within the 'T' command sent to the programmer. VALUE can be specified using the conventional number notation of the C programming language. no_blockmode Disables the default checking for block transfer capability. Use no_blockmode only if your AVR910 programmer creates errors during initial sequence. buspirate reset={cs,aux,aux2} The default setup assumes the BusPirate's CS output pin connected to the RESET pin on AVR side. It is however possible to have multiple AVRs connected to the same BP with MISO, MOSI and SCK lines common for all of them. In such a case one AVR should have its RESET connected to BusPirate's CS pin, second AVR's RESET connected to BusPirate's AUX pin and if your BusPirate has an AUX2 pin (only available on BusPirate version v1a with firmware 3.0 or newer) use that to activate RESET on the third AVR. It may be a good idea to decouple the BusPirate and the AVR's SPI buses from each other using a 3-state bus buffer. For example 74HC125 or 74HC244 are some good candidates with the latches driven by the appropriate reset pin (cs, aux or aux2). Otherwise the SPI traffic in one active circuit may interfere with programming the AVR in the other design. speed=<0..7> BusPirate to AVR SPI speed: 0 .. 30 kHz (default) 1 .. 125 kHz 2 .. 250 kHz 3 .. 1 MHz 4 .. 2 MHz 5 .. 2.6 MHz 6 .. 4 MHz 7 .. 8 MHz                          ascii Use ASCII mode even when the firmware supports BinMode (binary mode). BinMode is supported in firmware 2.7 and newer, older FW's either don't have BinMode or their BinMode is buggy. ASCII mode is slower and makes the above reset= and speed= parameters unavailable.","Process Name":"avrdude","Link":"https:\/\/linux.die.net\/man\/1\/avrdude"}},{"Process":{"Description":"AWFFull is a web server log analysis program based on The Webalizer. AWFFull produces usage statistics in HTML format for viewing with a browser. The results are presented in both columnar and graphical format, which facilitates interpretation. Yearly, monthly, daily and hourly usage statistics are presented, along with the ability to display usage by site, URL, referrer, user agent (browser), user name, search strings, entry\/exit pages, and country (some information may not be available if not present in the log file being processed). AWFFull supports the following log formats shown in the following variable list: CLF (common log format) log files Combined log formats as defined by NCSA and others, and variations of these which it attempts to handle intelligently xferlog wu-ftpd formatted log files allowing analysis of ftp servers, and squid proxy logs. Note Logs may also be compressed, via gzip. If a compressed log file is detected, it will be automatically uncompressed while it is read. Compressed logs must have the standard gzip extension of .gz. This documentation applies to AWFFull Version 3.8.2","Process Name":"awffull","Link":"https:\/\/linux.die.net\/man\/1\/awffull"}},{"Process":{"Description":"Gawk is the GNU Project's implementation of the AWK programming language. It conforms to the definition of the language in the POSIX 1003.1 Standard. This version in turn is based on the description in The AWK Programming Language, by Aho, Kernighan, and Weinberger, with the additional features found in the System V Release 4 version of UNIX awk. Gawk also provides more recent Bell Laboratories awk extensions, and a number of GNU -specific extensions. Pgawk is the profiling version of gawk. It is identical in every way to gawk, except that programs run more slowly, and it automatically produces an execution profile in the file awkprof.out when done. See the --profile option, below. The command line consists of options to gawk itself, the AWK program text (if not supplied via the -f or --file options), and values to be made available in the ARGC and ARGV pre-defined AWK variables.","Process Name":"awk","Link":"https:\/\/linux.die.net\/man\/1\/awk"}},{"Process":{"Description":"Axel is a program that downloads a file from a FTP or HTTP server through multiple connection, each connection downloads its own part of the file. Unlike most other programs, Axel downloads all the data directly to the destination file, using one single thread. It just saves some time at the end because the program doesn't have to concatenate all the downloaded parts.","Process Name":"axel","Link":"https:\/\/linux.die.net\/man\/1\/axel"}},{"Process":{"Description":"axfr-get is a DNS zone transfer client. It sends a zone transfer request in DNS-over-TCP format to descriptor 7, reads the result from descriptor 6 and saves the result in a file. axfr-get performs the zone transfer for the given domain, and writes the result to file - OUTFILE.TMP in a format that can be used as input to tinydns-data(1). If the zone transfer completes successfully, axfr-get renames OUTFILE.TMP to OUTFILE.","Process Name":"axfr-get","Link":"https:\/\/linux.die.net\/man\/1\/axfr-get"}}]